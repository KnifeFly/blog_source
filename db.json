{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"430e9d473b3bf041c04c58163a0bb2ca16156ea2","modified":1557935068192},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1535605693765},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1535605693765},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1535605693766},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1535605693766},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1535605693766},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1535605693766},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1535605693767},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1535605693767},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1535605693767},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1535605693767},{"_id":"themes/next/README.en.md","hash":"32d6cdfec1447f54aae1d7f1365ce6733dfcec8f","modified":1535605693767},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1535605693767},{"_id":"themes/next/_config.yml","hash":"cc31c820fac48d00c36136474b725be80afa5703","modified":1535605693768},{"_id":"themes/next/bower.json","hash":"7d7938f9da896fe710aa0e9120140e528bf058df","modified":1535605693768},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1535605693768},{"_id":"themes/next/package.json","hash":"193dad6f59a588908fac082cc46fe067dac1b84d","modified":1535605693782},{"_id":"source/_posts/CDH集群部署.md","hash":"1d6c0c8f621286a1921d84672fa671ac123f4203","modified":1547964036505},{"_id":"source/_posts/ClickHouse 系统架构概览.md","hash":"7706c9cc652ee7bf4f13dc105704345ebc26a386","modified":1558962131448},{"_id":"source/_posts/Golang垃圾回收剖析.md","hash":"beacca3c4499c333569eaf1d63ed557b386eaa9b","modified":1535557164650},{"_id":"source/_posts/Linux信号安全处理.md","hash":"35607ffcb9146857dd2d4a41c7cc29048fc45162","modified":1535557164650},{"_id":"source/_posts/kafka 高可用设计(上).md","hash":"40601bc0d7e570840780dc1611a95f658afe876a","modified":1550158357494},{"_id":"source/_posts/kafka运营-NotLeaderForPartitionException异常.md","hash":"2fd07fc83bcb0b1218d803367041f26d8efb06a9","modified":1547963830134},{"_id":"source/_posts/kafka高可用设计(下).md","hash":"b6b621de4b6fd2d7108bc1e09a8387ea9cf42518","modified":1550158356266},{"_id":"source/_posts/lighttpd-程序框架.md","hash":"f92330821eaa3dd1ef41e7e0887e9e720397004c","modified":1535557164651},{"_id":"source/_posts/linux-守护进程.md","hash":"2a6d1f0c8efe3fbf8c3e61acf85a51888828798f","modified":1535557164651},{"_id":"source/_posts/linux中最大文件描述符数.md","hash":"0d86599340ac5f7563702822b3736c9b70837d51","modified":1547964446607},{"_id":"source/_posts/nginx-信号处理.md","hash":"acadc8939694e77803a3a817e8c4a4571a6d59b8","modified":1535557164651},{"_id":"source/_posts/nginx-系统框架.md","hash":"6492583393321dba604e731c042c64fbe8ecbf6a","modified":1535557164651},{"_id":"source/_posts/nginx互斥锁的实现.md","hash":"6c0f9058c0368f92cdc750802afecee077880b3d","modified":1535557164651},{"_id":"source/_posts/python装饰器函数式编程.md","hash":"9d1f980923b36fef6cf4a3171370bfe634c4cfb9","modified":1535557164651},{"_id":"source/_posts/rsync 使用.md","hash":"ef2ebc5789bc0ed08981b7dac1988037956634f1","modified":1558449947814},{"_id":"source/_posts/sock5协议原理分析.md","hash":"86efb144c9d87da4f9c10ee7b7cde35f1c59ce62","modified":1535557164651},{"_id":"source/_posts/zookeeper分布式锁.md","hash":"ec6c4fcdfb009f37dac749799f58addbf7a8033a","modified":1547965266625},{"_id":"source/_posts/优秀的 Go存储开源项目和库.md","hash":"38c17cc865189bee8ffc98cb61eb61b034dd990d","modified":1558449397809},{"_id":"source/_posts/分布式锁的设计.md","hash":"8195b6fa66f427da14804d37f9979f4a9853b5b7","modified":1535557164652},{"_id":"source/_posts/理解channel.md","hash":"0031b82ad94c78c7e716f3cdb2f8718dd26b3b94","modified":1548173063452},{"_id":"source/_posts/理解spark闭包.md","hash":"98d33504516b7eebf886fbb7076ab71740194c05","modified":1547964312692},{"_id":"source/about/index.md","hash":"96c4b8cd78d7cb9837ebc8f21b4f6e10797c1c6a","modified":1535557164652},{"_id":"source/categories/index.md","hash":"85521e88a4a06f29199b9954c320633c3899690e","modified":1535557164652},{"_id":"source/tags/index.md","hash":"920732736ec790c2928e30fe1cfaa0d5fa45068e","modified":1535557164652},{"_id":"themes/next/.git/HEAD","hash":"75173e2dd18a6221ff84742ff53d01ac5c6e04b0","modified":1535605693761},{"_id":"themes/next/.git/config","hash":"510faaf0899b89e8a0a0a7ebeff0d4b0aa5ad38f","modified":1535605183432},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1535605181328},{"_id":"themes/next/.git/index","hash":"fdd554f100e0331af6bf464bbbdb403cc3d5b140","modified":1547963328377},{"_id":"themes/next/.git/packed-refs","hash":"69237944e31c16fe545d1f47b0b1e5b1d99660da","modified":1535605693759},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1535605693766},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b56c01cdfc6ee7ffea8a8a9fa149263f368caef6","modified":1535605693766},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"37bd0ec1d655c601946fc5f5ac2fe8ed1e529b77","modified":1535605693766},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1535605693768},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1535605693768},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1535605693768},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1535605693768},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1535605693769},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1535605693769},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1535605693769},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1535605693769},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1535605693769},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1535605693769},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1535605693769},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1535605693770},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1535605693770},{"_id":"themes/next/layout/_layout.swig","hash":"06b1eab2e00273e0b94bd32dc682bd92c1e0a747","modified":1535605693770},{"_id":"themes/next/layout/archive.swig","hash":"383f64deab105724fd5512371963bd9e9aafbffd","modified":1535605693780},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1535605693781},{"_id":"themes/next/layout/index.swig","hash":"03e8a2cda03bad42ac0cb827025eb81f95d496a2","modified":1535605693781},{"_id":"themes/next/layout/page.swig","hash":"37c874cd720acf0eda8d26e063278f2b6ae8d3a6","modified":1535605693781},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1535605693781},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1535605693781},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1535605693782},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1535605693782},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1535605693782},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1535605693848},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1535605693848},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1535605693849},{"_id":"source/_posts/Raft 一致性算法论文译文.md","hash":"90340dee67d5ca908e1357df39d0aa96374b226e","modified":1558449417656},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693807},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1535605181330},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1535605181329},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1535605181331},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1535605181331},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1535605181329},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1535605181331},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1535605181329},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1535605181330},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1535605181330},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1535605181331},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1535605181328},{"_id":"themes/next/.git/logs/HEAD","hash":"ece4ea86ac19263b6fe401109d321f49de46a4b7","modified":1535605693761},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1535605693770},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1535605693770},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1535605693771},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"53d4f83b2b7fb4387dfc9fe81519abd56fbce4ae","modified":1535605693771},{"_id":"themes/next/layout/_macro/post.swig","hash":"911363776867d9523a3e322cdf591d49cd166403","modified":1535605693771},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5d5f70deb6074cb4dd0438463e14ccf89213c282","modified":1535605693771},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"faa7886ccf986890cd776f4e9d70cb89fe9fda5f","modified":1535605693771},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1535605693771},{"_id":"themes/next/layout/_partials/comments.swig","hash":"ce7094ee05878161e7568a6dfae5b56ff3fbd6e1","modified":1535605693772},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1535605693772},{"_id":"themes/next/layout/_partials/head.swig","hash":"1f14d3f494b2dbbcee802fd6f6d1abd5b7e2304c","modified":1535605693772},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1535605693773},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1535605693773},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1535605693773},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1535605693773},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1535605693775},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1535605693775},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1535605693776},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1535605693778},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1535605693778},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1535605693779},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1535605693779},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1535605693779},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1535605693779},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1535605693783},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1535605693783},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1535605693783},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1535605693783},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1535605693783},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1535605693783},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1535605693784},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1535605693784},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1535605693784},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1535605693807},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1535605693807},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1535605693808},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1535605693808},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1535605693808},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1535605693808},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1535605693809},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1535605693809},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1535605693809},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1535605693809},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1535605693809},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1535605693809},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1535605693809},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1535605693810},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1535605693810},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693775},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693775},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693798},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693798},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693799},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693806},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1535605693807},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1535605693772},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1535605693772},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1535605693773},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1535605693773},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1535605693773},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1535605693774},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1535605693774},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1535605693774},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1535605693774},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1535605693775},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1535605693775},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1535605693775},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1535605693776},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1535605693777},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1535605693777},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1535605693777},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1535605693777},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1535605693777},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1535605693777},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1535605693777},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1535605693778},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1535605693778},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"ee63aa2e49507b884a2d56778479cf01c723d751","modified":1535605693778},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1535605693778},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1535605693778},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1535605693780},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1535605693780},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1535605693780},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1535605693780},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1535605693797},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1535605693798},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"eaedfaf06dae94ba77a8f4893e2e434bf8859bac","modified":1535605693798},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1535605693798},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"49b5210fa62d6cbc6a98f57d89d5067a06ab3561","modified":1535605693806},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1535605693806},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cfee25d790e4f9b7d57f0dc7e2ea9c1649f08f11","modified":1535605693807},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d477196c5699c8261b08e993a77ef67054d86166","modified":1535605693807},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1535605693810},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1535605693810},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1535605693810},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1535605693810},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1535605693811},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1535605693811},{"_id":"themes/next/source/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1535605693811},{"_id":"themes/next/source/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1535605693811},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1535605693812},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1535605693811},{"_id":"themes/next/source/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1535605693812},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1535605693815},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1535605693818},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1535605693818},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1535605693818},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1535605693818},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1535605693822},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1535605693822},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1535605693823},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1535605693823},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1535605693823},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1535605693824},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1535605693824},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1535605693824},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1535605693824},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1535605693834},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1535605693835},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1535605693835},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1535605693835},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1535605693835},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1535605693835},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1535605693836},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1535605693836},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1535605693836},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1535605693836},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1535605693837},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1535605693837},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1535605693837},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1535605693837},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1535605693837},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1535605693838},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1535605693838},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1535605693838},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1535605693838},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1535605693838},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1535605693838},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1535605693839},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1535605693839},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1535605693840},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1535605693845},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1535605693845},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1535605693847},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1535605693848},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1535605693848},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1535605693835},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1535605693760},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1535605693779},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1535605693779},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d026c8489f66ab6c12ad04bd37f1d5b6f2f3f0d1","modified":1535605693784},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1535605693793},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1535605693796},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"2915df7152ea095a6290ef69157fd67669e0e793","modified":1535605693796},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1535605693796},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"86b6fd7f1b1be3ae98f8af6b23a6b1299c670ce9","modified":1535605693797},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1535605693797},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1535605693797},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1535605693797},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bc8c388553bbcf95897459a466ba35bffd5ec5f0","modified":1535605693799},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1535605693800},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1535605693800},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1535605693801},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1535605693801},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1535605693801},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1535605693801},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1535605693801},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1535605693803},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1535605693803},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1535605693803},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1535605693804},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1535605693805},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1535605693805},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1535605693806},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1535605693811},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1535605693819},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1535605693819},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1535605693819},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1535605693820},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1535605693820},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1535605693820},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1535605693821},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1535605693822},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1535605693822},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1535605693823},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1535605693823},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1535605693825},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1535605693824},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1535605693825},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1535605693844},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1535605693845},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1535605693813},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1535605693814},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1535605693814},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1535605693814},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1535605693834},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1535605693833},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1535605693847},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"f3db8c817abcf3571b855daafea074b41581820c","modified":1535605693760},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1535605693785},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1535605693786},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1535605693787},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1535605693788},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1535605693789},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"ed88c8b51d0517759c777e71a6bfbe2907bcd994","modified":1535605693789},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1535605693789},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ee554b1031ef0070a5916477939021800e3c9d27","modified":1535605693789},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1535605693789},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/post/post-wordcount.styl","hash":"4fda5d38c6c8d910e3bf5c74a48a8d4a3f3dc73d","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"51eca243220cf57133a4becae9b78514bcfdc723","modified":1535605693790},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"65a64d5662637b66e2f039a5f58217afe7a6e800","modified":1535605693791},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1535605693791},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1535605693791},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1535605693792},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1535605693792},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1535605693792},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1535605693792},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1535605693792},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"57d2c8a060f5e4e1a0aef9aae11a0016cf7ac5ba","modified":1535605693793},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1535605693793},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1535605693793},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1535605693793},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1535605693793},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1535605693794},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1535605693794},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1535605693794},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1535605693794},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1535605693794},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1535605693795},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1535605693796},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1535605693796},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1535605693802},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1535605693803},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1535605693813},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1535605693813},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1535605693813},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1535605693820},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1535605693820},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1535605693821},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1535605693821},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1535605693821},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1535605693821},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1535605693827},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1535605693828},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1535605693832},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1535605693812},{"_id":"themes/next/.git/objects/pack/pack-c612b977fdee8f070a000761134eeb6350850a38.idx","hash":"baccc333989b5a1b1719965c6c3a9891cce61ff6","modified":1535605693751},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1535605693817},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1535605693831},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1535605693843},{"_id":"themes/next/.git/objects/pack/pack-c612b977fdee8f070a000761134eeb6350850a38.pack","hash":"1310fd9358c94811f585a7fa3724d7431f130898","modified":1535605693747},{"_id":"public/about/index.html","hash":"9ac2b020f2f398b3685199fc5c991d46a699d8ce","modified":1558962201571},{"_id":"public/categories/index.html","hash":"7b9c57e4289928ea0673fcdf414ff9d7871a34ea","modified":1558962201571},{"_id":"public/tags/index.html","hash":"a84de36427dc6e6d7bfc9b5041a8489f160c8f03","modified":1558962201571},{"_id":"public/archives/index.html","hash":"892cdbb035181d5a3963379ca538c88842c31231","modified":1558962201571},{"_id":"public/archives/page/2/index.html","hash":"0d339649c7d27a6b7ead6e9a25e7c3091e766eb2","modified":1558962201571},{"_id":"public/archives/page/3/index.html","hash":"bbfc102e643589a56599efe344419eae4d7e99d8","modified":1558962201572},{"_id":"public/archives/2016/index.html","hash":"85e8c185c507b2918fc375fe5d55e11bb7e4b52d","modified":1558962201572},{"_id":"public/archives/2016/01/index.html","hash":"7d11c9e8b013f4d4e737e797bf634fddb481c1eb","modified":1558962201572},{"_id":"public/archives/2016/04/index.html","hash":"35274e0be58d94b8c54a8b9ceb096d4a9b5648ba","modified":1558962201572},{"_id":"public/archives/2016/09/index.html","hash":"552a727f04c27e209d06d1ead5735bc03199d4d1","modified":1558962201572},{"_id":"public/archives/2016/10/index.html","hash":"7366733a42ebdedcca5d47dc7ecd20d1c8cb3f5c","modified":1558962201572},{"_id":"public/archives/2017/index.html","hash":"abf1450280dd667729981f543388f64a0251b9a7","modified":1558962201572},{"_id":"public/archives/2017/05/index.html","hash":"a0e12b80e19cfd168bc4baa525e6ef155378edc6","modified":1558962201572},{"_id":"public/archives/2017/07/index.html","hash":"b0b807b89ecb9cb97db336306b900b7dea69d391","modified":1558962201572},{"_id":"public/archives/2017/09/index.html","hash":"1d950736b28931a6e56c6e6169ea0f3f7c7c301c","modified":1558962201572},{"_id":"public/archives/2018/index.html","hash":"6c3169bfb1d38e7e9478f7403c4d16bdb06167cb","modified":1558962201572},{"_id":"public/archives/2018/02/index.html","hash":"ae2cf6e61d495d055cb475225b9e96f3702f1c25","modified":1558962201572},{"_id":"public/archives/2018/03/index.html","hash":"0dc78d8839e1f0b4e15507b4f7b01ba97544ac0c","modified":1558962201572},{"_id":"public/archives/2018/04/index.html","hash":"840272263ccd19283945f794a1e7384953528f26","modified":1558962201572},{"_id":"public/archives/2018/05/index.html","hash":"86558338d1be772b0817ccdfb9e1d8215677f831","modified":1558962201572},{"_id":"public/archives/2019/index.html","hash":"bbbe42de13608be37d9317b3df4c2258a125be62","modified":1558962201572},{"_id":"public/archives/2019/01/index.html","hash":"2d920084667ffb03b6cbd18ddaf6951c7a046c80","modified":1558962201572},{"_id":"public/archives/2019/02/index.html","hash":"302722bc5580907491eae649b714760644fddc09","modified":1558962201572},{"_id":"public/archives/2019/03/index.html","hash":"c97366ac79a75be74edf04dcb8300ce7ab9e8121","modified":1558962201573},{"_id":"public/archives/2019/04/index.html","hash":"df34beaab37f0109bfc981072637ef524c440bb6","modified":1558962201573},{"_id":"public/archives/2019/05/index.html","hash":"80f128ec700bde23792745020d124d4083975b62","modified":1558962201573},{"_id":"public/tags/大数据/index.html","hash":"38a717728ce2f6e5a8b83f80a53ae8c7cc119cd6","modified":1558962201573},{"_id":"public/tags/Linux/index.html","hash":"7931caefa1689b5a98614c278890bd70eaf68f77","modified":1558962201573},{"_id":"public/tags/kafka/index.html","hash":"2558e47ecb5f40dd636b433ac2c2097bb655e419","modified":1558962201573},{"_id":"public/tags/lighttpd/index.html","hash":"baf458de617b754bc44bc16a73d382182c0a4902","modified":1558962201573},{"_id":"public/tags/linux/index.html","hash":"6afad28b94ee6e3706de16fb323d8428bc6694bd","modified":1558962201573},{"_id":"public/tags/nginx/index.html","hash":"b7505dc07be2563d25cf2466a33f4256c74be9dd","modified":1558962201573},{"_id":"public/tags/go/index.html","hash":"4abb9bd1c68a2caca61c43abe1fc1daed54f724b","modified":1558962201573},{"_id":"public/tags/Spark/index.html","hash":"f39ce2fa2e00d64263a3e25eee4d8f4e88ebef88","modified":1558962201573},{"_id":"public/tags/python/index.html","hash":"1a961ff3516cb76ffc4b3c25a218dfd5e3cdd818","modified":1558962201573},{"_id":"public/tags/zookeeper/index.html","hash":"699e3b03a3808074fef4e195a36456a76100c749","modified":1558962201573},{"_id":"public/tags/分布式/index.html","hash":"c5d0ae7253d77415c69592e09f4d10d8ca489c3a","modified":1558962201573},{"_id":"public/tags/NoSQL/index.html","hash":"5f7e99991fab3f14550dcf010e84c92e8f07d9a9","modified":1558962201573},{"_id":"public/tags/raft/index.html","hash":"18ebc8da7bd48d95c068b4472cbb6a89bf437338","modified":1558962201573},{"_id":"public/2019/05/27/ClickHouse 系统架构概览/index.html","hash":"e471dcc45aacba3f9c54979a8b90e001a164cbd4","modified":1558962201573},{"_id":"public/2019/04/05/Raft 一致性算法论文译文/index.html","hash":"c1f958684adbd7b277942a41bf68941409d9b8fe","modified":1558962201573},{"_id":"public/2019/03/15/优秀的 Go存储开源项目和库/index.html","hash":"0babbdf851a9e6e3a1ebc0b3f665e62e3e475657","modified":1558962201574},{"_id":"public/2019/02/14/kafka高可用设计(下)/index.html","hash":"7a68228fb8a3b84116dbca006bb487d57e237d50","modified":1558962201574},{"_id":"public/2019/02/14/kafka 高可用设计(上)/index.html","hash":"8689b9b1c91e7ac678a40162e5946bb3d8b20b34","modified":1558962201574},{"_id":"public/2019/01/03/kafka运营-NotLeaderForPartitionException异常/index.html","hash":"96aa110359fb642ffa81728217bf0757c67f221c","modified":1558962201574},{"_id":"public/2018/05/23/理解channel/index.html","hash":"2ac714dba8ecfbc2fce5c9f5c2f446c4d3ce9ba9","modified":1558962201574},{"_id":"public/2018/05/21/理解spark闭包/index.html","hash":"51965853ae4180bd4a0a71a1da399c0646358721","modified":1558962201574},{"_id":"public/2018/05/16/zookeeper分布式锁/index.html","hash":"19ae7ef4bf48d0b896b873c9c03c4b9be08ee311","modified":1558962201574},{"_id":"public/2018/05/15/CDH集群部署/index.html","hash":"14c514df644643ddc3b3de6d67a2eeb5bb2fdadc","modified":1558962201574},{"_id":"public/2018/04/02/分布式锁的设计/index.html","hash":"722529c10f109ef189f6cf9d712900806b52a064","modified":1558962201574},{"_id":"public/2018/03/16/nginx互斥锁的实现/index.html","hash":"4560ec3f51665057bcbada151e2b622d54cc0d0f","modified":1558962201574},{"_id":"public/2018/02/15/Golang垃圾回收剖析/index.html","hash":"4d44c902ac5b65734bead591ed6b360f60157858","modified":1558962201574},{"_id":"public/2017/09/15/nginx-信号处理/index.html","hash":"11223a9035abbd73f341330c1f9d29adfc992383","modified":1558962201574},{"_id":"public/2017/07/15/nginx-系统框架/index.html","hash":"53cf6dda1f5c6926b6ef6b0302f7594900f23aba","modified":1558962201575},{"_id":"public/2017/05/15/Linux信号安全处理/index.html","hash":"4dc6a524d86ac6ebd0a07940744381f68df26127","modified":1558962201575},{"_id":"public/2016/10/15/lighttpd-程序框架/index.html","hash":"ed827282d288afe05c16220b70cfce68dcec1341","modified":1558962201575},{"_id":"public/2016/09/24/python装饰器函数式编程/index.html","hash":"6bef10ad6b3cd025ee896faeb2711f5bf5b58952","modified":1558962201575},{"_id":"public/2016/04/15/rsync 使用/index.html","hash":"bab69bd591d7e1712cedc6a35eb51ef166401c70","modified":1558962201575},{"_id":"public/2016/01/15/sock5协议原理分析/index.html","hash":"91e4fc723cae8e0146a217e15833a89ef53faf01","modified":1558962201575},{"_id":"public/2016/01/15/linux-守护进程/index.html","hash":"2d1f61be685e871cc5793f2fe795682671c434de","modified":1558962201575},{"_id":"public/2016/01/12/linux中最大文件描述符数/index.html","hash":"e5f8a8d5a956cbdf804bcf20bda357781fa39c30","modified":1558962201575},{"_id":"public/index.html","hash":"5db69912373cc217639b2913f804b2fc4f82e56a","modified":1558962201575},{"_id":"public/page/2/index.html","hash":"29c7a79c18598520b1f15e31a93c209c143d024d","modified":1558962201575},{"_id":"public/page/3/index.html","hash":"6ac3a871d9ad17f3bac9a6edfb16c4e124e8c13b","modified":1558962201575},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1558962201582},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1558962201582},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1558962201582},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1558962201582},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1558962201582},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1558962201582},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1558962201582},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1558962201582},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1558962201582},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1558962201582},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1558962201582},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1558962201582},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1558962201583},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1558962201583},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1558962201583},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1558962201583},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1558962201583},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1558962201583},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1558962201583},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1558962201583},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1558962201583},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1558962201583},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1558962201583},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1558962201583},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1558962201583},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1558962201583},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1558962201583},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1558962201583},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1558962201888},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1558962201889},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1558962201892},{"_id":"public/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1558962201893},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1558962201893},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1558962201893},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1558962201893},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1558962201893},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1558962201893},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1558962201893},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1558962201893},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1558962201893},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1558962201893},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1558962201893},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1558962201893},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1558962201893},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1558962201894},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1558962201894},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1558962201894},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1558962201894},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1558962201894},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1558962201894},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1558962201894},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1558962201894},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1558962201894},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1558962201894},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1558962201894},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1558962201894},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1558962201894},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1558962201894},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1558962201894},{"_id":"public/css/main.css","hash":"5f40a5ece4d6c6cdb05d09f01733845acc0f17f6","modified":1558962201894},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1558962201894},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1558962201894},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1558962201895},{"_id":"public/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1558962201898},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1558962201898},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1558962201898},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1558962201898},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1558962201898},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1558962201898},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1558962201899},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1558962201899},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1558962201902},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1558962201905},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1558962201905},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1558962201909},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1558962201909},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1558962201909},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1558962201909},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1558962201909},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1558962201909},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1558962201909},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1558962201910},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1558962201910},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1558962201911},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1558962201922},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1558962201926},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1558962201932},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1558962201936},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1558962201942},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1558962201945}],"Category":[],"Data":[],"Page":[{"date":"2017-09-11T13:47:20.000Z","_content":"\n一个来自南国的孩子\n","source":"about/index.md","raw":"---\ndate: 2017-09-11 21:47:20\n---\n\n一个来自南国的孩子\n","updated":"2018-08-29T15:39:24.652Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cjw6dubl90001amumpr692yiv","content":"<p>一个来自南国的孩子</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一个来自南国的孩子</p>\n"},{"date":"2017-09-11T13:48:38.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ndate: 2017-09-11 21:48:38\ntype: \"categories\"\n---\n","updated":"2018-08-29T15:39:24.652Z","path":"categories/index.html","title":"","comments":1,"layout":"page","_id":"cjw6dublb0003amumecf2w52g","content":"","site":{"data":{}},"excerpt":"","more":""},{"date":"2017-09-11T13:45:13.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ndate: 2017-09-11 21:45:13\ntype: \"tags\"\n---\n","updated":"2018-08-29T15:39:24.652Z","path":"tags/index.html","title":"","comments":1,"layout":"page","_id":"cjw6dubp60011amumkca1mpt9","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"CDH集群部署","date":"2018-05-15T02:01:29.000Z","_content":"\n\n# CDH集群部署\n\n## 安装步骤\n\n1. 安装JDK\n\n2. host修改\n\n   /etc/hosts配置文件\n\n3. NTP时间同步\n\n4. SSH免秘钥登录\n\n5. 安装mariadb\n\n   yum install install mariadb -y\n\n6. mariadb 建表\n\n   ```mysql\n   SET PASSWORD=PASSWORD('xcloud2017');\n   grant all privileges on *.* to 'root'@'%' identified by 'xcloud2017' with grant option; flush privileges;\n   create database scmdbn DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database report DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   ```\n\n<!--more-->\n\n7. 安装cloudera-manager\n\n   所有节点都需要安装cloudera-manage，选某个节点安装执行命令如下：\n\n   ```shell\n   for i in {53..54}; do scp -P 2223 /usr/local/src/cdh-install.tar.gz root@124.238.237.$i:/root ; done\n   for i in {53..54}; do ssh -p 2223 \"tar zxvf /usr/local/src/cdh-install.tar.gz -C /usr/local/src/\" ; done\n   for i in {54..57}; do ssh -p 2223 root@124.238.237.$i \"mkdir -p /opt/cloudera-manager\" ; done\n   for i in {54..57}; do ssh -p 2223 root@124.238.237.$i \"tar -axvf /usr/local/src/cdh/cloudera-manager-centos7-cm5.14.3_x86_64.tar.gz -C /opt/cloudera-manager\" ; done\n   \n   每台机器需要执行的操作：\n   1.useradd --system --home=/opt/cloudera-manager/cm-5.14.3/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm\n   \n   #更改manager master机器IP，以及绑定端口号，\n   2. vim /opt/cloudera-manager/cm-5.14.3/etc/cloudera-scm-agent/config.ini\n   \n   #创建必要目录\n   3. mkdir -p /opt/cloudera/parcels; chown cloudera-scm:cloudera-scm /opt/cloudera/parcels\n   \n   主节点机器执行的操作：\n   mkdir /var/cloudera-scm-server;\n   chown cloudera-scm:cloudera-scm /var/cloudera-scm-server;\n   chown cloudera-scm:cloudera-scm /opt/cloudera-manager;\n   \n   mkdir -p /opt/cloudera/parcel-repo;\n   chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo;\n   cp CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha manifest.json /opt/cloudera/parcel-repo ;\n   ```\n\n8. 启动cloudera组件\n\n   - 启动cloudera-manage\n\n   ```shell\n   cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server;\n   \n   chkconfig cloudera-scm-server on;\n   \n   # 更改CMF_DEFAULTS目录，更改为/opt/cloudera-manager/cm-5.14.3/etc/default\n   vi /etc/init.d/cloudera-scm-server\n   \n   chmod 755 /run/systemd/generator.late/cloudera-scm-*\n   \n   service cloudera-scm-server start\n   \n   ##如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-server/目录\n   ```\n\n   - 启动cloudera-agent\n\n   ```shell\n   每天机器agent机器执行：\n   for i in {53..57}; do ssh -p 2223 root@124.238.237.$i \" cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent\" ; done\n   \n   chkconfig cloudera-scm-agent on\n   \n   #CMF_DEFAULTS=${CMF_DEFAULTS:-/etc/default}改为=/opt/cloudera-manager/cm-5.14.3/etc/default\n   vi /etc/init.d/cloudera-scm-agent\n   \n   service cloudera-scm-agent start\n   \n   ##如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-agent/目录\n   ```\n\n----\n\n### Mysql配置文件\n\n```conf\n[mysqld]\ntransaction-isolation = READ-COMMITTED\n# Disabling symbolic-links is recommended to prevent assorted security risks;\n# to do so, uncomment this line:\n# symbolic-links = 0\n\nkey_buffer = 16M\nkey_buffer_size = 32M\nmax_allowed_packet = 32M\nthread_stack = 256K\nthread_cache_size = 64\nquery_cache_limit = 8M\nquery_cache_size = 64M\nquery_cache_type = 1\n\nmax_connections = 550\n#expire_logs_days = 10\n#max_binlog_size = 100M\n\n#log_bin should be on a disk with enough free space. Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your system\n#and chown the specified folder to the mysql user.\nlog_bin=/var/lib/mysql/mysql_binary_log\n\nbinlog_format = mixed\n\nread_buffer_size = 2M\nread_rnd_buffer_size = 16M\nsort_buffer_size = 8M\njoin_buffer_size = 8M\n\n# InnoDB settings\ninnodb_file_per_table = 1\ninnodb_flush_log_at_trx_commit  = 2\ninnodb_log_buffer_size = 64M\ninnodb_buffer_pool_size = 4G\ninnodb_thread_concurrency = 8\ninnodb_flush_method = O_DIRECT\ninnodb_log_file_size = 512M\n\n[mysqld_safe]\nlog-error=/var/log/mariadb/mariadb.log\npid-file=/var/run/mariadb/mariadb.pid\n```\n\n### 组件配置\n\n1. iptables配置\n\n  开启防火墙，避免被提交yarn任务\n\n```shell\n  iptables-restore <  /etc/sysconfig/iptables\n```\n\n2. kafka配置\n\n   num.partitions 配置为10\n\n   default.replication.factor 配置为3\n\n   其余默认配置即可\n\n3. flume配置\n\n  采集配置demo\n\n```\na1.sources = squid_source squidFlow_source \na1.sinks = squid_sink squidFlow_sink \na1.channels = squid_channel \n\n#################################################################################################\n# squid log source\na1.sources.squid_source.selector.type = replicating\na1.sources.squid_source.type = spooldir\na1.sources.squid_source.spoolDir = /xcloud-log/logFlume/squid\na1.sources.squid_source.fileHeader = true\na1.sources.squid_source.deletePolicy = immediate\na1.sources.squid_source.channels = squid_channel\na1.sources.squid_source.inputCharset = ASCII\na1.sources.squid_source.deserializer.inputCharset = ASCII\na1.sources.squid_source.decodeErrorPolicy = REPLACE\na1.sources.squid_source.deserializer.maxLineLength = 10240\na1.sources.squid_source.interceptors = i1\na1.sources.squid_source.interceptors.i1.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder\na1.sources.squid_source.interceptors.i1.headerName = key\na1.sources.squid_source.interceptors.i1.preserveExisting = false\n\n# squid log channel\na1.channels.squid_channel.type = memory\na1.channels.squid_channel.capacity = 10000\na1.channels.squid_channel.transactionCapacity = 1000\n\n\n# squid log sinks\na1.sinks.squid_sink.channel = squid_channel\na1.sinks.squid_sink.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.squid_sink.kafka.topic = kafka_squidlog_topic\na1.sinks.squid_sink.kafka.bootstrap.servers = 121.9.240.249:9092,121.9.240.250:9092,121.9.240.251:9092,121.9.240.252:9092,121.9.240.253:9092\na1.sinks.squid_sink.kafka.producer.acks = 1\na1.sinks.squid_sink.kafka.flumeBatchSize = 5000\n\n```\n\n","source":"_posts/CDH集群部署.md","raw":"---\ntitle: CDH集群部署\ndate: 2018-05-15 10:01:29\ntags: 大数据\n---\n\n\n# CDH集群部署\n\n## 安装步骤\n\n1. 安装JDK\n\n2. host修改\n\n   /etc/hosts配置文件\n\n3. NTP时间同步\n\n4. SSH免秘钥登录\n\n5. 安装mariadb\n\n   yum install install mariadb -y\n\n6. mariadb 建表\n\n   ```mysql\n   SET PASSWORD=PASSWORD('xcloud2017');\n   grant all privileges on *.* to 'root'@'%' identified by 'xcloud2017' with grant option; flush privileges;\n   create database scmdbn DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database report DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n   ```\n\n<!--more-->\n\n7. 安装cloudera-manager\n\n   所有节点都需要安装cloudera-manage，选某个节点安装执行命令如下：\n\n   ```shell\n   for i in {53..54}; do scp -P 2223 /usr/local/src/cdh-install.tar.gz root@124.238.237.$i:/root ; done\n   for i in {53..54}; do ssh -p 2223 \"tar zxvf /usr/local/src/cdh-install.tar.gz -C /usr/local/src/\" ; done\n   for i in {54..57}; do ssh -p 2223 root@124.238.237.$i \"mkdir -p /opt/cloudera-manager\" ; done\n   for i in {54..57}; do ssh -p 2223 root@124.238.237.$i \"tar -axvf /usr/local/src/cdh/cloudera-manager-centos7-cm5.14.3_x86_64.tar.gz -C /opt/cloudera-manager\" ; done\n   \n   每台机器需要执行的操作：\n   1.useradd --system --home=/opt/cloudera-manager/cm-5.14.3/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm\n   \n   #更改manager master机器IP，以及绑定端口号，\n   2. vim /opt/cloudera-manager/cm-5.14.3/etc/cloudera-scm-agent/config.ini\n   \n   #创建必要目录\n   3. mkdir -p /opt/cloudera/parcels; chown cloudera-scm:cloudera-scm /opt/cloudera/parcels\n   \n   主节点机器执行的操作：\n   mkdir /var/cloudera-scm-server;\n   chown cloudera-scm:cloudera-scm /var/cloudera-scm-server;\n   chown cloudera-scm:cloudera-scm /opt/cloudera-manager;\n   \n   mkdir -p /opt/cloudera/parcel-repo;\n   chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo;\n   cp CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha manifest.json /opt/cloudera/parcel-repo ;\n   ```\n\n8. 启动cloudera组件\n\n   - 启动cloudera-manage\n\n   ```shell\n   cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server;\n   \n   chkconfig cloudera-scm-server on;\n   \n   # 更改CMF_DEFAULTS目录，更改为/opt/cloudera-manager/cm-5.14.3/etc/default\n   vi /etc/init.d/cloudera-scm-server\n   \n   chmod 755 /run/systemd/generator.late/cloudera-scm-*\n   \n   service cloudera-scm-server start\n   \n   ##如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-server/目录\n   ```\n\n   - 启动cloudera-agent\n\n   ```shell\n   每天机器agent机器执行：\n   for i in {53..57}; do ssh -p 2223 root@124.238.237.$i \" cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent\" ; done\n   \n   chkconfig cloudera-scm-agent on\n   \n   #CMF_DEFAULTS=${CMF_DEFAULTS:-/etc/default}改为=/opt/cloudera-manager/cm-5.14.3/etc/default\n   vi /etc/init.d/cloudera-scm-agent\n   \n   service cloudera-scm-agent start\n   \n   ##如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-agent/目录\n   ```\n\n----\n\n### Mysql配置文件\n\n```conf\n[mysqld]\ntransaction-isolation = READ-COMMITTED\n# Disabling symbolic-links is recommended to prevent assorted security risks;\n# to do so, uncomment this line:\n# symbolic-links = 0\n\nkey_buffer = 16M\nkey_buffer_size = 32M\nmax_allowed_packet = 32M\nthread_stack = 256K\nthread_cache_size = 64\nquery_cache_limit = 8M\nquery_cache_size = 64M\nquery_cache_type = 1\n\nmax_connections = 550\n#expire_logs_days = 10\n#max_binlog_size = 100M\n\n#log_bin should be on a disk with enough free space. Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your system\n#and chown the specified folder to the mysql user.\nlog_bin=/var/lib/mysql/mysql_binary_log\n\nbinlog_format = mixed\n\nread_buffer_size = 2M\nread_rnd_buffer_size = 16M\nsort_buffer_size = 8M\njoin_buffer_size = 8M\n\n# InnoDB settings\ninnodb_file_per_table = 1\ninnodb_flush_log_at_trx_commit  = 2\ninnodb_log_buffer_size = 64M\ninnodb_buffer_pool_size = 4G\ninnodb_thread_concurrency = 8\ninnodb_flush_method = O_DIRECT\ninnodb_log_file_size = 512M\n\n[mysqld_safe]\nlog-error=/var/log/mariadb/mariadb.log\npid-file=/var/run/mariadb/mariadb.pid\n```\n\n### 组件配置\n\n1. iptables配置\n\n  开启防火墙，避免被提交yarn任务\n\n```shell\n  iptables-restore <  /etc/sysconfig/iptables\n```\n\n2. kafka配置\n\n   num.partitions 配置为10\n\n   default.replication.factor 配置为3\n\n   其余默认配置即可\n\n3. flume配置\n\n  采集配置demo\n\n```\na1.sources = squid_source squidFlow_source \na1.sinks = squid_sink squidFlow_sink \na1.channels = squid_channel \n\n#################################################################################################\n# squid log source\na1.sources.squid_source.selector.type = replicating\na1.sources.squid_source.type = spooldir\na1.sources.squid_source.spoolDir = /xcloud-log/logFlume/squid\na1.sources.squid_source.fileHeader = true\na1.sources.squid_source.deletePolicy = immediate\na1.sources.squid_source.channels = squid_channel\na1.sources.squid_source.inputCharset = ASCII\na1.sources.squid_source.deserializer.inputCharset = ASCII\na1.sources.squid_source.decodeErrorPolicy = REPLACE\na1.sources.squid_source.deserializer.maxLineLength = 10240\na1.sources.squid_source.interceptors = i1\na1.sources.squid_source.interceptors.i1.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder\na1.sources.squid_source.interceptors.i1.headerName = key\na1.sources.squid_source.interceptors.i1.preserveExisting = false\n\n# squid log channel\na1.channels.squid_channel.type = memory\na1.channels.squid_channel.capacity = 10000\na1.channels.squid_channel.transactionCapacity = 1000\n\n\n# squid log sinks\na1.sinks.squid_sink.channel = squid_channel\na1.sinks.squid_sink.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.squid_sink.kafka.topic = kafka_squidlog_topic\na1.sinks.squid_sink.kafka.bootstrap.servers = 121.9.240.249:9092,121.9.240.250:9092,121.9.240.251:9092,121.9.240.252:9092,121.9.240.253:9092\na1.sinks.squid_sink.kafka.producer.acks = 1\na1.sinks.squid_sink.kafka.flumeBatchSize = 5000\n\n```\n\n","slug":"CDH集群部署","published":1,"updated":"2019-01-20T06:00:36.505Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubl50000amumwyr0c0la","content":"<h1 id=\"CDH集群部署\"><a href=\"#CDH集群部署\" class=\"headerlink\" title=\"CDH集群部署\"></a>CDH集群部署</h1><h2 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h2><ol>\n<li><p>安装JDK</p>\n</li>\n<li><p>host修改</p>\n<p>/etc/hosts配置文件</p>\n</li>\n<li><p>NTP时间同步</p>\n</li>\n<li><p>SSH免秘钥登录</p>\n</li>\n<li><p>安装mariadb</p>\n<p>yum install install mariadb -y</p>\n</li>\n<li><p>mariadb 建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET PASSWORD=PASSWORD(&apos;xcloud2017&apos;);</span><br><span class=\"line\">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;xcloud2017&apos; with grant option; flush privileges;</span><br><span class=\"line\">create database scmdbn DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database report DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<a id=\"more\"></a>\n<ol>\n<li><p>安装cloudera-manager</p>\n<p>所有节点都需要安装cloudera-manage，选某个节点安装执行命令如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for i in &#123;53..54&#125;; do scp -P 2223 /usr/local/src/cdh-install.tar.gz root@124.238.237.$i:/root ; done</span><br><span class=\"line\">for i in &#123;53..54&#125;; do ssh -p 2223 \"tar zxvf /usr/local/src/cdh-install.tar.gz -C /usr/local/src/\" ; done</span><br><span class=\"line\">for i in &#123;54..57&#125;; do ssh -p 2223 root@124.238.237.$i \"mkdir -p /opt/cloudera-manager\" ; done</span><br><span class=\"line\">for i in &#123;54..57&#125;; do ssh -p 2223 root@124.238.237.$i \"tar -axvf /usr/local/src/cdh/cloudera-manager-centos7-cm5.14.3_x86_64.tar.gz -C /opt/cloudera-manager\" ; done</span><br><span class=\"line\"></span><br><span class=\"line\">每台机器需要执行的操作：</span><br><span class=\"line\">1.useradd --system --home=/opt/cloudera-manager/cm-5.14.3/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>更改manager master机器IP，以及绑定端口号，</span><br><span class=\"line\">2. vim /opt/cloudera-manager/cm-5.14.3/etc/cloudera-scm-agent/config.ini</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>创建必要目录</span><br><span class=\"line\">3. mkdir -p /opt/cloudera/parcels; chown cloudera-scm:cloudera-scm /opt/cloudera/parcels</span><br><span class=\"line\"></span><br><span class=\"line\">主节点机器执行的操作：</span><br><span class=\"line\">mkdir /var/cloudera-scm-server;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /var/cloudera-scm-server;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /opt/cloudera-manager;</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /opt/cloudera/parcel-repo;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo;</span><br><span class=\"line\">cp CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha manifest.json /opt/cloudera/parcel-repo ;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动cloudera组件</p>\n<ul>\n<li>启动cloudera-manage</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server;</span><br><span class=\"line\"></span><br><span class=\"line\">chkconfig cloudera-scm-server on;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span> 更改CMF_DEFAULTS目录，更改为/opt/cloudera-manager/cm-5.14.3/etc/default</span><br><span class=\"line\">vi /etc/init.d/cloudera-scm-server</span><br><span class=\"line\"></span><br><span class=\"line\">chmod 755 /run/systemd/generator.late/cloudera-scm-*</span><br><span class=\"line\"></span><br><span class=\"line\">service cloudera-scm-server start</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>#如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-server/目录</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动cloudera-agent</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">每天机器agent机器执行：</span><br><span class=\"line\">for i in &#123;53..57&#125;; do ssh -p 2223 root@124.238.237.$i \" cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent\" ; done</span><br><span class=\"line\"></span><br><span class=\"line\">chkconfig cloudera-scm-agent on</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>CMF_DEFAULTS=$&#123;CMF_DEFAULTS:-/etc/default&#125;改为=/opt/cloudera-manager/cm-5.14.3/etc/default</span><br><span class=\"line\">vi /etc/init.d/cloudera-scm-agent</span><br><span class=\"line\"></span><br><span class=\"line\">service cloudera-scm-agent start</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>#如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-agent/目录</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h3 id=\"Mysql配置文件\"><a href=\"#Mysql配置文件\" class=\"headerlink\" title=\"Mysql配置文件\"></a>Mysql配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mysqld]</span><br><span class=\"line\">transaction-isolation = READ-COMMITTED</span><br><span class=\"line\"># Disabling symbolic-links is recommended to prevent assorted security risks;</span><br><span class=\"line\"># to do so, uncomment this line:</span><br><span class=\"line\"># symbolic-links = 0</span><br><span class=\"line\"></span><br><span class=\"line\">key_buffer = 16M</span><br><span class=\"line\">key_buffer_size = 32M</span><br><span class=\"line\">max_allowed_packet = 32M</span><br><span class=\"line\">thread_stack = 256K</span><br><span class=\"line\">thread_cache_size = 64</span><br><span class=\"line\">query_cache_limit = 8M</span><br><span class=\"line\">query_cache_size = 64M</span><br><span class=\"line\">query_cache_type = 1</span><br><span class=\"line\"></span><br><span class=\"line\">max_connections = 550</span><br><span class=\"line\">#expire_logs_days = 10</span><br><span class=\"line\">#max_binlog_size = 100M</span><br><span class=\"line\"></span><br><span class=\"line\">#log_bin should be on a disk with enough free space. Replace &apos;/var/lib/mysql/mysql_binary_log&apos; with an appropriate path for your system</span><br><span class=\"line\">#and chown the specified folder to the mysql user.</span><br><span class=\"line\">log_bin=/var/lib/mysql/mysql_binary_log</span><br><span class=\"line\"></span><br><span class=\"line\">binlog_format = mixed</span><br><span class=\"line\"></span><br><span class=\"line\">read_buffer_size = 2M</span><br><span class=\"line\">read_rnd_buffer_size = 16M</span><br><span class=\"line\">sort_buffer_size = 8M</span><br><span class=\"line\">join_buffer_size = 8M</span><br><span class=\"line\"></span><br><span class=\"line\"># InnoDB settings</span><br><span class=\"line\">innodb_file_per_table = 1</span><br><span class=\"line\">innodb_flush_log_at_trx_commit  = 2</span><br><span class=\"line\">innodb_log_buffer_size = 64M</span><br><span class=\"line\">innodb_buffer_pool_size = 4G</span><br><span class=\"line\">innodb_thread_concurrency = 8</span><br><span class=\"line\">innodb_flush_method = O_DIRECT</span><br><span class=\"line\">innodb_log_file_size = 512M</span><br><span class=\"line\"></span><br><span class=\"line\">[mysqld_safe]</span><br><span class=\"line\">log-error=/var/log/mariadb/mariadb.log</span><br><span class=\"line\">pid-file=/var/run/mariadb/mariadb.pid</span><br></pre></td></tr></table></figure>\n<h3 id=\"组件配置\"><a href=\"#组件配置\" class=\"headerlink\" title=\"组件配置\"></a>组件配置</h3><ol>\n<li><p>iptables配置</p>\n<p>开启防火墙，避免被提交yarn任务</p>\n</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iptables-restore &lt;  /etc/sysconfig/iptables</span><br></pre></td></tr></table></figure>\n<ol>\n<li><p>kafka配置</p>\n<p>num.partitions 配置为10</p>\n<p>default.replication.factor 配置为3</p>\n<p>其余默认配置即可</p>\n</li>\n<li><p>flume配置</p>\n<p>采集配置demo</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a1.sources = squid_source squidFlow_source </span><br><span class=\"line\">a1.sinks = squid_sink squidFlow_sink </span><br><span class=\"line\">a1.channels = squid_channel </span><br><span class=\"line\"></span><br><span class=\"line\">#################################################################################################</span><br><span class=\"line\"># squid log source</span><br><span class=\"line\">a1.sources.squid_source.selector.type = replicating</span><br><span class=\"line\">a1.sources.squid_source.type = spooldir</span><br><span class=\"line\">a1.sources.squid_source.spoolDir = /xcloud-log/logFlume/squid</span><br><span class=\"line\">a1.sources.squid_source.fileHeader = true</span><br><span class=\"line\">a1.sources.squid_source.deletePolicy = immediate</span><br><span class=\"line\">a1.sources.squid_source.channels = squid_channel</span><br><span class=\"line\">a1.sources.squid_source.inputCharset = ASCII</span><br><span class=\"line\">a1.sources.squid_source.deserializer.inputCharset = ASCII</span><br><span class=\"line\">a1.sources.squid_source.decodeErrorPolicy = REPLACE</span><br><span class=\"line\">a1.sources.squid_source.deserializer.maxLineLength = 10240</span><br><span class=\"line\">a1.sources.squid_source.interceptors = i1</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.headerName = key</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.preserveExisting = false</span><br><span class=\"line\"></span><br><span class=\"line\"># squid log channel</span><br><span class=\"line\">a1.channels.squid_channel.type = memory</span><br><span class=\"line\">a1.channels.squid_channel.capacity = 10000</span><br><span class=\"line\">a1.channels.squid_channel.transactionCapacity = 1000</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># squid log sinks</span><br><span class=\"line\">a1.sinks.squid_sink.channel = squid_channel</span><br><span class=\"line\">a1.sinks.squid_sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.topic = kafka_squidlog_topic</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.bootstrap.servers = 121.9.240.249:9092,121.9.240.250:9092,121.9.240.251:9092,121.9.240.252:9092,121.9.240.253:9092</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.producer.acks = 1</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.flumeBatchSize = 5000</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"CDH集群部署\"><a href=\"#CDH集群部署\" class=\"headerlink\" title=\"CDH集群部署\"></a>CDH集群部署</h1><h2 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h2><ol>\n<li><p>安装JDK</p>\n</li>\n<li><p>host修改</p>\n<p>/etc/hosts配置文件</p>\n</li>\n<li><p>NTP时间同步</p>\n</li>\n<li><p>SSH免秘钥登录</p>\n</li>\n<li><p>安装mariadb</p>\n<p>yum install install mariadb -y</p>\n</li>\n<li><p>mariadb 建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET PASSWORD=PASSWORD(&apos;xcloud2017&apos;);</span><br><span class=\"line\">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;xcloud2017&apos; with grant option; flush privileges;</span><br><span class=\"line\">create database scmdbn DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database report DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class=\"line\">create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>","more":"<ol>\n<li><p>安装cloudera-manager</p>\n<p>所有节点都需要安装cloudera-manage，选某个节点安装执行命令如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for i in &#123;53..54&#125;; do scp -P 2223 /usr/local/src/cdh-install.tar.gz root@124.238.237.$i:/root ; done</span><br><span class=\"line\">for i in &#123;53..54&#125;; do ssh -p 2223 \"tar zxvf /usr/local/src/cdh-install.tar.gz -C /usr/local/src/\" ; done</span><br><span class=\"line\">for i in &#123;54..57&#125;; do ssh -p 2223 root@124.238.237.$i \"mkdir -p /opt/cloudera-manager\" ; done</span><br><span class=\"line\">for i in &#123;54..57&#125;; do ssh -p 2223 root@124.238.237.$i \"tar -axvf /usr/local/src/cdh/cloudera-manager-centos7-cm5.14.3_x86_64.tar.gz -C /opt/cloudera-manager\" ; done</span><br><span class=\"line\"></span><br><span class=\"line\">每台机器需要执行的操作：</span><br><span class=\"line\">1.useradd --system --home=/opt/cloudera-manager/cm-5.14.3/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>更改manager master机器IP，以及绑定端口号，</span><br><span class=\"line\">2. vim /opt/cloudera-manager/cm-5.14.3/etc/cloudera-scm-agent/config.ini</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>创建必要目录</span><br><span class=\"line\">3. mkdir -p /opt/cloudera/parcels; chown cloudera-scm:cloudera-scm /opt/cloudera/parcels</span><br><span class=\"line\"></span><br><span class=\"line\">主节点机器执行的操作：</span><br><span class=\"line\">mkdir /var/cloudera-scm-server;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /var/cloudera-scm-server;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /opt/cloudera-manager;</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /opt/cloudera/parcel-repo;</span><br><span class=\"line\">chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo;</span><br><span class=\"line\">cp CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha manifest.json /opt/cloudera/parcel-repo ;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动cloudera组件</p>\n<ul>\n<li>启动cloudera-manage</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server;</span><br><span class=\"line\"></span><br><span class=\"line\">chkconfig cloudera-scm-server on;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span> 更改CMF_DEFAULTS目录，更改为/opt/cloudera-manager/cm-5.14.3/etc/default</span><br><span class=\"line\">vi /etc/init.d/cloudera-scm-server</span><br><span class=\"line\"></span><br><span class=\"line\">chmod 755 /run/systemd/generator.late/cloudera-scm-*</span><br><span class=\"line\"></span><br><span class=\"line\">service cloudera-scm-server start</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>#如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-server/目录</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动cloudera-agent</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">每天机器agent机器执行：</span><br><span class=\"line\">for i in &#123;53..57&#125;; do ssh -p 2223 root@124.238.237.$i \" cp /opt/cloudera-manager/cm-5.14.3/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent\" ; done</span><br><span class=\"line\"></span><br><span class=\"line\">chkconfig cloudera-scm-agent on</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>CMF_DEFAULTS=$&#123;CMF_DEFAULTS:-/etc/default&#125;改为=/opt/cloudera-manager/cm-5.14.3/etc/default</span><br><span class=\"line\">vi /etc/init.d/cloudera-scm-agent</span><br><span class=\"line\"></span><br><span class=\"line\">service cloudera-scm-agent start</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span>#如果启动失败，查看/opt/cloudera-manager/cm-5.14.3/log/cloudera-scm-agent/目录</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h3 id=\"Mysql配置文件\"><a href=\"#Mysql配置文件\" class=\"headerlink\" title=\"Mysql配置文件\"></a>Mysql配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mysqld]</span><br><span class=\"line\">transaction-isolation = READ-COMMITTED</span><br><span class=\"line\"># Disabling symbolic-links is recommended to prevent assorted security risks;</span><br><span class=\"line\"># to do so, uncomment this line:</span><br><span class=\"line\"># symbolic-links = 0</span><br><span class=\"line\"></span><br><span class=\"line\">key_buffer = 16M</span><br><span class=\"line\">key_buffer_size = 32M</span><br><span class=\"line\">max_allowed_packet = 32M</span><br><span class=\"line\">thread_stack = 256K</span><br><span class=\"line\">thread_cache_size = 64</span><br><span class=\"line\">query_cache_limit = 8M</span><br><span class=\"line\">query_cache_size = 64M</span><br><span class=\"line\">query_cache_type = 1</span><br><span class=\"line\"></span><br><span class=\"line\">max_connections = 550</span><br><span class=\"line\">#expire_logs_days = 10</span><br><span class=\"line\">#max_binlog_size = 100M</span><br><span class=\"line\"></span><br><span class=\"line\">#log_bin should be on a disk with enough free space. Replace &apos;/var/lib/mysql/mysql_binary_log&apos; with an appropriate path for your system</span><br><span class=\"line\">#and chown the specified folder to the mysql user.</span><br><span class=\"line\">log_bin=/var/lib/mysql/mysql_binary_log</span><br><span class=\"line\"></span><br><span class=\"line\">binlog_format = mixed</span><br><span class=\"line\"></span><br><span class=\"line\">read_buffer_size = 2M</span><br><span class=\"line\">read_rnd_buffer_size = 16M</span><br><span class=\"line\">sort_buffer_size = 8M</span><br><span class=\"line\">join_buffer_size = 8M</span><br><span class=\"line\"></span><br><span class=\"line\"># InnoDB settings</span><br><span class=\"line\">innodb_file_per_table = 1</span><br><span class=\"line\">innodb_flush_log_at_trx_commit  = 2</span><br><span class=\"line\">innodb_log_buffer_size = 64M</span><br><span class=\"line\">innodb_buffer_pool_size = 4G</span><br><span class=\"line\">innodb_thread_concurrency = 8</span><br><span class=\"line\">innodb_flush_method = O_DIRECT</span><br><span class=\"line\">innodb_log_file_size = 512M</span><br><span class=\"line\"></span><br><span class=\"line\">[mysqld_safe]</span><br><span class=\"line\">log-error=/var/log/mariadb/mariadb.log</span><br><span class=\"line\">pid-file=/var/run/mariadb/mariadb.pid</span><br></pre></td></tr></table></figure>\n<h3 id=\"组件配置\"><a href=\"#组件配置\" class=\"headerlink\" title=\"组件配置\"></a>组件配置</h3><ol>\n<li><p>iptables配置</p>\n<p>开启防火墙，避免被提交yarn任务</p>\n</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iptables-restore &lt;  /etc/sysconfig/iptables</span><br></pre></td></tr></table></figure>\n<ol>\n<li><p>kafka配置</p>\n<p>num.partitions 配置为10</p>\n<p>default.replication.factor 配置为3</p>\n<p>其余默认配置即可</p>\n</li>\n<li><p>flume配置</p>\n<p>采集配置demo</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a1.sources = squid_source squidFlow_source </span><br><span class=\"line\">a1.sinks = squid_sink squidFlow_sink </span><br><span class=\"line\">a1.channels = squid_channel </span><br><span class=\"line\"></span><br><span class=\"line\">#################################################################################################</span><br><span class=\"line\"># squid log source</span><br><span class=\"line\">a1.sources.squid_source.selector.type = replicating</span><br><span class=\"line\">a1.sources.squid_source.type = spooldir</span><br><span class=\"line\">a1.sources.squid_source.spoolDir = /xcloud-log/logFlume/squid</span><br><span class=\"line\">a1.sources.squid_source.fileHeader = true</span><br><span class=\"line\">a1.sources.squid_source.deletePolicy = immediate</span><br><span class=\"line\">a1.sources.squid_source.channels = squid_channel</span><br><span class=\"line\">a1.sources.squid_source.inputCharset = ASCII</span><br><span class=\"line\">a1.sources.squid_source.deserializer.inputCharset = ASCII</span><br><span class=\"line\">a1.sources.squid_source.decodeErrorPolicy = REPLACE</span><br><span class=\"line\">a1.sources.squid_source.deserializer.maxLineLength = 10240</span><br><span class=\"line\">a1.sources.squid_source.interceptors = i1</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.headerName = key</span><br><span class=\"line\">a1.sources.squid_source.interceptors.i1.preserveExisting = false</span><br><span class=\"line\"></span><br><span class=\"line\"># squid log channel</span><br><span class=\"line\">a1.channels.squid_channel.type = memory</span><br><span class=\"line\">a1.channels.squid_channel.capacity = 10000</span><br><span class=\"line\">a1.channels.squid_channel.transactionCapacity = 1000</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># squid log sinks</span><br><span class=\"line\">a1.sinks.squid_sink.channel = squid_channel</span><br><span class=\"line\">a1.sinks.squid_sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.topic = kafka_squidlog_topic</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.bootstrap.servers = 121.9.240.249:9092,121.9.240.250:9092,121.9.240.251:9092,121.9.240.252:9092,121.9.240.253:9092</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.producer.acks = 1</span><br><span class=\"line\">a1.sinks.squid_sink.kafka.flumeBatchSize = 5000</span><br></pre></td></tr></table></figure>"},{"title":"Linux信号安全处理","date":"2017-05-15T07:37:21.000Z","_content":"\n## 信号处理机制\n\n在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n    - 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n    - 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n\n当内核发送一个信号给进程时，它将会修改进程的pending位向量，譬如说，当内核发送一个SIGINT信号给进程，那么它会将进程的pending[SIGINT]的值设置成 1。同样地，当进程屏蔽掉一个信号时，那么它会修改blocked位向量。当进程屏蔽掉一个信号之后，内核仍然可以发送这个信号给进程(保存在进程的pending位向量中)，但进程不会接收并处理这个信号。只有当进程解除了对这个信号的屏蔽之后，进程才会接收并处理这个信号。\n\n下面的程序一开始就屏蔽了SIGINT信号，所以即使内核发送SIGINT信号给这个程序，这个信号也不会得到处理。而当程序解除了对SIGINT的屏蔽之后，这个SIGINT信号才会得到处理：\n\n<!-- more -->\n\n\n```c\n#include <signal.h>\n#include <unistd.h>\n#include <string.h>\nvoid sigint_handler(int sig)\n{\n    const char *message = \"handle SIGINT signal\\n\";\n    write(STDOUT_FILENO, message, strlen(message));\n}\n\nint main()\n{\n    signal(SIGINT, sigint_handler);\n    sigset_t mask, prev_mask;\n    sigemptyset(&mask);\n    sigaddset(&mask, SIGINT);\n    // 屏蔽掉 SIGINT 信号\n    sigprocmask(SIG_BLOCK, &mask, &prev_mask);\n    // 假设此时接收到 SIGINT 信号\n    sleep(10);\n    // 解除对 SIGINT 的屏蔽之后，进程会开始处理 SIGINT 信号\n    sigprocmask(SIG_SETMASK, &prev_mask, NULL);\n    return 0;\n}\n```\n\n## 安全处理信号\n\n- 当进程接收到某个信号时，会调用这个信号的 handler，这会中断主程序的执行。\n- 当进程在执行某个信号 handler 的过程中，可能会被另一个信号 handler 中断。\n\n上面这两种情况都会带来并发安全的问题，因此在编写信号 handler 时，需要考虑到并发安全的问题。譬如说，由于信号 handler 会中断主程序的执行，如果信号 handler 与主程序共享全局变量，就可能带来并发安全的问题。\n\n信号 handler 与主程序共享全局变量是很常见的。譬如说，当进程在接收到SIGINT时，为了优雅地退出程序，这时可以使用一个全局变量记录是否接收到SIGINT信号。主程序每次进入循环时都会检查这个变量，如果发现进程接收到SIGINT信号，就释放好资源并退出程序\n\n上面的代码并不是并发安全的，可能导致两个问题：\n- 现代编译器通常会优化程序对变量的访问。主程序可能会将quit的副本存储在寄存器中，每次访问quit时就从寄存器中访问。那么即使信号 handler 修改了这个quit在内存中的值，主程序也可能不知道。\n\n- 主程序会读取quit的值，信号 handler 会改变quit的值，而这两个操作都不保证是原子的\n\n我们可以这样解决这两个问题：\n\n- 首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问\n\n- 首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问\n\n\n```c\nvolatile sig_atomic_t quit = 0;\n```\n\n## I/O 多路复用与信号\n\n在 Linux 中处理信号是极为麻烦的事情，正如 Linux 标准指出的，当select()、poll()和epoll_wait()被信号中断之后，它们是决不会重启的，所以说如果这些函数被信号中断，我们只好手动重启它们\n\n```c\nwhile (true) {\n    int n = epoll_wait(/** ... **/);\n    if (n == -1 && errno == EINTR) {\n        continue;\n    } else {\n        // ...\n    }\n}\n```\n\n所幸的是 Linux 提供了signalfd()函数，signalfd()可以将接收到的信号，转化为文件描述符的可读事件，所以signalfd()可以和 select/poll/epoll 配合使用，大大简化信号处理的难度。\n\n下面的例子将signalfd()与 epoll 配合使用，signalfd()负责将接收到的SIGINT和SIGHUP转换为文件描述符的可读事件：\n\n```c\n#include <unistd.h>\n#include <string.h>\n#include <sys/epoll.h>\n#include <signal.h>\n#include <sys/signalfd.h>\n#include <stdbool.h>\n#include <assert.h>\n#include <stdio.h>\n\nint main()\n{\n    // 屏蔽信号 SIGINT 和 SIGHUP\n    sigset_t mask;\n    sigemptyset(&mask);\n    sigaddset(&mask, SIGINT);\n    sigaddset(&mask, SIGHUP);\n    sigprocmask(SIG_BLOCK, &mask, NULL);\n    int signal_fd = signalfd(-1, &mask, SFD_NONBLOCK | SFD_CLOEXEC);\n    int epoll_fd = epoll_create1(EPOLL_CLOEXEC);\n    struct epoll_event event;\n    memset(&event, 0, sizeof(event));\n    event.events = EPOLLIN;\n    event.data.fd = signal_fd;\n    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, signal_fd, &event);\n    const int MAX_EVENTS = 64;\n    struct epoll_event events[MAX_EVENTS];\n    while (true)\n    {\n        int n = epoll_wait(epoll_fd, &events[0], MAX_EVENTS, 0);\n        for (int i = 0; i < n; ++i)\n        {\n            if (events[i].data.fd == signal_fd)\n            {\n                struct signalfd_siginfo info;\n                ssize_t bytes = read(signal_fd, &info, sizeof(info));\n                assert(bytes == sizeof(info));\n                if (info.ssi_signo == SIGINT)\n                {\n                    printf(\"receive signal SIGINT\\n\");\n                }\n                else if (info.ssi_signo == SIGHUP)\n                {\n                    printf(\"receive signal SIGHUP\\n\");\n                }\n                printf(\"Program quit!\\n\");\n                return 0;\n            }\n        }\n    }\n    return 0;\n}\n```\n\n\n参考资料：http://senlinzhan.github.io/2017/03/02/linux-signal/\n","source":"_posts/Linux信号安全处理.md","raw":"---\ntitle: Linux信号安全处理\ndate: 2017-05-15 15:37:21\ntags: Linux\n---\n\n## 信号处理机制\n\n在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n    - 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n    - 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n\n当内核发送一个信号给进程时，它将会修改进程的pending位向量，譬如说，当内核发送一个SIGINT信号给进程，那么它会将进程的pending[SIGINT]的值设置成 1。同样地，当进程屏蔽掉一个信号时，那么它会修改blocked位向量。当进程屏蔽掉一个信号之后，内核仍然可以发送这个信号给进程(保存在进程的pending位向量中)，但进程不会接收并处理这个信号。只有当进程解除了对这个信号的屏蔽之后，进程才会接收并处理这个信号。\n\n下面的程序一开始就屏蔽了SIGINT信号，所以即使内核发送SIGINT信号给这个程序，这个信号也不会得到处理。而当程序解除了对SIGINT的屏蔽之后，这个SIGINT信号才会得到处理：\n\n<!-- more -->\n\n\n```c\n#include <signal.h>\n#include <unistd.h>\n#include <string.h>\nvoid sigint_handler(int sig)\n{\n    const char *message = \"handle SIGINT signal\\n\";\n    write(STDOUT_FILENO, message, strlen(message));\n}\n\nint main()\n{\n    signal(SIGINT, sigint_handler);\n    sigset_t mask, prev_mask;\n    sigemptyset(&mask);\n    sigaddset(&mask, SIGINT);\n    // 屏蔽掉 SIGINT 信号\n    sigprocmask(SIG_BLOCK, &mask, &prev_mask);\n    // 假设此时接收到 SIGINT 信号\n    sleep(10);\n    // 解除对 SIGINT 的屏蔽之后，进程会开始处理 SIGINT 信号\n    sigprocmask(SIG_SETMASK, &prev_mask, NULL);\n    return 0;\n}\n```\n\n## 安全处理信号\n\n- 当进程接收到某个信号时，会调用这个信号的 handler，这会中断主程序的执行。\n- 当进程在执行某个信号 handler 的过程中，可能会被另一个信号 handler 中断。\n\n上面这两种情况都会带来并发安全的问题，因此在编写信号 handler 时，需要考虑到并发安全的问题。譬如说，由于信号 handler 会中断主程序的执行，如果信号 handler 与主程序共享全局变量，就可能带来并发安全的问题。\n\n信号 handler 与主程序共享全局变量是很常见的。譬如说，当进程在接收到SIGINT时，为了优雅地退出程序，这时可以使用一个全局变量记录是否接收到SIGINT信号。主程序每次进入循环时都会检查这个变量，如果发现进程接收到SIGINT信号，就释放好资源并退出程序\n\n上面的代码并不是并发安全的，可能导致两个问题：\n- 现代编译器通常会优化程序对变量的访问。主程序可能会将quit的副本存储在寄存器中，每次访问quit时就从寄存器中访问。那么即使信号 handler 修改了这个quit在内存中的值，主程序也可能不知道。\n\n- 主程序会读取quit的值，信号 handler 会改变quit的值，而这两个操作都不保证是原子的\n\n我们可以这样解决这两个问题：\n\n- 首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问\n\n- 首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问\n\n\n```c\nvolatile sig_atomic_t quit = 0;\n```\n\n## I/O 多路复用与信号\n\n在 Linux 中处理信号是极为麻烦的事情，正如 Linux 标准指出的，当select()、poll()和epoll_wait()被信号中断之后，它们是决不会重启的，所以说如果这些函数被信号中断，我们只好手动重启它们\n\n```c\nwhile (true) {\n    int n = epoll_wait(/** ... **/);\n    if (n == -1 && errno == EINTR) {\n        continue;\n    } else {\n        // ...\n    }\n}\n```\n\n所幸的是 Linux 提供了signalfd()函数，signalfd()可以将接收到的信号，转化为文件描述符的可读事件，所以signalfd()可以和 select/poll/epoll 配合使用，大大简化信号处理的难度。\n\n下面的例子将signalfd()与 epoll 配合使用，signalfd()负责将接收到的SIGINT和SIGHUP转换为文件描述符的可读事件：\n\n```c\n#include <unistd.h>\n#include <string.h>\n#include <sys/epoll.h>\n#include <signal.h>\n#include <sys/signalfd.h>\n#include <stdbool.h>\n#include <assert.h>\n#include <stdio.h>\n\nint main()\n{\n    // 屏蔽信号 SIGINT 和 SIGHUP\n    sigset_t mask;\n    sigemptyset(&mask);\n    sigaddset(&mask, SIGINT);\n    sigaddset(&mask, SIGHUP);\n    sigprocmask(SIG_BLOCK, &mask, NULL);\n    int signal_fd = signalfd(-1, &mask, SFD_NONBLOCK | SFD_CLOEXEC);\n    int epoll_fd = epoll_create1(EPOLL_CLOEXEC);\n    struct epoll_event event;\n    memset(&event, 0, sizeof(event));\n    event.events = EPOLLIN;\n    event.data.fd = signal_fd;\n    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, signal_fd, &event);\n    const int MAX_EVENTS = 64;\n    struct epoll_event events[MAX_EVENTS];\n    while (true)\n    {\n        int n = epoll_wait(epoll_fd, &events[0], MAX_EVENTS, 0);\n        for (int i = 0; i < n; ++i)\n        {\n            if (events[i].data.fd == signal_fd)\n            {\n                struct signalfd_siginfo info;\n                ssize_t bytes = read(signal_fd, &info, sizeof(info));\n                assert(bytes == sizeof(info));\n                if (info.ssi_signo == SIGINT)\n                {\n                    printf(\"receive signal SIGINT\\n\");\n                }\n                else if (info.ssi_signo == SIGHUP)\n                {\n                    printf(\"receive signal SIGHUP\\n\");\n                }\n                printf(\"Program quit!\\n\");\n                return 0;\n            }\n        }\n    }\n    return 0;\n}\n```\n\n\n参考资料：http://senlinzhan.github.io/2017/03/02/linux-signal/\n","slug":"Linux信号安全处理","published":1,"updated":"2018-08-29T15:39:24.650Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubla0002amumm7wnvpj4","content":"<h2 id=\"信号处理机制\"><a href=\"#信号处理机制\" class=\"headerlink\" title=\"信号处理机制\"></a>信号处理机制</h2><p>在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：</p>\n<pre><code>- 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n- 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n</code></pre><p>当内核发送一个信号给进程时，它将会修改进程的pending位向量，譬如说，当内核发送一个SIGINT信号给进程，那么它会将进程的pending[SIGINT]的值设置成 1。同样地，当进程屏蔽掉一个信号时，那么它会修改blocked位向量。当进程屏蔽掉一个信号之后，内核仍然可以发送这个信号给进程(保存在进程的pending位向量中)，但进程不会接收并处理这个信号。只有当进程解除了对这个信号的屏蔽之后，进程才会接收并处理这个信号。</p>\n<p>下面的程序一开始就屏蔽了SIGINT信号，所以即使内核发送SIGINT信号给这个程序，这个信号也不会得到处理。而当程序解除了对SIGINT的屏蔽之后，这个SIGINT信号才会得到处理：</p>\n<a id=\"more\"></a>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sigint_handler</span><span class=\"params\">(<span class=\"keyword\">int</span> sig)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *message = <span class=\"string\">\"handle SIGINT signal\\n\"</span>;</span><br><span class=\"line\">    write(STDOUT_FILENO, message, <span class=\"built_in\">strlen</span>(message));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    signal(SIGINT, sigint_handler);</span><br><span class=\"line\">    <span class=\"keyword\">sigset_t</span> mask, prev_mask;</span><br><span class=\"line\">    sigemptyset(&amp;mask);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGINT);</span><br><span class=\"line\">    <span class=\"comment\">// 屏蔽掉 SIGINT 信号</span></span><br><span class=\"line\">    sigprocmask(SIG_BLOCK, &amp;mask, &amp;prev_mask);</span><br><span class=\"line\">    <span class=\"comment\">// 假设此时接收到 SIGINT 信号</span></span><br><span class=\"line\">    sleep(<span class=\"number\">10</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 解除对 SIGINT 的屏蔽之后，进程会开始处理 SIGINT 信号</span></span><br><span class=\"line\">    sigprocmask(SIG_SETMASK, &amp;prev_mask, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"安全处理信号\"><a href=\"#安全处理信号\" class=\"headerlink\" title=\"安全处理信号\"></a>安全处理信号</h2><ul>\n<li>当进程接收到某个信号时，会调用这个信号的 handler，这会中断主程序的执行。</li>\n<li>当进程在执行某个信号 handler 的过程中，可能会被另一个信号 handler 中断。</li>\n</ul>\n<p>上面这两种情况都会带来并发安全的问题，因此在编写信号 handler 时，需要考虑到并发安全的问题。譬如说，由于信号 handler 会中断主程序的执行，如果信号 handler 与主程序共享全局变量，就可能带来并发安全的问题。</p>\n<p>信号 handler 与主程序共享全局变量是很常见的。譬如说，当进程在接收到SIGINT时，为了优雅地退出程序，这时可以使用一个全局变量记录是否接收到SIGINT信号。主程序每次进入循环时都会检查这个变量，如果发现进程接收到SIGINT信号，就释放好资源并退出程序</p>\n<p>上面的代码并不是并发安全的，可能导致两个问题：</p>\n<ul>\n<li><p>现代编译器通常会优化程序对变量的访问。主程序可能会将quit的副本存储在寄存器中，每次访问quit时就从寄存器中访问。那么即使信号 handler 修改了这个quit在内存中的值，主程序也可能不知道。</p>\n</li>\n<li><p>主程序会读取quit的值，信号 handler 会改变quit的值，而这两个操作都不保证是原子的</p>\n</li>\n</ul>\n<p>我们可以这样解决这两个问题：</p>\n<ul>\n<li><p>首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问</p>\n</li>\n<li><p>首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问</p>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">volatile</span> <span class=\"keyword\">sig_atomic_t</span> quit = <span class=\"number\">0</span>;</span><br></pre></td></tr></table></figure>\n<h2 id=\"I-O-多路复用与信号\"><a href=\"#I-O-多路复用与信号\" class=\"headerlink\" title=\"I/O 多路复用与信号\"></a>I/O 多路复用与信号</h2><p>在 Linux 中处理信号是极为麻烦的事情，正如 Linux 标准指出的，当select()、poll()和epoll_wait()被信号中断之后，它们是决不会重启的，所以说如果这些函数被信号中断，我们只好手动重启它们</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = epoll_wait(<span class=\"comment\">/** ... **/</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (n == <span class=\"number\">-1</span> &amp;&amp; errno == EINTR) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// ...</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所幸的是 Linux 提供了signalfd()函数，signalfd()可以将接收到的信号，转化为文件描述符的可读事件，所以signalfd()可以和 select/poll/epoll 配合使用，大大简化信号处理的难度。</p>\n<p>下面的例子将signalfd()与 epoll 配合使用，signalfd()负责将接收到的SIGINT和SIGHUP转换为文件描述符的可读事件：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/epoll.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/signalfd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdbool.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;assert.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 屏蔽信号 SIGINT 和 SIGHUP</span></span><br><span class=\"line\">    <span class=\"keyword\">sigset_t</span> mask;</span><br><span class=\"line\">    sigemptyset(&amp;mask);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGINT);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGHUP);</span><br><span class=\"line\">    sigprocmask(SIG_BLOCK, &amp;mask, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> signal_fd = signalfd(<span class=\"number\">-1</span>, &amp;mask, SFD_NONBLOCK | SFD_CLOEXEC);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> epoll_fd = epoll_create1(EPOLL_CLOEXEC);</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">epoll_event</span> <span class=\"title\">event</span>;</span></span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(&amp;event, <span class=\"number\">0</span>, <span class=\"keyword\">sizeof</span>(event));</span><br><span class=\"line\">    event.events = EPOLLIN;</span><br><span class=\"line\">    event.data.fd = signal_fd;</span><br><span class=\"line\">    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, signal_fd, &amp;event);</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> MAX_EVENTS = <span class=\"number\">64</span>;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">epoll_event</span> <span class=\"title\">events</span>[<span class=\"title\">MAX_EVENTS</span>];</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = epoll_wait(epoll_fd, &amp;events[<span class=\"number\">0</span>], MAX_EVENTS, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (events[i].data.fd == signal_fd)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">signalfd_siginfo</span> <span class=\"title\">info</span>;</span></span><br><span class=\"line\">                <span class=\"keyword\">ssize_t</span> bytes = read(signal_fd, &amp;info, <span class=\"keyword\">sizeof</span>(info));</span><br><span class=\"line\">                assert(bytes == <span class=\"keyword\">sizeof</span>(info));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (info.ssi_signo == SIGINT)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"built_in\">printf</span>(<span class=\"string\">\"receive signal SIGINT\\n\"</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (info.ssi_signo == SIGHUP)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"built_in\">printf</span>(<span class=\"string\">\"receive signal SIGHUP\\n\"</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"built_in\">printf</span>(<span class=\"string\">\"Program quit!\\n\"</span>);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>参考资料：<a href=\"http://senlinzhan.github.io/2017/03/02/linux-signal/\" target=\"_blank\" rel=\"noopener\">http://senlinzhan.github.io/2017/03/02/linux-signal/</a></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"信号处理机制\"><a href=\"#信号处理机制\" class=\"headerlink\" title=\"信号处理机制\"></a>信号处理机制</h2><p>在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：</p>\n<pre><code>- 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n- 在 Linux 中，每个进程都拥有两个位向量，这两个位向量共同决定了进程将如何处理信号：\n</code></pre><p>当内核发送一个信号给进程时，它将会修改进程的pending位向量，譬如说，当内核发送一个SIGINT信号给进程，那么它会将进程的pending[SIGINT]的值设置成 1。同样地，当进程屏蔽掉一个信号时，那么它会修改blocked位向量。当进程屏蔽掉一个信号之后，内核仍然可以发送这个信号给进程(保存在进程的pending位向量中)，但进程不会接收并处理这个信号。只有当进程解除了对这个信号的屏蔽之后，进程才会接收并处理这个信号。</p>\n<p>下面的程序一开始就屏蔽了SIGINT信号，所以即使内核发送SIGINT信号给这个程序，这个信号也不会得到处理。而当程序解除了对SIGINT的屏蔽之后，这个SIGINT信号才会得到处理：</p>","more":"<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sigint_handler</span><span class=\"params\">(<span class=\"keyword\">int</span> sig)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *message = <span class=\"string\">\"handle SIGINT signal\\n\"</span>;</span><br><span class=\"line\">    write(STDOUT_FILENO, message, <span class=\"built_in\">strlen</span>(message));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    signal(SIGINT, sigint_handler);</span><br><span class=\"line\">    <span class=\"keyword\">sigset_t</span> mask, prev_mask;</span><br><span class=\"line\">    sigemptyset(&amp;mask);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGINT);</span><br><span class=\"line\">    <span class=\"comment\">// 屏蔽掉 SIGINT 信号</span></span><br><span class=\"line\">    sigprocmask(SIG_BLOCK, &amp;mask, &amp;prev_mask);</span><br><span class=\"line\">    <span class=\"comment\">// 假设此时接收到 SIGINT 信号</span></span><br><span class=\"line\">    sleep(<span class=\"number\">10</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 解除对 SIGINT 的屏蔽之后，进程会开始处理 SIGINT 信号</span></span><br><span class=\"line\">    sigprocmask(SIG_SETMASK, &amp;prev_mask, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"安全处理信号\"><a href=\"#安全处理信号\" class=\"headerlink\" title=\"安全处理信号\"></a>安全处理信号</h2><ul>\n<li>当进程接收到某个信号时，会调用这个信号的 handler，这会中断主程序的执行。</li>\n<li>当进程在执行某个信号 handler 的过程中，可能会被另一个信号 handler 中断。</li>\n</ul>\n<p>上面这两种情况都会带来并发安全的问题，因此在编写信号 handler 时，需要考虑到并发安全的问题。譬如说，由于信号 handler 会中断主程序的执行，如果信号 handler 与主程序共享全局变量，就可能带来并发安全的问题。</p>\n<p>信号 handler 与主程序共享全局变量是很常见的。譬如说，当进程在接收到SIGINT时，为了优雅地退出程序，这时可以使用一个全局变量记录是否接收到SIGINT信号。主程序每次进入循环时都会检查这个变量，如果发现进程接收到SIGINT信号，就释放好资源并退出程序</p>\n<p>上面的代码并不是并发安全的，可能导致两个问题：</p>\n<ul>\n<li><p>现代编译器通常会优化程序对变量的访问。主程序可能会将quit的副本存储在寄存器中，每次访问quit时就从寄存器中访问。那么即使信号 handler 修改了这个quit在内存中的值，主程序也可能不知道。</p>\n</li>\n<li><p>主程序会读取quit的值，信号 handler 会改变quit的值，而这两个操作都不保证是原子的</p>\n</li>\n</ul>\n<p>我们可以这样解决这两个问题：</p>\n<ul>\n<li><p>首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问</p>\n</li>\n<li><p>首先将quit声明为volatile变量。volatile可以阻止编译器所做的优化，这样信号 handler 和主程序访问quit时都会从主内存中访问</p>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">volatile</span> <span class=\"keyword\">sig_atomic_t</span> quit = <span class=\"number\">0</span>;</span><br></pre></td></tr></table></figure>\n<h2 id=\"I-O-多路复用与信号\"><a href=\"#I-O-多路复用与信号\" class=\"headerlink\" title=\"I/O 多路复用与信号\"></a>I/O 多路复用与信号</h2><p>在 Linux 中处理信号是极为麻烦的事情，正如 Linux 标准指出的，当select()、poll()和epoll_wait()被信号中断之后，它们是决不会重启的，所以说如果这些函数被信号中断，我们只好手动重启它们</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = epoll_wait(<span class=\"comment\">/** ... **/</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (n == <span class=\"number\">-1</span> &amp;&amp; errno == EINTR) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// ...</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所幸的是 Linux 提供了signalfd()函数，signalfd()可以将接收到的信号，转化为文件描述符的可读事件，所以signalfd()可以和 select/poll/epoll 配合使用，大大简化信号处理的难度。</p>\n<p>下面的例子将signalfd()与 epoll 配合使用，signalfd()负责将接收到的SIGINT和SIGHUP转换为文件描述符的可读事件：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;string.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/epoll.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/signalfd.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdbool.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;assert.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 屏蔽信号 SIGINT 和 SIGHUP</span></span><br><span class=\"line\">    <span class=\"keyword\">sigset_t</span> mask;</span><br><span class=\"line\">    sigemptyset(&amp;mask);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGINT);</span><br><span class=\"line\">    sigaddset(&amp;mask, SIGHUP);</span><br><span class=\"line\">    sigprocmask(SIG_BLOCK, &amp;mask, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> signal_fd = signalfd(<span class=\"number\">-1</span>, &amp;mask, SFD_NONBLOCK | SFD_CLOEXEC);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> epoll_fd = epoll_create1(EPOLL_CLOEXEC);</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">epoll_event</span> <span class=\"title\">event</span>;</span></span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(&amp;event, <span class=\"number\">0</span>, <span class=\"keyword\">sizeof</span>(event));</span><br><span class=\"line\">    event.events = EPOLLIN;</span><br><span class=\"line\">    event.data.fd = signal_fd;</span><br><span class=\"line\">    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, signal_fd, &amp;event);</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> MAX_EVENTS = <span class=\"number\">64</span>;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">epoll_event</span> <span class=\"title\">events</span>[<span class=\"title\">MAX_EVENTS</span>];</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> n = epoll_wait(epoll_fd, &amp;events[<span class=\"number\">0</span>], MAX_EVENTS, <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (events[i].data.fd == signal_fd)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">signalfd_siginfo</span> <span class=\"title\">info</span>;</span></span><br><span class=\"line\">                <span class=\"keyword\">ssize_t</span> bytes = read(signal_fd, &amp;info, <span class=\"keyword\">sizeof</span>(info));</span><br><span class=\"line\">                assert(bytes == <span class=\"keyword\">sizeof</span>(info));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (info.ssi_signo == SIGINT)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"built_in\">printf</span>(<span class=\"string\">\"receive signal SIGINT\\n\"</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (info.ssi_signo == SIGHUP)</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"built_in\">printf</span>(<span class=\"string\">\"receive signal SIGHUP\\n\"</span>);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                <span class=\"built_in\">printf</span>(<span class=\"string\">\"Program quit!\\n\"</span>);</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>参考资料：<a href=\"http://senlinzhan.github.io/2017/03/02/linux-signal/\" target=\"_blank\" rel=\"noopener\">http://senlinzhan.github.io/2017/03/02/linux-signal/</a></p>"},{"title":"Kafka运营-NotLeaderForPartitionException异常","date":"2019-01-03T04:11:22.000Z","_content":"\n\n## Kafka NotLeaderForPartitionException异常\n\n### 异常分析\n\n1.Kafka日志分析\n\n发现Kafka日志中有比较多的org.apache.kafka.common.errors.NotLeaderForPartitionException异常信息，该异常从字面解释就是某个分区的leader找不到，具体异常信息如下：\n\n```java\n2019-01-18 22:01:00,802 ERROR kafka.server.ReplicaFetcherThread: [ReplicaFetcherThread-0-118]: Error for partition [kafka_custflow_topic_test,5] to broker 118:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.\n```\n\n通常来说该异常信息是由于kafka和zk连接存在超时，接着导致Controller重新选举导致获取元数据不正确，timed out的那台Broker所持有的partition就会出现NotLeaderForPartitionException，kafka中连接zk超时日志格式信息如下:\n\n```\n2019-01-18 21:59:55,916 WARN org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5\n2019-01-18 21:59:55,916 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5, closing socket connection and attempting reconnect\n```\n```\n2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server mfsmaster/121.9.240.249:2181. Will not attempt to authenticate using SASL (unknown error)\n2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to mfsmaster/121.9.240.249:2181, initiating session\n2019-01-18 21:59:56,080 WARN org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired\n2019-01-18 21:59:56,080 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (Expired)\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired, closing socket connection\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=SR-CNSX-GDFS-240-251:2181,SR-CNSX-GDFS-240-252:2181,SR-CNSX-GDFS-240-253:2181,mfslogger:2181,mfsmaster:2181/kafka sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7957dc72\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x2677edb3ac1f8d5\n```\n\n```\n2019-01-18 21:59:56,094 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (SyncConnected)\n2019-01-18 21:59:56,095 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: re-registering broker info in ZK for broker 121\n2019-01-18 21:59:56,095 INFO kafka.utils.ZKCheckedEphemeral: Creating /brokers/ids/121 (is it secure? false)\n2019-01-18 21:59:56,112 INFO kafka.utils.ZKCheckedEphemeral: Result of znode creation is: OK\n2019-01-18 21:59:56,113 INFO kafka.utils.ZkUtils: Registered broker 121 at path /brokers/ids/121 with addresses: EndPoint(SR-CNSX-GDFS-240-252,9092,ListenerName(PLAINTEXT),PLAINTEXT)\n2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: done re-registering broker\n2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: Subscribing to /brokers/topics path to watch for new topics\n```\n\n\n\n2.zk日志分析\n\n同时分析zk日志信息：\n\n```\n2019-01-18 21:59:55,961 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x2677edb3ac1f8d5, likely client has closed socket\n        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)\n        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)\n        at java.lang.Thread.run(Thread.java:748)\n2019-01-18 21:59:55,962 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /121.9.240.252:40660 which had sessionid 0x2677edb3ac1f8d5\n2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x2677edb3ac1f8d5, timeout of 6000ms exceeded\n2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x2677edb3ac1f8d5\n```\n\n综合zk和kafka日志信息，可以看出kafka和zk session超时之后，session会被zk主动关闭，之后kafka会重新连接到zk集群，基本是在1s之内kafka新的session已经建立，所以从短时间内kafka服务没问题。\n\n\n\n3.gc日志分析\n\nGc 日志中基本没有Full GC\n\n\n\n4.系统资源分析\n- 晚上9~10业务晚高峰，数据量通常比较大\n- CPU 内存正常\n- IO在晚高峰时存在突刺\n\n\n\n### 解决方案\n\n1.kafka增大session time out \n\n当前默认值是6s，可适当加大超时时间\n\n2.增加kafka磁盘\n\n3.增加磁盘IO处理线程数","source":"_posts/kafka运营-NotLeaderForPartitionException异常.md","raw":"---\ntitle: Kafka运营-NotLeaderForPartitionException异常\ndate: 2019-01-03 12:11:22\ntags: kafka\n---\n\n\n## Kafka NotLeaderForPartitionException异常\n\n### 异常分析\n\n1.Kafka日志分析\n\n发现Kafka日志中有比较多的org.apache.kafka.common.errors.NotLeaderForPartitionException异常信息，该异常从字面解释就是某个分区的leader找不到，具体异常信息如下：\n\n```java\n2019-01-18 22:01:00,802 ERROR kafka.server.ReplicaFetcherThread: [ReplicaFetcherThread-0-118]: Error for partition [kafka_custflow_topic_test,5] to broker 118:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.\n```\n\n通常来说该异常信息是由于kafka和zk连接存在超时，接着导致Controller重新选举导致获取元数据不正确，timed out的那台Broker所持有的partition就会出现NotLeaderForPartitionException，kafka中连接zk超时日志格式信息如下:\n\n```\n2019-01-18 21:59:55,916 WARN org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5\n2019-01-18 21:59:55,916 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5, closing socket connection and attempting reconnect\n```\n```\n2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server mfsmaster/121.9.240.249:2181. Will not attempt to authenticate using SASL (unknown error)\n2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to mfsmaster/121.9.240.249:2181, initiating session\n2019-01-18 21:59:56,080 WARN org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired\n2019-01-18 21:59:56,080 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (Expired)\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired, closing socket connection\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=SR-CNSX-GDFS-240-251:2181,SR-CNSX-GDFS-240-252:2181,SR-CNSX-GDFS-240-253:2181,mfslogger:2181,mfsmaster:2181/kafka sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7957dc72\n2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x2677edb3ac1f8d5\n```\n\n```\n2019-01-18 21:59:56,094 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (SyncConnected)\n2019-01-18 21:59:56,095 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: re-registering broker info in ZK for broker 121\n2019-01-18 21:59:56,095 INFO kafka.utils.ZKCheckedEphemeral: Creating /brokers/ids/121 (is it secure? false)\n2019-01-18 21:59:56,112 INFO kafka.utils.ZKCheckedEphemeral: Result of znode creation is: OK\n2019-01-18 21:59:56,113 INFO kafka.utils.ZkUtils: Registered broker 121 at path /brokers/ids/121 with addresses: EndPoint(SR-CNSX-GDFS-240-252,9092,ListenerName(PLAINTEXT),PLAINTEXT)\n2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: done re-registering broker\n2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: Subscribing to /brokers/topics path to watch for new topics\n```\n\n\n\n2.zk日志分析\n\n同时分析zk日志信息：\n\n```\n2019-01-18 21:59:55,961 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x2677edb3ac1f8d5, likely client has closed socket\n        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)\n        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)\n        at java.lang.Thread.run(Thread.java:748)\n2019-01-18 21:59:55,962 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /121.9.240.252:40660 which had sessionid 0x2677edb3ac1f8d5\n2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x2677edb3ac1f8d5, timeout of 6000ms exceeded\n2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x2677edb3ac1f8d5\n```\n\n综合zk和kafka日志信息，可以看出kafka和zk session超时之后，session会被zk主动关闭，之后kafka会重新连接到zk集群，基本是在1s之内kafka新的session已经建立，所以从短时间内kafka服务没问题。\n\n\n\n3.gc日志分析\n\nGc 日志中基本没有Full GC\n\n\n\n4.系统资源分析\n- 晚上9~10业务晚高峰，数据量通常比较大\n- CPU 内存正常\n- IO在晚高峰时存在突刺\n\n\n\n### 解决方案\n\n1.kafka增大session time out \n\n当前默认值是6s，可适当加大超时时间\n\n2.增加kafka磁盘\n\n3.增加磁盘IO处理线程数","slug":"kafka运营-NotLeaderForPartitionException异常","published":1,"updated":"2019-01-20T05:57:10.134Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubld0005amumubm9j4o3","content":"<h2 id=\"Kafka-NotLeaderForPartitionException异常\"><a href=\"#Kafka-NotLeaderForPartitionException异常\" class=\"headerlink\" title=\"Kafka NotLeaderForPartitionException异常\"></a>Kafka NotLeaderForPartitionException异常</h2><h3 id=\"异常分析\"><a href=\"#异常分析\" class=\"headerlink\" title=\"异常分析\"></a>异常分析</h3><p>1.Kafka日志分析</p>\n<p>发现Kafka日志中有比较多的org.apache.kafka.common.errors.NotLeaderForPartitionException异常信息，该异常从字面解释就是某个分区的leader找不到，具体异常信息如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2019</span>-<span class=\"number\">01</span>-<span class=\"number\">18</span> <span class=\"number\">22</span>:<span class=\"number\">01</span>:<span class=\"number\">00</span>,<span class=\"number\">802</span> ERROR kafka.server.ReplicaFetcherThread: [ReplicaFetcherThread-<span class=\"number\">0</span>-<span class=\"number\">118</span>]: Error <span class=\"keyword\">for</span> partition [kafka_custflow_topic_test,<span class=\"number\">5</span>] to broker <span class=\"number\">118</span>:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader <span class=\"keyword\">for</span> that topic-partition.</span><br></pre></td></tr></table></figure>\n<p>通常来说该异常信息是由于kafka和zk连接存在超时，接着导致Controller重新选举导致获取元数据不正确，timed out的那台Broker所持有的partition就会出现NotLeaderForPartitionException，kafka中连接zk超时日志格式信息如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:55,916 WARN org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5</span><br><span class=\"line\">2019-01-18 21:59:55,916 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5, closing socket connection and attempting reconnect</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server mfsmaster/121.9.240.249:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class=\"line\">2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to mfsmaster/121.9.240.249:2181, initiating session</span><br><span class=\"line\">2019-01-18 21:59:56,080 WARN org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (Expired)</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired, closing socket connection</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=SR-CNSX-GDFS-240-251:2181,SR-CNSX-GDFS-240-252:2181,SR-CNSX-GDFS-240-253:2181,mfslogger:2181,mfsmaster:2181/kafka sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7957dc72</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x2677edb3ac1f8d5</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:56,094 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (SyncConnected)</span><br><span class=\"line\">2019-01-18 21:59:56,095 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: re-registering broker info in ZK for broker 121</span><br><span class=\"line\">2019-01-18 21:59:56,095 INFO kafka.utils.ZKCheckedEphemeral: Creating /brokers/ids/121 (is it secure? false)</span><br><span class=\"line\">2019-01-18 21:59:56,112 INFO kafka.utils.ZKCheckedEphemeral: Result of znode creation is: OK</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.utils.ZkUtils: Registered broker 121 at path /brokers/ids/121 with addresses: EndPoint(SR-CNSX-GDFS-240-252,9092,ListenerName(PLAINTEXT),PLAINTEXT)</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: done re-registering broker</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: Subscribing to /brokers/topics path to watch for new topics</span><br></pre></td></tr></table></figure>\n<p>2.zk日志分析</p>\n<p>同时分析zk日志信息：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:55,961 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class=\"line\">EndOfStreamException: Unable to read additional data from client sessionid 0x2677edb3ac1f8d5, likely client has closed socket</span><br><span class=\"line\">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class=\"line\">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class=\"line\">        at java.lang.Thread.run(Thread.java:748)</span><br><span class=\"line\">2019-01-18 21:59:55,962 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /121.9.240.252:40660 which had sessionid 0x2677edb3ac1f8d5</span><br><span class=\"line\">2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x2677edb3ac1f8d5, timeout of 6000ms exceeded</span><br><span class=\"line\">2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x2677edb3ac1f8d5</span><br></pre></td></tr></table></figure>\n<p>综合zk和kafka日志信息，可以看出kafka和zk session超时之后，session会被zk主动关闭，之后kafka会重新连接到zk集群，基本是在1s之内kafka新的session已经建立，所以从短时间内kafka服务没问题。</p>\n<p>3.gc日志分析</p>\n<p>Gc 日志中基本没有Full GC</p>\n<p>4.系统资源分析</p>\n<ul>\n<li>晚上9~10业务晚高峰，数据量通常比较大</li>\n<li>CPU 内存正常</li>\n<li>IO在晚高峰时存在突刺</li>\n</ul>\n<h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>1.kafka增大session time out </p>\n<p>当前默认值是6s，可适当加大超时时间</p>\n<p>2.增加kafka磁盘</p>\n<p>3.增加磁盘IO处理线程数</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Kafka-NotLeaderForPartitionException异常\"><a href=\"#Kafka-NotLeaderForPartitionException异常\" class=\"headerlink\" title=\"Kafka NotLeaderForPartitionException异常\"></a>Kafka NotLeaderForPartitionException异常</h2><h3 id=\"异常分析\"><a href=\"#异常分析\" class=\"headerlink\" title=\"异常分析\"></a>异常分析</h3><p>1.Kafka日志分析</p>\n<p>发现Kafka日志中有比较多的org.apache.kafka.common.errors.NotLeaderForPartitionException异常信息，该异常从字面解释就是某个分区的leader找不到，具体异常信息如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">2019</span>-<span class=\"number\">01</span>-<span class=\"number\">18</span> <span class=\"number\">22</span>:<span class=\"number\">01</span>:<span class=\"number\">00</span>,<span class=\"number\">802</span> ERROR kafka.server.ReplicaFetcherThread: [ReplicaFetcherThread-<span class=\"number\">0</span>-<span class=\"number\">118</span>]: Error <span class=\"keyword\">for</span> partition [kafka_custflow_topic_test,<span class=\"number\">5</span>] to broker <span class=\"number\">118</span>:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader <span class=\"keyword\">for</span> that topic-partition.</span><br></pre></td></tr></table></figure>\n<p>通常来说该异常信息是由于kafka和zk连接存在超时，接着导致Controller重新选举导致获取元数据不正确，timed out的那台Broker所持有的partition就会出现NotLeaderForPartitionException，kafka中连接zk超时日志格式信息如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:55,916 WARN org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5</span><br><span class=\"line\">2019-01-18 21:59:55,916 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 7329ms for sessionid 0x2677edb3ac1f8d5, closing socket connection and attempting reconnect</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server mfsmaster/121.9.240.249:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class=\"line\">2019-01-18 21:59:56,075 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to mfsmaster/121.9.240.249:2181, initiating session</span><br><span class=\"line\">2019-01-18 21:59:56,080 WARN org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (Expired)</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2677edb3ac1f8d5 has expired, closing socket connection</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=SR-CNSX-GDFS-240-251:2181,SR-CNSX-GDFS-240-252:2181,SR-CNSX-GDFS-240-253:2181,mfslogger:2181,mfsmaster:2181/kafka sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7957dc72</span><br><span class=\"line\">2019-01-18 21:59:56,080 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x2677edb3ac1f8d5</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:56,094 INFO org.I0Itec.zkclient.ZkClient: zookeeper state changed (SyncConnected)</span><br><span class=\"line\">2019-01-18 21:59:56,095 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: re-registering broker info in ZK for broker 121</span><br><span class=\"line\">2019-01-18 21:59:56,095 INFO kafka.utils.ZKCheckedEphemeral: Creating /brokers/ids/121 (is it secure? false)</span><br><span class=\"line\">2019-01-18 21:59:56,112 INFO kafka.utils.ZKCheckedEphemeral: Result of znode creation is: OK</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.utils.ZkUtils: Registered broker 121 at path /brokers/ids/121 with addresses: EndPoint(SR-CNSX-GDFS-240-252,9092,ListenerName(PLAINTEXT),PLAINTEXT)</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: done re-registering broker</span><br><span class=\"line\">2019-01-18 21:59:56,113 INFO kafka.server.KafkaHealthcheck$SessionExpireListener: Subscribing to /brokers/topics path to watch for new topics</span><br></pre></td></tr></table></figure>\n<p>2.zk日志分析</p>\n<p>同时分析zk日志信息：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2019-01-18 21:59:55,961 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class=\"line\">EndOfStreamException: Unable to read additional data from client sessionid 0x2677edb3ac1f8d5, likely client has closed socket</span><br><span class=\"line\">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class=\"line\">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class=\"line\">        at java.lang.Thread.run(Thread.java:748)</span><br><span class=\"line\">2019-01-18 21:59:55,962 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /121.9.240.252:40660 which had sessionid 0x2677edb3ac1f8d5</span><br><span class=\"line\">2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.ZooKeeperServer: Expiring session 0x2677edb3ac1f8d5, timeout of 6000ms exceeded</span><br><span class=\"line\">2019-01-18 21:59:56,001 INFO org.apache.zookeeper.server.PrepRequestProcessor: Processed session termination for sessionid: 0x2677edb3ac1f8d5</span><br></pre></td></tr></table></figure>\n<p>综合zk和kafka日志信息，可以看出kafka和zk session超时之后，session会被zk主动关闭，之后kafka会重新连接到zk集群，基本是在1s之内kafka新的session已经建立，所以从短时间内kafka服务没问题。</p>\n<p>3.gc日志分析</p>\n<p>Gc 日志中基本没有Full GC</p>\n<p>4.系统资源分析</p>\n<ul>\n<li>晚上9~10业务晚高峰，数据量通常比较大</li>\n<li>CPU 内存正常</li>\n<li>IO在晚高峰时存在突刺</li>\n</ul>\n<h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>1.kafka增大session time out </p>\n<p>当前默认值是6s，可适当加大超时时间</p>\n<p>2.增加kafka磁盘</p>\n<p>3.增加磁盘IO处理线程数</p>\n"},{"title":"lighttpd 程序框架","date":"2016-10-15T03:37:28.000Z","_content":"\n由于历史的原因，公司部门在多个组件中使用了lighttpd，具体为何当时技术选型的时候没选nginx而选择lighttpd就不得而知了。lighttpd的社区相对nginx差距还是很大的，明显nginx的社区更活跃。lighttpd网上的资料以及第三方模块相对比较少，学习的成本会相对高一点。不过lighttpd的源码相对nginx会少一点，毕竟lighttpd比较轻量级，功能上没nginx那么多。网上nginx/lighttpd/appache 三种web server的测试结果，lighttpd占用内存最小，请求响应时间中等，apache最差。\n\n## 进程模型\nlighttpd采用master-worker进程模型，master进程主要负责加载配置、fork worker进程、管理worker进程，worker进程主要负责接收请求、处理请求、返回请求结果，worker进程个数可以在配置文件中配置，master进程会根据配置的个数，fork worker进程\n\n<!-- more -->\n\nmaster进程的主要逻辑：\n- 根据命令行完成各种初始化工作\n- daemonize\n- fork \n- wait子进程(当子进程退出时，再fork出一个子进程)\n\n主要逻辑代码：\n\n```\nnum_childs = srv->srvconf.max_worker;\nif (num_childs > 0) {\n    int child = 0;\n    //master进程上下文\n    while (!child && !srv_shutdown && !graceful_shutdown) {\n        if (num_childs > 0) {\n            switch (fork()) {\n            case -1:\n                return -1;\n            case 0:\n                //worker进程在这里退出，执行后面的程序逻辑\n                child = 1;\n                break;\n            default:\n                num_childs--;\n                break;\n            }\n        } else {\n            int status;\n\n            // master进程在这里阻塞等待，回收worker进程\n            if (-1 != wait(&status)) {\n                //如果一个worker进程挂了，master进程会重新fork一个worker进程\n                num_childs++;\n            } else {\n                switch (errno) {\n                case EINTR:\n                    if (handle_sig_hup) {\n                        handle_sig_hup = 0;\n\n                        log_error_cycle(srv);\n                        if (!forwarded_sig_hup && 0 != srv->srvconf.max_worker) {\n                            forwarded_sig_hup = 1;\n                            kill(0, SIGHUP);\n                        }\n                    }\n                    break;\n                default:\n                    break;\n                }\n            }\n        }\n    }\n\n    //master 进程在这里退出\n    if (!child) {\n        /** \n            * kill all children too \n            */\n        if (graceful_shutdown) {\n            kill(0, SIGINT);\n        } else if (srv_shutdown) {\n            kill(0, SIGTERM);\n        }\n\n        remove_pid_file(srv, &pid_fd);\n        log_error_close(srv);\n        network_close(srv);\n        connections_free(srv);\n        plugins_free(srv);\n        server_free(srv);\n        return 0;\n    }\n```\n\nmater 进程的主要工作逻辑还是非常清晰的，简单的说就是早fork完子进程后，阻塞，监控子进程\n\nworker进程的主要逻辑：\n- 初始化event模型(select/poll/pselect...)\n- 设置监听listen的套接字，注册listen套接字读写的回调函数\n- while(1)大循环：\n    1. 判断server服务是否终止,如果服务终止，删除pid文件、写日志、日志关闭、网络关闭等清理工作，然后程序退出;\n    2. 如果server服务未终止，先判断是否存在SIGHUP信号，调用各个插件的handle_sighup函数;\n    3. 判断是否产生了SIGALARM信号，若是执行各个插件的handle_trigger函数，再判断各个连接的超时;( 程序在处理连接超时的时候是每一秒中轮询所有的连接，判断其是否超时，这个效率其实低了);\n    4. 判断连接是否失效以及服务是否过载;\n    5. 启动事件轮询，等待各种IO事件的发生，包括文件读写，socket请求等，一旦有事件发生，调用相应的处理函数进行处理，这也是整个server逻辑最复杂的地方，根据连接状态机处理socket读写事件,lighttpd的事件模型以及插件模型需要再另外写篇文章来分析\n\n主要逻辑代码：\n\n```\n    //启动事件轮询,等待各种IO时间的发生,包括文件读写，socket请求等\n    if ((n = fdevent_poll(srv->ev, 1000)) > 0) {\n        /* n is the number of events */\n        int revents;\n        int fd_ndx;\n        last_active_ts = srv->cur_ts;\n        fd_ndx = -1;\n        do {\n            fdevent_handler handler;\n            void *context;\n\n            fd_ndx  = fdevent_event_next_fdndx (srv->ev, fd_ndx);\n            if (-1 == fd_ndx) break; /* not all fdevent handlers know how many fds got an event */\n\n            revents = fdevent_event_get_revent (srv->ev, fd_ndx);\n            fd      = fdevent_event_get_fd     (srv->ev, fd_ndx);\n            handler = fdevent_get_handler(srv->ev, fd);\n            context = fdevent_get_context(srv->ev, fd);\n\n            //一旦有事件发生，调用相应的处理函数进行处理。\n            if (NULL != handler) {\n                (*handler)(srv, context, revents);\n            }\n        } while (--n > 0);\n        fdevent_sched_run(srv, srv->ev);\n    }\n```\n","source":"_posts/lighttpd-程序框架.md","raw":"---\ntitle: lighttpd 程序框架\ndate: 2016-10-15 11:37:28\ntags: lighttpd\n---\n\n由于历史的原因，公司部门在多个组件中使用了lighttpd，具体为何当时技术选型的时候没选nginx而选择lighttpd就不得而知了。lighttpd的社区相对nginx差距还是很大的，明显nginx的社区更活跃。lighttpd网上的资料以及第三方模块相对比较少，学习的成本会相对高一点。不过lighttpd的源码相对nginx会少一点，毕竟lighttpd比较轻量级，功能上没nginx那么多。网上nginx/lighttpd/appache 三种web server的测试结果，lighttpd占用内存最小，请求响应时间中等，apache最差。\n\n## 进程模型\nlighttpd采用master-worker进程模型，master进程主要负责加载配置、fork worker进程、管理worker进程，worker进程主要负责接收请求、处理请求、返回请求结果，worker进程个数可以在配置文件中配置，master进程会根据配置的个数，fork worker进程\n\n<!-- more -->\n\nmaster进程的主要逻辑：\n- 根据命令行完成各种初始化工作\n- daemonize\n- fork \n- wait子进程(当子进程退出时，再fork出一个子进程)\n\n主要逻辑代码：\n\n```\nnum_childs = srv->srvconf.max_worker;\nif (num_childs > 0) {\n    int child = 0;\n    //master进程上下文\n    while (!child && !srv_shutdown && !graceful_shutdown) {\n        if (num_childs > 0) {\n            switch (fork()) {\n            case -1:\n                return -1;\n            case 0:\n                //worker进程在这里退出，执行后面的程序逻辑\n                child = 1;\n                break;\n            default:\n                num_childs--;\n                break;\n            }\n        } else {\n            int status;\n\n            // master进程在这里阻塞等待，回收worker进程\n            if (-1 != wait(&status)) {\n                //如果一个worker进程挂了，master进程会重新fork一个worker进程\n                num_childs++;\n            } else {\n                switch (errno) {\n                case EINTR:\n                    if (handle_sig_hup) {\n                        handle_sig_hup = 0;\n\n                        log_error_cycle(srv);\n                        if (!forwarded_sig_hup && 0 != srv->srvconf.max_worker) {\n                            forwarded_sig_hup = 1;\n                            kill(0, SIGHUP);\n                        }\n                    }\n                    break;\n                default:\n                    break;\n                }\n            }\n        }\n    }\n\n    //master 进程在这里退出\n    if (!child) {\n        /** \n            * kill all children too \n            */\n        if (graceful_shutdown) {\n            kill(0, SIGINT);\n        } else if (srv_shutdown) {\n            kill(0, SIGTERM);\n        }\n\n        remove_pid_file(srv, &pid_fd);\n        log_error_close(srv);\n        network_close(srv);\n        connections_free(srv);\n        plugins_free(srv);\n        server_free(srv);\n        return 0;\n    }\n```\n\nmater 进程的主要工作逻辑还是非常清晰的，简单的说就是早fork完子进程后，阻塞，监控子进程\n\nworker进程的主要逻辑：\n- 初始化event模型(select/poll/pselect...)\n- 设置监听listen的套接字，注册listen套接字读写的回调函数\n- while(1)大循环：\n    1. 判断server服务是否终止,如果服务终止，删除pid文件、写日志、日志关闭、网络关闭等清理工作，然后程序退出;\n    2. 如果server服务未终止，先判断是否存在SIGHUP信号，调用各个插件的handle_sighup函数;\n    3. 判断是否产生了SIGALARM信号，若是执行各个插件的handle_trigger函数，再判断各个连接的超时;( 程序在处理连接超时的时候是每一秒中轮询所有的连接，判断其是否超时，这个效率其实低了);\n    4. 判断连接是否失效以及服务是否过载;\n    5. 启动事件轮询，等待各种IO事件的发生，包括文件读写，socket请求等，一旦有事件发生，调用相应的处理函数进行处理，这也是整个server逻辑最复杂的地方，根据连接状态机处理socket读写事件,lighttpd的事件模型以及插件模型需要再另外写篇文章来分析\n\n主要逻辑代码：\n\n```\n    //启动事件轮询,等待各种IO时间的发生,包括文件读写，socket请求等\n    if ((n = fdevent_poll(srv->ev, 1000)) > 0) {\n        /* n is the number of events */\n        int revents;\n        int fd_ndx;\n        last_active_ts = srv->cur_ts;\n        fd_ndx = -1;\n        do {\n            fdevent_handler handler;\n            void *context;\n\n            fd_ndx  = fdevent_event_next_fdndx (srv->ev, fd_ndx);\n            if (-1 == fd_ndx) break; /* not all fdevent handlers know how many fds got an event */\n\n            revents = fdevent_event_get_revent (srv->ev, fd_ndx);\n            fd      = fdevent_event_get_fd     (srv->ev, fd_ndx);\n            handler = fdevent_get_handler(srv->ev, fd);\n            context = fdevent_get_context(srv->ev, fd);\n\n            //一旦有事件发生，调用相应的处理函数进行处理。\n            if (NULL != handler) {\n                (*handler)(srv, context, revents);\n            }\n        } while (--n > 0);\n        fdevent_sched_run(srv, srv->ev);\n    }\n```\n","slug":"lighttpd-程序框架","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6duble0006amumxtevoxh3","content":"<p>由于历史的原因，公司部门在多个组件中使用了lighttpd，具体为何当时技术选型的时候没选nginx而选择lighttpd就不得而知了。lighttpd的社区相对nginx差距还是很大的，明显nginx的社区更活跃。lighttpd网上的资料以及第三方模块相对比较少，学习的成本会相对高一点。不过lighttpd的源码相对nginx会少一点，毕竟lighttpd比较轻量级，功能上没nginx那么多。网上nginx/lighttpd/appache 三种web server的测试结果，lighttpd占用内存最小，请求响应时间中等，apache最差。</p>\n<h2 id=\"进程模型\"><a href=\"#进程模型\" class=\"headerlink\" title=\"进程模型\"></a>进程模型</h2><p>lighttpd采用master-worker进程模型，master进程主要负责加载配置、fork worker进程、管理worker进程，worker进程主要负责接收请求、处理请求、返回请求结果，worker进程个数可以在配置文件中配置，master进程会根据配置的个数，fork worker进程</p>\n<a id=\"more\"></a>\n<p>master进程的主要逻辑：</p>\n<ul>\n<li>根据命令行完成各种初始化工作</li>\n<li>daemonize</li>\n<li>fork </li>\n<li>wait子进程(当子进程退出时，再fork出一个子进程)</li>\n</ul>\n<p>主要逻辑代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_childs = srv-&gt;srvconf.max_worker;</span><br><span class=\"line\">if (num_childs &gt; 0) &#123;</span><br><span class=\"line\">    int child = 0;</span><br><span class=\"line\">    //master进程上下文</span><br><span class=\"line\">    while (!child &amp;&amp; !srv_shutdown &amp;&amp; !graceful_shutdown) &#123;</span><br><span class=\"line\">        if (num_childs &gt; 0) &#123;</span><br><span class=\"line\">            switch (fork()) &#123;</span><br><span class=\"line\">            case -1:</span><br><span class=\"line\">                return -1;</span><br><span class=\"line\">            case 0:</span><br><span class=\"line\">                //worker进程在这里退出，执行后面的程序逻辑</span><br><span class=\"line\">                child = 1;</span><br><span class=\"line\">                break;</span><br><span class=\"line\">            default:</span><br><span class=\"line\">                num_childs--;</span><br><span class=\"line\">                break;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            int status;</span><br><span class=\"line\"></span><br><span class=\"line\">            // master进程在这里阻塞等待，回收worker进程</span><br><span class=\"line\">            if (-1 != wait(&amp;status)) &#123;</span><br><span class=\"line\">                //如果一个worker进程挂了，master进程会重新fork一个worker进程</span><br><span class=\"line\">                num_childs++;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                switch (errno) &#123;</span><br><span class=\"line\">                case EINTR:</span><br><span class=\"line\">                    if (handle_sig_hup) &#123;</span><br><span class=\"line\">                        handle_sig_hup = 0;</span><br><span class=\"line\"></span><br><span class=\"line\">                        log_error_cycle(srv);</span><br><span class=\"line\">                        if (!forwarded_sig_hup &amp;&amp; 0 != srv-&gt;srvconf.max_worker) &#123;</span><br><span class=\"line\">                            forwarded_sig_hup = 1;</span><br><span class=\"line\">                            kill(0, SIGHUP);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    break;</span><br><span class=\"line\">                default:</span><br><span class=\"line\">                    break;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //master 进程在这里退出</span><br><span class=\"line\">    if (!child) &#123;</span><br><span class=\"line\">        /** </span><br><span class=\"line\">            * kill all children too </span><br><span class=\"line\">            */</span><br><span class=\"line\">        if (graceful_shutdown) &#123;</span><br><span class=\"line\">            kill(0, SIGINT);</span><br><span class=\"line\">        &#125; else if (srv_shutdown) &#123;</span><br><span class=\"line\">            kill(0, SIGTERM);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        remove_pid_file(srv, &amp;pid_fd);</span><br><span class=\"line\">        log_error_close(srv);</span><br><span class=\"line\">        network_close(srv);</span><br><span class=\"line\">        connections_free(srv);</span><br><span class=\"line\">        plugins_free(srv);</span><br><span class=\"line\">        server_free(srv);</span><br><span class=\"line\">        return 0;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>mater 进程的主要工作逻辑还是非常清晰的，简单的说就是早fork完子进程后，阻塞，监控子进程</p>\n<p>worker进程的主要逻辑：</p>\n<ul>\n<li>初始化event模型(select/poll/pselect…)</li>\n<li>设置监听listen的套接字，注册listen套接字读写的回调函数</li>\n<li>while(1)大循环：<ol>\n<li>判断server服务是否终止,如果服务终止，删除pid文件、写日志、日志关闭、网络关闭等清理工作，然后程序退出;</li>\n<li>如果server服务未终止，先判断是否存在SIGHUP信号，调用各个插件的handle_sighup函数;</li>\n<li>判断是否产生了SIGALARM信号，若是执行各个插件的handle_trigger函数，再判断各个连接的超时;( 程序在处理连接超时的时候是每一秒中轮询所有的连接，判断其是否超时，这个效率其实低了);</li>\n<li>判断连接是否失效以及服务是否过载;</li>\n<li>启动事件轮询，等待各种IO事件的发生，包括文件读写，socket请求等，一旦有事件发生，调用相应的处理函数进行处理，这也是整个server逻辑最复杂的地方，根据连接状态机处理socket读写事件,lighttpd的事件模型以及插件模型需要再另外写篇文章来分析</li>\n</ol>\n</li>\n</ul>\n<p>主要逻辑代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//启动事件轮询,等待各种IO时间的发生,包括文件读写，socket请求等</span><br><span class=\"line\">if ((n = fdevent_poll(srv-&gt;ev, 1000)) &gt; 0) &#123;</span><br><span class=\"line\">    /* n is the number of events */</span><br><span class=\"line\">    int revents;</span><br><span class=\"line\">    int fd_ndx;</span><br><span class=\"line\">    last_active_ts = srv-&gt;cur_ts;</span><br><span class=\"line\">    fd_ndx = -1;</span><br><span class=\"line\">    do &#123;</span><br><span class=\"line\">        fdevent_handler handler;</span><br><span class=\"line\">        void *context;</span><br><span class=\"line\"></span><br><span class=\"line\">        fd_ndx  = fdevent_event_next_fdndx (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        if (-1 == fd_ndx) break; /* not all fdevent handlers know how many fds got an event */</span><br><span class=\"line\"></span><br><span class=\"line\">        revents = fdevent_event_get_revent (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        fd      = fdevent_event_get_fd     (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        handler = fdevent_get_handler(srv-&gt;ev, fd);</span><br><span class=\"line\">        context = fdevent_get_context(srv-&gt;ev, fd);</span><br><span class=\"line\"></span><br><span class=\"line\">        //一旦有事件发生，调用相应的处理函数进行处理。</span><br><span class=\"line\">        if (NULL != handler) &#123;</span><br><span class=\"line\">            (*handler)(srv, context, revents);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; while (--n &gt; 0);</span><br><span class=\"line\">    fdevent_sched_run(srv, srv-&gt;ev);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>由于历史的原因，公司部门在多个组件中使用了lighttpd，具体为何当时技术选型的时候没选nginx而选择lighttpd就不得而知了。lighttpd的社区相对nginx差距还是很大的，明显nginx的社区更活跃。lighttpd网上的资料以及第三方模块相对比较少，学习的成本会相对高一点。不过lighttpd的源码相对nginx会少一点，毕竟lighttpd比较轻量级，功能上没nginx那么多。网上nginx/lighttpd/appache 三种web server的测试结果，lighttpd占用内存最小，请求响应时间中等，apache最差。</p>\n<h2 id=\"进程模型\"><a href=\"#进程模型\" class=\"headerlink\" title=\"进程模型\"></a>进程模型</h2><p>lighttpd采用master-worker进程模型，master进程主要负责加载配置、fork worker进程、管理worker进程，worker进程主要负责接收请求、处理请求、返回请求结果，worker进程个数可以在配置文件中配置，master进程会根据配置的个数，fork worker进程</p>","more":"<p>master进程的主要逻辑：</p>\n<ul>\n<li>根据命令行完成各种初始化工作</li>\n<li>daemonize</li>\n<li>fork </li>\n<li>wait子进程(当子进程退出时，再fork出一个子进程)</li>\n</ul>\n<p>主要逻辑代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_childs = srv-&gt;srvconf.max_worker;</span><br><span class=\"line\">if (num_childs &gt; 0) &#123;</span><br><span class=\"line\">    int child = 0;</span><br><span class=\"line\">    //master进程上下文</span><br><span class=\"line\">    while (!child &amp;&amp; !srv_shutdown &amp;&amp; !graceful_shutdown) &#123;</span><br><span class=\"line\">        if (num_childs &gt; 0) &#123;</span><br><span class=\"line\">            switch (fork()) &#123;</span><br><span class=\"line\">            case -1:</span><br><span class=\"line\">                return -1;</span><br><span class=\"line\">            case 0:</span><br><span class=\"line\">                //worker进程在这里退出，执行后面的程序逻辑</span><br><span class=\"line\">                child = 1;</span><br><span class=\"line\">                break;</span><br><span class=\"line\">            default:</span><br><span class=\"line\">                num_childs--;</span><br><span class=\"line\">                break;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            int status;</span><br><span class=\"line\"></span><br><span class=\"line\">            // master进程在这里阻塞等待，回收worker进程</span><br><span class=\"line\">            if (-1 != wait(&amp;status)) &#123;</span><br><span class=\"line\">                //如果一个worker进程挂了，master进程会重新fork一个worker进程</span><br><span class=\"line\">                num_childs++;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                switch (errno) &#123;</span><br><span class=\"line\">                case EINTR:</span><br><span class=\"line\">                    if (handle_sig_hup) &#123;</span><br><span class=\"line\">                        handle_sig_hup = 0;</span><br><span class=\"line\"></span><br><span class=\"line\">                        log_error_cycle(srv);</span><br><span class=\"line\">                        if (!forwarded_sig_hup &amp;&amp; 0 != srv-&gt;srvconf.max_worker) &#123;</span><br><span class=\"line\">                            forwarded_sig_hup = 1;</span><br><span class=\"line\">                            kill(0, SIGHUP);</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    break;</span><br><span class=\"line\">                default:</span><br><span class=\"line\">                    break;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //master 进程在这里退出</span><br><span class=\"line\">    if (!child) &#123;</span><br><span class=\"line\">        /** </span><br><span class=\"line\">            * kill all children too </span><br><span class=\"line\">            */</span><br><span class=\"line\">        if (graceful_shutdown) &#123;</span><br><span class=\"line\">            kill(0, SIGINT);</span><br><span class=\"line\">        &#125; else if (srv_shutdown) &#123;</span><br><span class=\"line\">            kill(0, SIGTERM);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        remove_pid_file(srv, &amp;pid_fd);</span><br><span class=\"line\">        log_error_close(srv);</span><br><span class=\"line\">        network_close(srv);</span><br><span class=\"line\">        connections_free(srv);</span><br><span class=\"line\">        plugins_free(srv);</span><br><span class=\"line\">        server_free(srv);</span><br><span class=\"line\">        return 0;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>mater 进程的主要工作逻辑还是非常清晰的，简单的说就是早fork完子进程后，阻塞，监控子进程</p>\n<p>worker进程的主要逻辑：</p>\n<ul>\n<li>初始化event模型(select/poll/pselect…)</li>\n<li>设置监听listen的套接字，注册listen套接字读写的回调函数</li>\n<li>while(1)大循环：<ol>\n<li>判断server服务是否终止,如果服务终止，删除pid文件、写日志、日志关闭、网络关闭等清理工作，然后程序退出;</li>\n<li>如果server服务未终止，先判断是否存在SIGHUP信号，调用各个插件的handle_sighup函数;</li>\n<li>判断是否产生了SIGALARM信号，若是执行各个插件的handle_trigger函数，再判断各个连接的超时;( 程序在处理连接超时的时候是每一秒中轮询所有的连接，判断其是否超时，这个效率其实低了);</li>\n<li>判断连接是否失效以及服务是否过载;</li>\n<li>启动事件轮询，等待各种IO事件的发生，包括文件读写，socket请求等，一旦有事件发生，调用相应的处理函数进行处理，这也是整个server逻辑最复杂的地方，根据连接状态机处理socket读写事件,lighttpd的事件模型以及插件模型需要再另外写篇文章来分析</li>\n</ol>\n</li>\n</ul>\n<p>主要逻辑代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//启动事件轮询,等待各种IO时间的发生,包括文件读写，socket请求等</span><br><span class=\"line\">if ((n = fdevent_poll(srv-&gt;ev, 1000)) &gt; 0) &#123;</span><br><span class=\"line\">    /* n is the number of events */</span><br><span class=\"line\">    int revents;</span><br><span class=\"line\">    int fd_ndx;</span><br><span class=\"line\">    last_active_ts = srv-&gt;cur_ts;</span><br><span class=\"line\">    fd_ndx = -1;</span><br><span class=\"line\">    do &#123;</span><br><span class=\"line\">        fdevent_handler handler;</span><br><span class=\"line\">        void *context;</span><br><span class=\"line\"></span><br><span class=\"line\">        fd_ndx  = fdevent_event_next_fdndx (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        if (-1 == fd_ndx) break; /* not all fdevent handlers know how many fds got an event */</span><br><span class=\"line\"></span><br><span class=\"line\">        revents = fdevent_event_get_revent (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        fd      = fdevent_event_get_fd     (srv-&gt;ev, fd_ndx);</span><br><span class=\"line\">        handler = fdevent_get_handler(srv-&gt;ev, fd);</span><br><span class=\"line\">        context = fdevent_get_context(srv-&gt;ev, fd);</span><br><span class=\"line\"></span><br><span class=\"line\">        //一旦有事件发生，调用相应的处理函数进行处理。</span><br><span class=\"line\">        if (NULL != handler) &#123;</span><br><span class=\"line\">            (*handler)(srv, context, revents);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; while (--n &gt; 0);</span><br><span class=\"line\">    fdevent_sched_run(srv, srv-&gt;ev);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"linux 守护进程","date":"2016-01-15T04:11:22.000Z","_content":"\n# 守护进程\nlinux 服务端程序很多都是以守护进程的方式对外提供服务, linux 系统本身也有很多守护进程,例如kthreadd用来创建内核进程, kswapd是内存换页守护进程,flush是dump内存中的脏页面到磁盘,jbd提供ext4文件系统的日志日志功能...守护进程命名大部分都是以d结尾. 大部分守护进程都是以root方式运行,没有控制终端,运行在后台. 大部分守护进程都是进程组的组长进程以及会话的首进程,而且是进程组和会话中的唯一进程. 守护进程的父进程一般是系统1号进程,例如initd或者systemd.\n\n# 编程规则\n为了让守护进程在后台运行,减少不必要的交互,守护进程的编写有一套编程规则:\n- umask将文件模式创建屏蔽字设置一个已知值,通常是0. 由于继承得来的文件模式屏蔽字可能会被设置为拒绝某些权限.\n- fork() 然后父进程exit\n    - 如果守护进程是以shell命令启动, 父进程exit会让shell认为这条命令已经执行完毕\n    - 虽然子进程继承了父进程的进程组ID, 但获得了一个新的进程ID,这保证了子进程不是一个进程组的组长进程  (setid调用的条件)\n- 调用setid创建一个新会话, 如果调用setsid的进程不是一个进程组的组长，此函数创建一个新的会话期setid,setid会让子进程执行三个步骤\n    - 让子进程成为新会话的首进程\n    - 让子进程成为新进程组的组长进程\n    - 让子进程没有控制终端,如果在调用setsid前，该进程有控制终端，那么与该终端的联系被解除。 如果该进程是一个进程组的组长，此函数返回错误\n    - 为了保证这一点，我们先调用fork()然后exit()，此时只有子进程在运行\n- 再次fork, 这个步骤有些守护进程没有. 此时进程已经成为无终端的会话组长,但它可以重新申请打开一个控制终端,为了使进程不再成为会话组长来禁止进程重新打开控制终端, 再次fork然后父进程exit\n- 设置工作目录为根目录, 从父进程继承过来的当前工作目录可能在一个挂载的文件系统中\n- 关闭不再需要的文件描述符\n- 打开/dev/null, 让文件描述符0 1 2都指向/dev/null\n- 处理SIGCHLD信号, 处理SIGCHLD信号并不是必须的\n\n<!-- more -->\n\nAPUE这本书中一个守护进程编程范例,不过一些程序实现守护进程的方式会省略一些步骤,相比这个范例会简单一些\n\n```\nvoid daemonize(const char *cmd)\n{\n    int                 i,fd0,fd1,fd2;\n    pid_t               pid;\n    struct              rlimit rl;\n    struct sigaction    sa;\n\n    //设置文件模式屏蔽字\n    umask(0);\n\n    if(getrlimit(RLIMIT_NOFILE,&rl)<0)\n        err_quit(\"%s: can't get file limit \",cmd);\n\n    //first blood :fork 一次，使父进程退出，让子进程成为孤儿进程，让子进程成为新会话的手进程，\n \n    if ((pid=fork())<0)\n        err_quit(\"%s:can't fork \",cmd);\n    else if (pid!=0)\n        exit(0);\n    setsid();\n\n    sa.sa_handler=SIG_IGN;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags=0;\n\n    if(sigaction(SIGHUP,&sa,NULL)<0)\n        err_quit(\"%s: can't ignore SIGHUP \",cmd);\n    if((pid=fork())<0)\n        err_quit(\"%s: fork error \",cmd);\n    else if(pid!=0)\n        exit(0);\n\n    if(chdir(\"/\")<0)\n        err_quit(\"%s: can;t cahnge directory to /\",cmd);\n\n    if(rl.rlim_max==RLIM_INFINITY)\n        rl.rlim_max=1024;\n    for(i=0;i<rl.rlim_max;i++)\n        close(i);\n\n    fd0=open(\"/dev/null\",O_RDWR);\n    fd1=dup(0);\n    fd2=dup(0);\n\n    openlog(cmd,LOG_CONS,LOG_DAEMON);\n    if(fd0!=0||fd1!=1||fd2!=2)\n    {\n        syslog(LOG_ERR,\"unexpected file decription %d %d  %d\",fd0,fd1,fd2);\n        exit(1);\n    }\n}\n```\n\nGlibc库提供了创建守护进程API daemon, 函数原型为:\n``` \n    int daemon(int nochdir, int noclose);\n```\n","source":"_posts/linux-守护进程.md","raw":"---\ntitle: linux 守护进程\ndate: 2016-01-15 12:11:22\ntags: linux\n---\n\n# 守护进程\nlinux 服务端程序很多都是以守护进程的方式对外提供服务, linux 系统本身也有很多守护进程,例如kthreadd用来创建内核进程, kswapd是内存换页守护进程,flush是dump内存中的脏页面到磁盘,jbd提供ext4文件系统的日志日志功能...守护进程命名大部分都是以d结尾. 大部分守护进程都是以root方式运行,没有控制终端,运行在后台. 大部分守护进程都是进程组的组长进程以及会话的首进程,而且是进程组和会话中的唯一进程. 守护进程的父进程一般是系统1号进程,例如initd或者systemd.\n\n# 编程规则\n为了让守护进程在后台运行,减少不必要的交互,守护进程的编写有一套编程规则:\n- umask将文件模式创建屏蔽字设置一个已知值,通常是0. 由于继承得来的文件模式屏蔽字可能会被设置为拒绝某些权限.\n- fork() 然后父进程exit\n    - 如果守护进程是以shell命令启动, 父进程exit会让shell认为这条命令已经执行完毕\n    - 虽然子进程继承了父进程的进程组ID, 但获得了一个新的进程ID,这保证了子进程不是一个进程组的组长进程  (setid调用的条件)\n- 调用setid创建一个新会话, 如果调用setsid的进程不是一个进程组的组长，此函数创建一个新的会话期setid,setid会让子进程执行三个步骤\n    - 让子进程成为新会话的首进程\n    - 让子进程成为新进程组的组长进程\n    - 让子进程没有控制终端,如果在调用setsid前，该进程有控制终端，那么与该终端的联系被解除。 如果该进程是一个进程组的组长，此函数返回错误\n    - 为了保证这一点，我们先调用fork()然后exit()，此时只有子进程在运行\n- 再次fork, 这个步骤有些守护进程没有. 此时进程已经成为无终端的会话组长,但它可以重新申请打开一个控制终端,为了使进程不再成为会话组长来禁止进程重新打开控制终端, 再次fork然后父进程exit\n- 设置工作目录为根目录, 从父进程继承过来的当前工作目录可能在一个挂载的文件系统中\n- 关闭不再需要的文件描述符\n- 打开/dev/null, 让文件描述符0 1 2都指向/dev/null\n- 处理SIGCHLD信号, 处理SIGCHLD信号并不是必须的\n\n<!-- more -->\n\nAPUE这本书中一个守护进程编程范例,不过一些程序实现守护进程的方式会省略一些步骤,相比这个范例会简单一些\n\n```\nvoid daemonize(const char *cmd)\n{\n    int                 i,fd0,fd1,fd2;\n    pid_t               pid;\n    struct              rlimit rl;\n    struct sigaction    sa;\n\n    //设置文件模式屏蔽字\n    umask(0);\n\n    if(getrlimit(RLIMIT_NOFILE,&rl)<0)\n        err_quit(\"%s: can't get file limit \",cmd);\n\n    //first blood :fork 一次，使父进程退出，让子进程成为孤儿进程，让子进程成为新会话的手进程，\n \n    if ((pid=fork())<0)\n        err_quit(\"%s:can't fork \",cmd);\n    else if (pid!=0)\n        exit(0);\n    setsid();\n\n    sa.sa_handler=SIG_IGN;\n    sigemptyset(&sa.sa_mask);\n    sa.sa_flags=0;\n\n    if(sigaction(SIGHUP,&sa,NULL)<0)\n        err_quit(\"%s: can't ignore SIGHUP \",cmd);\n    if((pid=fork())<0)\n        err_quit(\"%s: fork error \",cmd);\n    else if(pid!=0)\n        exit(0);\n\n    if(chdir(\"/\")<0)\n        err_quit(\"%s: can;t cahnge directory to /\",cmd);\n\n    if(rl.rlim_max==RLIM_INFINITY)\n        rl.rlim_max=1024;\n    for(i=0;i<rl.rlim_max;i++)\n        close(i);\n\n    fd0=open(\"/dev/null\",O_RDWR);\n    fd1=dup(0);\n    fd2=dup(0);\n\n    openlog(cmd,LOG_CONS,LOG_DAEMON);\n    if(fd0!=0||fd1!=1||fd2!=2)\n    {\n        syslog(LOG_ERR,\"unexpected file decription %d %d  %d\",fd0,fd1,fd2);\n        exit(1);\n    }\n}\n```\n\nGlibc库提供了创建守护进程API daemon, 函数原型为:\n``` \n    int daemon(int nochdir, int noclose);\n```\n","slug":"linux-守护进程","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6duble0007amumh6i3miaz","content":"<h1 id=\"守护进程\"><a href=\"#守护进程\" class=\"headerlink\" title=\"守护进程\"></a>守护进程</h1><p>linux 服务端程序很多都是以守护进程的方式对外提供服务, linux 系统本身也有很多守护进程,例如kthreadd用来创建内核进程, kswapd是内存换页守护进程,flush是dump内存中的脏页面到磁盘,jbd提供ext4文件系统的日志日志功能…守护进程命名大部分都是以d结尾. 大部分守护进程都是以root方式运行,没有控制终端,运行在后台. 大部分守护进程都是进程组的组长进程以及会话的首进程,而且是进程组和会话中的唯一进程. 守护进程的父进程一般是系统1号进程,例如initd或者systemd.</p>\n<h1 id=\"编程规则\"><a href=\"#编程规则\" class=\"headerlink\" title=\"编程规则\"></a>编程规则</h1><p>为了让守护进程在后台运行,减少不必要的交互,守护进程的编写有一套编程规则:</p>\n<ul>\n<li>umask将文件模式创建屏蔽字设置一个已知值,通常是0. 由于继承得来的文件模式屏蔽字可能会被设置为拒绝某些权限.</li>\n<li>fork() 然后父进程exit<ul>\n<li>如果守护进程是以shell命令启动, 父进程exit会让shell认为这条命令已经执行完毕</li>\n<li>虽然子进程继承了父进程的进程组ID, 但获得了一个新的进程ID,这保证了子进程不是一个进程组的组长进程  (setid调用的条件)</li>\n</ul>\n</li>\n<li>调用setid创建一个新会话, 如果调用setsid的进程不是一个进程组的组长，此函数创建一个新的会话期setid,setid会让子进程执行三个步骤<ul>\n<li>让子进程成为新会话的首进程</li>\n<li>让子进程成为新进程组的组长进程</li>\n<li>让子进程没有控制终端,如果在调用setsid前，该进程有控制终端，那么与该终端的联系被解除。 如果该进程是一个进程组的组长，此函数返回错误</li>\n<li>为了保证这一点，我们先调用fork()然后exit()，此时只有子进程在运行</li>\n</ul>\n</li>\n<li>再次fork, 这个步骤有些守护进程没有. 此时进程已经成为无终端的会话组长,但它可以重新申请打开一个控制终端,为了使进程不再成为会话组长来禁止进程重新打开控制终端, 再次fork然后父进程exit</li>\n<li>设置工作目录为根目录, 从父进程继承过来的当前工作目录可能在一个挂载的文件系统中</li>\n<li>关闭不再需要的文件描述符</li>\n<li>打开/dev/null, 让文件描述符0 1 2都指向/dev/null</li>\n<li>处理SIGCHLD信号, 处理SIGCHLD信号并不是必须的</li>\n</ul>\n<a id=\"more\"></a>\n<p>APUE这本书中一个守护进程编程范例,不过一些程序实现守护进程的方式会省略一些步骤,相比这个范例会简单一些</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void daemonize(const char *cmd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    int                 i,fd0,fd1,fd2;</span><br><span class=\"line\">    pid_t               pid;</span><br><span class=\"line\">    struct              rlimit rl;</span><br><span class=\"line\">    struct sigaction    sa;</span><br><span class=\"line\"></span><br><span class=\"line\">    //设置文件模式屏蔽字</span><br><span class=\"line\">    umask(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(getrlimit(RLIMIT_NOFILE,&amp;rl)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can&apos;t get file limit &quot;,cmd);</span><br><span class=\"line\"></span><br><span class=\"line\">    //first blood :fork 一次，使父进程退出，让子进程成为孤儿进程，让子进程成为新会话的手进程，</span><br><span class=\"line\"> </span><br><span class=\"line\">    if ((pid=fork())&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s:can&apos;t fork &quot;,cmd);</span><br><span class=\"line\">    else if (pid!=0)</span><br><span class=\"line\">        exit(0);</span><br><span class=\"line\">    setsid();</span><br><span class=\"line\"></span><br><span class=\"line\">    sa.sa_handler=SIG_IGN;</span><br><span class=\"line\">    sigemptyset(&amp;sa.sa_mask);</span><br><span class=\"line\">    sa.sa_flags=0;</span><br><span class=\"line\"></span><br><span class=\"line\">    if(sigaction(SIGHUP,&amp;sa,NULL)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can&apos;t ignore SIGHUP &quot;,cmd);</span><br><span class=\"line\">    if((pid=fork())&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: fork error &quot;,cmd);</span><br><span class=\"line\">    else if(pid!=0)</span><br><span class=\"line\">        exit(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(chdir(&quot;/&quot;)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can;t cahnge directory to /&quot;,cmd);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(rl.rlim_max==RLIM_INFINITY)</span><br><span class=\"line\">        rl.rlim_max=1024;</span><br><span class=\"line\">    for(i=0;i&lt;rl.rlim_max;i++)</span><br><span class=\"line\">        close(i);</span><br><span class=\"line\"></span><br><span class=\"line\">    fd0=open(&quot;/dev/null&quot;,O_RDWR);</span><br><span class=\"line\">    fd1=dup(0);</span><br><span class=\"line\">    fd2=dup(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    openlog(cmd,LOG_CONS,LOG_DAEMON);</span><br><span class=\"line\">    if(fd0!=0||fd1!=1||fd2!=2)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        syslog(LOG_ERR,&quot;unexpected file decription %d %d  %d&quot;,fd0,fd1,fd2);</span><br><span class=\"line\">        exit(1);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Glibc库提供了创建守护进程API daemon, 函数原型为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int daemon(int nochdir, int noclose);</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"守护进程\"><a href=\"#守护进程\" class=\"headerlink\" title=\"守护进程\"></a>守护进程</h1><p>linux 服务端程序很多都是以守护进程的方式对外提供服务, linux 系统本身也有很多守护进程,例如kthreadd用来创建内核进程, kswapd是内存换页守护进程,flush是dump内存中的脏页面到磁盘,jbd提供ext4文件系统的日志日志功能…守护进程命名大部分都是以d结尾. 大部分守护进程都是以root方式运行,没有控制终端,运行在后台. 大部分守护进程都是进程组的组长进程以及会话的首进程,而且是进程组和会话中的唯一进程. 守护进程的父进程一般是系统1号进程,例如initd或者systemd.</p>\n<h1 id=\"编程规则\"><a href=\"#编程规则\" class=\"headerlink\" title=\"编程规则\"></a>编程规则</h1><p>为了让守护进程在后台运行,减少不必要的交互,守护进程的编写有一套编程规则:</p>\n<ul>\n<li>umask将文件模式创建屏蔽字设置一个已知值,通常是0. 由于继承得来的文件模式屏蔽字可能会被设置为拒绝某些权限.</li>\n<li>fork() 然后父进程exit<ul>\n<li>如果守护进程是以shell命令启动, 父进程exit会让shell认为这条命令已经执行完毕</li>\n<li>虽然子进程继承了父进程的进程组ID, 但获得了一个新的进程ID,这保证了子进程不是一个进程组的组长进程  (setid调用的条件)</li>\n</ul>\n</li>\n<li>调用setid创建一个新会话, 如果调用setsid的进程不是一个进程组的组长，此函数创建一个新的会话期setid,setid会让子进程执行三个步骤<ul>\n<li>让子进程成为新会话的首进程</li>\n<li>让子进程成为新进程组的组长进程</li>\n<li>让子进程没有控制终端,如果在调用setsid前，该进程有控制终端，那么与该终端的联系被解除。 如果该进程是一个进程组的组长，此函数返回错误</li>\n<li>为了保证这一点，我们先调用fork()然后exit()，此时只有子进程在运行</li>\n</ul>\n</li>\n<li>再次fork, 这个步骤有些守护进程没有. 此时进程已经成为无终端的会话组长,但它可以重新申请打开一个控制终端,为了使进程不再成为会话组长来禁止进程重新打开控制终端, 再次fork然后父进程exit</li>\n<li>设置工作目录为根目录, 从父进程继承过来的当前工作目录可能在一个挂载的文件系统中</li>\n<li>关闭不再需要的文件描述符</li>\n<li>打开/dev/null, 让文件描述符0 1 2都指向/dev/null</li>\n<li>处理SIGCHLD信号, 处理SIGCHLD信号并不是必须的</li>\n</ul>","more":"<p>APUE这本书中一个守护进程编程范例,不过一些程序实现守护进程的方式会省略一些步骤,相比这个范例会简单一些</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void daemonize(const char *cmd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    int                 i,fd0,fd1,fd2;</span><br><span class=\"line\">    pid_t               pid;</span><br><span class=\"line\">    struct              rlimit rl;</span><br><span class=\"line\">    struct sigaction    sa;</span><br><span class=\"line\"></span><br><span class=\"line\">    //设置文件模式屏蔽字</span><br><span class=\"line\">    umask(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(getrlimit(RLIMIT_NOFILE,&amp;rl)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can&apos;t get file limit &quot;,cmd);</span><br><span class=\"line\"></span><br><span class=\"line\">    //first blood :fork 一次，使父进程退出，让子进程成为孤儿进程，让子进程成为新会话的手进程，</span><br><span class=\"line\"> </span><br><span class=\"line\">    if ((pid=fork())&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s:can&apos;t fork &quot;,cmd);</span><br><span class=\"line\">    else if (pid!=0)</span><br><span class=\"line\">        exit(0);</span><br><span class=\"line\">    setsid();</span><br><span class=\"line\"></span><br><span class=\"line\">    sa.sa_handler=SIG_IGN;</span><br><span class=\"line\">    sigemptyset(&amp;sa.sa_mask);</span><br><span class=\"line\">    sa.sa_flags=0;</span><br><span class=\"line\"></span><br><span class=\"line\">    if(sigaction(SIGHUP,&amp;sa,NULL)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can&apos;t ignore SIGHUP &quot;,cmd);</span><br><span class=\"line\">    if((pid=fork())&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: fork error &quot;,cmd);</span><br><span class=\"line\">    else if(pid!=0)</span><br><span class=\"line\">        exit(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(chdir(&quot;/&quot;)&lt;0)</span><br><span class=\"line\">        err_quit(&quot;%s: can;t cahnge directory to /&quot;,cmd);</span><br><span class=\"line\"></span><br><span class=\"line\">    if(rl.rlim_max==RLIM_INFINITY)</span><br><span class=\"line\">        rl.rlim_max=1024;</span><br><span class=\"line\">    for(i=0;i&lt;rl.rlim_max;i++)</span><br><span class=\"line\">        close(i);</span><br><span class=\"line\"></span><br><span class=\"line\">    fd0=open(&quot;/dev/null&quot;,O_RDWR);</span><br><span class=\"line\">    fd1=dup(0);</span><br><span class=\"line\">    fd2=dup(0);</span><br><span class=\"line\"></span><br><span class=\"line\">    openlog(cmd,LOG_CONS,LOG_DAEMON);</span><br><span class=\"line\">    if(fd0!=0||fd1!=1||fd2!=2)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        syslog(LOG_ERR,&quot;unexpected file decription %d %d  %d&quot;,fd0,fd1,fd2);</span><br><span class=\"line\">        exit(1);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Glibc库提供了创建守护进程API daemon, 函数原型为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int daemon(int nochdir, int noclose);</span><br></pre></td></tr></table></figure></p>"},{"title":"linux 最大文件描述符","date":"2016-01-12T12:41:42.000Z","_content":"\n\n# linux最大文件描述符数\n\n### 前言\n\n> 关于Linux下系统,进程能最大能打开的文件描述符数看过好多文章,但大都没有完整,详细说明每个值表示什么意思,在实践中该怎么设置.今天刚好有时间就通过Google来整理了如下的内容.如有错误请指出,谢谢.\n\n### 系统级别\n\nLinux系统级别限制所有用户进程能打开的文件描述符总数可以通过如下的命令查看\n\n```\n$ cat /proc/sys/fs/file-max \n2259544\n```\n\n有2中方法修改系统级别的限制：\n\n1. 通过命令动态修改(重启后失效)\n\n   sysctl -w fs.file-max=102400\n\n2.通过配置文件修改\n\n```\nvi /etc/sysctl.conf\n在文件末尾添加\nfs.file-max=102400\n保存退出后使用sysctl -p 命令使其生效\n```\n\n和`fs.file-max`有关的一个参数是`file-nr`, 该参数是只读的\n\n```\n$ cat /proc/sys/fs/file-nr \n3296    0       2259544    \n```\n\n`file-nr`的值由3部分组成：1，已经分配的文件描述符数；2，已经分配但未使用的文件描述符数；\n3，内核最大能分配的文件描述符数\n\n**注意：** 只要你的内存足够大，file-max的值可以非常大。\n\n### 用户级别\n\n用户级别的限制是通过可以通过命令`ulimit`命令和文件`/etc/security/limits.conf`\n\n```\n$ ulimit  -n\n655350\n//查看硬件资源限制\n$ ulimit  -Hn\n655350\n//软件资源限制\n$ ulimit  -Sn\n655350\n//设置软/硬件资源限制\nulimit  -Sn 655350 或 ulimit  -Hn 655350\n```\n\n查看limits.conf文件\n\n```\ncat /etc/security/limits.conf\n//输出\n* hard nofile 655350\n* soft nofile 655350    \n```\n\nlimits.conf 文件的格式是：\n\n```\n<domain> <type> <item> <value>\n```\n\n每个域的取值可以[参考](https://linux.die.net/man/5/limits.conf)\n\n如果domain的值是一个用户名，则可以限制该用户下的所有进程能打开的文件描述符总数，如果是`*`则表示针对每个用户都起作用\n\n**注意** 针对同一个item取值， soft的值不能大于hard\n\n### nr_open\n\n> This denotes the maximum number of file-handles a process can\n> allocate. Default value is 1024*1024 (1048576) which should be\n> enough for most machines. Actual limit depends on RLIMIT_NOFILE\n> resource limit.\n\n就是说nr_open表示一个进程做多能分配的文件句柄数，默认值是1048576。针对大多数的情况该值是足够的。\n\n### NR_FILE\n\n> NR_FILE is the limit on total number of files in the system at any given point in time\n\nNR_FILE 是系统在某一给定时刻，限制的文件总数\n\n> While initializing the kernel we setup the vfs cache with start_kernel\n> vfs_caches_init(num_physpages);\n> files_init(mempages);\n> fs/file_table.c says\n> /* One file with associated inode and dcache is very roughly 1K.\n>\n> - Per default don’t use more than 10% of our memory for files.\n>   n = (mempages * (PAGE_SIZE / 1024)) / 10;\n>   this n can never be greater than NR_FILE\n\n### ulimit 命令\n\n1.只对当前tty（终端有效），若要每次都生效的话，可以把ulimit参数放到对应用户的.bash_profile里面；\n2.ulimit命令本身就有分软硬设置，加-H就是硬，加-S就是软；\n3.默认显示的是软限制，如果运行ulimit命令修改的时候没有加上的话，就是两个参数一起改变.生效；\n\n命令参数\n-H 设置硬件资源限制.\n-S 设置软件资源限制.\n-a 显示当前所有的资源限制.\n-c size:设置core文件的最大值.单位:blocks\n-d size:设置数据段的最大值.单位:kbytes\n-f size:设置创建文件的最大值.单位:blocks\n-l size:设置在内存中锁定进程的最大值.单位:kbytes\n-m size:设置可以使用的常驻内存的最大值.单位:kbytes\n-n size:设置内核可以同时打开的文件描述符的最大值.单位:n\n-p size:设置管道缓冲区的最大值.单位:kbytes\n-s size:设置堆栈的最大值.单位:kbytes\n-t size:设置CPU使用时间的最大上限.单位:seconds\n-v size:设置虚拟内存的最大值.单位:kbytes\nunlimited 是一个特殊值，用于表示不限制\n\n### 总结 file-max, nr_open, nofile之间的关系\n\n1. 针对用户打开最大文件数的限制，可以通过修改文件`limits.conf`来实现\n2. nofile中soft的值小于hard, 最大值由nr_open来决定\n3. file-max表示内核针对整个系统，限制能所有进程能打开的文件描述符数\n4. nofile < nr_open < file-max","source":"_posts/linux中最大文件描述符数.md","raw":"---\ntitle: linux 最大文件描述符\ndate: 2016-01-12 20:41:42\ntags: linux\n---\n\n\n# linux最大文件描述符数\n\n### 前言\n\n> 关于Linux下系统,进程能最大能打开的文件描述符数看过好多文章,但大都没有完整,详细说明每个值表示什么意思,在实践中该怎么设置.今天刚好有时间就通过Google来整理了如下的内容.如有错误请指出,谢谢.\n\n### 系统级别\n\nLinux系统级别限制所有用户进程能打开的文件描述符总数可以通过如下的命令查看\n\n```\n$ cat /proc/sys/fs/file-max \n2259544\n```\n\n有2中方法修改系统级别的限制：\n\n1. 通过命令动态修改(重启后失效)\n\n   sysctl -w fs.file-max=102400\n\n2.通过配置文件修改\n\n```\nvi /etc/sysctl.conf\n在文件末尾添加\nfs.file-max=102400\n保存退出后使用sysctl -p 命令使其生效\n```\n\n和`fs.file-max`有关的一个参数是`file-nr`, 该参数是只读的\n\n```\n$ cat /proc/sys/fs/file-nr \n3296    0       2259544    \n```\n\n`file-nr`的值由3部分组成：1，已经分配的文件描述符数；2，已经分配但未使用的文件描述符数；\n3，内核最大能分配的文件描述符数\n\n**注意：** 只要你的内存足够大，file-max的值可以非常大。\n\n### 用户级别\n\n用户级别的限制是通过可以通过命令`ulimit`命令和文件`/etc/security/limits.conf`\n\n```\n$ ulimit  -n\n655350\n//查看硬件资源限制\n$ ulimit  -Hn\n655350\n//软件资源限制\n$ ulimit  -Sn\n655350\n//设置软/硬件资源限制\nulimit  -Sn 655350 或 ulimit  -Hn 655350\n```\n\n查看limits.conf文件\n\n```\ncat /etc/security/limits.conf\n//输出\n* hard nofile 655350\n* soft nofile 655350    \n```\n\nlimits.conf 文件的格式是：\n\n```\n<domain> <type> <item> <value>\n```\n\n每个域的取值可以[参考](https://linux.die.net/man/5/limits.conf)\n\n如果domain的值是一个用户名，则可以限制该用户下的所有进程能打开的文件描述符总数，如果是`*`则表示针对每个用户都起作用\n\n**注意** 针对同一个item取值， soft的值不能大于hard\n\n### nr_open\n\n> This denotes the maximum number of file-handles a process can\n> allocate. Default value is 1024*1024 (1048576) which should be\n> enough for most machines. Actual limit depends on RLIMIT_NOFILE\n> resource limit.\n\n就是说nr_open表示一个进程做多能分配的文件句柄数，默认值是1048576。针对大多数的情况该值是足够的。\n\n### NR_FILE\n\n> NR_FILE is the limit on total number of files in the system at any given point in time\n\nNR_FILE 是系统在某一给定时刻，限制的文件总数\n\n> While initializing the kernel we setup the vfs cache with start_kernel\n> vfs_caches_init(num_physpages);\n> files_init(mempages);\n> fs/file_table.c says\n> /* One file with associated inode and dcache is very roughly 1K.\n>\n> - Per default don’t use more than 10% of our memory for files.\n>   n = (mempages * (PAGE_SIZE / 1024)) / 10;\n>   this n can never be greater than NR_FILE\n\n### ulimit 命令\n\n1.只对当前tty（终端有效），若要每次都生效的话，可以把ulimit参数放到对应用户的.bash_profile里面；\n2.ulimit命令本身就有分软硬设置，加-H就是硬，加-S就是软；\n3.默认显示的是软限制，如果运行ulimit命令修改的时候没有加上的话，就是两个参数一起改变.生效；\n\n命令参数\n-H 设置硬件资源限制.\n-S 设置软件资源限制.\n-a 显示当前所有的资源限制.\n-c size:设置core文件的最大值.单位:blocks\n-d size:设置数据段的最大值.单位:kbytes\n-f size:设置创建文件的最大值.单位:blocks\n-l size:设置在内存中锁定进程的最大值.单位:kbytes\n-m size:设置可以使用的常驻内存的最大值.单位:kbytes\n-n size:设置内核可以同时打开的文件描述符的最大值.单位:n\n-p size:设置管道缓冲区的最大值.单位:kbytes\n-s size:设置堆栈的最大值.单位:kbytes\n-t size:设置CPU使用时间的最大上限.单位:seconds\n-v size:设置虚拟内存的最大值.单位:kbytes\nunlimited 是一个特殊值，用于表示不限制\n\n### 总结 file-max, nr_open, nofile之间的关系\n\n1. 针对用户打开最大文件数的限制，可以通过修改文件`limits.conf`来实现\n2. nofile中soft的值小于hard, 最大值由nr_open来决定\n3. file-max表示内核针对整个系统，限制能所有进程能打开的文件描述符数\n4. nofile < nr_open < file-max","slug":"linux中最大文件描述符数","published":1,"updated":"2019-01-20T06:07:26.607Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dublg000aamumwsy1z052","content":"<h1 id=\"linux最大文件描述符数\"><a href=\"#linux最大文件描述符数\" class=\"headerlink\" title=\"linux最大文件描述符数\"></a>linux最大文件描述符数</h1><h3 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h3><blockquote>\n<p>关于Linux下系统,进程能最大能打开的文件描述符数看过好多文章,但大都没有完整,详细说明每个值表示什么意思,在实践中该怎么设置.今天刚好有时间就通过Google来整理了如下的内容.如有错误请指出,谢谢.</p>\n</blockquote>\n<h3 id=\"系统级别\"><a href=\"#系统级别\" class=\"headerlink\" title=\"系统级别\"></a>系统级别</h3><p>Linux系统级别限制所有用户进程能打开的文件描述符总数可以通过如下的命令查看</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /proc/sys/fs/file-max </span><br><span class=\"line\">2259544</span><br></pre></td></tr></table></figure>\n<p>有2中方法修改系统级别的限制：</p>\n<ol>\n<li><p>通过命令动态修改(重启后失效)</p>\n<p>sysctl -w fs.file-max=102400</p>\n</li>\n</ol>\n<p>2.通过配置文件修改</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vi /etc/sysctl.conf</span><br><span class=\"line\">在文件末尾添加</span><br><span class=\"line\">fs.file-max=102400</span><br><span class=\"line\">保存退出后使用sysctl -p 命令使其生效</span><br></pre></td></tr></table></figure>\n<p>和<code>fs.file-max</code>有关的一个参数是<code>file-nr</code>, 该参数是只读的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /proc/sys/fs/file-nr </span><br><span class=\"line\">3296    0       2259544</span><br></pre></td></tr></table></figure>\n<p><code>file-nr</code>的值由3部分组成：1，已经分配的文件描述符数；2，已经分配但未使用的文件描述符数；<br>3，内核最大能分配的文件描述符数</p>\n<p><strong>注意：</strong> 只要你的内存足够大，file-max的值可以非常大。</p>\n<h3 id=\"用户级别\"><a href=\"#用户级别\" class=\"headerlink\" title=\"用户级别\"></a>用户级别</h3><p>用户级别的限制是通过可以通过命令<code>ulimit</code>命令和文件<code>/etc/security/limits.conf</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ulimit  -n</span><br><span class=\"line\">655350</span><br><span class=\"line\">//查看硬件资源限制</span><br><span class=\"line\">$ ulimit  -Hn</span><br><span class=\"line\">655350</span><br><span class=\"line\">//软件资源限制</span><br><span class=\"line\">$ ulimit  -Sn</span><br><span class=\"line\">655350</span><br><span class=\"line\">//设置软/硬件资源限制</span><br><span class=\"line\">ulimit  -Sn 655350 或 ulimit  -Hn 655350</span><br></pre></td></tr></table></figure>\n<p>查看limits.conf文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /etc/security/limits.conf</span><br><span class=\"line\">//输出</span><br><span class=\"line\">* hard nofile 655350</span><br><span class=\"line\">* soft nofile 655350</span><br></pre></td></tr></table></figure>\n<p>limits.conf 文件的格式是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt;</span><br></pre></td></tr></table></figure>\n<p>每个域的取值可以<a href=\"https://linux.die.net/man/5/limits.conf\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<p>如果domain的值是一个用户名，则可以限制该用户下的所有进程能打开的文件描述符总数，如果是<code>*</code>则表示针对每个用户都起作用</p>\n<p><strong>注意</strong> 针对同一个item取值， soft的值不能大于hard</p>\n<h3 id=\"nr-open\"><a href=\"#nr-open\" class=\"headerlink\" title=\"nr_open\"></a>nr_open</h3><blockquote>\n<p>This denotes the maximum number of file-handles a process can<br>allocate. Default value is 1024*1024 (1048576) which should be<br>enough for most machines. Actual limit depends on RLIMIT_NOFILE<br>resource limit.</p>\n</blockquote>\n<p>就是说nr_open表示一个进程做多能分配的文件句柄数，默认值是1048576。针对大多数的情况该值是足够的。</p>\n<h3 id=\"NR-FILE\"><a href=\"#NR-FILE\" class=\"headerlink\" title=\"NR_FILE\"></a>NR_FILE</h3><blockquote>\n<p>NR_FILE is the limit on total number of files in the system at any given point in time</p>\n</blockquote>\n<p>NR_FILE 是系统在某一给定时刻，限制的文件总数</p>\n<blockquote>\n<p>While initializing the kernel we setup the vfs cache with start_kernel<br>vfs_caches_init(num_physpages);<br>files_init(mempages);<br>fs/file_table.c says<br>/* One file with associated inode and dcache is very roughly 1K.</p>\n<ul>\n<li>Per default don’t use more than 10% of our memory for files.<br>n = (mempages * (PAGE_SIZE / 1024)) / 10;<br>this n can never be greater than NR_FILE</li>\n</ul>\n</blockquote>\n<h3 id=\"ulimit-命令\"><a href=\"#ulimit-命令\" class=\"headerlink\" title=\"ulimit 命令\"></a>ulimit 命令</h3><p>1.只对当前tty（终端有效），若要每次都生效的话，可以把ulimit参数放到对应用户的.bash_profile里面；<br>2.ulimit命令本身就有分软硬设置，加-H就是硬，加-S就是软；<br>3.默认显示的是软限制，如果运行ulimit命令修改的时候没有加上的话，就是两个参数一起改变.生效；</p>\n<p>命令参数<br>-H 设置硬件资源限制.<br>-S 设置软件资源限制.<br>-a 显示当前所有的资源限制.<br>-c size:设置core文件的最大值.单位:blocks<br>-d size:设置数据段的最大值.单位:kbytes<br>-f size:设置创建文件的最大值.单位:blocks<br>-l size:设置在内存中锁定进程的最大值.单位:kbytes<br>-m size:设置可以使用的常驻内存的最大值.单位:kbytes<br>-n size:设置内核可以同时打开的文件描述符的最大值.单位:n<br>-p size:设置管道缓冲区的最大值.单位:kbytes<br>-s size:设置堆栈的最大值.单位:kbytes<br>-t size:设置CPU使用时间的最大上限.单位:seconds<br>-v size:设置虚拟内存的最大值.单位:kbytes<br>unlimited 是一个特殊值，用于表示不限制</p>\n<h3 id=\"总结-file-max-nr-open-nofile之间的关系\"><a href=\"#总结-file-max-nr-open-nofile之间的关系\" class=\"headerlink\" title=\"总结 file-max, nr_open, nofile之间的关系\"></a>总结 file-max, nr_open, nofile之间的关系</h3><ol>\n<li>针对用户打开最大文件数的限制，可以通过修改文件<code>limits.conf</code>来实现</li>\n<li>nofile中soft的值小于hard, 最大值由nr_open来决定</li>\n<li>file-max表示内核针对整个系统，限制能所有进程能打开的文件描述符数</li>\n<li>nofile &lt; nr_open &lt; file-max</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"linux最大文件描述符数\"><a href=\"#linux最大文件描述符数\" class=\"headerlink\" title=\"linux最大文件描述符数\"></a>linux最大文件描述符数</h1><h3 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h3><blockquote>\n<p>关于Linux下系统,进程能最大能打开的文件描述符数看过好多文章,但大都没有完整,详细说明每个值表示什么意思,在实践中该怎么设置.今天刚好有时间就通过Google来整理了如下的内容.如有错误请指出,谢谢.</p>\n</blockquote>\n<h3 id=\"系统级别\"><a href=\"#系统级别\" class=\"headerlink\" title=\"系统级别\"></a>系统级别</h3><p>Linux系统级别限制所有用户进程能打开的文件描述符总数可以通过如下的命令查看</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /proc/sys/fs/file-max </span><br><span class=\"line\">2259544</span><br></pre></td></tr></table></figure>\n<p>有2中方法修改系统级别的限制：</p>\n<ol>\n<li><p>通过命令动态修改(重启后失效)</p>\n<p>sysctl -w fs.file-max=102400</p>\n</li>\n</ol>\n<p>2.通过配置文件修改</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vi /etc/sysctl.conf</span><br><span class=\"line\">在文件末尾添加</span><br><span class=\"line\">fs.file-max=102400</span><br><span class=\"line\">保存退出后使用sysctl -p 命令使其生效</span><br></pre></td></tr></table></figure>\n<p>和<code>fs.file-max</code>有关的一个参数是<code>file-nr</code>, 该参数是只读的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat /proc/sys/fs/file-nr </span><br><span class=\"line\">3296    0       2259544</span><br></pre></td></tr></table></figure>\n<p><code>file-nr</code>的值由3部分组成：1，已经分配的文件描述符数；2，已经分配但未使用的文件描述符数；<br>3，内核最大能分配的文件描述符数</p>\n<p><strong>注意：</strong> 只要你的内存足够大，file-max的值可以非常大。</p>\n<h3 id=\"用户级别\"><a href=\"#用户级别\" class=\"headerlink\" title=\"用户级别\"></a>用户级别</h3><p>用户级别的限制是通过可以通过命令<code>ulimit</code>命令和文件<code>/etc/security/limits.conf</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ulimit  -n</span><br><span class=\"line\">655350</span><br><span class=\"line\">//查看硬件资源限制</span><br><span class=\"line\">$ ulimit  -Hn</span><br><span class=\"line\">655350</span><br><span class=\"line\">//软件资源限制</span><br><span class=\"line\">$ ulimit  -Sn</span><br><span class=\"line\">655350</span><br><span class=\"line\">//设置软/硬件资源限制</span><br><span class=\"line\">ulimit  -Sn 655350 或 ulimit  -Hn 655350</span><br></pre></td></tr></table></figure>\n<p>查看limits.conf文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /etc/security/limits.conf</span><br><span class=\"line\">//输出</span><br><span class=\"line\">* hard nofile 655350</span><br><span class=\"line\">* soft nofile 655350</span><br></pre></td></tr></table></figure>\n<p>limits.conf 文件的格式是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt;</span><br></pre></td></tr></table></figure>\n<p>每个域的取值可以<a href=\"https://linux.die.net/man/5/limits.conf\" target=\"_blank\" rel=\"noopener\">参考</a></p>\n<p>如果domain的值是一个用户名，则可以限制该用户下的所有进程能打开的文件描述符总数，如果是<code>*</code>则表示针对每个用户都起作用</p>\n<p><strong>注意</strong> 针对同一个item取值， soft的值不能大于hard</p>\n<h3 id=\"nr-open\"><a href=\"#nr-open\" class=\"headerlink\" title=\"nr_open\"></a>nr_open</h3><blockquote>\n<p>This denotes the maximum number of file-handles a process can<br>allocate. Default value is 1024*1024 (1048576) which should be<br>enough for most machines. Actual limit depends on RLIMIT_NOFILE<br>resource limit.</p>\n</blockquote>\n<p>就是说nr_open表示一个进程做多能分配的文件句柄数，默认值是1048576。针对大多数的情况该值是足够的。</p>\n<h3 id=\"NR-FILE\"><a href=\"#NR-FILE\" class=\"headerlink\" title=\"NR_FILE\"></a>NR_FILE</h3><blockquote>\n<p>NR_FILE is the limit on total number of files in the system at any given point in time</p>\n</blockquote>\n<p>NR_FILE 是系统在某一给定时刻，限制的文件总数</p>\n<blockquote>\n<p>While initializing the kernel we setup the vfs cache with start_kernel<br>vfs_caches_init(num_physpages);<br>files_init(mempages);<br>fs/file_table.c says<br>/* One file with associated inode and dcache is very roughly 1K.</p>\n<ul>\n<li>Per default don’t use more than 10% of our memory for files.<br>n = (mempages * (PAGE_SIZE / 1024)) / 10;<br>this n can never be greater than NR_FILE</li>\n</ul>\n</blockquote>\n<h3 id=\"ulimit-命令\"><a href=\"#ulimit-命令\" class=\"headerlink\" title=\"ulimit 命令\"></a>ulimit 命令</h3><p>1.只对当前tty（终端有效），若要每次都生效的话，可以把ulimit参数放到对应用户的.bash_profile里面；<br>2.ulimit命令本身就有分软硬设置，加-H就是硬，加-S就是软；<br>3.默认显示的是软限制，如果运行ulimit命令修改的时候没有加上的话，就是两个参数一起改变.生效；</p>\n<p>命令参数<br>-H 设置硬件资源限制.<br>-S 设置软件资源限制.<br>-a 显示当前所有的资源限制.<br>-c size:设置core文件的最大值.单位:blocks<br>-d size:设置数据段的最大值.单位:kbytes<br>-f size:设置创建文件的最大值.单位:blocks<br>-l size:设置在内存中锁定进程的最大值.单位:kbytes<br>-m size:设置可以使用的常驻内存的最大值.单位:kbytes<br>-n size:设置内核可以同时打开的文件描述符的最大值.单位:n<br>-p size:设置管道缓冲区的最大值.单位:kbytes<br>-s size:设置堆栈的最大值.单位:kbytes<br>-t size:设置CPU使用时间的最大上限.单位:seconds<br>-v size:设置虚拟内存的最大值.单位:kbytes<br>unlimited 是一个特殊值，用于表示不限制</p>\n<h3 id=\"总结-file-max-nr-open-nofile之间的关系\"><a href=\"#总结-file-max-nr-open-nofile之间的关系\" class=\"headerlink\" title=\"总结 file-max, nr_open, nofile之间的关系\"></a>总结 file-max, nr_open, nofile之间的关系</h3><ol>\n<li>针对用户打开最大文件数的限制，可以通过修改文件<code>limits.conf</code>来实现</li>\n<li>nofile中soft的值小于hard, 最大值由nr_open来决定</li>\n<li>file-max表示内核针对整个系统，限制能所有进程能打开的文件描述符数</li>\n<li>nofile &lt; nr_open &lt; file-max</li>\n</ol>\n"},{"title":"nginx 信号处理","date":"2017-09-15T03:37:28.000Z","_content":"\n​\t因为Nginx是利用信号来实现平滑升级、更换日志文件、配置文件实时生效、重启服务等功能的，所以在Nginx的启动过程中会向操作系统内核注册所使用到的信号，其代码实现如下\n\n```c\nint ngx_cdecl\nmain(int argc, char *const *argv)\n{\n    ……\n    // 初始化信号\n    if (ngx_init_signals(cycle->log) != NGX_OK) {\n        return 1;\n    }\n    ……\n}\n```\n\n<!-- more -->\n\n​\tsignals是一个全局数组，保存的是ngx_signal_t数据类型，定义了各种信号信息以及处理函数，ngx_init_signals在程序main入口处调用，注册各种信号，ngx_init_signals()实现如下：\n\n```c\nngx_int_t\nngx_init_signals(ngx_log_t *log)\n{\n    ngx_signal_t      *sig;\n    struct sigaction   sa;  \n\n    // 遍历signals数组，注册各种信号处理函数\n    for (sig = signals; sig->signo != 0; sig++) {\n        ngx_memzero(&sa, sizeof(struct sigaction));\n        sa.sa_handler = sig->handler;  \n        sigemptyset(&sa.sa_mask);\n        //向内核注册信号的回调方法\n        if (sigaction(sig->signo, &sa, NULL) == -1) {\n\t\t\t……\n        }\n    }\n    return NGX_OK;\n}\n```\n\n​\tNginx采用ngx_signal_t结构体来保存信号id 信号名称 信号处理函数，signals是一个ngx_signal_t类型的全局数组，用来保存各个信号的信息，不同信号的处理函数都是ngx_signal_handler，**ngx_signal_handler函数的逻辑主要是根据信号类型来设置不同的全局标志变量，ngx_terminate等全局标志变量都是sig_atomic_t数据类型，sig_atomic_t可以在操作系统层面提供的读写原子性，保证在对ngx_terminate变量的读的时候不会被信号中断，不过比较奇怪的是ngx_terminate变量没有添加volatile关键字，编译器会对优化变量的访问方式，如果ngx_terminate的赋值只是更改了寄存器中变量的值，而主循环中读的却是内存中的变量值，岂不是出问题？**\n\n```c\n// ngx_terminate的声明：\nsig_atomic_t  ngx_sigalrm;\nsig_atomic_t  ngx_terminate;\nsig_atomic_t  ngx_quit;\n....\n\n// 信号处理函数大概逻辑\nswitch (signo) {\n    case ngx_signal_value(NGX_SHUTDOWN_SIGNAL):\n        ngx_quit = 1;\n        action = \", shutting down\";\n        break;\n\n    case ngx_signal_value(NGX_TERMINATE_SIGNAL):\n    case SIGINT:\n        ngx_terminate = 1;\n        action = \", exiting\";\n        break;\n    ...\n    break;\n```\n​\tnginx命令行启动中提供了参数-s参数来搞一些事情，例如stop/quit/reopen/reload，如果nginx进程已经起来了，则可以通过-s参数来给master进程发送信号来控制nginx。ngx_signal_process函数主要是读master进程的pid文件，获取进程的pid，最后给master进程发送相关信号。\n\n```c\n// 如果启动的进程，有信号相关的参数，则像已经存在的masrer进程发送信号，然后自己退出\nif (ngx_signal) {\n    return ngx_signal_process(cycle, ngx_signal);\n}\n```\n\n​\t在初始化流程中设置好了信号相关处理函数。**master进程在派生出worker进程之前会屏蔽一大堆信号，例如SIGCHLD/SIGALRM/SIGINT等，防止在创建worker进程时受到干扰，worker进程的ngx_worker_process_init初始化函数中会重新设置worker进程的信号屏蔽字，把屏蔽字设置为空以便worker进程也可以正常处理各种信号**。\n\n​\tmaster进程的主循环会在调用sigsuspend(&set)函数时阻塞，sigsuspend函数的功能主要是先用已经被清空的信号集代替当前进程中的信号集，然后阻塞等待信号，等信号处理函数返回之后会用老的信号屏蔽字替换新的信号屏蔽字。\n\n​\tmaster进程对信号的设置大概如下：\n\n```c\n// ngx_master_process_cycle 函数\n...\nsigemptyset(&set);\nsigaddset(&set, SIGCHLD);\nsigaddset(&set, SIGALRM);\nsigaddset(&set, SIGIO);\nsigaddset(&set, SIGINT);\nsigaddset(&set, ngx_signal_value(NGX_RECONFIGURE_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_REOPEN_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_NOACCEPT_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_TERMINATE_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_SHUTDOWN_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_CHANGEBIN_SIGNAL));\n\n//上面屏蔽一系列的信号，以防创建worker进程时，被打扰。\nif (sigprocmask(SIG_BLOCK, &set, NULL) == -1) {\n    ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,\n                  \"sigprocmask() failed\");\n}\n//清空信号集\nsigemptyset(&set);\n...\nfor(;;) {\n\t...        \n\tsigsuspend(&set); // 阻塞等待信号\n}\n```\n\n​\tworker进程对信号设置大概如下：\n\n```c\n// ngx_worker_process_cycle() -> ngx_worker_process_init()\n\n// ngx_worker_process_init函数\n...\n// worker进程对从父进程继承而来的信号屏蔽字重新设置为空，以可以处理各种信号\nsigemptyset(&set);\n\nif (sigprocmask(SIG_SETMASK, &set, NULL) == -1) {\n    ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,\n                  \"sigprocmask() failed\");\n}\n...\n```\n\n​\tworker进程的主循环也会不停地检测各种信号全局标志变量，具体可参考nginx程序框架部分\n","source":"_posts/nginx-信号处理.md","raw":"---\ntitle: nginx 信号处理\ndate: 2017-09-15 11:37:28\ntags: nginx\n---\n\n​\t因为Nginx是利用信号来实现平滑升级、更换日志文件、配置文件实时生效、重启服务等功能的，所以在Nginx的启动过程中会向操作系统内核注册所使用到的信号，其代码实现如下\n\n```c\nint ngx_cdecl\nmain(int argc, char *const *argv)\n{\n    ……\n    // 初始化信号\n    if (ngx_init_signals(cycle->log) != NGX_OK) {\n        return 1;\n    }\n    ……\n}\n```\n\n<!-- more -->\n\n​\tsignals是一个全局数组，保存的是ngx_signal_t数据类型，定义了各种信号信息以及处理函数，ngx_init_signals在程序main入口处调用，注册各种信号，ngx_init_signals()实现如下：\n\n```c\nngx_int_t\nngx_init_signals(ngx_log_t *log)\n{\n    ngx_signal_t      *sig;\n    struct sigaction   sa;  \n\n    // 遍历signals数组，注册各种信号处理函数\n    for (sig = signals; sig->signo != 0; sig++) {\n        ngx_memzero(&sa, sizeof(struct sigaction));\n        sa.sa_handler = sig->handler;  \n        sigemptyset(&sa.sa_mask);\n        //向内核注册信号的回调方法\n        if (sigaction(sig->signo, &sa, NULL) == -1) {\n\t\t\t……\n        }\n    }\n    return NGX_OK;\n}\n```\n\n​\tNginx采用ngx_signal_t结构体来保存信号id 信号名称 信号处理函数，signals是一个ngx_signal_t类型的全局数组，用来保存各个信号的信息，不同信号的处理函数都是ngx_signal_handler，**ngx_signal_handler函数的逻辑主要是根据信号类型来设置不同的全局标志变量，ngx_terminate等全局标志变量都是sig_atomic_t数据类型，sig_atomic_t可以在操作系统层面提供的读写原子性，保证在对ngx_terminate变量的读的时候不会被信号中断，不过比较奇怪的是ngx_terminate变量没有添加volatile关键字，编译器会对优化变量的访问方式，如果ngx_terminate的赋值只是更改了寄存器中变量的值，而主循环中读的却是内存中的变量值，岂不是出问题？**\n\n```c\n// ngx_terminate的声明：\nsig_atomic_t  ngx_sigalrm;\nsig_atomic_t  ngx_terminate;\nsig_atomic_t  ngx_quit;\n....\n\n// 信号处理函数大概逻辑\nswitch (signo) {\n    case ngx_signal_value(NGX_SHUTDOWN_SIGNAL):\n        ngx_quit = 1;\n        action = \", shutting down\";\n        break;\n\n    case ngx_signal_value(NGX_TERMINATE_SIGNAL):\n    case SIGINT:\n        ngx_terminate = 1;\n        action = \", exiting\";\n        break;\n    ...\n    break;\n```\n​\tnginx命令行启动中提供了参数-s参数来搞一些事情，例如stop/quit/reopen/reload，如果nginx进程已经起来了，则可以通过-s参数来给master进程发送信号来控制nginx。ngx_signal_process函数主要是读master进程的pid文件，获取进程的pid，最后给master进程发送相关信号。\n\n```c\n// 如果启动的进程，有信号相关的参数，则像已经存在的masrer进程发送信号，然后自己退出\nif (ngx_signal) {\n    return ngx_signal_process(cycle, ngx_signal);\n}\n```\n\n​\t在初始化流程中设置好了信号相关处理函数。**master进程在派生出worker进程之前会屏蔽一大堆信号，例如SIGCHLD/SIGALRM/SIGINT等，防止在创建worker进程时受到干扰，worker进程的ngx_worker_process_init初始化函数中会重新设置worker进程的信号屏蔽字，把屏蔽字设置为空以便worker进程也可以正常处理各种信号**。\n\n​\tmaster进程的主循环会在调用sigsuspend(&set)函数时阻塞，sigsuspend函数的功能主要是先用已经被清空的信号集代替当前进程中的信号集，然后阻塞等待信号，等信号处理函数返回之后会用老的信号屏蔽字替换新的信号屏蔽字。\n\n​\tmaster进程对信号的设置大概如下：\n\n```c\n// ngx_master_process_cycle 函数\n...\nsigemptyset(&set);\nsigaddset(&set, SIGCHLD);\nsigaddset(&set, SIGALRM);\nsigaddset(&set, SIGIO);\nsigaddset(&set, SIGINT);\nsigaddset(&set, ngx_signal_value(NGX_RECONFIGURE_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_REOPEN_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_NOACCEPT_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_TERMINATE_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_SHUTDOWN_SIGNAL));\nsigaddset(&set, ngx_signal_value(NGX_CHANGEBIN_SIGNAL));\n\n//上面屏蔽一系列的信号，以防创建worker进程时，被打扰。\nif (sigprocmask(SIG_BLOCK, &set, NULL) == -1) {\n    ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,\n                  \"sigprocmask() failed\");\n}\n//清空信号集\nsigemptyset(&set);\n...\nfor(;;) {\n\t...        \n\tsigsuspend(&set); // 阻塞等待信号\n}\n```\n\n​\tworker进程对信号设置大概如下：\n\n```c\n// ngx_worker_process_cycle() -> ngx_worker_process_init()\n\n// ngx_worker_process_init函数\n...\n// worker进程对从父进程继承而来的信号屏蔽字重新设置为空，以可以处理各种信号\nsigemptyset(&set);\n\nif (sigprocmask(SIG_SETMASK, &set, NULL) == -1) {\n    ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,\n                  \"sigprocmask() failed\");\n}\n...\n```\n\n​\tworker进程的主循环也会不停地检测各种信号全局标志变量，具体可参考nginx程序框架部分\n","slug":"nginx-信号处理","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dublh000bamum9gph33mp","content":"<p>​    因为Nginx是利用信号来实现平滑升级、更换日志文件、配置文件实时生效、重启服务等功能的，所以在Nginx的启动过程中会向操作系统内核注册所使用到的信号，其代码实现如下</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> ngx_cdecl</span><br><span class=\"line\">main(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span> *<span class=\"keyword\">const</span> *argv)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">    <span class=\"comment\">// 初始化信号</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_init_signals(cycle-&gt;<span class=\"built_in\">log</span>) != NGX_OK) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<p>​    signals是一个全局数组，保存的是ngx_signal_t数据类型，定义了各种信号信息以及处理函数，ngx_init_signals在程序main入口处调用，注册各种信号，ngx_init_signals()实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_init_signals(<span class=\"keyword\">ngx_log_t</span> *<span class=\"built_in\">log</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_signal_t</span>      *sig;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sigaction</span>   <span class=\"title\">sa</span>;</span>  </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 遍历signals数组，注册各种信号处理函数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (sig = signals; sig-&gt;signo != <span class=\"number\">0</span>; sig++) &#123;</span><br><span class=\"line\">        ngx_memzero(&amp;sa, <span class=\"keyword\">sizeof</span>(struct sigaction));</span><br><span class=\"line\">        sa.sa_handler = sig-&gt;handler;  </span><br><span class=\"line\">        sigemptyset(&amp;sa.sa_mask);</span><br><span class=\"line\">        <span class=\"comment\">//向内核注册信号的回调方法</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sigaction(sig-&gt;signo, &amp;sa, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">\t\t\t……</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    Nginx采用ngx_signal_t结构体来保存信号id 信号名称 信号处理函数，signals是一个ngx_signal_t类型的全局数组，用来保存各个信号的信息，不同信号的处理函数都是ngx_signal_handler，<strong>ngx_signal_handler函数的逻辑主要是根据信号类型来设置不同的全局标志变量，ngx_terminate等全局标志变量都是sig_atomic_t数据类型，sig_atomic_t可以在操作系统层面提供的读写原子性，保证在对ngx_terminate变量的读的时候不会被信号中断，不过比较奇怪的是ngx_terminate变量没有添加volatile关键字，编译器会对优化变量的访问方式，如果ngx_terminate的赋值只是更改了寄存器中变量的值，而主循环中读的却是内存中的变量值，岂不是出问题？</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_terminate的声明：</span></span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_sigalrm;</span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_terminate;</span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_quit;</span><br><span class=\"line\">....</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 信号处理函数大概逻辑</span></span><br><span class=\"line\"><span class=\"keyword\">switch</span> (signo) &#123;</span><br><span class=\"line\">    case ngx_signal_value(NGX_SHUTDOWN_SIGNAL):</span><br><span class=\"line\">        ngx_quit = <span class=\"number\">1</span>;</span><br><span class=\"line\">        action = <span class=\"string\">\", shutting down\"</span>;</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    case ngx_signal_value(NGX_TERMINATE_SIGNAL):</span><br><span class=\"line\">    <span class=\"keyword\">case</span> SIGINT:</span><br><span class=\"line\">        ngx_terminate = <span class=\"number\">1</span>;</span><br><span class=\"line\">        action = <span class=\"string\">\", exiting\"</span>;</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">break</span>;</span><br></pre></td></tr></table></figure>\n<p>​    nginx命令行启动中提供了参数-s参数来搞一些事情，例如stop/quit/reopen/reload，如果nginx进程已经起来了，则可以通过-s参数来给master进程发送信号来控制nginx。ngx_signal_process函数主要是读master进程的pid文件，获取进程的pid，最后给master进程发送相关信号。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果启动的进程，有信号相关的参数，则像已经存在的masrer进程发送信号，然后自己退出</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (ngx_signal) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ngx_signal_process(cycle, ngx_signal);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    在初始化流程中设置好了信号相关处理函数。<strong>master进程在派生出worker进程之前会屏蔽一大堆信号，例如SIGCHLD/SIGALRM/SIGINT等，防止在创建worker进程时受到干扰，worker进程的ngx_worker_process_init初始化函数中会重新设置worker进程的信号屏蔽字，把屏蔽字设置为空以便worker进程也可以正常处理各种信号</strong>。</p>\n<p>​    master进程的主循环会在调用sigsuspend(&amp;set)函数时阻塞，sigsuspend函数的功能主要是先用已经被清空的信号集代替当前进程中的信号集，然后阻塞等待信号，等信号处理函数返回之后会用老的信号屏蔽字替换新的信号屏蔽字。</p>\n<p>​    master进程对信号的设置大概如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_master_process_cycle 函数</span></span><br><span class=\"line\">...</span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGCHLD);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGALRM);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGIO);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGINT);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_RECONFIGURE_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_REOPEN_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_NOACCEPT_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_TERMINATE_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_SHUTDOWN_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_CHANGEBIN_SIGNAL));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//上面屏蔽一系列的信号，以防创建worker进程时，被打扰。</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (sigprocmask(SIG_BLOCK, &amp;<span class=\"built_in\">set</span>, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">    ngx_log_error(NGX_LOG_ALERT, cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                  <span class=\"string\">\"sigprocmask() failed\"</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//清空信号集</span></span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span>(;;) &#123;</span><br><span class=\"line\">\t...        </span><br><span class=\"line\">\tsigsuspend(&amp;<span class=\"built_in\">set</span>); <span class=\"comment\">// 阻塞等待信号</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    worker进程对信号设置大概如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_worker_process_cycle() -&gt; ngx_worker_process_init()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ngx_worker_process_init函数</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"comment\">// worker进程对从父进程继承而来的信号屏蔽字重新设置为空，以可以处理各种信号</span></span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (sigprocmask(SIG_SETMASK, &amp;<span class=\"built_in\">set</span>, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">    ngx_log_error(NGX_LOG_ALERT, cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                  <span class=\"string\">\"sigprocmask() failed\"</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>​    worker进程的主循环也会不停地检测各种信号全局标志变量，具体可参考nginx程序框架部分</p>\n","site":{"data":{}},"excerpt":"<p>​    因为Nginx是利用信号来实现平滑升级、更换日志文件、配置文件实时生效、重启服务等功能的，所以在Nginx的启动过程中会向操作系统内核注册所使用到的信号，其代码实现如下</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> ngx_cdecl</span><br><span class=\"line\">main(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span> *<span class=\"keyword\">const</span> *argv)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">    <span class=\"comment\">// 初始化信号</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_init_signals(cycle-&gt;<span class=\"built_in\">log</span>) != NGX_OK) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"<p>​    signals是一个全局数组，保存的是ngx_signal_t数据类型，定义了各种信号信息以及处理函数，ngx_init_signals在程序main入口处调用，注册各种信号，ngx_init_signals()实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_init_signals(<span class=\"keyword\">ngx_log_t</span> *<span class=\"built_in\">log</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_signal_t</span>      *sig;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sigaction</span>   <span class=\"title\">sa</span>;</span>  </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 遍历signals数组，注册各种信号处理函数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (sig = signals; sig-&gt;signo != <span class=\"number\">0</span>; sig++) &#123;</span><br><span class=\"line\">        ngx_memzero(&amp;sa, <span class=\"keyword\">sizeof</span>(struct sigaction));</span><br><span class=\"line\">        sa.sa_handler = sig-&gt;handler;  </span><br><span class=\"line\">        sigemptyset(&amp;sa.sa_mask);</span><br><span class=\"line\">        <span class=\"comment\">//向内核注册信号的回调方法</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sigaction(sig-&gt;signo, &amp;sa, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">\t\t\t……</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    Nginx采用ngx_signal_t结构体来保存信号id 信号名称 信号处理函数，signals是一个ngx_signal_t类型的全局数组，用来保存各个信号的信息，不同信号的处理函数都是ngx_signal_handler，<strong>ngx_signal_handler函数的逻辑主要是根据信号类型来设置不同的全局标志变量，ngx_terminate等全局标志变量都是sig_atomic_t数据类型，sig_atomic_t可以在操作系统层面提供的读写原子性，保证在对ngx_terminate变量的读的时候不会被信号中断，不过比较奇怪的是ngx_terminate变量没有添加volatile关键字，编译器会对优化变量的访问方式，如果ngx_terminate的赋值只是更改了寄存器中变量的值，而主循环中读的却是内存中的变量值，岂不是出问题？</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_terminate的声明：</span></span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_sigalrm;</span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_terminate;</span><br><span class=\"line\"><span class=\"keyword\">sig_atomic_t</span>  ngx_quit;</span><br><span class=\"line\">....</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 信号处理函数大概逻辑</span></span><br><span class=\"line\"><span class=\"keyword\">switch</span> (signo) &#123;</span><br><span class=\"line\">    case ngx_signal_value(NGX_SHUTDOWN_SIGNAL):</span><br><span class=\"line\">        ngx_quit = <span class=\"number\">1</span>;</span><br><span class=\"line\">        action = <span class=\"string\">\", shutting down\"</span>;</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    case ngx_signal_value(NGX_TERMINATE_SIGNAL):</span><br><span class=\"line\">    <span class=\"keyword\">case</span> SIGINT:</span><br><span class=\"line\">        ngx_terminate = <span class=\"number\">1</span>;</span><br><span class=\"line\">        action = <span class=\"string\">\", exiting\"</span>;</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">break</span>;</span><br></pre></td></tr></table></figure>\n<p>​    nginx命令行启动中提供了参数-s参数来搞一些事情，例如stop/quit/reopen/reload，如果nginx进程已经起来了，则可以通过-s参数来给master进程发送信号来控制nginx。ngx_signal_process函数主要是读master进程的pid文件，获取进程的pid，最后给master进程发送相关信号。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 如果启动的进程，有信号相关的参数，则像已经存在的masrer进程发送信号，然后自己退出</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (ngx_signal) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ngx_signal_process(cycle, ngx_signal);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    在初始化流程中设置好了信号相关处理函数。<strong>master进程在派生出worker进程之前会屏蔽一大堆信号，例如SIGCHLD/SIGALRM/SIGINT等，防止在创建worker进程时受到干扰，worker进程的ngx_worker_process_init初始化函数中会重新设置worker进程的信号屏蔽字，把屏蔽字设置为空以便worker进程也可以正常处理各种信号</strong>。</p>\n<p>​    master进程的主循环会在调用sigsuspend(&amp;set)函数时阻塞，sigsuspend函数的功能主要是先用已经被清空的信号集代替当前进程中的信号集，然后阻塞等待信号，等信号处理函数返回之后会用老的信号屏蔽字替换新的信号屏蔽字。</p>\n<p>​    master进程对信号的设置大概如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_master_process_cycle 函数</span></span><br><span class=\"line\">...</span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGCHLD);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGALRM);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGIO);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, SIGINT);</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_RECONFIGURE_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_REOPEN_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_NOACCEPT_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_TERMINATE_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_SHUTDOWN_SIGNAL));</span><br><span class=\"line\">sigaddset(&amp;<span class=\"built_in\">set</span>, ngx_signal_value(NGX_CHANGEBIN_SIGNAL));</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//上面屏蔽一系列的信号，以防创建worker进程时，被打扰。</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (sigprocmask(SIG_BLOCK, &amp;<span class=\"built_in\">set</span>, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">    ngx_log_error(NGX_LOG_ALERT, cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                  <span class=\"string\">\"sigprocmask() failed\"</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//清空信号集</span></span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span>(;;) &#123;</span><br><span class=\"line\">\t...        </span><br><span class=\"line\">\tsigsuspend(&amp;<span class=\"built_in\">set</span>); <span class=\"comment\">// 阻塞等待信号</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>​    worker进程对信号设置大概如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ngx_worker_process_cycle() -&gt; ngx_worker_process_init()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ngx_worker_process_init函数</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"comment\">// worker进程对从父进程继承而来的信号屏蔽字重新设置为空，以可以处理各种信号</span></span><br><span class=\"line\">sigemptyset(&amp;<span class=\"built_in\">set</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (sigprocmask(SIG_SETMASK, &amp;<span class=\"built_in\">set</span>, <span class=\"literal\">NULL</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">    ngx_log_error(NGX_LOG_ALERT, cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                  <span class=\"string\">\"sigprocmask() failed\"</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>​    worker进程的主循环也会不停地检测各种信号全局标志变量，具体可参考nginx程序框架部分</p>"},{"title":"nginx 系统框架","date":"2017-07-15T12:35:19.000Z","_content":"\n#  Nginx 进程模型\n\nNginx 进程模型采用master/worker进程模型，一个master进程，多个worker进程。master进程主要工作就是重载配置文件、监听信号(重载配置文件/重新打开日志文件等)、监控worker，而worker进程核心就是处理网络事件以及定时相关操作，maser和worker之间通信机制采用socketpair + 共享内存 + 信号，worker和worker之间通信机制采用socketpair通信。\n\n---\n\n## 1. Nginx启动流程\n\n- 命令行参数解析，获取主配置文件路径等，ngx_process_options\n- 各种初始化工作，调用各个模块create_conf和init_conf方法、创建pid文件、创建共享内存、监听套接字、调用各个模块init_module方法...，初始化工作主要在ngx_init_cycle这个函数中完成\n- 注册各种信号处理函数，主要在ngx_init_signals函数中完成，在signals全局数组中保存了各种信号以及相应的信号处理函数\n- 创建后台daemon进程\n- 创建pid文件\n- 根据启动参数判断启动哪种模式，master/worker进程模式则进入ngx_master_process_cycle函数\n\n<!-- more -->\n\n---\n\n## 2. master进程\n\nmaster进程的主要工作逻辑是在ngx_master_process_cycle函数中执行，大概的执行步骤：\n\n- 阻塞各种信号，Nginx中有用到的信号基本都先屏蔽，目的主要是为了防止master在fork worker进程时受到干扰\n- 按照配置文件中worker进程个数，fork相应个数的worker进程，每fork一个worker进程会向所有woker进程广播当前worker进程的进程信息(pid、socketfd)\n- master进程进入主循环，sigsuspend挂起进程等待处理各种信号事件，主要处理以下事件：\n  - 收到SIGCHLD信号，有worker进程异常退出，则重启之\n  - 收到SIGTERM信号或者SIGINT信号，则通知所有worker退出，并且等待worker退出\n  - 收到了SIGHUP信号, 重载配置文件\n  - 收到SIGUSR1信号，重新打开log文件\n  - 收到SIGUSR2信号，热代码替换\n  - 收到SIGWINCH信号，不再接收请求，worker退出，master不退出\n\n---\n\n## 3.worker进程\n\nworker进程主要工作逻辑在ngx_worker_process_cycle函数中完成，大概的执行步骤：\n\n- woker初始化工作，根据全局的配置信息设置执行环境、优先级、限制、setgid、setuid、信号初始化等，在fork子进程之前，信号都被屏蔽了，所以在worker初始化时需要解除阻塞\n- 调用所有模块的init_process钩子函数\n- 关闭不会使用到的socket，关闭当前worker的channel[0]句柄和其他worker的channel[1]句柄。(当前worker会使用其他worker的channel[0]句柄发送消息，使用当前worker的channel[1]句柄监听可读事件)\n- worker进程进入主循环\n  - 判断是否关闭worker进程\n  - 处理事件和定时器事件，ngx_process_events_and_timers()，worker核心部分\n  - 处理maser进程发给worker进程的命令，master在收到信号后会给worker进程发送命令\n    - 判断是否强制关闭进程(SIGTERM信号)\n    - 判断是否优雅地关闭进程(SIGQUIT信号)\n    - 判断是否重新打开文件(切换日志文件)(SIGUSR1信号)\n\n----\n\n## 4.master和worker通信\n\nmaster和worker之间通信方式有channel机制  + 共享内存机制，master进程向worker发送命令采用的是无名管道的channel机制，这里简要介绍无名管道机制。\n\n`int socketpair(int domain, int type, int protocol, int sv[2])`\n\n无名管道机制是双工的，socketpair创建了一对无名的套接字描述符描述符存储于一个二元数组s[2] ，这对套接字可以进行双工通信，每一个描述符既可以读也可以写。这个在同一个进程中也可以进行通信，向s[0]中写入，就可以从s[1]中读取（只能从s[1]中读取），也可以在s[1]中写入，然后从s[0]中读取；但是，若没有在0端写入，而从1端读取，则1端的读取操作会阻塞，即使在1端写入，也不能从1读取，仍然阻塞，反之亦然......\n\n在master进程fork worker进程的时候，也把这个套接字传给了worker，也就是说在master向worker的sv[0]写数据，那么worker便可以在自己的s[1]中读到数据。Nginx中主要是master向管道中写命令，worker从管道中读，命令主要有：\n\n- NGX_CMD_QUIT\n- NGX_CMD_TERMINATE\n- NGX_CMD_REOPEN\n- NGX_CMD_OPEN_CHANNEL\n- NGX_CMD_CLOSE_CHANNEL\n- NGX_CMD_PIPE_BROKEN\n\n","source":"_posts/nginx-系统框架.md","raw":"---\ntitle: nginx 系统框架\ndate: 2017-07-15 20:35:19\ntags: nginx\n---\n\n#  Nginx 进程模型\n\nNginx 进程模型采用master/worker进程模型，一个master进程，多个worker进程。master进程主要工作就是重载配置文件、监听信号(重载配置文件/重新打开日志文件等)、监控worker，而worker进程核心就是处理网络事件以及定时相关操作，maser和worker之间通信机制采用socketpair + 共享内存 + 信号，worker和worker之间通信机制采用socketpair通信。\n\n---\n\n## 1. Nginx启动流程\n\n- 命令行参数解析，获取主配置文件路径等，ngx_process_options\n- 各种初始化工作，调用各个模块create_conf和init_conf方法、创建pid文件、创建共享内存、监听套接字、调用各个模块init_module方法...，初始化工作主要在ngx_init_cycle这个函数中完成\n- 注册各种信号处理函数，主要在ngx_init_signals函数中完成，在signals全局数组中保存了各种信号以及相应的信号处理函数\n- 创建后台daemon进程\n- 创建pid文件\n- 根据启动参数判断启动哪种模式，master/worker进程模式则进入ngx_master_process_cycle函数\n\n<!-- more -->\n\n---\n\n## 2. master进程\n\nmaster进程的主要工作逻辑是在ngx_master_process_cycle函数中执行，大概的执行步骤：\n\n- 阻塞各种信号，Nginx中有用到的信号基本都先屏蔽，目的主要是为了防止master在fork worker进程时受到干扰\n- 按照配置文件中worker进程个数，fork相应个数的worker进程，每fork一个worker进程会向所有woker进程广播当前worker进程的进程信息(pid、socketfd)\n- master进程进入主循环，sigsuspend挂起进程等待处理各种信号事件，主要处理以下事件：\n  - 收到SIGCHLD信号，有worker进程异常退出，则重启之\n  - 收到SIGTERM信号或者SIGINT信号，则通知所有worker退出，并且等待worker退出\n  - 收到了SIGHUP信号, 重载配置文件\n  - 收到SIGUSR1信号，重新打开log文件\n  - 收到SIGUSR2信号，热代码替换\n  - 收到SIGWINCH信号，不再接收请求，worker退出，master不退出\n\n---\n\n## 3.worker进程\n\nworker进程主要工作逻辑在ngx_worker_process_cycle函数中完成，大概的执行步骤：\n\n- woker初始化工作，根据全局的配置信息设置执行环境、优先级、限制、setgid、setuid、信号初始化等，在fork子进程之前，信号都被屏蔽了，所以在worker初始化时需要解除阻塞\n- 调用所有模块的init_process钩子函数\n- 关闭不会使用到的socket，关闭当前worker的channel[0]句柄和其他worker的channel[1]句柄。(当前worker会使用其他worker的channel[0]句柄发送消息，使用当前worker的channel[1]句柄监听可读事件)\n- worker进程进入主循环\n  - 判断是否关闭worker进程\n  - 处理事件和定时器事件，ngx_process_events_and_timers()，worker核心部分\n  - 处理maser进程发给worker进程的命令，master在收到信号后会给worker进程发送命令\n    - 判断是否强制关闭进程(SIGTERM信号)\n    - 判断是否优雅地关闭进程(SIGQUIT信号)\n    - 判断是否重新打开文件(切换日志文件)(SIGUSR1信号)\n\n----\n\n## 4.master和worker通信\n\nmaster和worker之间通信方式有channel机制  + 共享内存机制，master进程向worker发送命令采用的是无名管道的channel机制，这里简要介绍无名管道机制。\n\n`int socketpair(int domain, int type, int protocol, int sv[2])`\n\n无名管道机制是双工的，socketpair创建了一对无名的套接字描述符描述符存储于一个二元数组s[2] ，这对套接字可以进行双工通信，每一个描述符既可以读也可以写。这个在同一个进程中也可以进行通信，向s[0]中写入，就可以从s[1]中读取（只能从s[1]中读取），也可以在s[1]中写入，然后从s[0]中读取；但是，若没有在0端写入，而从1端读取，则1端的读取操作会阻塞，即使在1端写入，也不能从1读取，仍然阻塞，反之亦然......\n\n在master进程fork worker进程的时候，也把这个套接字传给了worker，也就是说在master向worker的sv[0]写数据，那么worker便可以在自己的s[1]中读到数据。Nginx中主要是master向管道中写命令，worker从管道中读，命令主要有：\n\n- NGX_CMD_QUIT\n- NGX_CMD_TERMINATE\n- NGX_CMD_REOPEN\n- NGX_CMD_OPEN_CHANNEL\n- NGX_CMD_CLOSE_CHANNEL\n- NGX_CMD_PIPE_BROKEN\n\n","slug":"nginx-系统框架","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubli000damumanv3gj9r","content":"<h1 id=\"Nginx-进程模型\"><a href=\"#Nginx-进程模型\" class=\"headerlink\" title=\"Nginx 进程模型\"></a>Nginx 进程模型</h1><p>Nginx 进程模型采用master/worker进程模型，一个master进程，多个worker进程。master进程主要工作就是重载配置文件、监听信号(重载配置文件/重新打开日志文件等)、监控worker，而worker进程核心就是处理网络事件以及定时相关操作，maser和worker之间通信机制采用socketpair + 共享内存 + 信号，worker和worker之间通信机制采用socketpair通信。</p>\n<hr>\n<h2 id=\"1-Nginx启动流程\"><a href=\"#1-Nginx启动流程\" class=\"headerlink\" title=\"1. Nginx启动流程\"></a>1. Nginx启动流程</h2><ul>\n<li>命令行参数解析，获取主配置文件路径等，ngx_process_options</li>\n<li>各种初始化工作，调用各个模块create_conf和init_conf方法、创建pid文件、创建共享内存、监听套接字、调用各个模块init_module方法…，初始化工作主要在ngx_init_cycle这个函数中完成</li>\n<li>注册各种信号处理函数，主要在ngx_init_signals函数中完成，在signals全局数组中保存了各种信号以及相应的信号处理函数</li>\n<li>创建后台daemon进程</li>\n<li>创建pid文件</li>\n<li>根据启动参数判断启动哪种模式，master/worker进程模式则进入ngx_master_process_cycle函数</li>\n</ul>\n<a id=\"more\"></a>\n<hr>\n<h2 id=\"2-master进程\"><a href=\"#2-master进程\" class=\"headerlink\" title=\"2. master进程\"></a>2. master进程</h2><p>master进程的主要工作逻辑是在ngx_master_process_cycle函数中执行，大概的执行步骤：</p>\n<ul>\n<li>阻塞各种信号，Nginx中有用到的信号基本都先屏蔽，目的主要是为了防止master在fork worker进程时受到干扰</li>\n<li>按照配置文件中worker进程个数，fork相应个数的worker进程，每fork一个worker进程会向所有woker进程广播当前worker进程的进程信息(pid、socketfd)</li>\n<li>master进程进入主循环，sigsuspend挂起进程等待处理各种信号事件，主要处理以下事件：<ul>\n<li>收到SIGCHLD信号，有worker进程异常退出，则重启之</li>\n<li>收到SIGTERM信号或者SIGINT信号，则通知所有worker退出，并且等待worker退出</li>\n<li>收到了SIGHUP信号, 重载配置文件</li>\n<li>收到SIGUSR1信号，重新打开log文件</li>\n<li>收到SIGUSR2信号，热代码替换</li>\n<li>收到SIGWINCH信号，不再接收请求，worker退出，master不退出</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-worker进程\"><a href=\"#3-worker进程\" class=\"headerlink\" title=\"3.worker进程\"></a>3.worker进程</h2><p>worker进程主要工作逻辑在ngx_worker_process_cycle函数中完成，大概的执行步骤：</p>\n<ul>\n<li>woker初始化工作，根据全局的配置信息设置执行环境、优先级、限制、setgid、setuid、信号初始化等，在fork子进程之前，信号都被屏蔽了，所以在worker初始化时需要解除阻塞</li>\n<li>调用所有模块的init_process钩子函数</li>\n<li>关闭不会使用到的socket，关闭当前worker的channel[0]句柄和其他worker的channel[1]句柄。(当前worker会使用其他worker的channel[0]句柄发送消息，使用当前worker的channel[1]句柄监听可读事件)</li>\n<li>worker进程进入主循环<ul>\n<li>判断是否关闭worker进程</li>\n<li>处理事件和定时器事件，ngx_process_events_and_timers()，worker核心部分</li>\n<li>处理maser进程发给worker进程的命令，master在收到信号后会给worker进程发送命令<ul>\n<li>判断是否强制关闭进程(SIGTERM信号)</li>\n<li>判断是否优雅地关闭进程(SIGQUIT信号)</li>\n<li>判断是否重新打开文件(切换日志文件)(SIGUSR1信号)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-master和worker通信\"><a href=\"#4-master和worker通信\" class=\"headerlink\" title=\"4.master和worker通信\"></a>4.master和worker通信</h2><p>master和worker之间通信方式有channel机制  + 共享内存机制，master进程向worker发送命令采用的是无名管道的channel机制，这里简要介绍无名管道机制。</p>\n<p><code>int socketpair(int domain, int type, int protocol, int sv[2])</code></p>\n<p>无名管道机制是双工的，socketpair创建了一对无名的套接字描述符描述符存储于一个二元数组s[2] ，这对套接字可以进行双工通信，每一个描述符既可以读也可以写。这个在同一个进程中也可以进行通信，向s[0]中写入，就可以从s[1]中读取（只能从s[1]中读取），也可以在s[1]中写入，然后从s[0]中读取；但是，若没有在0端写入，而从1端读取，则1端的读取操作会阻塞，即使在1端写入，也不能从1读取，仍然阻塞，反之亦然……</p>\n<p>在master进程fork worker进程的时候，也把这个套接字传给了worker，也就是说在master向worker的sv[0]写数据，那么worker便可以在自己的s[1]中读到数据。Nginx中主要是master向管道中写命令，worker从管道中读，命令主要有：</p>\n<ul>\n<li>NGX_CMD_QUIT</li>\n<li>NGX_CMD_TERMINATE</li>\n<li>NGX_CMD_REOPEN</li>\n<li>NGX_CMD_OPEN_CHANNEL</li>\n<li>NGX_CMD_CLOSE_CHANNEL</li>\n<li>NGX_CMD_PIPE_BROKEN</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Nginx-进程模型\"><a href=\"#Nginx-进程模型\" class=\"headerlink\" title=\"Nginx 进程模型\"></a>Nginx 进程模型</h1><p>Nginx 进程模型采用master/worker进程模型，一个master进程，多个worker进程。master进程主要工作就是重载配置文件、监听信号(重载配置文件/重新打开日志文件等)、监控worker，而worker进程核心就是处理网络事件以及定时相关操作，maser和worker之间通信机制采用socketpair + 共享内存 + 信号，worker和worker之间通信机制采用socketpair通信。</p>\n<hr>\n<h2 id=\"1-Nginx启动流程\"><a href=\"#1-Nginx启动流程\" class=\"headerlink\" title=\"1. Nginx启动流程\"></a>1. Nginx启动流程</h2><ul>\n<li>命令行参数解析，获取主配置文件路径等，ngx_process_options</li>\n<li>各种初始化工作，调用各个模块create_conf和init_conf方法、创建pid文件、创建共享内存、监听套接字、调用各个模块init_module方法…，初始化工作主要在ngx_init_cycle这个函数中完成</li>\n<li>注册各种信号处理函数，主要在ngx_init_signals函数中完成，在signals全局数组中保存了各种信号以及相应的信号处理函数</li>\n<li>创建后台daemon进程</li>\n<li>创建pid文件</li>\n<li>根据启动参数判断启动哪种模式，master/worker进程模式则进入ngx_master_process_cycle函数</li>\n</ul>","more":"<hr>\n<h2 id=\"2-master进程\"><a href=\"#2-master进程\" class=\"headerlink\" title=\"2. master进程\"></a>2. master进程</h2><p>master进程的主要工作逻辑是在ngx_master_process_cycle函数中执行，大概的执行步骤：</p>\n<ul>\n<li>阻塞各种信号，Nginx中有用到的信号基本都先屏蔽，目的主要是为了防止master在fork worker进程时受到干扰</li>\n<li>按照配置文件中worker进程个数，fork相应个数的worker进程，每fork一个worker进程会向所有woker进程广播当前worker进程的进程信息(pid、socketfd)</li>\n<li>master进程进入主循环，sigsuspend挂起进程等待处理各种信号事件，主要处理以下事件：<ul>\n<li>收到SIGCHLD信号，有worker进程异常退出，则重启之</li>\n<li>收到SIGTERM信号或者SIGINT信号，则通知所有worker退出，并且等待worker退出</li>\n<li>收到了SIGHUP信号, 重载配置文件</li>\n<li>收到SIGUSR1信号，重新打开log文件</li>\n<li>收到SIGUSR2信号，热代码替换</li>\n<li>收到SIGWINCH信号，不再接收请求，worker退出，master不退出</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"3-worker进程\"><a href=\"#3-worker进程\" class=\"headerlink\" title=\"3.worker进程\"></a>3.worker进程</h2><p>worker进程主要工作逻辑在ngx_worker_process_cycle函数中完成，大概的执行步骤：</p>\n<ul>\n<li>woker初始化工作，根据全局的配置信息设置执行环境、优先级、限制、setgid、setuid、信号初始化等，在fork子进程之前，信号都被屏蔽了，所以在worker初始化时需要解除阻塞</li>\n<li>调用所有模块的init_process钩子函数</li>\n<li>关闭不会使用到的socket，关闭当前worker的channel[0]句柄和其他worker的channel[1]句柄。(当前worker会使用其他worker的channel[0]句柄发送消息，使用当前worker的channel[1]句柄监听可读事件)</li>\n<li>worker进程进入主循环<ul>\n<li>判断是否关闭worker进程</li>\n<li>处理事件和定时器事件，ngx_process_events_and_timers()，worker核心部分</li>\n<li>处理maser进程发给worker进程的命令，master在收到信号后会给worker进程发送命令<ul>\n<li>判断是否强制关闭进程(SIGTERM信号)</li>\n<li>判断是否优雅地关闭进程(SIGQUIT信号)</li>\n<li>判断是否重新打开文件(切换日志文件)(SIGUSR1信号)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-master和worker通信\"><a href=\"#4-master和worker通信\" class=\"headerlink\" title=\"4.master和worker通信\"></a>4.master和worker通信</h2><p>master和worker之间通信方式有channel机制  + 共享内存机制，master进程向worker发送命令采用的是无名管道的channel机制，这里简要介绍无名管道机制。</p>\n<p><code>int socketpair(int domain, int type, int protocol, int sv[2])</code></p>\n<p>无名管道机制是双工的，socketpair创建了一对无名的套接字描述符描述符存储于一个二元数组s[2] ，这对套接字可以进行双工通信，每一个描述符既可以读也可以写。这个在同一个进程中也可以进行通信，向s[0]中写入，就可以从s[1]中读取（只能从s[1]中读取），也可以在s[1]中写入，然后从s[0]中读取；但是，若没有在0端写入，而从1端读取，则1端的读取操作会阻塞，即使在1端写入，也不能从1读取，仍然阻塞，反之亦然……</p>\n<p>在master进程fork worker进程的时候，也把这个套接字传给了worker，也就是说在master向worker的sv[0]写数据，那么worker便可以在自己的s[1]中读到数据。Nginx中主要是master向管道中写命令，worker从管道中读，命令主要有：</p>\n<ul>\n<li>NGX_CMD_QUIT</li>\n<li>NGX_CMD_TERMINATE</li>\n<li>NGX_CMD_REOPEN</li>\n<li>NGX_CMD_OPEN_CHANNEL</li>\n<li>NGX_CMD_CLOSE_CHANNEL</li>\n<li>NGX_CMD_PIPE_BROKEN</li>\n</ul>"},{"title":"Rsync使用","date":"2016-04-15T13:01:29.000Z","_content":"\n# Rsync 使用\n\n1、服务端与客户端的定义\n\n​    A机器向B机器发送数据，  或者A机器向B机器拉取数据\n\n​    那么A均为客户端， B均为服务端\n\n \n\n2、是否需要启动rsync服务\n\n​    客户端不需要， 服务端需要\n\n​    启动rsync进程： systemctl start rsyncd\n\n​    设置rsync开机自启动： systemctl enable rsyncd\n\n \n\n3、rsync相关配置\n\n​    1）模块配置， 固定为/etc/rsyncd.conf, 权限644即可\n\n​    2）用户密码配置， 可以在/etc/rsyncd.conf配置该文件路径， 推荐使用/etc/rsyncd.secret, 权限必须为600\n\n​    3）日志回滚配置 /etc/logrotate.d/rsyncd.rotate， 按需添加\n\n \n\n4、rsyncd.conf举例说明\n\n> \\###全局配置， 生效于每个模块， 并会被模块中相同的配置项覆盖\n>\n> uid = root\n>\n> gid = root\n>\n> use chroot = no\n>\n> pid file = /var/run/rsyncd.pid\n>\n> \\#设置rsync用户密码配置文件路径\n>\n> secrets file = /etc/rsyncd.secret\n>\n> timeout = 300\n>\n> read only = yes\n>\n> write only = no\n>\n> \\#设置最大连接数\n>\n> max connections = 2048\n>\n> dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2\n>\n> \\#设置rsync日志文件， 若未设置， 则默认将日志写入syslog， 即/var/log/message，  当前版本的rsync不支持关闭日志， 所以若设置了该项， 记得添加日志回滚， 避免日志文件挤爆磁盘\n>\n> log file = /var/log/rsync.log\n>\n> syslog facility = local3\n>\n>  \n>\n> \\###分模块配置， 注意rsync配置时，注释必须另起一行 \n>\n> \\#模块名\n>\n> [test-module]\n>\n> \\#模块根目录，注意该目录若不存在， rsync不会自动创建， 传输将报错\n>\n> path = /mylog/test/ori_data/\n>\n> \\#设置为可写\n>\n> read only = no\n>\n> \\#rsync用户名\n>\n> auth users = mylog\n\n\n\n5、rsyncd.secret举例说明\n\n格式为 用户名:密码 ， 例如\n\nmylog:abc123\n\nmylog2:abc456\n\n \n\n6、rsync基础命令\n\n在使用rsync传输之前， 需要在本机创建一个密码文件， 名称随意，例如命名为rsync.key， 权限必须是600， 并且内容只包含要使用的密码（例如 abc123）\n\n1. 这条命令， 表示将本机/home/test/目录下的所有内容，使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test/ mylog@192.168.1.2::test-module\n   ```\n\n   \n\n2. 注意这里/home/test/改为/home/test ， test-module模块名后面添加了一层子目录/test/ ， 这条命令表示将本机的/home/test目录本身， 使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/test/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test mylog@192.168.1.2::test-module/test/\n   ```\n\n   \n\n3. 相比于第一条命令， 这里修改了最后两个参数的顺序， 表示从192.168.1.2拉取数据到本机的/home/test/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key  mylog@192.168.1.2::test-module /home/test/\n   ```","source":"_posts/rsync 使用.md","raw":"---\ntitle: Rsync使用\ndate: 2016-04-15 21:01:29\ntags: linux\n---\n\n# Rsync 使用\n\n1、服务端与客户端的定义\n\n​    A机器向B机器发送数据，  或者A机器向B机器拉取数据\n\n​    那么A均为客户端， B均为服务端\n\n \n\n2、是否需要启动rsync服务\n\n​    客户端不需要， 服务端需要\n\n​    启动rsync进程： systemctl start rsyncd\n\n​    设置rsync开机自启动： systemctl enable rsyncd\n\n \n\n3、rsync相关配置\n\n​    1）模块配置， 固定为/etc/rsyncd.conf, 权限644即可\n\n​    2）用户密码配置， 可以在/etc/rsyncd.conf配置该文件路径， 推荐使用/etc/rsyncd.secret, 权限必须为600\n\n​    3）日志回滚配置 /etc/logrotate.d/rsyncd.rotate， 按需添加\n\n \n\n4、rsyncd.conf举例说明\n\n> \\###全局配置， 生效于每个模块， 并会被模块中相同的配置项覆盖\n>\n> uid = root\n>\n> gid = root\n>\n> use chroot = no\n>\n> pid file = /var/run/rsyncd.pid\n>\n> \\#设置rsync用户密码配置文件路径\n>\n> secrets file = /etc/rsyncd.secret\n>\n> timeout = 300\n>\n> read only = yes\n>\n> write only = no\n>\n> \\#设置最大连接数\n>\n> max connections = 2048\n>\n> dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2\n>\n> \\#设置rsync日志文件， 若未设置， 则默认将日志写入syslog， 即/var/log/message，  当前版本的rsync不支持关闭日志， 所以若设置了该项， 记得添加日志回滚， 避免日志文件挤爆磁盘\n>\n> log file = /var/log/rsync.log\n>\n> syslog facility = local3\n>\n>  \n>\n> \\###分模块配置， 注意rsync配置时，注释必须另起一行 \n>\n> \\#模块名\n>\n> [test-module]\n>\n> \\#模块根目录，注意该目录若不存在， rsync不会自动创建， 传输将报错\n>\n> path = /mylog/test/ori_data/\n>\n> \\#设置为可写\n>\n> read only = no\n>\n> \\#rsync用户名\n>\n> auth users = mylog\n\n\n\n5、rsyncd.secret举例说明\n\n格式为 用户名:密码 ， 例如\n\nmylog:abc123\n\nmylog2:abc456\n\n \n\n6、rsync基础命令\n\n在使用rsync传输之前， 需要在本机创建一个密码文件， 名称随意，例如命名为rsync.key， 权限必须是600， 并且内容只包含要使用的密码（例如 abc123）\n\n1. 这条命令， 表示将本机/home/test/目录下的所有内容，使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test/ mylog@192.168.1.2::test-module\n   ```\n\n   \n\n2. 注意这里/home/test/改为/home/test ， test-module模块名后面添加了一层子目录/test/ ， 这条命令表示将本机的/home/test目录本身， 使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/test/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test mylog@192.168.1.2::test-module/test/\n   ```\n\n   \n\n3. 相比于第一条命令， 这里修改了最后两个参数的顺序， 表示从192.168.1.2拉取数据到本机的/home/test/目录下\n\n   ```shell\n   /bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key  mylog@192.168.1.2::test-module /home/test/\n   ```","slug":"rsync 使用","published":1,"updated":"2019-05-21T14:45:47.814Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dublk000famum7ma2hxog","content":"<h1 id=\"Rsync-使用\"><a href=\"#Rsync-使用\" class=\"headerlink\" title=\"Rsync 使用\"></a>Rsync 使用</h1><p>1、服务端与客户端的定义</p>\n<p>​    A机器向B机器发送数据，  或者A机器向B机器拉取数据</p>\n<p>​    那么A均为客户端， B均为服务端</p>\n<p>2、是否需要启动rsync服务</p>\n<p>​    客户端不需要， 服务端需要</p>\n<p>​    启动rsync进程： systemctl start rsyncd</p>\n<p>​    设置rsync开机自启动： systemctl enable rsyncd</p>\n<p>3、rsync相关配置</p>\n<p>​    1）模块配置， 固定为/etc/rsyncd.conf, 权限644即可</p>\n<p>​    2）用户密码配置， 可以在/etc/rsyncd.conf配置该文件路径， 推荐使用/etc/rsyncd.secret, 权限必须为600</p>\n<p>​    3）日志回滚配置 /etc/logrotate.d/rsyncd.rotate， 按需添加</p>\n<p>4、rsyncd.conf举例说明</p>\n<blockquote>\n<p>###全局配置， 生效于每个模块， 并会被模块中相同的配置项覆盖</p>\n<p>uid = root</p>\n<p>gid = root</p>\n<p>use chroot = no</p>\n<p>pid file = /var/run/rsyncd.pid</p>\n<p>#设置rsync用户密码配置文件路径</p>\n<p>secrets file = /etc/rsyncd.secret</p>\n<p>timeout = 300</p>\n<p>read only = yes</p>\n<p>write only = no</p>\n<p>#设置最大连接数</p>\n<p>max connections = 2048</p>\n<p>dont compress = <em>.gz </em>.tgz <em>.zip </em>.z <em>.Z </em>.rpm <em>.deb </em>.bz2</p>\n<p>#设置rsync日志文件， 若未设置， 则默认将日志写入syslog， 即/var/log/message，  当前版本的rsync不支持关闭日志， 所以若设置了该项， 记得添加日志回滚， 避免日志文件挤爆磁盘</p>\n<p>log file = /var/log/rsync.log</p>\n<p>syslog facility = local3</p>\n<p>###分模块配置， 注意rsync配置时，注释必须另起一行 </p>\n<p>#模块名</p>\n<p>[test-module]</p>\n<p>#模块根目录，注意该目录若不存在， rsync不会自动创建， 传输将报错</p>\n<p>path = /mylog/test/ori_data/</p>\n<p>#设置为可写</p>\n<p>read only = no</p>\n<p>#rsync用户名</p>\n<p>auth users = mylog</p>\n</blockquote>\n<p>5、rsyncd.secret举例说明</p>\n<p>格式为 用户名:密码 ， 例如</p>\n<p>mylog:abc123</p>\n<p>mylog2:abc456</p>\n<p>6、rsync基础命令</p>\n<p>在使用rsync传输之前， 需要在本机创建一个密码文件， 名称随意，例如命名为rsync.key， 权限必须是600， 并且内容只包含要使用的密码（例如 abc123）</p>\n<ol>\n<li><p>这条命令， 表示将本机/home/test/目录下的所有内容，使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test/ mylog@192.168.1.2::test-module</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>注意这里/home/test/改为/home/test ， test-module模块名后面添加了一层子目录/test/ ， 这条命令表示将本机的/home/test目录本身， 使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/test/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test mylog@192.168.1.2::test-module/test/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>相比于第一条命令， 这里修改了最后两个参数的顺序， 表示从192.168.1.2拉取数据到本机的/home/test/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key  mylog@192.168.1.2::test-module /home/test/</span><br></pre></td></tr></table></figure></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Rsync-使用\"><a href=\"#Rsync-使用\" class=\"headerlink\" title=\"Rsync 使用\"></a>Rsync 使用</h1><p>1、服务端与客户端的定义</p>\n<p>​    A机器向B机器发送数据，  或者A机器向B机器拉取数据</p>\n<p>​    那么A均为客户端， B均为服务端</p>\n<p>2、是否需要启动rsync服务</p>\n<p>​    客户端不需要， 服务端需要</p>\n<p>​    启动rsync进程： systemctl start rsyncd</p>\n<p>​    设置rsync开机自启动： systemctl enable rsyncd</p>\n<p>3、rsync相关配置</p>\n<p>​    1）模块配置， 固定为/etc/rsyncd.conf, 权限644即可</p>\n<p>​    2）用户密码配置， 可以在/etc/rsyncd.conf配置该文件路径， 推荐使用/etc/rsyncd.secret, 权限必须为600</p>\n<p>​    3）日志回滚配置 /etc/logrotate.d/rsyncd.rotate， 按需添加</p>\n<p>4、rsyncd.conf举例说明</p>\n<blockquote>\n<p>###全局配置， 生效于每个模块， 并会被模块中相同的配置项覆盖</p>\n<p>uid = root</p>\n<p>gid = root</p>\n<p>use chroot = no</p>\n<p>pid file = /var/run/rsyncd.pid</p>\n<p>#设置rsync用户密码配置文件路径</p>\n<p>secrets file = /etc/rsyncd.secret</p>\n<p>timeout = 300</p>\n<p>read only = yes</p>\n<p>write only = no</p>\n<p>#设置最大连接数</p>\n<p>max connections = 2048</p>\n<p>dont compress = <em>.gz </em>.tgz <em>.zip </em>.z <em>.Z </em>.rpm <em>.deb </em>.bz2</p>\n<p>#设置rsync日志文件， 若未设置， 则默认将日志写入syslog， 即/var/log/message，  当前版本的rsync不支持关闭日志， 所以若设置了该项， 记得添加日志回滚， 避免日志文件挤爆磁盘</p>\n<p>log file = /var/log/rsync.log</p>\n<p>syslog facility = local3</p>\n<p>###分模块配置， 注意rsync配置时，注释必须另起一行 </p>\n<p>#模块名</p>\n<p>[test-module]</p>\n<p>#模块根目录，注意该目录若不存在， rsync不会自动创建， 传输将报错</p>\n<p>path = /mylog/test/ori_data/</p>\n<p>#设置为可写</p>\n<p>read only = no</p>\n<p>#rsync用户名</p>\n<p>auth users = mylog</p>\n</blockquote>\n<p>5、rsyncd.secret举例说明</p>\n<p>格式为 用户名:密码 ， 例如</p>\n<p>mylog:abc123</p>\n<p>mylog2:abc456</p>\n<p>6、rsync基础命令</p>\n<p>在使用rsync传输之前， 需要在本机创建一个密码文件， 名称随意，例如命名为rsync.key， 权限必须是600， 并且内容只包含要使用的密码（例如 abc123）</p>\n<ol>\n<li><p>这条命令， 表示将本机/home/test/目录下的所有内容，使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test/ mylog@192.168.1.2::test-module</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>注意这里/home/test/改为/home/test ， test-module模块名后面添加了一层子目录/test/ ， 这条命令表示将本机的/home/test目录本身， 使用模块test-module传输到192.168.1.2， 根据模块配置， 这些内容将被传输到/mylog/test/ori_data/test/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key /home/test mylog@192.168.1.2::test-module/test/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>相比于第一条命令， 这里修改了最后两个参数的顺序， 表示从192.168.1.2拉取数据到本机的/home/test/目录下</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/bin/rsync-az–timeout=30–contimeout=15–password-file=rsync.key  mylog@192.168.1.2::test-module /home/test/</span><br></pre></td></tr></table></figure></li>\n</ol>\n"},{"title":"sock5协议原理分析","date":"2016-01-15T14:47:03.000Z","_content":"\n早点两年前就开始使用shadowsocks了,那时候买了搬瓦工一年19$的vps,由于搬瓦工数据中心比较少,且大部分位于美国,经过测试选了个在凤凰城的机房,但是网速一直不稳定时不时的断线,前阵子买了vultr位于东京数据中心的vps,网络稳定,看youtube 1080p 毫无压力.虽然用了很久的shandowsock5,但是一直对sock5协知之甚少,决定解下这个牛逼的翻墙协议,简单的说就是在墙外搭一个sock5服务器,墙内的客户端把tcp数据加密后发往墙外的服务器,server按照sock5协议解密后再发往目标机器\n\n\n<div style=\"align: center\">\n<img src=\"https://yuerblog.cc/wp-content/uploads/2016/11/WechatIMG938.jpeg\" width=\"100%\" height=\"100%\">\n</div>\n\n1. PC客户端（即你的电脑）发出请求基于Socks5协议跟SS-Local端进行通讯，由于这个SS-Local一般是本机或路由器等局域网的其他机器，不经过GFW，所以解决GFW通过特征分析进行干扰的问题\n2. SS-Local和SS-Server两端通过多种可选的加密方法(\"aes-256-cfb\"等)进行通讯，经过GFW的时候因为是常规的TCP包，没有明显特征码GFW也无法对通讯数据进行解密\n3. SS-Server将收到的加密数据进行解密，还原初始请求，再发送到用户需要访问的服务网站，获取响应原路再返回SS-04，返回途中依然使用了加密，使得流量是普通TCP包，并成功穿过GFW防火墙\n\n<!-- more -->\n\n# sock5协议简介\n>wiki:SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递,根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间\n\n根据[rfc1928](https://www.ietf.org/rfc/rfc1928.txt \"rfc1928\")文档的说明,sock5协议设计之初就是为了解决翻墙问题,该协议工作在应用层和传输层,不提供例如转发ICMP消息的网关服务,sock5在sock4协议基础上增加了对udp和ipv6的支持,rfc文档中制定了报文格式:\n1. sock5客户端先给服务端发版本协商报文\n\n| VER   |  NMETHODS  | METHODS  |\n| ---   | --------   | ------   |\n|   1   |    1       |  1-255   | \n- VER字段为0x05\n- NMETHODSB表示加密算法的格式\n- METHODS表示具体的加密算法\n\n\n2. 服务端版本协商响应报文\n\n| VER   |  METHOD  | \n| :---: | :--------: | \n| 1     |    1       |\n\nMETHOD字段表示含义\n- X'00' NO AUTHENTICATION REQUIRED\n- X'01' GSSAPI\n- X'02' USERNAME/PASSWORD\n- X'03' to X'7F' IANA ASSIGNED\n- X'80' to X'FE' RESERVED FOR PRIVATE METHODS\n- X'FF' NO ACCEPTABLE METHODS\n\n3. 协商完毕后client的正常请求\n\n| VER | CMD | RSV | ATYP | DST ADDR | DST PORT |\n| --- |  ---| --- | ---  |  ------- |  ------- |\n|  1  |  1  | 00  |   1  | Variable |    2     |\n\n- VER   protocol version: X'05'\n- CMD \n    - CONNECT X'01'\n    - BIND X'02'\n    - UDP ASSOCIATE X'03'\n- RSV    RESERVED\n- ATYP   address type of following address\n    - IP V4 address: X'01'\n    - DOMAINNAME: X'03'\n    - P V6 address: X'04'\n- DST.ADDR       desired destination address\n- DST.PORT desired destination port in network octet order\n\n当客户端和服务端建立连接后,client会给server发送这样上面格式的请求\n\n4. server的响应\n\n| VER | REP | RSV | ATPY | BND ADDR | BND PORT |\n|  -- | --- | --- | ---  | ----     |  ----    |\n|  1  |  1  | 00  |  1   | Variable |    2     |\n\n- VER    protocol version: X'05'\n- REP    Reply field:\n  - X'00' succeeded\n  - X'01' general SOCKS server failure\n  - X'02' connection not allowed by ruleset\n  - X'03' Network unreachable\n  - X'04' Host unreachable\n  - X'05' Connection refused\n  - X'06' TTL expired\n  - X'07' Command not supported\n  - X'08' Address type not supported\n  - X'09' to X'FF' unassigned\n- RSV    RESERVED\n- ATYP   address type of following address\n\n具体的协议抓包可以参考这篇博客[sock5抓包分析](https://www.skyreal.me/tong-guo-wireshark-zhua-bao-xue-xi-socks5-xie-yi/ \"sock5抓包分析\")\n","source":"_posts/sock5协议原理分析.md","raw":"---\ntitle: sock5协议原理分析\ndate: 2016-01-15 22:47:03\ntags:\n---\n\n早点两年前就开始使用shadowsocks了,那时候买了搬瓦工一年19$的vps,由于搬瓦工数据中心比较少,且大部分位于美国,经过测试选了个在凤凰城的机房,但是网速一直不稳定时不时的断线,前阵子买了vultr位于东京数据中心的vps,网络稳定,看youtube 1080p 毫无压力.虽然用了很久的shandowsock5,但是一直对sock5协知之甚少,决定解下这个牛逼的翻墙协议,简单的说就是在墙外搭一个sock5服务器,墙内的客户端把tcp数据加密后发往墙外的服务器,server按照sock5协议解密后再发往目标机器\n\n\n<div style=\"align: center\">\n<img src=\"https://yuerblog.cc/wp-content/uploads/2016/11/WechatIMG938.jpeg\" width=\"100%\" height=\"100%\">\n</div>\n\n1. PC客户端（即你的电脑）发出请求基于Socks5协议跟SS-Local端进行通讯，由于这个SS-Local一般是本机或路由器等局域网的其他机器，不经过GFW，所以解决GFW通过特征分析进行干扰的问题\n2. SS-Local和SS-Server两端通过多种可选的加密方法(\"aes-256-cfb\"等)进行通讯，经过GFW的时候因为是常规的TCP包，没有明显特征码GFW也无法对通讯数据进行解密\n3. SS-Server将收到的加密数据进行解密，还原初始请求，再发送到用户需要访问的服务网站，获取响应原路再返回SS-04，返回途中依然使用了加密，使得流量是普通TCP包，并成功穿过GFW防火墙\n\n<!-- more -->\n\n# sock5协议简介\n>wiki:SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递,根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间\n\n根据[rfc1928](https://www.ietf.org/rfc/rfc1928.txt \"rfc1928\")文档的说明,sock5协议设计之初就是为了解决翻墙问题,该协议工作在应用层和传输层,不提供例如转发ICMP消息的网关服务,sock5在sock4协议基础上增加了对udp和ipv6的支持,rfc文档中制定了报文格式:\n1. sock5客户端先给服务端发版本协商报文\n\n| VER   |  NMETHODS  | METHODS  |\n| ---   | --------   | ------   |\n|   1   |    1       |  1-255   | \n- VER字段为0x05\n- NMETHODSB表示加密算法的格式\n- METHODS表示具体的加密算法\n\n\n2. 服务端版本协商响应报文\n\n| VER   |  METHOD  | \n| :---: | :--------: | \n| 1     |    1       |\n\nMETHOD字段表示含义\n- X'00' NO AUTHENTICATION REQUIRED\n- X'01' GSSAPI\n- X'02' USERNAME/PASSWORD\n- X'03' to X'7F' IANA ASSIGNED\n- X'80' to X'FE' RESERVED FOR PRIVATE METHODS\n- X'FF' NO ACCEPTABLE METHODS\n\n3. 协商完毕后client的正常请求\n\n| VER | CMD | RSV | ATYP | DST ADDR | DST PORT |\n| --- |  ---| --- | ---  |  ------- |  ------- |\n|  1  |  1  | 00  |   1  | Variable |    2     |\n\n- VER   protocol version: X'05'\n- CMD \n    - CONNECT X'01'\n    - BIND X'02'\n    - UDP ASSOCIATE X'03'\n- RSV    RESERVED\n- ATYP   address type of following address\n    - IP V4 address: X'01'\n    - DOMAINNAME: X'03'\n    - P V6 address: X'04'\n- DST.ADDR       desired destination address\n- DST.PORT desired destination port in network octet order\n\n当客户端和服务端建立连接后,client会给server发送这样上面格式的请求\n\n4. server的响应\n\n| VER | REP | RSV | ATPY | BND ADDR | BND PORT |\n|  -- | --- | --- | ---  | ----     |  ----    |\n|  1  |  1  | 00  |  1   | Variable |    2     |\n\n- VER    protocol version: X'05'\n- REP    Reply field:\n  - X'00' succeeded\n  - X'01' general SOCKS server failure\n  - X'02' connection not allowed by ruleset\n  - X'03' Network unreachable\n  - X'04' Host unreachable\n  - X'05' Connection refused\n  - X'06' TTL expired\n  - X'07' Command not supported\n  - X'08' Address type not supported\n  - X'09' to X'FF' unassigned\n- RSV    RESERVED\n- ATYP   address type of following address\n\n具体的协议抓包可以参考这篇博客[sock5抓包分析](https://www.skyreal.me/tong-guo-wireshark-zhua-bao-xue-xi-socks5-xie-yi/ \"sock5抓包分析\")\n","slug":"sock5协议原理分析","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubll000hamumfxx9coim","content":"<p>早点两年前就开始使用shadowsocks了,那时候买了搬瓦工一年19$的vps,由于搬瓦工数据中心比较少,且大部分位于美国,经过测试选了个在凤凰城的机房,但是网速一直不稳定时不时的断线,前阵子买了vultr位于东京数据中心的vps,网络稳定,看youtube 1080p 毫无压力.虽然用了很久的shandowsock5,但是一直对sock5协知之甚少,决定解下这个牛逼的翻墙协议,简单的说就是在墙外搭一个sock5服务器,墙内的客户端把tcp数据加密后发往墙外的服务器,server按照sock5协议解密后再发往目标机器</p>\n<div style=\"align: center\"><br><img src=\"https://yuerblog.cc/wp-content/uploads/2016/11/WechatIMG938.jpeg\" width=\"100%\" height=\"100%\"><br></div>\n\n<ol>\n<li>PC客户端（即你的电脑）发出请求基于Socks5协议跟SS-Local端进行通讯，由于这个SS-Local一般是本机或路由器等局域网的其他机器，不经过GFW，所以解决GFW通过特征分析进行干扰的问题</li>\n<li>SS-Local和SS-Server两端通过多种可选的加密方法(“aes-256-cfb”等)进行通讯，经过GFW的时候因为是常规的TCP包，没有明显特征码GFW也无法对通讯数据进行解密</li>\n<li>SS-Server将收到的加密数据进行解密，还原初始请求，再发送到用户需要访问的服务网站，获取响应原路再返回SS-04，返回途中依然使用了加密，使得流量是普通TCP包，并成功穿过GFW防火墙</li>\n</ol>\n<a id=\"more\"></a>\n<h1 id=\"sock5协议简介\"><a href=\"#sock5协议简介\" class=\"headerlink\" title=\"sock5协议简介\"></a>sock5协议简介</h1><blockquote>\n<p>wiki:SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递,根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间</p>\n</blockquote>\n<p>根据<a href=\"https://www.ietf.org/rfc/rfc1928.txt\" title=\"rfc1928\" target=\"_blank\" rel=\"noopener\">rfc1928</a>文档的说明,sock5协议设计之初就是为了解决翻墙问题,该协议工作在应用层和传输层,不提供例如转发ICMP消息的网关服务,sock5在sock4协议基础上增加了对udp和ipv6的支持,rfc文档中制定了报文格式:</p>\n<ol>\n<li>sock5客户端先给服务端发版本协商报文</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>NMETHODS</th>\n<th>METHODS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1-255</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER字段为0x05</li>\n<li>NMETHODSB表示加密算法的格式</li>\n<li>METHODS表示具体的加密算法</li>\n</ul>\n<ol>\n<li>服务端版本协商响应报文</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">VER</th>\n<th style=\"text-align:center\">METHOD</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<p>METHOD字段表示含义</p>\n<ul>\n<li>X’00’ NO AUTHENTICATION REQUIRED</li>\n<li>X’01’ GSSAPI</li>\n<li>X’02’ USERNAME/PASSWORD</li>\n<li>X’03’ to X’7F’ IANA ASSIGNED</li>\n<li>X’80’ to X’FE’ RESERVED FOR PRIVATE METHODS</li>\n<li>X’FF’ NO ACCEPTABLE METHODS</li>\n</ul>\n<ol>\n<li>协商完毕后client的正常请求</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>CMD</th>\n<th>RSV</th>\n<th>ATYP</th>\n<th>DST ADDR</th>\n<th>DST PORT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>00</td>\n<td>1</td>\n<td>Variable</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER   protocol version: X’05’</li>\n<li>CMD <ul>\n<li>CONNECT X’01’</li>\n<li>BIND X’02’</li>\n<li>UDP ASSOCIATE X’03’</li>\n</ul>\n</li>\n<li>RSV    RESERVED</li>\n<li>ATYP   address type of following address<ul>\n<li>IP V4 address: X’01’</li>\n<li>DOMAINNAME: X’03’</li>\n<li>P V6 address: X’04’</li>\n</ul>\n</li>\n<li>DST.ADDR       desired destination address</li>\n<li>DST.PORT desired destination port in network octet order</li>\n</ul>\n<p>当客户端和服务端建立连接后,client会给server发送这样上面格式的请求</p>\n<ol>\n<li>server的响应</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>REP</th>\n<th>RSV</th>\n<th>ATPY</th>\n<th>BND ADDR</th>\n<th>BND PORT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>00</td>\n<td>1</td>\n<td>Variable</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER    protocol version: X’05’</li>\n<li>REP    Reply field:<ul>\n<li>X’00’ succeeded</li>\n<li>X’01’ general SOCKS server failure</li>\n<li>X’02’ connection not allowed by ruleset</li>\n<li>X’03’ Network unreachable</li>\n<li>X’04’ Host unreachable</li>\n<li>X’05’ Connection refused</li>\n<li>X’06’ TTL expired</li>\n<li>X’07’ Command not supported</li>\n<li>X’08’ Address type not supported</li>\n<li>X’09’ to X’FF’ unassigned</li>\n</ul>\n</li>\n<li>RSV    RESERVED</li>\n<li>ATYP   address type of following address</li>\n</ul>\n<p>具体的协议抓包可以参考这篇博客<a href=\"https://www.skyreal.me/tong-guo-wireshark-zhua-bao-xue-xi-socks5-xie-yi/\" title=\"sock5抓包分析\" target=\"_blank\" rel=\"noopener\">sock5抓包分析</a></p>\n","site":{"data":{}},"excerpt":"<p>早点两年前就开始使用shadowsocks了,那时候买了搬瓦工一年19$的vps,由于搬瓦工数据中心比较少,且大部分位于美国,经过测试选了个在凤凰城的机房,但是网速一直不稳定时不时的断线,前阵子买了vultr位于东京数据中心的vps,网络稳定,看youtube 1080p 毫无压力.虽然用了很久的shandowsock5,但是一直对sock5协知之甚少,决定解下这个牛逼的翻墙协议,简单的说就是在墙外搭一个sock5服务器,墙内的客户端把tcp数据加密后发往墙外的服务器,server按照sock5协议解密后再发往目标机器</p>\n<div style=\"align: center\"><br><img src=\"https://yuerblog.cc/wp-content/uploads/2016/11/WechatIMG938.jpeg\" width=\"100%\" height=\"100%\"><br></div>\n\n<ol>\n<li>PC客户端（即你的电脑）发出请求基于Socks5协议跟SS-Local端进行通讯，由于这个SS-Local一般是本机或路由器等局域网的其他机器，不经过GFW，所以解决GFW通过特征分析进行干扰的问题</li>\n<li>SS-Local和SS-Server两端通过多种可选的加密方法(“aes-256-cfb”等)进行通讯，经过GFW的时候因为是常规的TCP包，没有明显特征码GFW也无法对通讯数据进行解密</li>\n<li>SS-Server将收到的加密数据进行解密，还原初始请求，再发送到用户需要访问的服务网站，获取响应原路再返回SS-04，返回途中依然使用了加密，使得流量是普通TCP包，并成功穿过GFW防火墙</li>\n</ol>","more":"<h1 id=\"sock5协议简介\"><a href=\"#sock5协议简介\" class=\"headerlink\" title=\"sock5协议简介\"></a>sock5协议简介</h1><blockquote>\n<p>wiki:SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递,根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间</p>\n</blockquote>\n<p>根据<a href=\"https://www.ietf.org/rfc/rfc1928.txt\" title=\"rfc1928\" target=\"_blank\" rel=\"noopener\">rfc1928</a>文档的说明,sock5协议设计之初就是为了解决翻墙问题,该协议工作在应用层和传输层,不提供例如转发ICMP消息的网关服务,sock5在sock4协议基础上增加了对udp和ipv6的支持,rfc文档中制定了报文格式:</p>\n<ol>\n<li>sock5客户端先给服务端发版本协商报文</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>NMETHODS</th>\n<th>METHODS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1-255</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER字段为0x05</li>\n<li>NMETHODSB表示加密算法的格式</li>\n<li>METHODS表示具体的加密算法</li>\n</ul>\n<ol>\n<li>服务端版本协商响应报文</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">VER</th>\n<th style=\"text-align:center\">METHOD</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n<p>METHOD字段表示含义</p>\n<ul>\n<li>X’00’ NO AUTHENTICATION REQUIRED</li>\n<li>X’01’ GSSAPI</li>\n<li>X’02’ USERNAME/PASSWORD</li>\n<li>X’03’ to X’7F’ IANA ASSIGNED</li>\n<li>X’80’ to X’FE’ RESERVED FOR PRIVATE METHODS</li>\n<li>X’FF’ NO ACCEPTABLE METHODS</li>\n</ul>\n<ol>\n<li>协商完毕后client的正常请求</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>CMD</th>\n<th>RSV</th>\n<th>ATYP</th>\n<th>DST ADDR</th>\n<th>DST PORT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>00</td>\n<td>1</td>\n<td>Variable</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER   protocol version: X’05’</li>\n<li>CMD <ul>\n<li>CONNECT X’01’</li>\n<li>BIND X’02’</li>\n<li>UDP ASSOCIATE X’03’</li>\n</ul>\n</li>\n<li>RSV    RESERVED</li>\n<li>ATYP   address type of following address<ul>\n<li>IP V4 address: X’01’</li>\n<li>DOMAINNAME: X’03’</li>\n<li>P V6 address: X’04’</li>\n</ul>\n</li>\n<li>DST.ADDR       desired destination address</li>\n<li>DST.PORT desired destination port in network octet order</li>\n</ul>\n<p>当客户端和服务端建立连接后,client会给server发送这样上面格式的请求</p>\n<ol>\n<li>server的响应</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>VER</th>\n<th>REP</th>\n<th>RSV</th>\n<th>ATPY</th>\n<th>BND ADDR</th>\n<th>BND PORT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>00</td>\n<td>1</td>\n<td>Variable</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>VER    protocol version: X’05’</li>\n<li>REP    Reply field:<ul>\n<li>X’00’ succeeded</li>\n<li>X’01’ general SOCKS server failure</li>\n<li>X’02’ connection not allowed by ruleset</li>\n<li>X’03’ Network unreachable</li>\n<li>X’04’ Host unreachable</li>\n<li>X’05’ Connection refused</li>\n<li>X’06’ TTL expired</li>\n<li>X’07’ Command not supported</li>\n<li>X’08’ Address type not supported</li>\n<li>X’09’ to X’FF’ unassigned</li>\n</ul>\n</li>\n<li>RSV    RESERVED</li>\n<li>ATYP   address type of following address</li>\n</ul>\n<p>具体的协议抓包可以参考这篇博客<a href=\"https://www.skyreal.me/tong-guo-wireshark-zhua-bao-xue-xi-socks5-xie-yi/\" title=\"sock5抓包分析\" target=\"_blank\" rel=\"noopener\">sock5抓包分析</a></p>"},{"title":"理解Go Channel","date":"2018-05-23T13:21:46.000Z","_content":"\n\n## 理解Go Channel\n\nCSP 是 Communicating Sequential Process 的简称，中文可以叫做通信顺序进程，是一种并发编程模型，由 [Tony Hoare](https://en.wikipedia.org/wiki/Tony_Hoare) 于 1977 年提出。简单来说，CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里发送消息时使用的就是通道，或者叫 channel。CSP 模型的关键是关注 channel，而不关注发送消息的实体。Go 语言实现了 CSP 部分理论，goroutine 对应 CSP 中并发执行的实体，channel 也就对应着 CSP 中的 channel。\n\n### Channel类型\n\n```go\nchan T          // 可以接收和发送类型为 T 的数据\nchan<- float64  // 只可以用来发送 float64 类型的数据\n<-chan int      // 只可以用来接收 int 类型的数据\n```\n\n<-总是优先和最左边的类型结合\n\n```go\nchan<- chan int    // 等价 chan<- (chan int)\nchan<- <-chan int  // 等价 chan<- (<-chan int)\n<-chan <-chan int  // 等价 <-chan (<-chan int)\nchan (<-chan int)\n```\n\n\n\n### Channel创建\n\n使用`make`初始化Channel,并且可以设置容量\n\n```go\nunBufferChan := make(chan int)  // 1\nbufferChan := make(chan int, N) // 2\n```\n\n上面的方式 1 创建的是无缓冲 channel，方式 2 创建的是缓冲 channel。如果使用 channel 之前没有 make，会出现 dead lock 错误。\n\n```\nfatal error: all goroutines are asleep - deadlock!\n\ngoroutine 1 [chan receive (nil chan)]:\nmain.main()\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:8 +0x4a\n\ngoroutine 4 [chan send (nil chan)]:\nmain.main.func1(0x0)\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:6 +0x37\ncreated by main.main\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:5 +0x3e\nexit status 2\n```\n\n\n\n### Channel发送和接收\n\n```go\nch := make(chan int, 10)\n\n// 读操作\nx <- ch\n\n// 写操作\nch <- x\n```\n\nchannel 分为无缓冲 channel 和有缓冲 channel。\n\n- 无缓冲：发送和接收动作是同时发生的。如果没有 goroutine 读取 channel （<- channel），则发送者 (channel <-) 会一直阻塞\n- 缓冲：缓冲 channel 类似一个有容量的队列。当队列满的时候发送者会阻塞；当队列空的时候接收者会阻塞。\n\n\n\n### Channel关闭\n\n```go\nch := make(chan int)\n\n// 关闭\nclose(ch)\n```\n\n- 重复关闭 channel 会导致 panic\n- 向关闭的 channel 发送数据会 panic\n- 从关闭的 channel 读数据不会 panic，读出 channel 中已有的数据之后再读就是 channel 类似的默认值，比如 chan int 类型的 channel 关闭之后读取到的值为 0\n\n```go\nch := make(chan int, 10)\n...\nclose(ch)\n\n// ok-idiom \nval, ok := <-ch\nif ok == false {\n    // channel closed\n}\n```\n\n\n\n### Channel Range\n\n```go\nfunc consumer(ch chan int) {\n    ...\n    for x := range ch {\n        fmt.Println(x)\n        ...\n    }\n}\n\nfunc producer(ch chan int) {\n  ...\n  for _, v := range values {\n      ch <- v\n  }  \n}\n```\n\n\n\n### Channel Select\n\nselect会一致阻塞直到有case满足条件，select通常和for循环一起用。for + select + time.After可以实现超时，time.After返回一个类型为`<-chan Time`的单向的channel\n\n```go\nfor {\n    select {\n        case a <- ch1:\n        \tbreak\n    \tcase b <- ch2:\n        \tbreak\n        case <- time.After(2 * time.Second)\n        \tbreak\n        default:\n        \tbreak\n    }\n}\n```\n\n\n\n### 参考\n1. [Go Concurrency Patterns: Pipelines and cancellation](https://blog.golang.org/pipelines)\n1. [Go Channel 详解](https://colobu.com/2016/04/14/Golang-Channels/)\n2. [深入理解Go Channel](http://legendtkl.com/2017/07/30/understanding-golang-channel/)","source":"_posts/理解channel.md","raw":"---\ntitle: 理解Go Channel\ndate: 2018-05-23 21:21:46\ntags: go\n---\n\n\n## 理解Go Channel\n\nCSP 是 Communicating Sequential Process 的简称，中文可以叫做通信顺序进程，是一种并发编程模型，由 [Tony Hoare](https://en.wikipedia.org/wiki/Tony_Hoare) 于 1977 年提出。简单来说，CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里发送消息时使用的就是通道，或者叫 channel。CSP 模型的关键是关注 channel，而不关注发送消息的实体。Go 语言实现了 CSP 部分理论，goroutine 对应 CSP 中并发执行的实体，channel 也就对应着 CSP 中的 channel。\n\n### Channel类型\n\n```go\nchan T          // 可以接收和发送类型为 T 的数据\nchan<- float64  // 只可以用来发送 float64 类型的数据\n<-chan int      // 只可以用来接收 int 类型的数据\n```\n\n<-总是优先和最左边的类型结合\n\n```go\nchan<- chan int    // 等价 chan<- (chan int)\nchan<- <-chan int  // 等价 chan<- (<-chan int)\n<-chan <-chan int  // 等价 <-chan (<-chan int)\nchan (<-chan int)\n```\n\n\n\n### Channel创建\n\n使用`make`初始化Channel,并且可以设置容量\n\n```go\nunBufferChan := make(chan int)  // 1\nbufferChan := make(chan int, N) // 2\n```\n\n上面的方式 1 创建的是无缓冲 channel，方式 2 创建的是缓冲 channel。如果使用 channel 之前没有 make，会出现 dead lock 错误。\n\n```\nfatal error: all goroutines are asleep - deadlock!\n\ngoroutine 1 [chan receive (nil chan)]:\nmain.main()\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:8 +0x4a\n\ngoroutine 4 [chan send (nil chan)]:\nmain.main.func1(0x0)\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:6 +0x37\ncreated by main.main\n\t/Users/knife/Work/GoWorkplace/src/test/go.go:5 +0x3e\nexit status 2\n```\n\n\n\n### Channel发送和接收\n\n```go\nch := make(chan int, 10)\n\n// 读操作\nx <- ch\n\n// 写操作\nch <- x\n```\n\nchannel 分为无缓冲 channel 和有缓冲 channel。\n\n- 无缓冲：发送和接收动作是同时发生的。如果没有 goroutine 读取 channel （<- channel），则发送者 (channel <-) 会一直阻塞\n- 缓冲：缓冲 channel 类似一个有容量的队列。当队列满的时候发送者会阻塞；当队列空的时候接收者会阻塞。\n\n\n\n### Channel关闭\n\n```go\nch := make(chan int)\n\n// 关闭\nclose(ch)\n```\n\n- 重复关闭 channel 会导致 panic\n- 向关闭的 channel 发送数据会 panic\n- 从关闭的 channel 读数据不会 panic，读出 channel 中已有的数据之后再读就是 channel 类似的默认值，比如 chan int 类型的 channel 关闭之后读取到的值为 0\n\n```go\nch := make(chan int, 10)\n...\nclose(ch)\n\n// ok-idiom \nval, ok := <-ch\nif ok == false {\n    // channel closed\n}\n```\n\n\n\n### Channel Range\n\n```go\nfunc consumer(ch chan int) {\n    ...\n    for x := range ch {\n        fmt.Println(x)\n        ...\n    }\n}\n\nfunc producer(ch chan int) {\n  ...\n  for _, v := range values {\n      ch <- v\n  }  \n}\n```\n\n\n\n### Channel Select\n\nselect会一致阻塞直到有case满足条件，select通常和for循环一起用。for + select + time.After可以实现超时，time.After返回一个类型为`<-chan Time`的单向的channel\n\n```go\nfor {\n    select {\n        case a <- ch1:\n        \tbreak\n    \tcase b <- ch2:\n        \tbreak\n        case <- time.After(2 * time.Second)\n        \tbreak\n        default:\n        \tbreak\n    }\n}\n```\n\n\n\n### 参考\n1. [Go Concurrency Patterns: Pipelines and cancellation](https://blog.golang.org/pipelines)\n1. [Go Channel 详解](https://colobu.com/2016/04/14/Golang-Channels/)\n2. [深入理解Go Channel](http://legendtkl.com/2017/07/30/understanding-golang-channel/)","slug":"理解channel","published":1,"updated":"2019-01-22T16:04:23.452Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dublm000jamum5yuk8ixk","content":"<h2 id=\"理解Go-Channel\"><a href=\"#理解Go-Channel\" class=\"headerlink\" title=\"理解Go Channel\"></a>理解Go Channel</h2><p>CSP 是 Communicating Sequential Process 的简称，中文可以叫做通信顺序进程，是一种并发编程模型，由 <a href=\"https://en.wikipedia.org/wiki/Tony_Hoare\" target=\"_blank\" rel=\"noopener\">Tony Hoare</a> 于 1977 年提出。简单来说，CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里发送消息时使用的就是通道，或者叫 channel。CSP 模型的关键是关注 channel，而不关注发送消息的实体。Go 语言实现了 CSP 部分理论，goroutine 对应 CSP 中并发执行的实体，channel 也就对应着 CSP 中的 channel。</p>\n<h3 id=\"Channel类型\"><a href=\"#Channel类型\" class=\"headerlink\" title=\"Channel类型\"></a>Channel类型</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">chan</span> T          <span class=\"comment\">// 可以接收和发送类型为 T 的数据</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- <span class=\"keyword\">float64</span>  <span class=\"comment\">// 只可以用来发送 float64 类型的数据</span></span><br><span class=\"line\">&lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>      <span class=\"comment\">// 只可以用来接收 int 类型的数据</span></span><br></pre></td></tr></table></figure>\n<p>&lt;-总是优先和最左边的类型结合</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>    <span class=\"comment\">// 等价 chan&lt;- (chan int)</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- &lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>  <span class=\"comment\">// 等价 chan&lt;- (&lt;-chan int)</span></span><br><span class=\"line\">&lt;-<span class=\"keyword\">chan</span> &lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>  <span class=\"comment\">// 等价 &lt;-chan (&lt;-chan int)</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span> (&lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel创建\"><a href=\"#Channel创建\" class=\"headerlink\" title=\"Channel创建\"></a>Channel创建</h3><p>使用<code>make</code>初始化Channel,并且可以设置容量</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unBufferChan := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)  <span class=\"comment\">// 1</span></span><br><span class=\"line\">bufferChan := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, N) <span class=\"comment\">// 2</span></span><br></pre></td></tr></table></figure>\n<p>上面的方式 1 创建的是无缓冲 channel，方式 2 创建的是缓冲 channel。如果使用 channel 之前没有 make，会出现 dead lock 错误。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fatal error: all goroutines are asleep - deadlock!</span><br><span class=\"line\"></span><br><span class=\"line\">goroutine 1 [chan receive (nil chan)]:</span><br><span class=\"line\">main.main()</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:8 +0x4a</span><br><span class=\"line\"></span><br><span class=\"line\">goroutine 4 [chan send (nil chan)]:</span><br><span class=\"line\">main.main.func1(0x0)</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:6 +0x37</span><br><span class=\"line\">created by main.main</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:5 +0x3e</span><br><span class=\"line\">exit status 2</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel发送和接收\"><a href=\"#Channel发送和接收\" class=\"headerlink\" title=\"Channel发送和接收\"></a>Channel发送和接收</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 读操作</span></span><br><span class=\"line\">x &lt;- ch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 写操作</span></span><br><span class=\"line\">ch &lt;- x</span><br></pre></td></tr></table></figure>\n<p>channel 分为无缓冲 channel 和有缓冲 channel。</p>\n<ul>\n<li>无缓冲：发送和接收动作是同时发生的。如果没有 goroutine 读取 channel （&lt;- channel），则发送者 (channel &lt;-) 会一直阻塞</li>\n<li>缓冲：缓冲 channel 类似一个有容量的队列。当队列满的时候发送者会阻塞；当队列空的时候接收者会阻塞。</li>\n</ul>\n<h3 id=\"Channel关闭\"><a href=\"#Channel关闭\" class=\"headerlink\" title=\"Channel关闭\"></a>Channel关闭</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 关闭</span></span><br><span class=\"line\"><span class=\"built_in\">close</span>(ch)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>重复关闭 channel 会导致 panic</li>\n<li>向关闭的 channel 发送数据会 panic</li>\n<li>从关闭的 channel 读数据不会 panic，读出 channel 中已有的数据之后再读就是 channel 类似的默认值，比如 chan int 类型的 channel 关闭之后读取到的值为 0</li>\n</ul>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"built_in\">close</span>(ch)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ok-idiom </span></span><br><span class=\"line\">val, ok := &lt;-ch</span><br><span class=\"line\"><span class=\"keyword\">if</span> ok == <span class=\"literal\">false</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// channel closed</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel-Range\"><a href=\"#Channel-Range\" class=\"headerlink\" title=\"Channel Range\"></a>Channel Range</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">consumer</span><span class=\"params\">(ch <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x := <span class=\"keyword\">range</span> ch &#123;</span><br><span class=\"line\">        fmt.Println(x)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">producer</span><span class=\"params\">(ch <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  <span class=\"keyword\">for</span> _, v := <span class=\"keyword\">range</span> values &#123;</span><br><span class=\"line\">      ch &lt;- v</span><br><span class=\"line\">  &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel-Select\"><a href=\"#Channel-Select\" class=\"headerlink\" title=\"Channel Select\"></a>Channel Select</h3><p>select会一致阻塞直到有case满足条件，select通常和for循环一起用。for + select + time.After可以实现超时，time.After返回一个类型为<code>&lt;-chan Time</code>的单向的channel</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> a &lt;- ch1:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">    \t<span class=\"keyword\">case</span> b &lt;- ch2:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">case</span> &lt;- time.After(<span class=\"number\">2</span> * time.Second)</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ol>\n<li><a href=\"https://blog.golang.org/pipelines\" target=\"_blank\" rel=\"noopener\">Go Concurrency Patterns: Pipelines and cancellation</a></li>\n<li><a href=\"https://colobu.com/2016/04/14/Golang-Channels/\" target=\"_blank\" rel=\"noopener\">Go Channel 详解</a></li>\n<li><a href=\"http://legendtkl.com/2017/07/30/understanding-golang-channel/\" target=\"_blank\" rel=\"noopener\">深入理解Go Channel</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"理解Go-Channel\"><a href=\"#理解Go-Channel\" class=\"headerlink\" title=\"理解Go Channel\"></a>理解Go Channel</h2><p>CSP 是 Communicating Sequential Process 的简称，中文可以叫做通信顺序进程，是一种并发编程模型，由 <a href=\"https://en.wikipedia.org/wiki/Tony_Hoare\" target=\"_blank\" rel=\"noopener\">Tony Hoare</a> 于 1977 年提出。简单来说，CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里发送消息时使用的就是通道，或者叫 channel。CSP 模型的关键是关注 channel，而不关注发送消息的实体。Go 语言实现了 CSP 部分理论，goroutine 对应 CSP 中并发执行的实体，channel 也就对应着 CSP 中的 channel。</p>\n<h3 id=\"Channel类型\"><a href=\"#Channel类型\" class=\"headerlink\" title=\"Channel类型\"></a>Channel类型</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">chan</span> T          <span class=\"comment\">// 可以接收和发送类型为 T 的数据</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- <span class=\"keyword\">float64</span>  <span class=\"comment\">// 只可以用来发送 float64 类型的数据</span></span><br><span class=\"line\">&lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>      <span class=\"comment\">// 只可以用来接收 int 类型的数据</span></span><br></pre></td></tr></table></figure>\n<p>&lt;-总是优先和最左边的类型结合</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>    <span class=\"comment\">// 等价 chan&lt;- (chan int)</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span>&lt;- &lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>  <span class=\"comment\">// 等价 chan&lt;- (&lt;-chan int)</span></span><br><span class=\"line\">&lt;-<span class=\"keyword\">chan</span> &lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>  <span class=\"comment\">// 等价 &lt;-chan (&lt;-chan int)</span></span><br><span class=\"line\"><span class=\"keyword\">chan</span> (&lt;-<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel创建\"><a href=\"#Channel创建\" class=\"headerlink\" title=\"Channel创建\"></a>Channel创建</h3><p>使用<code>make</code>初始化Channel,并且可以设置容量</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unBufferChan := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)  <span class=\"comment\">// 1</span></span><br><span class=\"line\">bufferChan := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, N) <span class=\"comment\">// 2</span></span><br></pre></td></tr></table></figure>\n<p>上面的方式 1 创建的是无缓冲 channel，方式 2 创建的是缓冲 channel。如果使用 channel 之前没有 make，会出现 dead lock 错误。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fatal error: all goroutines are asleep - deadlock!</span><br><span class=\"line\"></span><br><span class=\"line\">goroutine 1 [chan receive (nil chan)]:</span><br><span class=\"line\">main.main()</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:8 +0x4a</span><br><span class=\"line\"></span><br><span class=\"line\">goroutine 4 [chan send (nil chan)]:</span><br><span class=\"line\">main.main.func1(0x0)</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:6 +0x37</span><br><span class=\"line\">created by main.main</span><br><span class=\"line\">\t/Users/knife/Work/GoWorkplace/src/test/go.go:5 +0x3e</span><br><span class=\"line\">exit status 2</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel发送和接收\"><a href=\"#Channel发送和接收\" class=\"headerlink\" title=\"Channel发送和接收\"></a>Channel发送和接收</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 读操作</span></span><br><span class=\"line\">x &lt;- ch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 写操作</span></span><br><span class=\"line\">ch &lt;- x</span><br></pre></td></tr></table></figure>\n<p>channel 分为无缓冲 channel 和有缓冲 channel。</p>\n<ul>\n<li>无缓冲：发送和接收动作是同时发生的。如果没有 goroutine 读取 channel （&lt;- channel），则发送者 (channel &lt;-) 会一直阻塞</li>\n<li>缓冲：缓冲 channel 类似一个有容量的队列。当队列满的时候发送者会阻塞；当队列空的时候接收者会阻塞。</li>\n</ul>\n<h3 id=\"Channel关闭\"><a href=\"#Channel关闭\" class=\"headerlink\" title=\"Channel关闭\"></a>Channel关闭</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 关闭</span></span><br><span class=\"line\"><span class=\"built_in\">close</span>(ch)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>重复关闭 channel 会导致 panic</li>\n<li>向关闭的 channel 发送数据会 panic</li>\n<li>从关闭的 channel 读数据不会 panic，读出 channel 中已有的数据之后再读就是 channel 类似的默认值，比如 chan int 类型的 channel 关闭之后读取到的值为 0</li>\n</ul>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ch := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"built_in\">close</span>(ch)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// ok-idiom </span></span><br><span class=\"line\">val, ok := &lt;-ch</span><br><span class=\"line\"><span class=\"keyword\">if</span> ok == <span class=\"literal\">false</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// channel closed</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel-Range\"><a href=\"#Channel-Range\" class=\"headerlink\" title=\"Channel Range\"></a>Channel Range</h3><figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">consumer</span><span class=\"params\">(ch <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> x := <span class=\"keyword\">range</span> ch &#123;</span><br><span class=\"line\">        fmt.Println(x)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">producer</span><span class=\"params\">(ch <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  <span class=\"keyword\">for</span> _, v := <span class=\"keyword\">range</span> values &#123;</span><br><span class=\"line\">      ch &lt;- v</span><br><span class=\"line\">  &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Channel-Select\"><a href=\"#Channel-Select\" class=\"headerlink\" title=\"Channel Select\"></a>Channel Select</h3><p>select会一致阻塞直到有case满足条件，select通常和for循环一起用。for + select + time.After可以实现超时，time.After返回一个类型为<code>&lt;-chan Time</code>的单向的channel</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> a &lt;- ch1:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">    \t<span class=\"keyword\">case</span> b &lt;- ch2:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">case</span> &lt;- time.After(<span class=\"number\">2</span> * time.Second)</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">        \t<span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ol>\n<li><a href=\"https://blog.golang.org/pipelines\" target=\"_blank\" rel=\"noopener\">Go Concurrency Patterns: Pipelines and cancellation</a></li>\n<li><a href=\"https://colobu.com/2016/04/14/Golang-Channels/\" target=\"_blank\" rel=\"noopener\">Go Channel 详解</a></li>\n<li><a href=\"http://legendtkl.com/2017/07/30/understanding-golang-channel/\" target=\"_blank\" rel=\"noopener\">深入理解Go Channel</a></li>\n</ol>\n"},{"title":"理解Spark闭包","date":"2018-05-21T11:15:39.000Z","_content":"\n\n# 理解spark闭包\n\n什么叫闭包： 跨作用域访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。\n\nSpark闭包的问题引出： \n在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。这个和我们编写单机程序的认识有很大不同。为什么呢？\n\n```\nobject Test {\n  def main(args:Array[String]):Unit = {\n      val conf = new SparkConf().setAppName(\"test\");\n      val sc = new SparkContext(conf)\n\n      val rdd = sc.parallelize(List(1,2,3))\n      var counter = 0\n      //warn: don't do this\n      rdd.foreach(x => counter += x)\n      println(\"Counter value: \"+counter)\n\n      sc.stop()\n    }\n}1234567891011121314\n```\n\n问题分析： \ncounter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的“跨域”问题，也就是闭包问题。\n\n问题解释： \n对于上面程序中的counter变量，由于在`main函数`和在`rdd对象的foreach函数`是属于不同“闭包”的，所以，传进foreach中的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终打印出来的counter为0.\n\n当用户提交了一个用scala语言写的Spark程序，Spark框架会调用哪些组件呢？首先，这个Spark程序就是一个“Application”，程序里面的mian函数就是“Driver Program”， 前面已经讲到它的作用，只是，dirver程序的可能运行在客户端，也有可有可能运行在spark集群中，这取决于spark作业提交时参数的选定，比如，yarn-client和yarn-cluster就是分别运行在客户端和spark集群中。在driver程序中会有RDD对象的相关代码操作，比如下面代码的newRDD.map()\n\n```\nclass Test{\n  def main(args: Array[String]) {\n    val sc = new SparkContext(new SparkConf())\n    val newRDD = sc.textFile(\"\")\n\n    newRDD.map(data => {\n      //do something\n      println(data.toString)\n    })\n  }\n}\n```\n\n涉及到RDD的代码，比如上面RDD的map操作，它们是在Worker节点上面运行的，所以spark会透明地帮用户把这些涉及到RDD操作的代码传给相应的worker节点。如果在RDD map函数中调用了在函数外部定义的对象，因为这些对象需要通过网络从driver所在节点传给其他的worker节点，所以要求这些类是可序列化的，比如在Java或者scala中实现Serializable类，除了java这种序列化机制，还可以选择其他方式，使得序列化工作更加高效。worker节点接收到程序之后，在spark资源管理器的指挥下运行RDD程序。不同worker节点之间的运行操作是并行的。\n\n 在worker节点上所运行的RDD中代码的变量是保存在worker节点上面的，在spark编程中，很多时候用户需要在driver程序中进行相关数据操作之后把该数据传给RDD对象的方法以做进一步处理，这时候，spark框架会自动帮用户把这些数据通过网络传给相应的worker节点。除了这种以变量的形式定义传输数据到worker节点之外，spark还另外提供了两种机制，分别是broadcast和accumulator。相比于变量的方式，在一定场景下使用broadcast比较有优势，因为所广播的数据在每一个worker节点上面只存一个副本，而在spark算子中使用到的外部变量会在每一个用到它的task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。\n\n\n\n**理解闭包**\n\n \n\n​      Spark中理解起来比较困难的一点是当代码在集群上运行时变量和方法的生命周期和作用域(scope)。当作用于RDD上的操作修改了超出它们作用域范围的变量时，会引起一些混淆。为了说明这个问题，使用下面的例子。该例中使用foreach()，对counter(计数器)进行增加，相同的问题也会发生在其他操作中。\n\n \n\n \n\n**例子**\n\n​      下面的例子在以本地模式运行(--master = local[n]) 和将它部署到集群中 (例如通过 spark-submit 提交到 YARN)对比发现会产生不同的结果。\n\n```scala\nvar counter =  0 \nvar rdd = sc.parallelize(data)\n// 错误,请不要这样做！！\nrdd.foreach(x => counter += x)\nprintln( \"Counter value: \"  + counter)\n```\n\n\n\n**本地模式 vs. 集群模式**\n\n​      这里主要的挑战是上面代码的行为是有歧义的。以本地模式运行在单个JVM上，上面的代码会将RDD中的值进行累加，并且将它存储到counter中。这是因为RDD和变量counter在driver节点的相同内存空间中。\n      然而，以集群模式运行时，会更加复杂，上面的代码的结果也许不会如我们预期的那样。当执行一个作业(job)时,Spark会将RDD分成多个任务(task)--每一个任务都会由一个executor来执行。在执行之前，Spark会计算闭包(closure)。闭包是对executors可见的那部分变量和方法，executors会用闭包来执行RDD上的计算(在这个例子中，闭包是foreach())。这个闭包是被序列化的，并且发送给每个executor。在本地模式中，只有一个executor，所以共享相同的闭包。然而，在集群模式中，就不是这样了。executors会运行在各自的worker节点中，每个executor都有闭包的一个复本。\n      发送给每个executor的闭包中的变量其实也是复本。每个foreach函数中引用的counter不再是driver节点上的counter。当然，在driver节点的内存中仍然存在这一个counter，但是这个counter对于executors来说是不可见的。executors只能看到自己的闭包中的复本。这样，counter最后的值仍旧是0，因为所有在counter的操作只引用了序列化闭包中的值。\n      为了在这样的场景中，确保这些行为正确，应该使用累加变量(Accumulator)。在集群中跨节点工作时，Spark中的累加变量提供了一种安全的机制来更新变量。所以可变的全局状态应该使用累加变量来定义。\n\n所以上面的例子可以这样写：\n\n```scala\n// counter现在是累加变量\nvar counter = sc.accumulator( 0) \nvar rdd = sc.parallelize(data) \nrdd.foreach(x => counter += x) \nprintln( \"Counter value: \" + counter)\n```\n\n","source":"_posts/理解spark闭包.md","raw":"---\ntitle: 理解Spark闭包\ndate: 2018-05-21 19:15:39\ntags: Spark\n---\n\n\n# 理解spark闭包\n\n什么叫闭包： 跨作用域访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。\n\nSpark闭包的问题引出： \n在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。这个和我们编写单机程序的认识有很大不同。为什么呢？\n\n```\nobject Test {\n  def main(args:Array[String]):Unit = {\n      val conf = new SparkConf().setAppName(\"test\");\n      val sc = new SparkContext(conf)\n\n      val rdd = sc.parallelize(List(1,2,3))\n      var counter = 0\n      //warn: don't do this\n      rdd.foreach(x => counter += x)\n      println(\"Counter value: \"+counter)\n\n      sc.stop()\n    }\n}1234567891011121314\n```\n\n问题分析： \ncounter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的“跨域”问题，也就是闭包问题。\n\n问题解释： \n对于上面程序中的counter变量，由于在`main函数`和在`rdd对象的foreach函数`是属于不同“闭包”的，所以，传进foreach中的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终打印出来的counter为0.\n\n当用户提交了一个用scala语言写的Spark程序，Spark框架会调用哪些组件呢？首先，这个Spark程序就是一个“Application”，程序里面的mian函数就是“Driver Program”， 前面已经讲到它的作用，只是，dirver程序的可能运行在客户端，也有可有可能运行在spark集群中，这取决于spark作业提交时参数的选定，比如，yarn-client和yarn-cluster就是分别运行在客户端和spark集群中。在driver程序中会有RDD对象的相关代码操作，比如下面代码的newRDD.map()\n\n```\nclass Test{\n  def main(args: Array[String]) {\n    val sc = new SparkContext(new SparkConf())\n    val newRDD = sc.textFile(\"\")\n\n    newRDD.map(data => {\n      //do something\n      println(data.toString)\n    })\n  }\n}\n```\n\n涉及到RDD的代码，比如上面RDD的map操作，它们是在Worker节点上面运行的，所以spark会透明地帮用户把这些涉及到RDD操作的代码传给相应的worker节点。如果在RDD map函数中调用了在函数外部定义的对象，因为这些对象需要通过网络从driver所在节点传给其他的worker节点，所以要求这些类是可序列化的，比如在Java或者scala中实现Serializable类，除了java这种序列化机制，还可以选择其他方式，使得序列化工作更加高效。worker节点接收到程序之后，在spark资源管理器的指挥下运行RDD程序。不同worker节点之间的运行操作是并行的。\n\n 在worker节点上所运行的RDD中代码的变量是保存在worker节点上面的，在spark编程中，很多时候用户需要在driver程序中进行相关数据操作之后把该数据传给RDD对象的方法以做进一步处理，这时候，spark框架会自动帮用户把这些数据通过网络传给相应的worker节点。除了这种以变量的形式定义传输数据到worker节点之外，spark还另外提供了两种机制，分别是broadcast和accumulator。相比于变量的方式，在一定场景下使用broadcast比较有优势，因为所广播的数据在每一个worker节点上面只存一个副本，而在spark算子中使用到的外部变量会在每一个用到它的task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。\n\n\n\n**理解闭包**\n\n \n\n​      Spark中理解起来比较困难的一点是当代码在集群上运行时变量和方法的生命周期和作用域(scope)。当作用于RDD上的操作修改了超出它们作用域范围的变量时，会引起一些混淆。为了说明这个问题，使用下面的例子。该例中使用foreach()，对counter(计数器)进行增加，相同的问题也会发生在其他操作中。\n\n \n\n \n\n**例子**\n\n​      下面的例子在以本地模式运行(--master = local[n]) 和将它部署到集群中 (例如通过 spark-submit 提交到 YARN)对比发现会产生不同的结果。\n\n```scala\nvar counter =  0 \nvar rdd = sc.parallelize(data)\n// 错误,请不要这样做！！\nrdd.foreach(x => counter += x)\nprintln( \"Counter value: \"  + counter)\n```\n\n\n\n**本地模式 vs. 集群模式**\n\n​      这里主要的挑战是上面代码的行为是有歧义的。以本地模式运行在单个JVM上，上面的代码会将RDD中的值进行累加，并且将它存储到counter中。这是因为RDD和变量counter在driver节点的相同内存空间中。\n      然而，以集群模式运行时，会更加复杂，上面的代码的结果也许不会如我们预期的那样。当执行一个作业(job)时,Spark会将RDD分成多个任务(task)--每一个任务都会由一个executor来执行。在执行之前，Spark会计算闭包(closure)。闭包是对executors可见的那部分变量和方法，executors会用闭包来执行RDD上的计算(在这个例子中，闭包是foreach())。这个闭包是被序列化的，并且发送给每个executor。在本地模式中，只有一个executor，所以共享相同的闭包。然而，在集群模式中，就不是这样了。executors会运行在各自的worker节点中，每个executor都有闭包的一个复本。\n      发送给每个executor的闭包中的变量其实也是复本。每个foreach函数中引用的counter不再是driver节点上的counter。当然，在driver节点的内存中仍然存在这一个counter，但是这个counter对于executors来说是不可见的。executors只能看到自己的闭包中的复本。这样，counter最后的值仍旧是0，因为所有在counter的操作只引用了序列化闭包中的值。\n      为了在这样的场景中，确保这些行为正确，应该使用累加变量(Accumulator)。在集群中跨节点工作时，Spark中的累加变量提供了一种安全的机制来更新变量。所以可变的全局状态应该使用累加变量来定义。\n\n所以上面的例子可以这样写：\n\n```scala\n// counter现在是累加变量\nvar counter = sc.accumulator( 0) \nvar rdd = sc.parallelize(data) \nrdd.foreach(x => counter += x) \nprintln( \"Counter value: \" + counter)\n```\n\n","slug":"理解spark闭包","published":1,"updated":"2019-01-20T06:05:12.692Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubln000lamumuoymnvzo","content":"<h1 id=\"理解spark闭包\"><a href=\"#理解spark闭包\" class=\"headerlink\" title=\"理解spark闭包\"></a>理解spark闭包</h1><p>什么叫闭包： 跨作用域访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。</p>\n<p>Spark闭包的问题引出：<br>在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。这个和我们编写单机程序的认识有很大不同。为什么呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object Test &#123;</span><br><span class=\"line\">  def main(args:Array[String]):Unit = &#123;</span><br><span class=\"line\">      val conf = new SparkConf().setAppName(&quot;test&quot;);</span><br><span class=\"line\">      val sc = new SparkContext(conf)</span><br><span class=\"line\"></span><br><span class=\"line\">      val rdd = sc.parallelize(List(1,2,3))</span><br><span class=\"line\">      var counter = 0</span><br><span class=\"line\">      //warn: don&apos;t do this</span><br><span class=\"line\">      rdd.foreach(x =&gt; counter += x)</span><br><span class=\"line\">      println(&quot;Counter value: &quot;+counter)</span><br><span class=\"line\"></span><br><span class=\"line\">      sc.stop()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;1234567891011121314</span><br></pre></td></tr></table></figure>\n<p>问题分析：<br>counter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的“跨域”问题，也就是闭包问题。</p>\n<p>问题解释：<br>对于上面程序中的counter变量，由于在<code>main函数</code>和在<code>rdd对象的foreach函数</code>是属于不同“闭包”的，所以，传进foreach中的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终打印出来的counter为0.</p>\n<p>当用户提交了一个用scala语言写的Spark程序，Spark框架会调用哪些组件呢？首先，这个Spark程序就是一个“Application”，程序里面的mian函数就是“Driver Program”， 前面已经讲到它的作用，只是，dirver程序的可能运行在客户端，也有可有可能运行在spark集群中，这取决于spark作业提交时参数的选定，比如，yarn-client和yarn-cluster就是分别运行在客户端和spark集群中。在driver程序中会有RDD对象的相关代码操作，比如下面代码的newRDD.map()</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Test&#123;</span><br><span class=\"line\">  def main(args: Array[String]) &#123;</span><br><span class=\"line\">    val sc = new SparkContext(new SparkConf())</span><br><span class=\"line\">    val newRDD = sc.textFile(&quot;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    newRDD.map(data =&gt; &#123;</span><br><span class=\"line\">      //do something</span><br><span class=\"line\">      println(data.toString)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>涉及到RDD的代码，比如上面RDD的map操作，它们是在Worker节点上面运行的，所以spark会透明地帮用户把这些涉及到RDD操作的代码传给相应的worker节点。如果在RDD map函数中调用了在函数外部定义的对象，因为这些对象需要通过网络从driver所在节点传给其他的worker节点，所以要求这些类是可序列化的，比如在Java或者scala中实现Serializable类，除了java这种序列化机制，还可以选择其他方式，使得序列化工作更加高效。worker节点接收到程序之后，在spark资源管理器的指挥下运行RDD程序。不同worker节点之间的运行操作是并行的。</p>\n<p> 在worker节点上所运行的RDD中代码的变量是保存在worker节点上面的，在spark编程中，很多时候用户需要在driver程序中进行相关数据操作之后把该数据传给RDD对象的方法以做进一步处理，这时候，spark框架会自动帮用户把这些数据通过网络传给相应的worker节点。除了这种以变量的形式定义传输数据到worker节点之外，spark还另外提供了两种机制，分别是broadcast和accumulator。相比于变量的方式，在一定场景下使用broadcast比较有优势，因为所广播的数据在每一个worker节点上面只存一个副本，而在spark算子中使用到的外部变量会在每一个用到它的task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。</p>\n<p><strong>理解闭包</strong></p>\n<p>​      Spark中理解起来比较困难的一点是当代码在集群上运行时变量和方法的生命周期和作用域(scope)。当作用于RDD上的操作修改了超出它们作用域范围的变量时，会引起一些混淆。为了说明这个问题，使用下面的例子。该例中使用foreach()，对counter(计数器)进行增加，相同的问题也会发生在其他操作中。</p>\n<p><strong>例子</strong></p>\n<p>​      下面的例子在以本地模式运行(–master = local[n]) 和将它部署到集群中 (例如通过 spark-submit 提交到 YARN)对比发现会产生不同的结果。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> counter =  <span class=\"number\">0</span> </span><br><span class=\"line\"><span class=\"keyword\">var</span> rdd = sc.parallelize(data)</span><br><span class=\"line\"><span class=\"comment\">// 错误,请不要这样做！！</span></span><br><span class=\"line\">rdd.foreach(x =&gt; counter += x)</span><br><span class=\"line\">println( <span class=\"string\">\"Counter value: \"</span>  + counter)</span><br></pre></td></tr></table></figure>\n<p><strong>本地模式 vs. 集群模式</strong></p>\n<p>​      这里主要的挑战是上面代码的行为是有歧义的。以本地模式运行在单个JVM上，上面的代码会将RDD中的值进行累加，并且将它存储到counter中。这是因为RDD和变量counter在driver节点的相同内存空间中。<br>      然而，以集群模式运行时，会更加复杂，上面的代码的结果也许不会如我们预期的那样。当执行一个作业(job)时,Spark会将RDD分成多个任务(task)–每一个任务都会由一个executor来执行。在执行之前，Spark会计算闭包(closure)。闭包是对executors可见的那部分变量和方法，executors会用闭包来执行RDD上的计算(在这个例子中，闭包是foreach())。这个闭包是被序列化的，并且发送给每个executor。在本地模式中，只有一个executor，所以共享相同的闭包。然而，在集群模式中，就不是这样了。executors会运行在各自的worker节点中，每个executor都有闭包的一个复本。<br>      发送给每个executor的闭包中的变量其实也是复本。每个foreach函数中引用的counter不再是driver节点上的counter。当然，在driver节点的内存中仍然存在这一个counter，但是这个counter对于executors来说是不可见的。executors只能看到自己的闭包中的复本。这样，counter最后的值仍旧是0，因为所有在counter的操作只引用了序列化闭包中的值。<br>      为了在这样的场景中，确保这些行为正确，应该使用累加变量(Accumulator)。在集群中跨节点工作时，Spark中的累加变量提供了一种安全的机制来更新变量。所以可变的全局状态应该使用累加变量来定义。</p>\n<p>所以上面的例子可以这样写：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// counter现在是累加变量</span></span><br><span class=\"line\"><span class=\"keyword\">var</span> counter = sc.accumulator( <span class=\"number\">0</span>) </span><br><span class=\"line\"><span class=\"keyword\">var</span> rdd = sc.parallelize(data) </span><br><span class=\"line\">rdd.foreach(x =&gt; counter += x) </span><br><span class=\"line\">println( <span class=\"string\">\"Counter value: \"</span> + counter)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"理解spark闭包\"><a href=\"#理解spark闭包\" class=\"headerlink\" title=\"理解spark闭包\"></a>理解spark闭包</h1><p>什么叫闭包： 跨作用域访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。</p>\n<p>Spark闭包的问题引出：<br>在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。这个和我们编写单机程序的认识有很大不同。为什么呢？</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object Test &#123;</span><br><span class=\"line\">  def main(args:Array[String]):Unit = &#123;</span><br><span class=\"line\">      val conf = new SparkConf().setAppName(&quot;test&quot;);</span><br><span class=\"line\">      val sc = new SparkContext(conf)</span><br><span class=\"line\"></span><br><span class=\"line\">      val rdd = sc.parallelize(List(1,2,3))</span><br><span class=\"line\">      var counter = 0</span><br><span class=\"line\">      //warn: don&apos;t do this</span><br><span class=\"line\">      rdd.foreach(x =&gt; counter += x)</span><br><span class=\"line\">      println(&quot;Counter value: &quot;+counter)</span><br><span class=\"line\"></span><br><span class=\"line\">      sc.stop()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;1234567891011121314</span><br></pre></td></tr></table></figure>\n<p>问题分析：<br>counter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的“跨域”问题，也就是闭包问题。</p>\n<p>问题解释：<br>对于上面程序中的counter变量，由于在<code>main函数</code>和在<code>rdd对象的foreach函数</code>是属于不同“闭包”的，所以，传进foreach中的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终打印出来的counter为0.</p>\n<p>当用户提交了一个用scala语言写的Spark程序，Spark框架会调用哪些组件呢？首先，这个Spark程序就是一个“Application”，程序里面的mian函数就是“Driver Program”， 前面已经讲到它的作用，只是，dirver程序的可能运行在客户端，也有可有可能运行在spark集群中，这取决于spark作业提交时参数的选定，比如，yarn-client和yarn-cluster就是分别运行在客户端和spark集群中。在driver程序中会有RDD对象的相关代码操作，比如下面代码的newRDD.map()</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Test&#123;</span><br><span class=\"line\">  def main(args: Array[String]) &#123;</span><br><span class=\"line\">    val sc = new SparkContext(new SparkConf())</span><br><span class=\"line\">    val newRDD = sc.textFile(&quot;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    newRDD.map(data =&gt; &#123;</span><br><span class=\"line\">      //do something</span><br><span class=\"line\">      println(data.toString)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>涉及到RDD的代码，比如上面RDD的map操作，它们是在Worker节点上面运行的，所以spark会透明地帮用户把这些涉及到RDD操作的代码传给相应的worker节点。如果在RDD map函数中调用了在函数外部定义的对象，因为这些对象需要通过网络从driver所在节点传给其他的worker节点，所以要求这些类是可序列化的，比如在Java或者scala中实现Serializable类，除了java这种序列化机制，还可以选择其他方式，使得序列化工作更加高效。worker节点接收到程序之后，在spark资源管理器的指挥下运行RDD程序。不同worker节点之间的运行操作是并行的。</p>\n<p> 在worker节点上所运行的RDD中代码的变量是保存在worker节点上面的，在spark编程中，很多时候用户需要在driver程序中进行相关数据操作之后把该数据传给RDD对象的方法以做进一步处理，这时候，spark框架会自动帮用户把这些数据通过网络传给相应的worker节点。除了这种以变量的形式定义传输数据到worker节点之外，spark还另外提供了两种机制，分别是broadcast和accumulator。相比于变量的方式，在一定场景下使用broadcast比较有优势，因为所广播的数据在每一个worker节点上面只存一个副本，而在spark算子中使用到的外部变量会在每一个用到它的task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。</p>\n<p><strong>理解闭包</strong></p>\n<p>​      Spark中理解起来比较困难的一点是当代码在集群上运行时变量和方法的生命周期和作用域(scope)。当作用于RDD上的操作修改了超出它们作用域范围的变量时，会引起一些混淆。为了说明这个问题，使用下面的例子。该例中使用foreach()，对counter(计数器)进行增加，相同的问题也会发生在其他操作中。</p>\n<p><strong>例子</strong></p>\n<p>​      下面的例子在以本地模式运行(–master = local[n]) 和将它部署到集群中 (例如通过 spark-submit 提交到 YARN)对比发现会产生不同的结果。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> counter =  <span class=\"number\">0</span> </span><br><span class=\"line\"><span class=\"keyword\">var</span> rdd = sc.parallelize(data)</span><br><span class=\"line\"><span class=\"comment\">// 错误,请不要这样做！！</span></span><br><span class=\"line\">rdd.foreach(x =&gt; counter += x)</span><br><span class=\"line\">println( <span class=\"string\">\"Counter value: \"</span>  + counter)</span><br></pre></td></tr></table></figure>\n<p><strong>本地模式 vs. 集群模式</strong></p>\n<p>​      这里主要的挑战是上面代码的行为是有歧义的。以本地模式运行在单个JVM上，上面的代码会将RDD中的值进行累加，并且将它存储到counter中。这是因为RDD和变量counter在driver节点的相同内存空间中。<br>      然而，以集群模式运行时，会更加复杂，上面的代码的结果也许不会如我们预期的那样。当执行一个作业(job)时,Spark会将RDD分成多个任务(task)–每一个任务都会由一个executor来执行。在执行之前，Spark会计算闭包(closure)。闭包是对executors可见的那部分变量和方法，executors会用闭包来执行RDD上的计算(在这个例子中，闭包是foreach())。这个闭包是被序列化的，并且发送给每个executor。在本地模式中，只有一个executor，所以共享相同的闭包。然而，在集群模式中，就不是这样了。executors会运行在各自的worker节点中，每个executor都有闭包的一个复本。<br>      发送给每个executor的闭包中的变量其实也是复本。每个foreach函数中引用的counter不再是driver节点上的counter。当然，在driver节点的内存中仍然存在这一个counter，但是这个counter对于executors来说是不可见的。executors只能看到自己的闭包中的复本。这样，counter最后的值仍旧是0，因为所有在counter的操作只引用了序列化闭包中的值。<br>      为了在这样的场景中，确保这些行为正确，应该使用累加变量(Accumulator)。在集群中跨节点工作时，Spark中的累加变量提供了一种安全的机制来更新变量。所以可变的全局状态应该使用累加变量来定义。</p>\n<p>所以上面的例子可以这样写：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// counter现在是累加变量</span></span><br><span class=\"line\"><span class=\"keyword\">var</span> counter = sc.accumulator( <span class=\"number\">0</span>) </span><br><span class=\"line\"><span class=\"keyword\">var</span> rdd = sc.parallelize(data) </span><br><span class=\"line\">rdd.foreach(x =&gt; counter += x) </span><br><span class=\"line\">println( <span class=\"string\">\"Counter value: \"</span> + counter)</span><br></pre></td></tr></table></figure>\n"},{"title":"python装饰器函数式编程","date":"2016-09-24T05:52:36.000Z","_content":"\n# 函数式编程\n> 函数式编程（英语：functional programming）或称函数程序设计,又称泛函编程,是一种编程典范,它将电脑运算视为数学上的函数计算,并且避免使用程序状态以及易变对象,函数编程语言最重要的基础是λ演算（lambda calculus,而且λ演算的函数可以接受函数当作输入（引数）和输出（传出值）\n\n函数式编程范式相对命令式编程(Imperative programming),函数式编程更加强调程序执行的结果而非执行的过程,倡导利用若干简单的执行单元让计算结果不断渐进,逐层推导复杂的运算,而不是设计一个复杂的执行过程.而命令式编程使用各种变量以及复杂的控制语句来编写逻辑代码,最初图灵机的设计就是属于命令式编程,在纸带上面刻各种孔,然后机器根据纸带上的孔执行各种命令.C/C++ Java Python等各种面向对象编程其实都属于命令式编程范围,虽然现在这些高级语言或多或少都已经支持Lambda表达式以开始支持函数式编程,但是相比Lisp Haskell Clojure等这些纯正函数式语言,Python在函数式编程方面支持的相对少一点.\n\n## Python Lambda\n\n> λ演算（英语：lambda calculus，λ-calculus）是一套从数学逻辑中发展，以变量绑定和替换的规则，来研究函数如何抽象化定义、函数如何被应用以及递归的形式系统。它由数学家阿隆佐·邱奇在20世纪30年代首次发表。Lambda演算作为一种广泛用途的计算模型，可以清晰地定义什么是一个可计算函数，而任何可计算函数都能以这种形式表达和求值，它能模拟单一磁带图灵机的计算过程；尽管如此，Lambda演算强调的是变换规则的运用，而非实现它们的具体机器。\n\n说到Python支持函数式编程必须得说道Lambda表达式,Lambda按照wiki上说法比较复杂,其实简单的说Lambda表达式就是匿名函数\n\n```\n    lambda argument: manipulate(argument)\n```\n---\n```\n    squares = map(lambda x: x * x, range(9))\n    print squares\n```\n\n---\n```\n    number_list = range(-5, 5)\n    less_than_zero = list(filter(lambda x: x < 0, number_list))\n    print(less_than_zero)\n```\n---\n\n```\n    number_list = range(-5, 5)\n    less_than_zero = list(filter(lambda x: x < 0, number_list))\n    print(less_than_zero)\n```\n\n上面三个代码片段就是map/reduce/filter和lambda表达式结合的一个例子,python在支持lambda时,为了不让开发者乱用这个特性,lambda设计的比较简单,就是在lamba后面加函数以及操作,只要使用得当lambda可以让代码看起来更加简洁优雅,但是如果使用不当代码可读性会很差\n\n<!-- more -->\n\n## 函数式编程特性\n\n其实对于纯正的函数式编程,我接触的比较少,像lisp Haskell Scheme以及更兴起的Clojure都不是很了解,下次可以好好的学习下Clojure\n函数式编程主要有三大特性:\n- immutable data 不可变数据: 不像命令式编程那样数据都是有状态的,函数式编程中采用无状态数据,像Clojure中变量是不可变的\n- first class functions: 函数和变量一样使用,只要变量有的特性函数都有\n- 尾递归优化: 使用尾递归优化技术——每次递归时都会重用stack,这样一来能够提升性能,Python就不支持\n\n函数式编程的几个技术:\n- map & reduce: python中有这两个函数\n- pipeline: 这个技术的意思是，把函数实例成一个一个的action，然后，把一组action放到一个数组或是列表中，然后把数据传给这个action list，数据就像一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果\n- recursing 递归: 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓\n- currying：把一个函数的多个参数分解成多个函数， 然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样,可以简化函数的多个参数\n- higher order function 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出，就像面向对象对象满天飞一样\n\n---\n\n# 函数装饰器\n\npython中的函数也是一个对象,可以用来传递,例如:\n\n```\ndef foo():\n    print(\"foo\")\n\ndef bar(func):\n    func()\n\nbar(foo)\n```\n\n> 装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能\n\n说到装饰器,网上举的例子大部分都是关于日志打印的,这个确实很经典,简单的说就是想要在一个已经定义好的函数中打印一些日志,需要自己再封装一个函数,然后在该函数中再调用,但是采用这种方法的话会破坏原先代码的逻辑,于是装饰器方案诞生了:\n\n```\ndef use_logging(func):\n\n    def wrapper():\n        logging.warn(\"%s is running\" % func.__name__)\n        return func()\n    return wrapper      #返回函数\n\ndef foo():\n    print('i am foo')\n\nfoo = use_logging(foo)  #函数作为参数传递\nfoo()                   #还是调用foo() 没有破坏原程序逻辑 调用foo()可以打印日志信息还执行了原代码 \n```\n\npython中对装饰器的支持采用@这个符号,这个是个语法糖,把@放在use_logging函数定义的开头,相当于foo = use_logging(foo), 只需要在foo()函数的开头加上@use_logging即可在不改变原函数逻辑的情况实现日志打印ongn     \n```\ndef use_logging(func):\n\n    def wrapper():\n        logging.warn(\"%s is running\" % func.__name__)\n        return func()\n    return wrapper\n\n@use_logging\ndef foo():\n    print(\"i am foo\")\n\nfoo()\n```\n\n如果foo()函数需要传参,*args/**kargs 利用这这两个参数可以传类似多个参数或者关键字参数,这个时候use_logging(func)中的内部函数可以更改为:\n```\n    def wrapper(*args, **kargs):\n        logging.warn(\"%s is running\" % func.__name__)\n        return func(*args, **kargs)\n    return wrapper \n```\n\n装饰器的使用非常灵活,例如还可以定义带参数的装饰器.拿上面日志的例子来说,如果日志打印需要打印不同的等级信息,那么use_logging参数需要有一个日志等级参数.\n```\n    def use_logging(level):\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                if level == \"warn\":\n                    logging.warn(\"%s is running\" % func.__name__)\n                elif level == \"info\":\n                    logging.info(\"%s is running\" % func.__name__)\n                return func(*args)\n            return wrapper\n\n    return decorator\n\n    @use_logging(level=\"warn\")\n    def foo(name='foo'):\n        print(\"i am %s\" % name)\n\n    foo()\n```\n\nuse_logging函数定义看起来有点绕, @use_logging(level=\"warn\") 会被python解析为foo = use_logging(level)(foo) 这样的话use_logging(level)需要返回一个装饰器, 也就不难理解上面的那段代码了\n\n---\n# 类装饰器\n\n装饰器也可用来装饰类,此时被修饰的函数会调用类的__call__方法\n```\nclass Foo(object):\n    def __init__(self, func):\n        print \"sinit in Foo\"\n        self._func = func\n\n    def __call__(self):\n        print ('class decorator runing')\n        self._func()\n        print ('class decorator ending')\n\n@Foo\ndef bar():\n    print ('bar')\n\nbar()\n```\n上面代码执行完毕后会打印:\n```\ninit in Foo\nclass decorator runing\nbar\nclass decorator ending\n```\nFoo函数定义了两个函数:\n1. 一个是__init__()，这个方法是在我们给某个函数decorator时被调用，所以，需要有一个fn的参数，也就是被decorator的函数\n2. 一个是__call__()，这个方法是在我们调用被decorator函数时被调用的\n\n# 装饰器副作用\ndecorator函数中原函数的信息被破坏了 例如__name__ docstring 参数列表等, 不过functools.wraps可以解决这个问题\n```\n    def use_logging(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if level == \"warn\":\n                logging.warn(\"%s is running\" % func.__name__)\n            elif level == \"info\":\n                logging.info(\"%s is running\" % func.__name__)\n            return func(*args)\n        return wrapper\n```\n\n参考文章: \n1. https://foofish.net/python-decorator.html\n2. https://coolshell.cn/articles/11265.html\n3. https://coolshell.cn/articles/10822.html\n4. http://book.pythontips.com/en/latest/lambdas.html\n\n","source":"_posts/python装饰器函数式编程.md","raw":"---\ntitle: python装饰器函数式编程\ndate: 2016-09-24 13:52:36\ntags: python\n---\n\n# 函数式编程\n> 函数式编程（英语：functional programming）或称函数程序设计,又称泛函编程,是一种编程典范,它将电脑运算视为数学上的函数计算,并且避免使用程序状态以及易变对象,函数编程语言最重要的基础是λ演算（lambda calculus,而且λ演算的函数可以接受函数当作输入（引数）和输出（传出值）\n\n函数式编程范式相对命令式编程(Imperative programming),函数式编程更加强调程序执行的结果而非执行的过程,倡导利用若干简单的执行单元让计算结果不断渐进,逐层推导复杂的运算,而不是设计一个复杂的执行过程.而命令式编程使用各种变量以及复杂的控制语句来编写逻辑代码,最初图灵机的设计就是属于命令式编程,在纸带上面刻各种孔,然后机器根据纸带上的孔执行各种命令.C/C++ Java Python等各种面向对象编程其实都属于命令式编程范围,虽然现在这些高级语言或多或少都已经支持Lambda表达式以开始支持函数式编程,但是相比Lisp Haskell Clojure等这些纯正函数式语言,Python在函数式编程方面支持的相对少一点.\n\n## Python Lambda\n\n> λ演算（英语：lambda calculus，λ-calculus）是一套从数学逻辑中发展，以变量绑定和替换的规则，来研究函数如何抽象化定义、函数如何被应用以及递归的形式系统。它由数学家阿隆佐·邱奇在20世纪30年代首次发表。Lambda演算作为一种广泛用途的计算模型，可以清晰地定义什么是一个可计算函数，而任何可计算函数都能以这种形式表达和求值，它能模拟单一磁带图灵机的计算过程；尽管如此，Lambda演算强调的是变换规则的运用，而非实现它们的具体机器。\n\n说到Python支持函数式编程必须得说道Lambda表达式,Lambda按照wiki上说法比较复杂,其实简单的说Lambda表达式就是匿名函数\n\n```\n    lambda argument: manipulate(argument)\n```\n---\n```\n    squares = map(lambda x: x * x, range(9))\n    print squares\n```\n\n---\n```\n    number_list = range(-5, 5)\n    less_than_zero = list(filter(lambda x: x < 0, number_list))\n    print(less_than_zero)\n```\n---\n\n```\n    number_list = range(-5, 5)\n    less_than_zero = list(filter(lambda x: x < 0, number_list))\n    print(less_than_zero)\n```\n\n上面三个代码片段就是map/reduce/filter和lambda表达式结合的一个例子,python在支持lambda时,为了不让开发者乱用这个特性,lambda设计的比较简单,就是在lamba后面加函数以及操作,只要使用得当lambda可以让代码看起来更加简洁优雅,但是如果使用不当代码可读性会很差\n\n<!-- more -->\n\n## 函数式编程特性\n\n其实对于纯正的函数式编程,我接触的比较少,像lisp Haskell Scheme以及更兴起的Clojure都不是很了解,下次可以好好的学习下Clojure\n函数式编程主要有三大特性:\n- immutable data 不可变数据: 不像命令式编程那样数据都是有状态的,函数式编程中采用无状态数据,像Clojure中变量是不可变的\n- first class functions: 函数和变量一样使用,只要变量有的特性函数都有\n- 尾递归优化: 使用尾递归优化技术——每次递归时都会重用stack,这样一来能够提升性能,Python就不支持\n\n函数式编程的几个技术:\n- map & reduce: python中有这两个函数\n- pipeline: 这个技术的意思是，把函数实例成一个一个的action，然后，把一组action放到一个数组或是列表中，然后把数据传给这个action list，数据就像一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果\n- recursing 递归: 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓\n- currying：把一个函数的多个参数分解成多个函数， 然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样,可以简化函数的多个参数\n- higher order function 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出，就像面向对象对象满天飞一样\n\n---\n\n# 函数装饰器\n\npython中的函数也是一个对象,可以用来传递,例如:\n\n```\ndef foo():\n    print(\"foo\")\n\ndef bar(func):\n    func()\n\nbar(foo)\n```\n\n> 装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能\n\n说到装饰器,网上举的例子大部分都是关于日志打印的,这个确实很经典,简单的说就是想要在一个已经定义好的函数中打印一些日志,需要自己再封装一个函数,然后在该函数中再调用,但是采用这种方法的话会破坏原先代码的逻辑,于是装饰器方案诞生了:\n\n```\ndef use_logging(func):\n\n    def wrapper():\n        logging.warn(\"%s is running\" % func.__name__)\n        return func()\n    return wrapper      #返回函数\n\ndef foo():\n    print('i am foo')\n\nfoo = use_logging(foo)  #函数作为参数传递\nfoo()                   #还是调用foo() 没有破坏原程序逻辑 调用foo()可以打印日志信息还执行了原代码 \n```\n\npython中对装饰器的支持采用@这个符号,这个是个语法糖,把@放在use_logging函数定义的开头,相当于foo = use_logging(foo), 只需要在foo()函数的开头加上@use_logging即可在不改变原函数逻辑的情况实现日志打印ongn     \n```\ndef use_logging(func):\n\n    def wrapper():\n        logging.warn(\"%s is running\" % func.__name__)\n        return func()\n    return wrapper\n\n@use_logging\ndef foo():\n    print(\"i am foo\")\n\nfoo()\n```\n\n如果foo()函数需要传参,*args/**kargs 利用这这两个参数可以传类似多个参数或者关键字参数,这个时候use_logging(func)中的内部函数可以更改为:\n```\n    def wrapper(*args, **kargs):\n        logging.warn(\"%s is running\" % func.__name__)\n        return func(*args, **kargs)\n    return wrapper \n```\n\n装饰器的使用非常灵活,例如还可以定义带参数的装饰器.拿上面日志的例子来说,如果日志打印需要打印不同的等级信息,那么use_logging参数需要有一个日志等级参数.\n```\n    def use_logging(level):\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                if level == \"warn\":\n                    logging.warn(\"%s is running\" % func.__name__)\n                elif level == \"info\":\n                    logging.info(\"%s is running\" % func.__name__)\n                return func(*args)\n            return wrapper\n\n    return decorator\n\n    @use_logging(level=\"warn\")\n    def foo(name='foo'):\n        print(\"i am %s\" % name)\n\n    foo()\n```\n\nuse_logging函数定义看起来有点绕, @use_logging(level=\"warn\") 会被python解析为foo = use_logging(level)(foo) 这样的话use_logging(level)需要返回一个装饰器, 也就不难理解上面的那段代码了\n\n---\n# 类装饰器\n\n装饰器也可用来装饰类,此时被修饰的函数会调用类的__call__方法\n```\nclass Foo(object):\n    def __init__(self, func):\n        print \"sinit in Foo\"\n        self._func = func\n\n    def __call__(self):\n        print ('class decorator runing')\n        self._func()\n        print ('class decorator ending')\n\n@Foo\ndef bar():\n    print ('bar')\n\nbar()\n```\n上面代码执行完毕后会打印:\n```\ninit in Foo\nclass decorator runing\nbar\nclass decorator ending\n```\nFoo函数定义了两个函数:\n1. 一个是__init__()，这个方法是在我们给某个函数decorator时被调用，所以，需要有一个fn的参数，也就是被decorator的函数\n2. 一个是__call__()，这个方法是在我们调用被decorator函数时被调用的\n\n# 装饰器副作用\ndecorator函数中原函数的信息被破坏了 例如__name__ docstring 参数列表等, 不过functools.wraps可以解决这个问题\n```\n    def use_logging(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if level == \"warn\":\n                logging.warn(\"%s is running\" % func.__name__)\n            elif level == \"info\":\n                logging.info(\"%s is running\" % func.__name__)\n            return func(*args)\n        return wrapper\n```\n\n参考文章: \n1. https://foofish.net/python-decorator.html\n2. https://coolshell.cn/articles/11265.html\n3. https://coolshell.cn/articles/10822.html\n4. http://book.pythontips.com/en/latest/lambdas.html\n\n","slug":"python装饰器函数式编程","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubp50010amumw7uzhhfu","content":"<h1 id=\"函数式编程\"><a href=\"#函数式编程\" class=\"headerlink\" title=\"函数式编程\"></a>函数式编程</h1><blockquote>\n<p>函数式编程（英语：functional programming）或称函数程序设计,又称泛函编程,是一种编程典范,它将电脑运算视为数学上的函数计算,并且避免使用程序状态以及易变对象,函数编程语言最重要的基础是λ演算（lambda calculus,而且λ演算的函数可以接受函数当作输入（引数）和输出（传出值）</p>\n</blockquote>\n<p>函数式编程范式相对命令式编程(Imperative programming),函数式编程更加强调程序执行的结果而非执行的过程,倡导利用若干简单的执行单元让计算结果不断渐进,逐层推导复杂的运算,而不是设计一个复杂的执行过程.而命令式编程使用各种变量以及复杂的控制语句来编写逻辑代码,最初图灵机的设计就是属于命令式编程,在纸带上面刻各种孔,然后机器根据纸带上的孔执行各种命令.C/C++ Java Python等各种面向对象编程其实都属于命令式编程范围,虽然现在这些高级语言或多或少都已经支持Lambda表达式以开始支持函数式编程,但是相比Lisp Haskell Clojure等这些纯正函数式语言,Python在函数式编程方面支持的相对少一点.</p>\n<h2 id=\"Python-Lambda\"><a href=\"#Python-Lambda\" class=\"headerlink\" title=\"Python Lambda\"></a>Python Lambda</h2><blockquote>\n<p>λ演算（英语：lambda calculus，λ-calculus）是一套从数学逻辑中发展，以变量绑定和替换的规则，来研究函数如何抽象化定义、函数如何被应用以及递归的形式系统。它由数学家阿隆佐·邱奇在20世纪30年代首次发表。Lambda演算作为一种广泛用途的计算模型，可以清晰地定义什么是一个可计算函数，而任何可计算函数都能以这种形式表达和求值，它能模拟单一磁带图灵机的计算过程；尽管如此，Lambda演算强调的是变换规则的运用，而非实现它们的具体机器。</p>\n</blockquote>\n<p>说到Python支持函数式编程必须得说道Lambda表达式,Lambda按照wiki上说法比较复杂,其实简单的说Lambda表达式就是匿名函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lambda argument: manipulate(argument)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squares = map(lambda x: x * x, range(9))</span><br><span class=\"line\">print squares</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">number_list = range(-5, 5)</span><br><span class=\"line\">less_than_zero = list(filter(lambda x: x &lt; 0, number_list))</span><br><span class=\"line\">print(less_than_zero)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">number_list = range(-5, 5)</span><br><span class=\"line\">less_than_zero = list(filter(lambda x: x &lt; 0, number_list))</span><br><span class=\"line\">print(less_than_zero)</span><br></pre></td></tr></table></figure>\n<p>上面三个代码片段就是map/reduce/filter和lambda表达式结合的一个例子,python在支持lambda时,为了不让开发者乱用这个特性,lambda设计的比较简单,就是在lamba后面加函数以及操作,只要使用得当lambda可以让代码看起来更加简洁优雅,但是如果使用不当代码可读性会很差</p>\n<a id=\"more\"></a>\n<h2 id=\"函数式编程特性\"><a href=\"#函数式编程特性\" class=\"headerlink\" title=\"函数式编程特性\"></a>函数式编程特性</h2><p>其实对于纯正的函数式编程,我接触的比较少,像lisp Haskell Scheme以及更兴起的Clojure都不是很了解,下次可以好好的学习下Clojure<br>函数式编程主要有三大特性:</p>\n<ul>\n<li>immutable data 不可变数据: 不像命令式编程那样数据都是有状态的,函数式编程中采用无状态数据,像Clojure中变量是不可变的</li>\n<li>first class functions: 函数和变量一样使用,只要变量有的特性函数都有</li>\n<li>尾递归优化: 使用尾递归优化技术——每次递归时都会重用stack,这样一来能够提升性能,Python就不支持</li>\n</ul>\n<p>函数式编程的几个技术:</p>\n<ul>\n<li>map &amp; reduce: python中有这两个函数</li>\n<li>pipeline: 这个技术的意思是，把函数实例成一个一个的action，然后，把一组action放到一个数组或是列表中，然后把数据传给这个action list，数据就像一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果</li>\n<li>recursing 递归: 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓</li>\n<li>currying：把一个函数的多个参数分解成多个函数， 然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样,可以简化函数的多个参数</li>\n<li>higher order function 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出，就像面向对象对象满天飞一样</li>\n</ul>\n<hr>\n<h1 id=\"函数装饰器\"><a href=\"#函数装饰器\" class=\"headerlink\" title=\"函数装饰器\"></a>函数装饰器</h1><p>python中的函数也是一个对象,可以用来传递,例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&quot;foo&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">def bar(func):</span><br><span class=\"line\">    func()</span><br><span class=\"line\"></span><br><span class=\"line\">bar(foo)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能</p>\n</blockquote>\n<p>说到装饰器,网上举的例子大部分都是关于日志打印的,这个确实很经典,简单的说就是想要在一个已经定义好的函数中打印一些日志,需要自己再封装一个函数,然后在该函数中再调用,但是采用这种方法的话会破坏原先代码的逻辑,于是装饰器方案诞生了:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\"></span><br><span class=\"line\">    def wrapper():</span><br><span class=\"line\">        logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func()</span><br><span class=\"line\">    return wrapper      #返回函数</span><br><span class=\"line\"></span><br><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&apos;i am foo&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">foo = use_logging(foo)  #函数作为参数传递</span><br><span class=\"line\">foo()                   #还是调用foo() 没有破坏原程序逻辑 调用foo()可以打印日志信息还执行了原代码</span><br></pre></td></tr></table></figure>\n<p>python中对装饰器的支持采用@这个符号,这个是个语法糖,把@放在use_logging函数定义的开头,相当于foo = use_logging(foo), 只需要在foo()函数的开头加上@use_logging即可在不改变原函数逻辑的情况实现日志打印ongn<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\"></span><br><span class=\"line\">    def wrapper():</span><br><span class=\"line\">        logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func()</span><br><span class=\"line\">    return wrapper</span><br><span class=\"line\"></span><br><span class=\"line\">@use_logging</span><br><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&quot;i am foo&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">foo()</span><br></pre></td></tr></table></figure></p>\n<p>如果foo()函数需要传参,<em>args/*</em>kargs 利用这这两个参数可以传类似多个参数或者关键字参数,这个时候use_logging(func)中的内部函数可以更改为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def wrapper(*args, **kargs):</span><br><span class=\"line\">    logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">    return func(*args, **kargs)</span><br><span class=\"line\">return wrapper</span><br></pre></td></tr></table></figure></p>\n<p>装饰器的使用非常灵活,例如还可以定义带参数的装饰器.拿上面日志的例子来说,如果日志打印需要打印不同的等级信息,那么use_logging参数需要有一个日志等级参数.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(level):</span><br><span class=\"line\">    def decorator(func):</span><br><span class=\"line\">        def wrapper(*args, **kwargs):</span><br><span class=\"line\">            if level == &quot;warn&quot;:</span><br><span class=\"line\">                logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">            elif level == &quot;info&quot;:</span><br><span class=\"line\">                logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">            return func(*args)</span><br><span class=\"line\">        return wrapper</span><br><span class=\"line\"></span><br><span class=\"line\">return decorator</span><br><span class=\"line\"></span><br><span class=\"line\">@use_logging(level=&quot;warn&quot;)</span><br><span class=\"line\">def foo(name=&apos;foo&apos;):</span><br><span class=\"line\">    print(&quot;i am %s&quot; % name)</span><br><span class=\"line\"></span><br><span class=\"line\">foo()</span><br></pre></td></tr></table></figure></p>\n<p>use_logging函数定义看起来有点绕, @use_logging(level=”warn”) 会被python解析为foo = use_logging(level)(foo) 这样的话use_logging(level)需要返回一个装饰器, 也就不难理解上面的那段代码了</p>\n<hr>\n<h1 id=\"类装饰器\"><a href=\"#类装饰器\" class=\"headerlink\" title=\"类装饰器\"></a>类装饰器</h1><p>装饰器也可用来装饰类,此时被修饰的函数会调用类的<strong>call</strong>方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Foo(object):</span><br><span class=\"line\">    def __init__(self, func):</span><br><span class=\"line\">        print &quot;sinit in Foo&quot;</span><br><span class=\"line\">        self._func = func</span><br><span class=\"line\"></span><br><span class=\"line\">    def __call__(self):</span><br><span class=\"line\">        print (&apos;class decorator runing&apos;)</span><br><span class=\"line\">        self._func()</span><br><span class=\"line\">        print (&apos;class decorator ending&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">@Foo</span><br><span class=\"line\">def bar():</span><br><span class=\"line\">    print (&apos;bar&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">bar()</span><br></pre></td></tr></table></figure></p>\n<p>上面代码执行完毕后会打印:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">init in Foo</span><br><span class=\"line\">class decorator runing</span><br><span class=\"line\">bar</span><br><span class=\"line\">class decorator ending</span><br></pre></td></tr></table></figure></p>\n<p>Foo函数定义了两个函数:</p>\n<ol>\n<li>一个是<strong>init</strong>()，这个方法是在我们给某个函数decorator时被调用，所以，需要有一个fn的参数，也就是被decorator的函数</li>\n<li>一个是<strong>call</strong>()，这个方法是在我们调用被decorator函数时被调用的</li>\n</ol>\n<h1 id=\"装饰器副作用\"><a href=\"#装饰器副作用\" class=\"headerlink\" title=\"装饰器副作用\"></a>装饰器副作用</h1><p>decorator函数中原函数的信息被破坏了 例如<strong>name</strong> docstring 参数列表等, 不过functools.wraps可以解决这个问题<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\">    @wraps(func)</span><br><span class=\"line\">    def wrapper(*args, **kwargs):</span><br><span class=\"line\">        if level == &quot;warn&quot;:</span><br><span class=\"line\">            logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        elif level == &quot;info&quot;:</span><br><span class=\"line\">            logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func(*args)</span><br><span class=\"line\">    return wrapper</span><br></pre></td></tr></table></figure></p>\n<p>参考文章: </p>\n<ol>\n<li><a href=\"https://foofish.net/python-decorator.html\" target=\"_blank\" rel=\"noopener\">https://foofish.net/python-decorator.html</a></li>\n<li><a href=\"https://coolshell.cn/articles/11265.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/11265.html</a></li>\n<li><a href=\"https://coolshell.cn/articles/10822.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/10822.html</a></li>\n<li><a href=\"http://book.pythontips.com/en/latest/lambdas.html\" target=\"_blank\" rel=\"noopener\">http://book.pythontips.com/en/latest/lambdas.html</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"函数式编程\"><a href=\"#函数式编程\" class=\"headerlink\" title=\"函数式编程\"></a>函数式编程</h1><blockquote>\n<p>函数式编程（英语：functional programming）或称函数程序设计,又称泛函编程,是一种编程典范,它将电脑运算视为数学上的函数计算,并且避免使用程序状态以及易变对象,函数编程语言最重要的基础是λ演算（lambda calculus,而且λ演算的函数可以接受函数当作输入（引数）和输出（传出值）</p>\n</blockquote>\n<p>函数式编程范式相对命令式编程(Imperative programming),函数式编程更加强调程序执行的结果而非执行的过程,倡导利用若干简单的执行单元让计算结果不断渐进,逐层推导复杂的运算,而不是设计一个复杂的执行过程.而命令式编程使用各种变量以及复杂的控制语句来编写逻辑代码,最初图灵机的设计就是属于命令式编程,在纸带上面刻各种孔,然后机器根据纸带上的孔执行各种命令.C/C++ Java Python等各种面向对象编程其实都属于命令式编程范围,虽然现在这些高级语言或多或少都已经支持Lambda表达式以开始支持函数式编程,但是相比Lisp Haskell Clojure等这些纯正函数式语言,Python在函数式编程方面支持的相对少一点.</p>\n<h2 id=\"Python-Lambda\"><a href=\"#Python-Lambda\" class=\"headerlink\" title=\"Python Lambda\"></a>Python Lambda</h2><blockquote>\n<p>λ演算（英语：lambda calculus，λ-calculus）是一套从数学逻辑中发展，以变量绑定和替换的规则，来研究函数如何抽象化定义、函数如何被应用以及递归的形式系统。它由数学家阿隆佐·邱奇在20世纪30年代首次发表。Lambda演算作为一种广泛用途的计算模型，可以清晰地定义什么是一个可计算函数，而任何可计算函数都能以这种形式表达和求值，它能模拟单一磁带图灵机的计算过程；尽管如此，Lambda演算强调的是变换规则的运用，而非实现它们的具体机器。</p>\n</blockquote>\n<p>说到Python支持函数式编程必须得说道Lambda表达式,Lambda按照wiki上说法比较复杂,其实简单的说Lambda表达式就是匿名函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lambda argument: manipulate(argument)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squares = map(lambda x: x * x, range(9))</span><br><span class=\"line\">print squares</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">number_list = range(-5, 5)</span><br><span class=\"line\">less_than_zero = list(filter(lambda x: x &lt; 0, number_list))</span><br><span class=\"line\">print(less_than_zero)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">number_list = range(-5, 5)</span><br><span class=\"line\">less_than_zero = list(filter(lambda x: x &lt; 0, number_list))</span><br><span class=\"line\">print(less_than_zero)</span><br></pre></td></tr></table></figure>\n<p>上面三个代码片段就是map/reduce/filter和lambda表达式结合的一个例子,python在支持lambda时,为了不让开发者乱用这个特性,lambda设计的比较简单,就是在lamba后面加函数以及操作,只要使用得当lambda可以让代码看起来更加简洁优雅,但是如果使用不当代码可读性会很差</p>","more":"<h2 id=\"函数式编程特性\"><a href=\"#函数式编程特性\" class=\"headerlink\" title=\"函数式编程特性\"></a>函数式编程特性</h2><p>其实对于纯正的函数式编程,我接触的比较少,像lisp Haskell Scheme以及更兴起的Clojure都不是很了解,下次可以好好的学习下Clojure<br>函数式编程主要有三大特性:</p>\n<ul>\n<li>immutable data 不可变数据: 不像命令式编程那样数据都是有状态的,函数式编程中采用无状态数据,像Clojure中变量是不可变的</li>\n<li>first class functions: 函数和变量一样使用,只要变量有的特性函数都有</li>\n<li>尾递归优化: 使用尾递归优化技术——每次递归时都会重用stack,这样一来能够提升性能,Python就不支持</li>\n</ul>\n<p>函数式编程的几个技术:</p>\n<ul>\n<li>map &amp; reduce: python中有这两个函数</li>\n<li>pipeline: 这个技术的意思是，把函数实例成一个一个的action，然后，把一组action放到一个数组或是列表中，然后把数据传给这个action list，数据就像一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果</li>\n<li>recursing 递归: 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓</li>\n<li>currying：把一个函数的多个参数分解成多个函数， 然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样,可以简化函数的多个参数</li>\n<li>higher order function 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出，就像面向对象对象满天飞一样</li>\n</ul>\n<hr>\n<h1 id=\"函数装饰器\"><a href=\"#函数装饰器\" class=\"headerlink\" title=\"函数装饰器\"></a>函数装饰器</h1><p>python中的函数也是一个对象,可以用来传递,例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&quot;foo&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">def bar(func):</span><br><span class=\"line\">    func()</span><br><span class=\"line\"></span><br><span class=\"line\">bar(foo)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能</p>\n</blockquote>\n<p>说到装饰器,网上举的例子大部分都是关于日志打印的,这个确实很经典,简单的说就是想要在一个已经定义好的函数中打印一些日志,需要自己再封装一个函数,然后在该函数中再调用,但是采用这种方法的话会破坏原先代码的逻辑,于是装饰器方案诞生了:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\"></span><br><span class=\"line\">    def wrapper():</span><br><span class=\"line\">        logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func()</span><br><span class=\"line\">    return wrapper      #返回函数</span><br><span class=\"line\"></span><br><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&apos;i am foo&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">foo = use_logging(foo)  #函数作为参数传递</span><br><span class=\"line\">foo()                   #还是调用foo() 没有破坏原程序逻辑 调用foo()可以打印日志信息还执行了原代码</span><br></pre></td></tr></table></figure>\n<p>python中对装饰器的支持采用@这个符号,这个是个语法糖,把@放在use_logging函数定义的开头,相当于foo = use_logging(foo), 只需要在foo()函数的开头加上@use_logging即可在不改变原函数逻辑的情况实现日志打印ongn<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\"></span><br><span class=\"line\">    def wrapper():</span><br><span class=\"line\">        logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func()</span><br><span class=\"line\">    return wrapper</span><br><span class=\"line\"></span><br><span class=\"line\">@use_logging</span><br><span class=\"line\">def foo():</span><br><span class=\"line\">    print(&quot;i am foo&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">foo()</span><br></pre></td></tr></table></figure></p>\n<p>如果foo()函数需要传参,<em>args/*</em>kargs 利用这这两个参数可以传类似多个参数或者关键字参数,这个时候use_logging(func)中的内部函数可以更改为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def wrapper(*args, **kargs):</span><br><span class=\"line\">    logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">    return func(*args, **kargs)</span><br><span class=\"line\">return wrapper</span><br></pre></td></tr></table></figure></p>\n<p>装饰器的使用非常灵活,例如还可以定义带参数的装饰器.拿上面日志的例子来说,如果日志打印需要打印不同的等级信息,那么use_logging参数需要有一个日志等级参数.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(level):</span><br><span class=\"line\">    def decorator(func):</span><br><span class=\"line\">        def wrapper(*args, **kwargs):</span><br><span class=\"line\">            if level == &quot;warn&quot;:</span><br><span class=\"line\">                logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">            elif level == &quot;info&quot;:</span><br><span class=\"line\">                logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">            return func(*args)</span><br><span class=\"line\">        return wrapper</span><br><span class=\"line\"></span><br><span class=\"line\">return decorator</span><br><span class=\"line\"></span><br><span class=\"line\">@use_logging(level=&quot;warn&quot;)</span><br><span class=\"line\">def foo(name=&apos;foo&apos;):</span><br><span class=\"line\">    print(&quot;i am %s&quot; % name)</span><br><span class=\"line\"></span><br><span class=\"line\">foo()</span><br></pre></td></tr></table></figure></p>\n<p>use_logging函数定义看起来有点绕, @use_logging(level=”warn”) 会被python解析为foo = use_logging(level)(foo) 这样的话use_logging(level)需要返回一个装饰器, 也就不难理解上面的那段代码了</p>\n<hr>\n<h1 id=\"类装饰器\"><a href=\"#类装饰器\" class=\"headerlink\" title=\"类装饰器\"></a>类装饰器</h1><p>装饰器也可用来装饰类,此时被修饰的函数会调用类的<strong>call</strong>方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Foo(object):</span><br><span class=\"line\">    def __init__(self, func):</span><br><span class=\"line\">        print &quot;sinit in Foo&quot;</span><br><span class=\"line\">        self._func = func</span><br><span class=\"line\"></span><br><span class=\"line\">    def __call__(self):</span><br><span class=\"line\">        print (&apos;class decorator runing&apos;)</span><br><span class=\"line\">        self._func()</span><br><span class=\"line\">        print (&apos;class decorator ending&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">@Foo</span><br><span class=\"line\">def bar():</span><br><span class=\"line\">    print (&apos;bar&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">bar()</span><br></pre></td></tr></table></figure></p>\n<p>上面代码执行完毕后会打印:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">init in Foo</span><br><span class=\"line\">class decorator runing</span><br><span class=\"line\">bar</span><br><span class=\"line\">class decorator ending</span><br></pre></td></tr></table></figure></p>\n<p>Foo函数定义了两个函数:</p>\n<ol>\n<li>一个是<strong>init</strong>()，这个方法是在我们给某个函数decorator时被调用，所以，需要有一个fn的参数，也就是被decorator的函数</li>\n<li>一个是<strong>call</strong>()，这个方法是在我们调用被decorator函数时被调用的</li>\n</ol>\n<h1 id=\"装饰器副作用\"><a href=\"#装饰器副作用\" class=\"headerlink\" title=\"装饰器副作用\"></a>装饰器副作用</h1><p>decorator函数中原函数的信息被破坏了 例如<strong>name</strong> docstring 参数列表等, 不过functools.wraps可以解决这个问题<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def use_logging(func):</span><br><span class=\"line\">    @wraps(func)</span><br><span class=\"line\">    def wrapper(*args, **kwargs):</span><br><span class=\"line\">        if level == &quot;warn&quot;:</span><br><span class=\"line\">            logging.warn(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        elif level == &quot;info&quot;:</span><br><span class=\"line\">            logging.info(&quot;%s is running&quot; % func.__name__)</span><br><span class=\"line\">        return func(*args)</span><br><span class=\"line\">    return wrapper</span><br></pre></td></tr></table></figure></p>\n<p>参考文章: </p>\n<ol>\n<li><a href=\"https://foofish.net/python-decorator.html\" target=\"_blank\" rel=\"noopener\">https://foofish.net/python-decorator.html</a></li>\n<li><a href=\"https://coolshell.cn/articles/11265.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/11265.html</a></li>\n<li><a href=\"https://coolshell.cn/articles/10822.html\" target=\"_blank\" rel=\"noopener\">https://coolshell.cn/articles/10822.html</a></li>\n<li><a href=\"http://book.pythontips.com/en/latest/lambdas.html\" target=\"_blank\" rel=\"noopener\">http://book.pythontips.com/en/latest/lambdas.html</a></li>\n</ol>"},{"title":"zookeeper实现分布式锁","date":"2018-05-16T11:21:51.000Z","_content":"\n\n# 基于Zookeeper的分布式锁\n\n[原文地址](http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html)\n\n实现分布式锁目前有三种流行方案，分别为基于数据库、Redis、Zookeeper的方案，其中前两种方案网络上有很多资料可以参考，本文不做展开。我们来看下使用Zookeeper如何实现分布式锁。\n\n## 什么是Zookeeper？\n\nZookeeper（业界简称zk）是一种提供配置管理、分布式协同以及命名的中心化服务，这些提供的功能都是分布式系统中非常底层且必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。因此zookeeper提供了这些功能，开发者在zookeeper之上构建自己的各种分布式系统。\n\n虽然zookeeper的实现比较复杂，但是它提供的模型抽象却是非常简单的。Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。\n\n而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。\n\n<!--more-->\n\n架构简图如下所示：\n\n![zk-framework](http://www.dengshenyu.com/assets/zookeeper-distributed-lock/zk-framework.png)\n\n客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。\n\n有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。\n\n## 如何使用zookeeper实现分布式锁？\n\n在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质：\n\n1. 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。\n2. 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。\n3. 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。\n\n下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：\n\n1. 客户端连接zookeeper，并在/lock下创建**临时的**且**有序的**子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。\n2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中**序号最小**的子节点，如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；\n3. 执行业务代码；\n4. 完成业务流程后，删除对应的子节点释放锁。\n\n步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。\n\n另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。因为zookeeper提供的API中设置监听器的操作与读操作是**原子执行**的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。\n\n最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。应该怎么做呢？在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。\n\n所以调整后的分布式锁算法流程如下：\n\n1. 客户端连接zookeeper，并在/lock下创建**临时的**且**有序的**子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。\n2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中**序号最小**的子节点，如果是则认为获得锁，否则**监听刚好在自己之前一位的子节点删除消息**，获得子节点变更通知后重复此步骤直至获得锁；\n3. 执行业务代码；\n4. 完成业务流程后，删除对应的子节点释放锁。\n\n## Curator的源码分析\n\n虽然zookeeper原生客户端暴露的API已经非常简洁了，但是实现一个分布式锁还是比较麻烦的…我们可以直接使用[curator](http://curator.apache.org/index.html)这个开源项目提供的zookeeper分布式锁实现。\n\n我们只需要引入下面这个包（基于maven）：\n\n```\n<dependency>\n    <groupId>org.apache.curator</groupId>\n    <artifactId>curator-recipes</artifactId>\n    <version>4.0.0</version>\n</dependency>\n```\n\n然后就可以用啦！代码如下：\n\n```\npublic static void main(String[] args) throws Exception {\n    //创建zookeeper的客户端\n    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n    CuratorFramework client = CuratorFrameworkFactory.newClient(\"10.21.41.181:2181,10.21.42.47:2181,10.21.49.252:2181\", retryPolicy);\n    client.start();\n\n    //创建分布式锁, 锁空间的根节点路径为/curator/lock\n    InterProcessMutex mutex = new InterProcessMutex(client, \"/curator/lock\");\n    mutex.acquire();\n    //获得了锁, 进行业务流程\n    System.out.println(\"Enter mutex\");\n    //完成业务流程, 释放锁\n    mutex.release();\n    \n    //关闭客户端\n    client.close();\n}\n```\n\n可以看到关键的核心操作就只有mutex.acquire()和mutex.release()，简直太方便了！\n\n下面来分析下获取锁的源码实现。acquire的方法如下：\n\n```\n/*\n * 获取锁，当锁被占用时会阻塞等待，这个操作支持同线程的可重入（也就是重复获取锁），acquire的次数需要与release的次数相同。\n * @throws Exception ZK errors, connection interruptions\n */\n@Override\npublic void acquire() throws Exception\n{\n    if ( !internalLock(-1, null) )\n    {\n        throw new IOException(\"Lost connection while trying to acquire lock: \" + basePath);\n    }\n}\n```\n\n这里有个地方需要注意，当与zookeeper通信存在异常时，acquire会直接抛出异常，需要使用者自身做重试策略。代码中调用了internalLock(-1, null)，参数表明在锁被占用时永久阻塞等待。internalLock的代码如下：\n\n```\nprivate boolean internalLock(long time, TimeUnit unit) throws Exception\n{\n\n    //这里处理同线程的可重入性，如果已经获得锁，那么只是在对应的数据结构中增加acquire的次数统计，直接返回成功\n    Thread currentThread = Thread.currentThread();\n    LockData lockData = threadData.get(currentThread);\n    if ( lockData != null )\n    {\n        // re-entering\n        lockData.lockCount.incrementAndGet();\n        return true;\n    }\n\n    //这里才真正去zookeeper中获取锁\n    String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());\n    if ( lockPath != null )\n    {\n        //获得锁之后，记录当前的线程获得锁的信息，在重入时只需在LockData中增加次数统计即可\n        LockData newLockData = new LockData(currentThread, lockPath);\n        threadData.put(currentThread, newLockData);\n        return true;\n    }\n\n    //在阻塞返回时仍然获取不到锁，这里上下文的处理隐含的意思为zookeeper通信异常\n    return false;\n}\n```\n\n代码中增加了具体注释，不做展开。看下zookeeper获取锁的具体实现：\n\n```\nString attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception\n{\n    //参数初始化，此处省略\n    //...\n   \n    //自旋获取锁\n    while ( !isDone )\n    {\n        isDone = true;\n\n        try\n        {\n            //在锁空间下创建临时且有序的子节点\n            ourPath = driver.createsTheLock(client, path, localLockNodeBytes);\n            //判断是否获得锁（子节点序号最小），获得锁则直接返回，否则阻塞等待前一个子节点删除通知\n            hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath);\n        }\n        catch ( KeeperException.NoNodeException e )\n        {\n            //对于NoNodeException，代码中确保了只有发生session过期才会在这里抛出NoNodeException，因此这里根据重试策略进行重试\n            if ( client.getZookeeperClient().getRetryPolicy().allowRetry(retryCount++, System.currentTimeMillis() - startMillis, RetryLoop.getDefaultRetrySleeper()) )\n            {\n                isDone = false;\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }\n\n    //如果获得锁则返回该子节点的路径\n    if ( hasTheLock )\n    {\n        return ourPath;\n    }\n\n    return null;\n}\n```\n\n上面代码中主要有两步操作：\n\n- driver.createsTheLock：创建临时且有序的子节点，里面实现比较简单不做展开，主要关注几种节点的模式：1）PERSISTENT（永久）；2）PERSISTENT_SEQUENTIAL（永久且有序）；3）EPHEMERAL（临时）；4）EPHEMERAL_SEQUENTIAL（临时且有序）。\n- internalLockLoop：阻塞等待直到获得锁。\n\n看下internalLockLoop是怎么判断锁以及阻塞等待的，这里删除了一些无关代码，只保留主流程：\n\n```\n//自旋直至获得锁\nwhile ( (client.getState() == CuratorFrameworkState.STARTED) && !haveTheLock )\n{\n    //获取所有的子节点列表，并且按序号从小到大排序\n    List<String>        children = getSortedChildren();\n    \n    //根据序号判断当前子节点是否为最小子节点\n    String              sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash\n    PredicateResults    predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases);\n    if ( predicateResults.getsTheLock() )\n    {\n        //如果为最小子节点则认为获得锁\n        haveTheLock = true;\n    }\n    else\n    {\n        //否则获取前一个子节点\n        String  previousSequencePath = basePath + \"/\" + predicateResults.getPathToWatch();\n\n        //这里使用对象监视器做线程同步，当获取不到锁时监听前一个子节点删除消息并且进行wait()，当前一个子节点删除（也就是锁释放）时，回调会通过notifyAll唤醒此线程，此线程继续自旋判断是否获得锁\n        synchronized(this)\n        {\n            try \n            {\n                //这里使用getData()接口而不是checkExists()是因为，如果前一个子节点已经被删除了那么会抛出异常而且不会设置事件监听器，而checkExists虽然也可以获取到节点是否存在的信息但是同时设置了监听器，这个监听器其实永远不会触发，对于zookeeper来说属于资源泄露\n                client.getData().usingWatcher(watcher).forPath(previousSequencePath);\n\n                //如果设置了阻塞等待的时间\n                if ( millisToWait != null )\n                {\n                    millisToWait -= (System.currentTimeMillis() - startMillis);\n                    startMillis = System.currentTimeMillis();\n                    if ( millisToWait <= 0 )\n                    {\n                        doDelete = true;    // 等待时间到达，删除对应的子节点\n                        break;\n                    }\n                    \n                    //等待相应的时间\n                    wait(millisToWait);\n                }\n                else\n                {\n                   //永远等待\n                    wait();\n                }\n            }\n            catch ( KeeperException.NoNodeException e ) \n            {\n                //上面使用getData来设置监听器时，如果前一个子节点已经被删除那么会抛出NoNodeException，只需要自旋一次即可，无需额外处理\n            }\n        }\n    }\n}\n```\n\n\n","source":"_posts/zookeeper分布式锁.md","raw":"---\ntitle: zookeeper实现分布式锁\ndate: 2018-05-16 19:21:51\ntags: zookeeper\n---\n\n\n# 基于Zookeeper的分布式锁\n\n[原文地址](http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html)\n\n实现分布式锁目前有三种流行方案，分别为基于数据库、Redis、Zookeeper的方案，其中前两种方案网络上有很多资料可以参考，本文不做展开。我们来看下使用Zookeeper如何实现分布式锁。\n\n## 什么是Zookeeper？\n\nZookeeper（业界简称zk）是一种提供配置管理、分布式协同以及命名的中心化服务，这些提供的功能都是分布式系统中非常底层且必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。因此zookeeper提供了这些功能，开发者在zookeeper之上构建自己的各种分布式系统。\n\n虽然zookeeper的实现比较复杂，但是它提供的模型抽象却是非常简单的。Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。\n\n而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。\n\n<!--more-->\n\n架构简图如下所示：\n\n![zk-framework](http://www.dengshenyu.com/assets/zookeeper-distributed-lock/zk-framework.png)\n\n客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。\n\n有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。\n\n## 如何使用zookeeper实现分布式锁？\n\n在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质：\n\n1. 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。\n2. 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。\n3. 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。\n\n下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：\n\n1. 客户端连接zookeeper，并在/lock下创建**临时的**且**有序的**子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。\n2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中**序号最小**的子节点，如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；\n3. 执行业务代码；\n4. 完成业务流程后，删除对应的子节点释放锁。\n\n步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。\n\n另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。因为zookeeper提供的API中设置监听器的操作与读操作是**原子执行**的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。\n\n最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。应该怎么做呢？在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。\n\n所以调整后的分布式锁算法流程如下：\n\n1. 客户端连接zookeeper，并在/lock下创建**临时的**且**有序的**子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。\n2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中**序号最小**的子节点，如果是则认为获得锁，否则**监听刚好在自己之前一位的子节点删除消息**，获得子节点变更通知后重复此步骤直至获得锁；\n3. 执行业务代码；\n4. 完成业务流程后，删除对应的子节点释放锁。\n\n## Curator的源码分析\n\n虽然zookeeper原生客户端暴露的API已经非常简洁了，但是实现一个分布式锁还是比较麻烦的…我们可以直接使用[curator](http://curator.apache.org/index.html)这个开源项目提供的zookeeper分布式锁实现。\n\n我们只需要引入下面这个包（基于maven）：\n\n```\n<dependency>\n    <groupId>org.apache.curator</groupId>\n    <artifactId>curator-recipes</artifactId>\n    <version>4.0.0</version>\n</dependency>\n```\n\n然后就可以用啦！代码如下：\n\n```\npublic static void main(String[] args) throws Exception {\n    //创建zookeeper的客户端\n    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n    CuratorFramework client = CuratorFrameworkFactory.newClient(\"10.21.41.181:2181,10.21.42.47:2181,10.21.49.252:2181\", retryPolicy);\n    client.start();\n\n    //创建分布式锁, 锁空间的根节点路径为/curator/lock\n    InterProcessMutex mutex = new InterProcessMutex(client, \"/curator/lock\");\n    mutex.acquire();\n    //获得了锁, 进行业务流程\n    System.out.println(\"Enter mutex\");\n    //完成业务流程, 释放锁\n    mutex.release();\n    \n    //关闭客户端\n    client.close();\n}\n```\n\n可以看到关键的核心操作就只有mutex.acquire()和mutex.release()，简直太方便了！\n\n下面来分析下获取锁的源码实现。acquire的方法如下：\n\n```\n/*\n * 获取锁，当锁被占用时会阻塞等待，这个操作支持同线程的可重入（也就是重复获取锁），acquire的次数需要与release的次数相同。\n * @throws Exception ZK errors, connection interruptions\n */\n@Override\npublic void acquire() throws Exception\n{\n    if ( !internalLock(-1, null) )\n    {\n        throw new IOException(\"Lost connection while trying to acquire lock: \" + basePath);\n    }\n}\n```\n\n这里有个地方需要注意，当与zookeeper通信存在异常时，acquire会直接抛出异常，需要使用者自身做重试策略。代码中调用了internalLock(-1, null)，参数表明在锁被占用时永久阻塞等待。internalLock的代码如下：\n\n```\nprivate boolean internalLock(long time, TimeUnit unit) throws Exception\n{\n\n    //这里处理同线程的可重入性，如果已经获得锁，那么只是在对应的数据结构中增加acquire的次数统计，直接返回成功\n    Thread currentThread = Thread.currentThread();\n    LockData lockData = threadData.get(currentThread);\n    if ( lockData != null )\n    {\n        // re-entering\n        lockData.lockCount.incrementAndGet();\n        return true;\n    }\n\n    //这里才真正去zookeeper中获取锁\n    String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());\n    if ( lockPath != null )\n    {\n        //获得锁之后，记录当前的线程获得锁的信息，在重入时只需在LockData中增加次数统计即可\n        LockData newLockData = new LockData(currentThread, lockPath);\n        threadData.put(currentThread, newLockData);\n        return true;\n    }\n\n    //在阻塞返回时仍然获取不到锁，这里上下文的处理隐含的意思为zookeeper通信异常\n    return false;\n}\n```\n\n代码中增加了具体注释，不做展开。看下zookeeper获取锁的具体实现：\n\n```\nString attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception\n{\n    //参数初始化，此处省略\n    //...\n   \n    //自旋获取锁\n    while ( !isDone )\n    {\n        isDone = true;\n\n        try\n        {\n            //在锁空间下创建临时且有序的子节点\n            ourPath = driver.createsTheLock(client, path, localLockNodeBytes);\n            //判断是否获得锁（子节点序号最小），获得锁则直接返回，否则阻塞等待前一个子节点删除通知\n            hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath);\n        }\n        catch ( KeeperException.NoNodeException e )\n        {\n            //对于NoNodeException，代码中确保了只有发生session过期才会在这里抛出NoNodeException，因此这里根据重试策略进行重试\n            if ( client.getZookeeperClient().getRetryPolicy().allowRetry(retryCount++, System.currentTimeMillis() - startMillis, RetryLoop.getDefaultRetrySleeper()) )\n            {\n                isDone = false;\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }\n\n    //如果获得锁则返回该子节点的路径\n    if ( hasTheLock )\n    {\n        return ourPath;\n    }\n\n    return null;\n}\n```\n\n上面代码中主要有两步操作：\n\n- driver.createsTheLock：创建临时且有序的子节点，里面实现比较简单不做展开，主要关注几种节点的模式：1）PERSISTENT（永久）；2）PERSISTENT_SEQUENTIAL（永久且有序）；3）EPHEMERAL（临时）；4）EPHEMERAL_SEQUENTIAL（临时且有序）。\n- internalLockLoop：阻塞等待直到获得锁。\n\n看下internalLockLoop是怎么判断锁以及阻塞等待的，这里删除了一些无关代码，只保留主流程：\n\n```\n//自旋直至获得锁\nwhile ( (client.getState() == CuratorFrameworkState.STARTED) && !haveTheLock )\n{\n    //获取所有的子节点列表，并且按序号从小到大排序\n    List<String>        children = getSortedChildren();\n    \n    //根据序号判断当前子节点是否为最小子节点\n    String              sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash\n    PredicateResults    predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases);\n    if ( predicateResults.getsTheLock() )\n    {\n        //如果为最小子节点则认为获得锁\n        haveTheLock = true;\n    }\n    else\n    {\n        //否则获取前一个子节点\n        String  previousSequencePath = basePath + \"/\" + predicateResults.getPathToWatch();\n\n        //这里使用对象监视器做线程同步，当获取不到锁时监听前一个子节点删除消息并且进行wait()，当前一个子节点删除（也就是锁释放）时，回调会通过notifyAll唤醒此线程，此线程继续自旋判断是否获得锁\n        synchronized(this)\n        {\n            try \n            {\n                //这里使用getData()接口而不是checkExists()是因为，如果前一个子节点已经被删除了那么会抛出异常而且不会设置事件监听器，而checkExists虽然也可以获取到节点是否存在的信息但是同时设置了监听器，这个监听器其实永远不会触发，对于zookeeper来说属于资源泄露\n                client.getData().usingWatcher(watcher).forPath(previousSequencePath);\n\n                //如果设置了阻塞等待的时间\n                if ( millisToWait != null )\n                {\n                    millisToWait -= (System.currentTimeMillis() - startMillis);\n                    startMillis = System.currentTimeMillis();\n                    if ( millisToWait <= 0 )\n                    {\n                        doDelete = true;    // 等待时间到达，删除对应的子节点\n                        break;\n                    }\n                    \n                    //等待相应的时间\n                    wait(millisToWait);\n                }\n                else\n                {\n                   //永远等待\n                    wait();\n                }\n            }\n            catch ( KeeperException.NoNodeException e ) \n            {\n                //上面使用getData来设置监听器时，如果前一个子节点已经被删除那么会抛出NoNodeException，只需要自旋一次即可，无需额外处理\n            }\n        }\n    }\n}\n```\n\n\n","slug":"zookeeper分布式锁","published":1,"updated":"2019-01-20T06:21:06.625Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubp70012amuma89f9mh4","content":"<h1 id=\"基于Zookeeper的分布式锁\"><a href=\"#基于Zookeeper的分布式锁\" class=\"headerlink\" title=\"基于Zookeeper的分布式锁\"></a>基于Zookeeper的分布式锁</h1><p><a href=\"http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html\" target=\"_blank\" rel=\"noopener\">原文地址</a></p>\n<p>实现分布式锁目前有三种流行方案，分别为基于数据库、Redis、Zookeeper的方案，其中前两种方案网络上有很多资料可以参考，本文不做展开。我们来看下使用Zookeeper如何实现分布式锁。</p>\n<h2 id=\"什么是Zookeeper？\"><a href=\"#什么是Zookeeper？\" class=\"headerlink\" title=\"什么是Zookeeper？\"></a>什么是Zookeeper？</h2><p>Zookeeper（业界简称zk）是一种提供配置管理、分布式协同以及命名的中心化服务，这些提供的功能都是分布式系统中非常底层且必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。因此zookeeper提供了这些功能，开发者在zookeeper之上构建自己的各种分布式系统。</p>\n<p>虽然zookeeper的实现比较复杂，但是它提供的模型抽象却是非常简单的。Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。</p>\n<p>而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。</p>\n<a id=\"more\"></a>\n<p>架构简图如下所示：</p>\n<p><img src=\"http://www.dengshenyu.com/assets/zookeeper-distributed-lock/zk-framework.png\" alt=\"zk-framework\"></p>\n<p>客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。</p>\n<p>有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。</p>\n<h2 id=\"如何使用zookeeper实现分布式锁？\"><a href=\"#如何使用zookeeper实现分布式锁？\" class=\"headerlink\" title=\"如何使用zookeeper实现分布式锁？\"></a>如何使用zookeeper实现分布式锁？</h2><p>在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质：</p>\n<ol>\n<li>有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。</li>\n<li>临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。</li>\n<li>事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。</li>\n</ol>\n<p>下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：</p>\n<ol>\n<li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li>\n<li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；</li>\n<li>执行业务代码；</li>\n<li>完成业务流程后，删除对应的子节点释放锁。</li>\n</ol>\n<p>步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。</p>\n<p>另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。因为zookeeper提供的API中设置监听器的操作与读操作是<strong>原子执行</strong>的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。</p>\n<p>最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。应该怎么做呢？在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。</p>\n<p>所以调整后的分布式锁算法流程如下：</p>\n<ol>\n<li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li>\n<li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则<strong>监听刚好在自己之前一位的子节点删除消息</strong>，获得子节点变更通知后重复此步骤直至获得锁；</li>\n<li>执行业务代码；</li>\n<li>完成业务流程后，删除对应的子节点释放锁。</li>\n</ol>\n<h2 id=\"Curator的源码分析\"><a href=\"#Curator的源码分析\" class=\"headerlink\" title=\"Curator的源码分析\"></a>Curator的源码分析</h2><p>虽然zookeeper原生客户端暴露的API已经非常简洁了，但是实现一个分布式锁还是比较麻烦的…我们可以直接使用<a href=\"http://curator.apache.org/index.html\" target=\"_blank\" rel=\"noopener\">curator</a>这个开源项目提供的zookeeper分布式锁实现。</p>\n<p>我们只需要引入下面这个包（基于maven）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;4.0.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>然后就可以用啦！代码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static void main(String[] args) throws Exception &#123;</span><br><span class=\"line\">    //创建zookeeper的客户端</span><br><span class=\"line\">    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);</span><br><span class=\"line\">    CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;10.21.41.181:2181,10.21.42.47:2181,10.21.49.252:2181&quot;, retryPolicy);</span><br><span class=\"line\">    client.start();</span><br><span class=\"line\"></span><br><span class=\"line\">    //创建分布式锁, 锁空间的根节点路径为/curator/lock</span><br><span class=\"line\">    InterProcessMutex mutex = new InterProcessMutex(client, &quot;/curator/lock&quot;);</span><br><span class=\"line\">    mutex.acquire();</span><br><span class=\"line\">    //获得了锁, 进行业务流程</span><br><span class=\"line\">    System.out.println(&quot;Enter mutex&quot;);</span><br><span class=\"line\">    //完成业务流程, 释放锁</span><br><span class=\"line\">    mutex.release();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //关闭客户端</span><br><span class=\"line\">    client.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到关键的核心操作就只有mutex.acquire()和mutex.release()，简直太方便了！</p>\n<p>下面来分析下获取锁的源码实现。acquire的方法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * 获取锁，当锁被占用时会阻塞等待，这个操作支持同线程的可重入（也就是重复获取锁），acquire的次数需要与release的次数相同。</span><br><span class=\"line\"> * @throws Exception ZK errors, connection interruptions</span><br><span class=\"line\"> */</span><br><span class=\"line\">@Override</span><br><span class=\"line\">public void acquire() throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    if ( !internalLock(-1, null) )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        throw new IOException(&quot;Lost connection while trying to acquire lock: &quot; + basePath);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里有个地方需要注意，当与zookeeper通信存在异常时，acquire会直接抛出异常，需要使用者自身做重试策略。代码中调用了internalLock(-1, null)，参数表明在锁被占用时永久阻塞等待。internalLock的代码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean internalLock(long time, TimeUnit unit) throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    //这里处理同线程的可重入性，如果已经获得锁，那么只是在对应的数据结构中增加acquire的次数统计，直接返回成功</span><br><span class=\"line\">    Thread currentThread = Thread.currentThread();</span><br><span class=\"line\">    LockData lockData = threadData.get(currentThread);</span><br><span class=\"line\">    if ( lockData != null )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        // re-entering</span><br><span class=\"line\">        lockData.lockCount.incrementAndGet();</span><br><span class=\"line\">        return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //这里才真正去zookeeper中获取锁</span><br><span class=\"line\">    String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());</span><br><span class=\"line\">    if ( lockPath != null )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //获得锁之后，记录当前的线程获得锁的信息，在重入时只需在LockData中增加次数统计即可</span><br><span class=\"line\">        LockData newLockData = new LockData(currentThread, lockPath);</span><br><span class=\"line\">        threadData.put(currentThread, newLockData);</span><br><span class=\"line\">        return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //在阻塞返回时仍然获取不到锁，这里上下文的处理隐含的意思为zookeeper通信异常</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>代码中增加了具体注释，不做展开。看下zookeeper获取锁的具体实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    //参数初始化，此处省略</span><br><span class=\"line\">    //...</span><br><span class=\"line\">   </span><br><span class=\"line\">    //自旋获取锁</span><br><span class=\"line\">    while ( !isDone )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        isDone = true;</span><br><span class=\"line\"></span><br><span class=\"line\">        try</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            //在锁空间下创建临时且有序的子节点</span><br><span class=\"line\">            ourPath = driver.createsTheLock(client, path, localLockNodeBytes);</span><br><span class=\"line\">            //判断是否获得锁（子节点序号最小），获得锁则直接返回，否则阻塞等待前一个子节点删除通知</span><br><span class=\"line\">            hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        catch ( KeeperException.NoNodeException e )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            //对于NoNodeException，代码中确保了只有发生session过期才会在这里抛出NoNodeException，因此这里根据重试策略进行重试</span><br><span class=\"line\">            if ( client.getZookeeperClient().getRetryPolicy().allowRetry(retryCount++, System.currentTimeMillis() - startMillis, RetryLoop.getDefaultRetrySleeper()) )</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                isDone = false;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            else</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                throw e;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //如果获得锁则返回该子节点的路径</span><br><span class=\"line\">    if ( hasTheLock )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        return ourPath;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return null;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码中主要有两步操作：</p>\n<ul>\n<li>driver.createsTheLock：创建临时且有序的子节点，里面实现比较简单不做展开，主要关注几种节点的模式：1）PERSISTENT（永久）；2）PERSISTENT_SEQUENTIAL（永久且有序）；3）EPHEMERAL（临时）；4）EPHEMERAL_SEQUENTIAL（临时且有序）。</li>\n<li>internalLockLoop：阻塞等待直到获得锁。</li>\n</ul>\n<p>看下internalLockLoop是怎么判断锁以及阻塞等待的，这里删除了一些无关代码，只保留主流程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//自旋直至获得锁</span><br><span class=\"line\">while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    //获取所有的子节点列表，并且按序号从小到大排序</span><br><span class=\"line\">    List&lt;String&gt;        children = getSortedChildren();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //根据序号判断当前子节点是否为最小子节点</span><br><span class=\"line\">    String              sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash</span><br><span class=\"line\">    PredicateResults    predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases);</span><br><span class=\"line\">    if ( predicateResults.getsTheLock() )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //如果为最小子节点则认为获得锁</span><br><span class=\"line\">        haveTheLock = true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    else</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //否则获取前一个子节点</span><br><span class=\"line\">        String  previousSequencePath = basePath + &quot;/&quot; + predicateResults.getPathToWatch();</span><br><span class=\"line\"></span><br><span class=\"line\">        //这里使用对象监视器做线程同步，当获取不到锁时监听前一个子节点删除消息并且进行wait()，当前一个子节点删除（也就是锁释放）时，回调会通过notifyAll唤醒此线程，此线程继续自旋判断是否获得锁</span><br><span class=\"line\">        synchronized(this)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            try </span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                //这里使用getData()接口而不是checkExists()是因为，如果前一个子节点已经被删除了那么会抛出异常而且不会设置事件监听器，而checkExists虽然也可以获取到节点是否存在的信息但是同时设置了监听器，这个监听器其实永远不会触发，对于zookeeper来说属于资源泄露</span><br><span class=\"line\">                client.getData().usingWatcher(watcher).forPath(previousSequencePath);</span><br><span class=\"line\"></span><br><span class=\"line\">                //如果设置了阻塞等待的时间</span><br><span class=\"line\">                if ( millisToWait != null )</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    millisToWait -= (System.currentTimeMillis() - startMillis);</span><br><span class=\"line\">                    startMillis = System.currentTimeMillis();</span><br><span class=\"line\">                    if ( millisToWait &lt;= 0 )</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        doDelete = true;    // 等待时间到达，删除对应的子节点</span><br><span class=\"line\">                        break;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    </span><br><span class=\"line\">                    //等待相应的时间</span><br><span class=\"line\">                    wait(millisToWait);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                else</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                   //永远等待</span><br><span class=\"line\">                    wait();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            catch ( KeeperException.NoNodeException e ) </span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                //上面使用getData来设置监听器时，如果前一个子节点已经被删除那么会抛出NoNodeException，只需要自旋一次即可，无需额外处理</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"基于Zookeeper的分布式锁\"><a href=\"#基于Zookeeper的分布式锁\" class=\"headerlink\" title=\"基于Zookeeper的分布式锁\"></a>基于Zookeeper的分布式锁</h1><p><a href=\"http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html\" target=\"_blank\" rel=\"noopener\">原文地址</a></p>\n<p>实现分布式锁目前有三种流行方案，分别为基于数据库、Redis、Zookeeper的方案，其中前两种方案网络上有很多资料可以参考，本文不做展开。我们来看下使用Zookeeper如何实现分布式锁。</p>\n<h2 id=\"什么是Zookeeper？\"><a href=\"#什么是Zookeeper？\" class=\"headerlink\" title=\"什么是Zookeeper？\"></a>什么是Zookeeper？</h2><p>Zookeeper（业界简称zk）是一种提供配置管理、分布式协同以及命名的中心化服务，这些提供的功能都是分布式系统中非常底层且必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。因此zookeeper提供了这些功能，开发者在zookeeper之上构建自己的各种分布式系统。</p>\n<p>虽然zookeeper的实现比较复杂，但是它提供的模型抽象却是非常简单的。Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。</p>\n<p>而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。</p>","more":"<p>架构简图如下所示：</p>\n<p><img src=\"http://www.dengshenyu.com/assets/zookeeper-distributed-lock/zk-framework.png\" alt=\"zk-framework\"></p>\n<p>客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。</p>\n<p>有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。</p>\n<h2 id=\"如何使用zookeeper实现分布式锁？\"><a href=\"#如何使用zookeeper实现分布式锁？\" class=\"headerlink\" title=\"如何使用zookeeper实现分布式锁？\"></a>如何使用zookeeper实现分布式锁？</h2><p>在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质：</p>\n<ol>\n<li>有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。</li>\n<li>临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。</li>\n<li>事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。</li>\n</ol>\n<p>下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：</p>\n<ol>\n<li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li>\n<li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；</li>\n<li>执行业务代码；</li>\n<li>完成业务流程后，删除对应的子节点释放锁。</li>\n</ol>\n<p>步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。</p>\n<p>另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。因为zookeeper提供的API中设置监听器的操作与读操作是<strong>原子执行</strong>的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。</p>\n<p>最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。应该怎么做呢？在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。</p>\n<p>所以调整后的分布式锁算法流程如下：</p>\n<ol>\n<li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li>\n<li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则<strong>监听刚好在自己之前一位的子节点删除消息</strong>，获得子节点变更通知后重复此步骤直至获得锁；</li>\n<li>执行业务代码；</li>\n<li>完成业务流程后，删除对应的子节点释放锁。</li>\n</ol>\n<h2 id=\"Curator的源码分析\"><a href=\"#Curator的源码分析\" class=\"headerlink\" title=\"Curator的源码分析\"></a>Curator的源码分析</h2><p>虽然zookeeper原生客户端暴露的API已经非常简洁了，但是实现一个分布式锁还是比较麻烦的…我们可以直接使用<a href=\"http://curator.apache.org/index.html\" target=\"_blank\" rel=\"noopener\">curator</a>这个开源项目提供的zookeeper分布式锁实现。</p>\n<p>我们只需要引入下面这个包（基于maven）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;4.0.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n<p>然后就可以用啦！代码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static void main(String[] args) throws Exception &#123;</span><br><span class=\"line\">    //创建zookeeper的客户端</span><br><span class=\"line\">    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);</span><br><span class=\"line\">    CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;10.21.41.181:2181,10.21.42.47:2181,10.21.49.252:2181&quot;, retryPolicy);</span><br><span class=\"line\">    client.start();</span><br><span class=\"line\"></span><br><span class=\"line\">    //创建分布式锁, 锁空间的根节点路径为/curator/lock</span><br><span class=\"line\">    InterProcessMutex mutex = new InterProcessMutex(client, &quot;/curator/lock&quot;);</span><br><span class=\"line\">    mutex.acquire();</span><br><span class=\"line\">    //获得了锁, 进行业务流程</span><br><span class=\"line\">    System.out.println(&quot;Enter mutex&quot;);</span><br><span class=\"line\">    //完成业务流程, 释放锁</span><br><span class=\"line\">    mutex.release();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //关闭客户端</span><br><span class=\"line\">    client.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到关键的核心操作就只有mutex.acquire()和mutex.release()，简直太方便了！</p>\n<p>下面来分析下获取锁的源码实现。acquire的方法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * 获取锁，当锁被占用时会阻塞等待，这个操作支持同线程的可重入（也就是重复获取锁），acquire的次数需要与release的次数相同。</span><br><span class=\"line\"> * @throws Exception ZK errors, connection interruptions</span><br><span class=\"line\"> */</span><br><span class=\"line\">@Override</span><br><span class=\"line\">public void acquire() throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    if ( !internalLock(-1, null) )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        throw new IOException(&quot;Lost connection while trying to acquire lock: &quot; + basePath);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里有个地方需要注意，当与zookeeper通信存在异常时，acquire会直接抛出异常，需要使用者自身做重试策略。代码中调用了internalLock(-1, null)，参数表明在锁被占用时永久阻塞等待。internalLock的代码如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean internalLock(long time, TimeUnit unit) throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    //这里处理同线程的可重入性，如果已经获得锁，那么只是在对应的数据结构中增加acquire的次数统计，直接返回成功</span><br><span class=\"line\">    Thread currentThread = Thread.currentThread();</span><br><span class=\"line\">    LockData lockData = threadData.get(currentThread);</span><br><span class=\"line\">    if ( lockData != null )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        // re-entering</span><br><span class=\"line\">        lockData.lockCount.incrementAndGet();</span><br><span class=\"line\">        return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //这里才真正去zookeeper中获取锁</span><br><span class=\"line\">    String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());</span><br><span class=\"line\">    if ( lockPath != null )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //获得锁之后，记录当前的线程获得锁的信息，在重入时只需在LockData中增加次数统计即可</span><br><span class=\"line\">        LockData newLockData = new LockData(currentThread, lockPath);</span><br><span class=\"line\">        threadData.put(currentThread, newLockData);</span><br><span class=\"line\">        return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //在阻塞返回时仍然获取不到锁，这里上下文的处理隐含的意思为zookeeper通信异常</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>代码中增加了具体注释，不做展开。看下zookeeper获取锁的具体实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    //参数初始化，此处省略</span><br><span class=\"line\">    //...</span><br><span class=\"line\">   </span><br><span class=\"line\">    //自旋获取锁</span><br><span class=\"line\">    while ( !isDone )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        isDone = true;</span><br><span class=\"line\"></span><br><span class=\"line\">        try</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            //在锁空间下创建临时且有序的子节点</span><br><span class=\"line\">            ourPath = driver.createsTheLock(client, path, localLockNodeBytes);</span><br><span class=\"line\">            //判断是否获得锁（子节点序号最小），获得锁则直接返回，否则阻塞等待前一个子节点删除通知</span><br><span class=\"line\">            hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        catch ( KeeperException.NoNodeException e )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            //对于NoNodeException，代码中确保了只有发生session过期才会在这里抛出NoNodeException，因此这里根据重试策略进行重试</span><br><span class=\"line\">            if ( client.getZookeeperClient().getRetryPolicy().allowRetry(retryCount++, System.currentTimeMillis() - startMillis, RetryLoop.getDefaultRetrySleeper()) )</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                isDone = false;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            else</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                throw e;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //如果获得锁则返回该子节点的路径</span><br><span class=\"line\">    if ( hasTheLock )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        return ourPath;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return null;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码中主要有两步操作：</p>\n<ul>\n<li>driver.createsTheLock：创建临时且有序的子节点，里面实现比较简单不做展开，主要关注几种节点的模式：1）PERSISTENT（永久）；2）PERSISTENT_SEQUENTIAL（永久且有序）；3）EPHEMERAL（临时）；4）EPHEMERAL_SEQUENTIAL（临时且有序）。</li>\n<li>internalLockLoop：阻塞等待直到获得锁。</li>\n</ul>\n<p>看下internalLockLoop是怎么判断锁以及阻塞等待的，这里删除了一些无关代码，只保留主流程：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//自旋直至获得锁</span><br><span class=\"line\">while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    //获取所有的子节点列表，并且按序号从小到大排序</span><br><span class=\"line\">    List&lt;String&gt;        children = getSortedChildren();</span><br><span class=\"line\">    </span><br><span class=\"line\">    //根据序号判断当前子节点是否为最小子节点</span><br><span class=\"line\">    String              sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash</span><br><span class=\"line\">    PredicateResults    predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases);</span><br><span class=\"line\">    if ( predicateResults.getsTheLock() )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //如果为最小子节点则认为获得锁</span><br><span class=\"line\">        haveTheLock = true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    else</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        //否则获取前一个子节点</span><br><span class=\"line\">        String  previousSequencePath = basePath + &quot;/&quot; + predicateResults.getPathToWatch();</span><br><span class=\"line\"></span><br><span class=\"line\">        //这里使用对象监视器做线程同步，当获取不到锁时监听前一个子节点删除消息并且进行wait()，当前一个子节点删除（也就是锁释放）时，回调会通过notifyAll唤醒此线程，此线程继续自旋判断是否获得锁</span><br><span class=\"line\">        synchronized(this)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            try </span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                //这里使用getData()接口而不是checkExists()是因为，如果前一个子节点已经被删除了那么会抛出异常而且不会设置事件监听器，而checkExists虽然也可以获取到节点是否存在的信息但是同时设置了监听器，这个监听器其实永远不会触发，对于zookeeper来说属于资源泄露</span><br><span class=\"line\">                client.getData().usingWatcher(watcher).forPath(previousSequencePath);</span><br><span class=\"line\"></span><br><span class=\"line\">                //如果设置了阻塞等待的时间</span><br><span class=\"line\">                if ( millisToWait != null )</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    millisToWait -= (System.currentTimeMillis() - startMillis);</span><br><span class=\"line\">                    startMillis = System.currentTimeMillis();</span><br><span class=\"line\">                    if ( millisToWait &lt;= 0 )</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        doDelete = true;    // 等待时间到达，删除对应的子节点</span><br><span class=\"line\">                        break;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                    </span><br><span class=\"line\">                    //等待相应的时间</span><br><span class=\"line\">                    wait(millisToWait);</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                else</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                   //永远等待</span><br><span class=\"line\">                    wait();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            catch ( KeeperException.NoNodeException e ) </span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                //上面使用getData来设置监听器时，如果前一个子节点已经被删除那么会抛出NoNodeException，只需要自旋一次即可，无需额外处理</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"优秀的 Go存储开源项目和库","date":"2019-03-15T12:18:29.000Z","_content":"\n# 优秀的 Go存储开源项目和库\n\nA curated list of awesome Go storage projects and libraries. Inspired by [awesome-go](https://github.com/avelino/awesome-go).\n\n### Contributing\n\nPlease take a quick gander at the [contribution guidelines](https://github.com/gostor/awesome-go-storage/blob/master/CONTRIBUTING.md) first. Thanks to all [contributors](https://github.com/gostor/awesome-go-storage/graphs/contributors); you rock!\n\n#### *If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!*\n\n### Contents\n\n- Awesome Go Storage\n  - [Storage Server](https://github.com/gostor/awesome-go-storage#storage-server)\n  - [Key-Value Store](https://github.com/gostor/awesome-go-storage#key-value-store)\n  - [File System](https://github.com/gostor/awesome-go-storage#file-system)\n  - [Database](https://github.com/gostor/awesome-go-storage#database)\n  - [Database Drivers](https://github.com/gostor/awesome-go-storage#database-drivers)\n\n## Storage Server\n\n*Storage Servers implemented in Go.*\n\n- [minio](https://github.com/minio/minio) - Minio is an open source object storage server compatible with Amazon S3 APIs.\n- [rclone](https://github.com/ncw/rclone) - \"rsync for cloud storage\" - Google Drive, Amazon Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Cloudfile…\n- [perkeep](https://github.com/perkeep/perkeep) - Perkeep is your personal storage system for life: a way of storing, syncing, sharing, modelling and backing up content.\n- [s3git](https://github.com/s3git/s3git) - Git for Cloud Storage. Distributed Version Control for Data.\n- [storj](https://github.com/storj/storj) - Decentralized cloud object storage that is affordable, easy to use, private, and secure.\n- [rook](https://github.com/rook/rook) - Open, Cloud Native, and Universal Distributed Storage.\n- [longhorn](https://github.com/rancher/longhorn) Longhorn is an open source persistent block storage server delivered via containers.\n\n<!--more-->\n\n## Key-Value Store\n\n*Key-Value Store implemented in Go.*\n\n- [Bitcask](https://github.com/prologic/bitcask) - Bitcask is an embeddable, persistent and fast key-value (KV) database written in pure Go with predictable read/write performance, low latency and high throughput thanks to the bitcask on-disk layout (LSM+WAL).\n- [BadgerDB](https://github.com/dgraph-io/badger) - BadgerDB is an embeddable, persistent, simple and fast key-value (KV) database written in pure Go. It's meant to be a performant alternative to non-Go-based key-value stores like RocksDB.\n- [biscuit](https://github.com/dcoker/biscuit) - Biscuit is a multi-region HA key-value store for your AWS infrastructure secrets.\n- [consul](https://github.com/hashicorp/consul) - Distributed consistent replicated key-value store for service discovery and configuration.\n- [diskv](https://github.com/peterbourgon/diskv) - A disk-backed key-value store.\n- [etcd](https://github.com/coreos/etcd) - Distributed reliable key-value store for the most critical data of a distributed system.\n- [go-cache](https://github.com/patrickmn/go-cache) - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.\n\n## File System\n\n*File Systems implemented in Go.*\n\n- [git-lfs](https://github.com/git-lfs/git-lfs) - Git extension for versioning large files.\n- [seaweedfs](https://github.com/chrislusf/seaweedfs) - SeaweedFS is a simple and highly scalable distributed file system for small files.\n- [fsnotify](https://github.com/fsnotify/fsnotify) - Cross-platform file system notifications for Go.\n- [goofys](https://github.com/kahing/goofys) - A high-performance, POSIX-ish Amazon S3 file system written in Go.\n- [go-systemd](https://github.com/coreos/go-systemd) - Go bindings to systemd socket activation, journal, D-Bus, and unit files.\n- [gcsfuse](https://github.com/GoogleCloudPlatform/gcsfuse) - A user-space file system for interacting with Google Cloud Storage.\n- [svfs](https://github.com/ovh/svfs) - A virtual file system over Openstack Swift built upon fuse.\n- [afero](https://github.com/spf13/afero) - A FileSystem Abstraction System for Go\n\n## Database\n\n*Databases implemented in Go.*\n\n- [BigCache](https://github.com/allegro/bigcache) - Efficient key/value cache for gigabytes of data.\n- [bolt](https://github.com/boltdb/bolt) - A low-level key/value database for Go. This original version by Ben Johnson has been marked as unmaintained and forked by [CoreOS bbolt](https://github.com/coreos/bbolt).\n- [buntdb](https://github.com/tidwall/buntdb) - A fast, embeddable, in-memory key/value database for Go with custom indexing and spatial support.\n- [cache2go](https://github.com/muesli/cache2go) - An in-memory key:value cache which supports automatic invalidation based on timeouts.\n- [cockroach](https://github.com/cockroachdb/cockroach) - A Scalable, Geo-Replicated, Transactional Datastore\n- [couchcache](https://github.com/codingsince1985/couchcache) - A RESTful caching micro-service backed by Couchbase server.\n- [CovenantSQL](https://github.com/CovenantSQL/CovenantSQL) - A SQL Database with Blockchain features.\n- [dgraph](https://github.com/dgraph-io/dgraph) - Scalable, Distributed, Low Latency, High Throughput Graph Database.\n- [diskv](https://github.com/peterbourgon/diskv) - A home-grown disk-backed key-value store.\n- [eliasdb](https://github.com/krotik/eliasdb) - Dependency-free, transactional graph database with REST API, phrase search and SQL-like query language.\n- [emitter](https://github.com/emitter-io/emitter) - Scalable, low-latency, distributed & secure pub/sub database with time-series message storage, suitable for IoT, gaming, apps and real-time web.\n- [forestdb](https://github.com/couchbase/goforestdb) - Go bindings for ForestDB.\n- [GCache](https://github.com/bluele/gcache) - Cache library with support for expirable Cache, LFU, LRU and ARC.\n- [geocache](https://github.com/melihmucuk/geocache) - An in-memory cache that is suitable for geolocation based applications.\n- [go-cache](https://github.com/pmylund/go-cache) - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.\n- [goleveldb](https://github.com/syndtr/goleveldb) - An implementation of the [LevelDB](https://github.com/google/leveldb) key/value database in the Go.\n- [groupcache](https://github.com/golang/groupcache) - Groupcache is a caching and cache-filling library, intended as a replacement for memcached in many cases.\n- [influxdb](https://github.com/influxdb/influxdb) - Scalable datastore for metrics, events, and real-time analytics\n- [ledisdb](https://github.com/siddontang/ledisdb) - Ledisdb is a high performance NoSQL like Redis based on LevelDB.\n- [levigo](https://github.com/jmhodges/levigo) - Levigo is a Go wrapper for LevelDB.\n- [moss](https://github.com/couchbase/moss) - Moss is a simple LSM key-value storage engine written in 100% Go.\n- [noms](https://github.com/attic-labs/noms) - The versioned, forkable, syncable database.\n- [piladb](https://github.com/fern4lvarez/piladb) - Lightweight RESTful database engine based on stack data structures.\n- [pREST](https://github.com/nuveo/prest) - Serve a RESTful API from any PostgreSQL database.\n- [prometheus](https://github.com/prometheus/prometheus) - Monitoring system and time series database.\n- [rqlite](https://github.com/rqlite/rqlite) - The lightweight, distributed, relational database built on SQLite.\n- [scribble](https://github.com/nanobox-io/golang-scribble) - A tiny flat file JSON store.\n- [tidb](https://github.com/pingcap/tidb) - TiDB is a distributed SQL database. Inspired by the design of Google F1.\n- [tiedot](https://github.com/HouzuoGuo/tiedot) - Your NoSQL database powered by Golang.\n- [Tile38](https://github.com/tidwall/tile38) - A geolocation DB with spatial index and realtime geofencing.\n\n*Database schema migration.*\n\n- [darwin](https://github.com/GuiaBolso/darwin) - Database schema evolution library for Go\n- [goose](https://github.com/steinbacher/goose) - Database migration tool. You can manage your database's evolution by creating incremental SQL or Go scripts.\n- [gormigrate](https://github.com/go-gormigrate/gormigrate) - Database schema migration helper for Gorm ORM.\n- [migrate](https://github.com/mattes/migrate) - Database migration handling in Golang support MySQL, PostgreSQL, Cassandra, and SQLite.\n- [pravasan](https://github.com/pravasan/pravasan) - Simple Migration tool - currently for MySQL but planning to support soon for Postgres, SQLite, MongoDB, etc.,\n- [soda](https://github.com/markbates/pop/tree/master/soda) - Database migration, creation, ORM, etc... for MySQL, PostgreSQL, and SQLite.\n- [sql-migrate](https://github.com/rubenv/sql-migrate) - Database migration tool. Allows embedding migrations into the application using go-bindata.\n\n*Database tools.*\n\n- [go-mysql](https://github.com/siddontang/go-mysql) - A go toolset to handle MySQL protocol and replication.\n- [go-mysql-elasticsearch](https://github.com/siddontang/go-mysql-elasticsearch) - Sync your MySQL data into Elasticsearch automatically.\n- [kingshard](https://github.com/flike/kingshard) - kingshard is a high performance proxy for MySQL powered by Golang.\n- [myreplication](https://github.com/2tvenom/myreplication) - MySql binary log replication listener. Support statement and row based replication.\n- [orchestrator](https://github.com/outbrain/orchestrator) - MySQL replication topology manager & visualizer\n- [pgweb](https://github.com/sosedoff/pgweb) - A web-based PostgreSQL database browser\n- [vitess](https://github.com/youtube/vitess) - vitess provides servers and tools which facilitate scaling of MySQL databases for large scale web services.\n- [usql](https://github.com/xo/usql) - universal command-line interface for SQL databases\n\n*SQL query builder, libraries for building and using SQL.*\n\n- [dat](https://github.com/mgutz/dat) - Go Postgres Data Access Toolkit\n- [Dotsql](https://github.com/gchaincl/dotsql) - Go library that helps you keep sql files in one place and use it with ease.\n- [goqu](https://github.com/doug-martin/goqu) - An idiomatic SQL builder and query library.\n- [igor](https://github.com/galeone/igor) - Abstraction layer for PostgreSQL that supports advanced functionality and uses gorm-like syntax.\n- [ozzo-dbx](https://github.com/go-ozzo/ozzo-dbx) - Powerful data retrieval methods as well as DB-agnostic query building capabilities.\n- [scaneo](https://github.com/variadico/scaneo) - Generate Go code to convert database rows into arbitrary structs.\n- [sqrl](https://github.com/elgris/sqrl) - SQL query builder, fork of Squirrel with improved performance.\n- [Squirrel](https://github.com/Masterminds/squirrel) - Go library that helps you build SQL queries.\n- [xo](https://github.com/knq/xo) - Generate idiomatic Go code for databases based on existing schema definitions or custom queries supporting PostgreSQL, MySQL, SQLite, Oracle, and Microsoft SQL Server.\n\n## Database Drivers\n\n*Libraries for connecting and operating databases.*\n\n- Relational Databases\n  - [bgc](https://github.com/viant/bgc) - Datastore Connectivity for BigQuery for go.\n  - [firebirdsql](https://github.com/nakagami/firebirdsql) - Firebird RDBMS SQL driver for Go\n  - [go-adodb](https://github.com/mattn/go-adodb) - Microsoft ActiveX Object DataBase driver for go that using database/sql.\n  - [go-bqstreamer](https://github.com/rounds/go-bqstreamer) - BigQuery fast and concurrent stream insert.\n  - [go-mssqldb](https://github.com/denisenkom/go-mssqldb) - Microsoft MSSQL driver in go language.\n  - [go-oci8](https://github.com/mattn/go-oci8) - Oracle driver for go that using database/sql.\n  - [go-sql-driver/mysql](https://github.com/go-sql-driver/mysql) - MySQL driver for Go.\n  - [go-sqlite3](https://github.com/mattn/go-sqlite3) - SQLite3 driver for go that using database/sql.\n  - [gofreetds](https://github.com/minus5/gofreetds) Microsoft MSSQL driver. Go wrapper over [FreeTDS](http://www.freetds.org/).\n  - [pgx](https://github.com/jackc/pgx) - PostgreSQL driver supporting features beyond those exposed by database/sql.\n  - [pq](https://github.com/lib/pq) - Pure Go Postgres driver for database/sql.\n- NoSQL Databases\n  - [aerospike-client-go](https://github.com/aerospike/aerospike-client-go) - Aerospike client in Go language.\n  - [arangolite](https://github.com/solher/arangolite) - Lightweight golang driver for ArangoDB.\n  - [asc](https://github.com/viant/asc) - Datastore Connectivity for Aerospike for go.\n  - [cayley](https://github.com/google/cayley) - A graph database with support for multiple backends.\n  - [dsc](https://github.com/viant/dsc) - Datastore connectivity for SQL, NoSQL, structured files.\n  - [dynago](https://github.com/underarmour/dynago) - Dynago is a principle of least surprise client for DynamoDB\n  - [go-couchbase](https://github.com/couchbase/go-couchbase) - Couchbase client in Go\n  - [go-couchdb](https://github.com/fjl/go-couchdb) - Yet another CouchDB HTTP API wrapper for Go\n  - [gocb](https://github.com/couchbase/gocb) - Official Couchbase Go SDK\n  - [gocql](http://gocql.github.io/) - A Go language driver for Apache Cassandra.\n  - [gomemcache](https://github.com/bradfitz/gomemcache/) - memcache client library for the Go programming language.\n  - [gorethink](https://github.com/GoRethink/gorethink) - Go language driver for RethinkDB\n  - [goriak](https://github.com/zegl/goriak) - Go language driver for Riak KV\n  - [mgo](https://godoc.org/labix.org/v2/mgo) - MongoDB driver for the Go language that implements a rich and well tested selection of features under a very simple API following standard Go idioms.\n  - [neo4j](https://github.com/cihangir/neo4j) - Neo4j Rest API Bindings for Golang\n  - [Neo4j-GO](https://github.com/davemeehan/Neo4j-GO) - Neo4j REST Client in golang.\n  - [neoism](https://github.com/jmcvetta/neoism) - Neo4j client for Golang\n  - [redigo](https://github.com/gomodule/redigo) - Redigo is a Go client for the Redis database.\n  - [redis](https://github.com/go-redis/redis) - Redis client for Golang\n  - [redis](https://github.com/hoisie/redis) - A simple, powerful Redis client for Go.\n  - [redis](https://github.com/bsm/redeo) - Redis-protocol compatible TCP servers/services.\n- Search and Analytic Databases\n  - [bleve](https://github.com/blevesearch/bleve) - A modern text indexing library for go.\n  - [elastic](https://github.com/olivere/elastic) - Elasticsearch client for Go.\n  - [elastigo](https://github.com/mattbaird/elastigo) - A Elasticsearch client library.\n  - [goes](https://github.com/belogik/goes) - A library to interact with Elasticsearch.\n  - [skizze](https://github.com/seiflotfy/skizze) - A probabilistic data-structures service and storage.","source":"_posts/优秀的 Go存储开源项目和库.md","raw":"---\ntitle: 优秀的 Go存储开源项目和库\ndate: 2019-03-15 20:18:29\ntags: go\n---\n\n# 优秀的 Go存储开源项目和库\n\nA curated list of awesome Go storage projects and libraries. Inspired by [awesome-go](https://github.com/avelino/awesome-go).\n\n### Contributing\n\nPlease take a quick gander at the [contribution guidelines](https://github.com/gostor/awesome-go-storage/blob/master/CONTRIBUTING.md) first. Thanks to all [contributors](https://github.com/gostor/awesome-go-storage/graphs/contributors); you rock!\n\n#### *If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!*\n\n### Contents\n\n- Awesome Go Storage\n  - [Storage Server](https://github.com/gostor/awesome-go-storage#storage-server)\n  - [Key-Value Store](https://github.com/gostor/awesome-go-storage#key-value-store)\n  - [File System](https://github.com/gostor/awesome-go-storage#file-system)\n  - [Database](https://github.com/gostor/awesome-go-storage#database)\n  - [Database Drivers](https://github.com/gostor/awesome-go-storage#database-drivers)\n\n## Storage Server\n\n*Storage Servers implemented in Go.*\n\n- [minio](https://github.com/minio/minio) - Minio is an open source object storage server compatible with Amazon S3 APIs.\n- [rclone](https://github.com/ncw/rclone) - \"rsync for cloud storage\" - Google Drive, Amazon Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Cloudfile…\n- [perkeep](https://github.com/perkeep/perkeep) - Perkeep is your personal storage system for life: a way of storing, syncing, sharing, modelling and backing up content.\n- [s3git](https://github.com/s3git/s3git) - Git for Cloud Storage. Distributed Version Control for Data.\n- [storj](https://github.com/storj/storj) - Decentralized cloud object storage that is affordable, easy to use, private, and secure.\n- [rook](https://github.com/rook/rook) - Open, Cloud Native, and Universal Distributed Storage.\n- [longhorn](https://github.com/rancher/longhorn) Longhorn is an open source persistent block storage server delivered via containers.\n\n<!--more-->\n\n## Key-Value Store\n\n*Key-Value Store implemented in Go.*\n\n- [Bitcask](https://github.com/prologic/bitcask) - Bitcask is an embeddable, persistent and fast key-value (KV) database written in pure Go with predictable read/write performance, low latency and high throughput thanks to the bitcask on-disk layout (LSM+WAL).\n- [BadgerDB](https://github.com/dgraph-io/badger) - BadgerDB is an embeddable, persistent, simple and fast key-value (KV) database written in pure Go. It's meant to be a performant alternative to non-Go-based key-value stores like RocksDB.\n- [biscuit](https://github.com/dcoker/biscuit) - Biscuit is a multi-region HA key-value store for your AWS infrastructure secrets.\n- [consul](https://github.com/hashicorp/consul) - Distributed consistent replicated key-value store for service discovery and configuration.\n- [diskv](https://github.com/peterbourgon/diskv) - A disk-backed key-value store.\n- [etcd](https://github.com/coreos/etcd) - Distributed reliable key-value store for the most critical data of a distributed system.\n- [go-cache](https://github.com/patrickmn/go-cache) - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.\n\n## File System\n\n*File Systems implemented in Go.*\n\n- [git-lfs](https://github.com/git-lfs/git-lfs) - Git extension for versioning large files.\n- [seaweedfs](https://github.com/chrislusf/seaweedfs) - SeaweedFS is a simple and highly scalable distributed file system for small files.\n- [fsnotify](https://github.com/fsnotify/fsnotify) - Cross-platform file system notifications for Go.\n- [goofys](https://github.com/kahing/goofys) - A high-performance, POSIX-ish Amazon S3 file system written in Go.\n- [go-systemd](https://github.com/coreos/go-systemd) - Go bindings to systemd socket activation, journal, D-Bus, and unit files.\n- [gcsfuse](https://github.com/GoogleCloudPlatform/gcsfuse) - A user-space file system for interacting with Google Cloud Storage.\n- [svfs](https://github.com/ovh/svfs) - A virtual file system over Openstack Swift built upon fuse.\n- [afero](https://github.com/spf13/afero) - A FileSystem Abstraction System for Go\n\n## Database\n\n*Databases implemented in Go.*\n\n- [BigCache](https://github.com/allegro/bigcache) - Efficient key/value cache for gigabytes of data.\n- [bolt](https://github.com/boltdb/bolt) - A low-level key/value database for Go. This original version by Ben Johnson has been marked as unmaintained and forked by [CoreOS bbolt](https://github.com/coreos/bbolt).\n- [buntdb](https://github.com/tidwall/buntdb) - A fast, embeddable, in-memory key/value database for Go with custom indexing and spatial support.\n- [cache2go](https://github.com/muesli/cache2go) - An in-memory key:value cache which supports automatic invalidation based on timeouts.\n- [cockroach](https://github.com/cockroachdb/cockroach) - A Scalable, Geo-Replicated, Transactional Datastore\n- [couchcache](https://github.com/codingsince1985/couchcache) - A RESTful caching micro-service backed by Couchbase server.\n- [CovenantSQL](https://github.com/CovenantSQL/CovenantSQL) - A SQL Database with Blockchain features.\n- [dgraph](https://github.com/dgraph-io/dgraph) - Scalable, Distributed, Low Latency, High Throughput Graph Database.\n- [diskv](https://github.com/peterbourgon/diskv) - A home-grown disk-backed key-value store.\n- [eliasdb](https://github.com/krotik/eliasdb) - Dependency-free, transactional graph database with REST API, phrase search and SQL-like query language.\n- [emitter](https://github.com/emitter-io/emitter) - Scalable, low-latency, distributed & secure pub/sub database with time-series message storage, suitable for IoT, gaming, apps and real-time web.\n- [forestdb](https://github.com/couchbase/goforestdb) - Go bindings for ForestDB.\n- [GCache](https://github.com/bluele/gcache) - Cache library with support for expirable Cache, LFU, LRU and ARC.\n- [geocache](https://github.com/melihmucuk/geocache) - An in-memory cache that is suitable for geolocation based applications.\n- [go-cache](https://github.com/pmylund/go-cache) - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.\n- [goleveldb](https://github.com/syndtr/goleveldb) - An implementation of the [LevelDB](https://github.com/google/leveldb) key/value database in the Go.\n- [groupcache](https://github.com/golang/groupcache) - Groupcache is a caching and cache-filling library, intended as a replacement for memcached in many cases.\n- [influxdb](https://github.com/influxdb/influxdb) - Scalable datastore for metrics, events, and real-time analytics\n- [ledisdb](https://github.com/siddontang/ledisdb) - Ledisdb is a high performance NoSQL like Redis based on LevelDB.\n- [levigo](https://github.com/jmhodges/levigo) - Levigo is a Go wrapper for LevelDB.\n- [moss](https://github.com/couchbase/moss) - Moss is a simple LSM key-value storage engine written in 100% Go.\n- [noms](https://github.com/attic-labs/noms) - The versioned, forkable, syncable database.\n- [piladb](https://github.com/fern4lvarez/piladb) - Lightweight RESTful database engine based on stack data structures.\n- [pREST](https://github.com/nuveo/prest) - Serve a RESTful API from any PostgreSQL database.\n- [prometheus](https://github.com/prometheus/prometheus) - Monitoring system and time series database.\n- [rqlite](https://github.com/rqlite/rqlite) - The lightweight, distributed, relational database built on SQLite.\n- [scribble](https://github.com/nanobox-io/golang-scribble) - A tiny flat file JSON store.\n- [tidb](https://github.com/pingcap/tidb) - TiDB is a distributed SQL database. Inspired by the design of Google F1.\n- [tiedot](https://github.com/HouzuoGuo/tiedot) - Your NoSQL database powered by Golang.\n- [Tile38](https://github.com/tidwall/tile38) - A geolocation DB with spatial index and realtime geofencing.\n\n*Database schema migration.*\n\n- [darwin](https://github.com/GuiaBolso/darwin) - Database schema evolution library for Go\n- [goose](https://github.com/steinbacher/goose) - Database migration tool. You can manage your database's evolution by creating incremental SQL or Go scripts.\n- [gormigrate](https://github.com/go-gormigrate/gormigrate) - Database schema migration helper for Gorm ORM.\n- [migrate](https://github.com/mattes/migrate) - Database migration handling in Golang support MySQL, PostgreSQL, Cassandra, and SQLite.\n- [pravasan](https://github.com/pravasan/pravasan) - Simple Migration tool - currently for MySQL but planning to support soon for Postgres, SQLite, MongoDB, etc.,\n- [soda](https://github.com/markbates/pop/tree/master/soda) - Database migration, creation, ORM, etc... for MySQL, PostgreSQL, and SQLite.\n- [sql-migrate](https://github.com/rubenv/sql-migrate) - Database migration tool. Allows embedding migrations into the application using go-bindata.\n\n*Database tools.*\n\n- [go-mysql](https://github.com/siddontang/go-mysql) - A go toolset to handle MySQL protocol and replication.\n- [go-mysql-elasticsearch](https://github.com/siddontang/go-mysql-elasticsearch) - Sync your MySQL data into Elasticsearch automatically.\n- [kingshard](https://github.com/flike/kingshard) - kingshard is a high performance proxy for MySQL powered by Golang.\n- [myreplication](https://github.com/2tvenom/myreplication) - MySql binary log replication listener. Support statement and row based replication.\n- [orchestrator](https://github.com/outbrain/orchestrator) - MySQL replication topology manager & visualizer\n- [pgweb](https://github.com/sosedoff/pgweb) - A web-based PostgreSQL database browser\n- [vitess](https://github.com/youtube/vitess) - vitess provides servers and tools which facilitate scaling of MySQL databases for large scale web services.\n- [usql](https://github.com/xo/usql) - universal command-line interface for SQL databases\n\n*SQL query builder, libraries for building and using SQL.*\n\n- [dat](https://github.com/mgutz/dat) - Go Postgres Data Access Toolkit\n- [Dotsql](https://github.com/gchaincl/dotsql) - Go library that helps you keep sql files in one place and use it with ease.\n- [goqu](https://github.com/doug-martin/goqu) - An idiomatic SQL builder and query library.\n- [igor](https://github.com/galeone/igor) - Abstraction layer for PostgreSQL that supports advanced functionality and uses gorm-like syntax.\n- [ozzo-dbx](https://github.com/go-ozzo/ozzo-dbx) - Powerful data retrieval methods as well as DB-agnostic query building capabilities.\n- [scaneo](https://github.com/variadico/scaneo) - Generate Go code to convert database rows into arbitrary structs.\n- [sqrl](https://github.com/elgris/sqrl) - SQL query builder, fork of Squirrel with improved performance.\n- [Squirrel](https://github.com/Masterminds/squirrel) - Go library that helps you build SQL queries.\n- [xo](https://github.com/knq/xo) - Generate idiomatic Go code for databases based on existing schema definitions or custom queries supporting PostgreSQL, MySQL, SQLite, Oracle, and Microsoft SQL Server.\n\n## Database Drivers\n\n*Libraries for connecting and operating databases.*\n\n- Relational Databases\n  - [bgc](https://github.com/viant/bgc) - Datastore Connectivity for BigQuery for go.\n  - [firebirdsql](https://github.com/nakagami/firebirdsql) - Firebird RDBMS SQL driver for Go\n  - [go-adodb](https://github.com/mattn/go-adodb) - Microsoft ActiveX Object DataBase driver for go that using database/sql.\n  - [go-bqstreamer](https://github.com/rounds/go-bqstreamer) - BigQuery fast and concurrent stream insert.\n  - [go-mssqldb](https://github.com/denisenkom/go-mssqldb) - Microsoft MSSQL driver in go language.\n  - [go-oci8](https://github.com/mattn/go-oci8) - Oracle driver for go that using database/sql.\n  - [go-sql-driver/mysql](https://github.com/go-sql-driver/mysql) - MySQL driver for Go.\n  - [go-sqlite3](https://github.com/mattn/go-sqlite3) - SQLite3 driver for go that using database/sql.\n  - [gofreetds](https://github.com/minus5/gofreetds) Microsoft MSSQL driver. Go wrapper over [FreeTDS](http://www.freetds.org/).\n  - [pgx](https://github.com/jackc/pgx) - PostgreSQL driver supporting features beyond those exposed by database/sql.\n  - [pq](https://github.com/lib/pq) - Pure Go Postgres driver for database/sql.\n- NoSQL Databases\n  - [aerospike-client-go](https://github.com/aerospike/aerospike-client-go) - Aerospike client in Go language.\n  - [arangolite](https://github.com/solher/arangolite) - Lightweight golang driver for ArangoDB.\n  - [asc](https://github.com/viant/asc) - Datastore Connectivity for Aerospike for go.\n  - [cayley](https://github.com/google/cayley) - A graph database with support for multiple backends.\n  - [dsc](https://github.com/viant/dsc) - Datastore connectivity for SQL, NoSQL, structured files.\n  - [dynago](https://github.com/underarmour/dynago) - Dynago is a principle of least surprise client for DynamoDB\n  - [go-couchbase](https://github.com/couchbase/go-couchbase) - Couchbase client in Go\n  - [go-couchdb](https://github.com/fjl/go-couchdb) - Yet another CouchDB HTTP API wrapper for Go\n  - [gocb](https://github.com/couchbase/gocb) - Official Couchbase Go SDK\n  - [gocql](http://gocql.github.io/) - A Go language driver for Apache Cassandra.\n  - [gomemcache](https://github.com/bradfitz/gomemcache/) - memcache client library for the Go programming language.\n  - [gorethink](https://github.com/GoRethink/gorethink) - Go language driver for RethinkDB\n  - [goriak](https://github.com/zegl/goriak) - Go language driver for Riak KV\n  - [mgo](https://godoc.org/labix.org/v2/mgo) - MongoDB driver for the Go language that implements a rich and well tested selection of features under a very simple API following standard Go idioms.\n  - [neo4j](https://github.com/cihangir/neo4j) - Neo4j Rest API Bindings for Golang\n  - [Neo4j-GO](https://github.com/davemeehan/Neo4j-GO) - Neo4j REST Client in golang.\n  - [neoism](https://github.com/jmcvetta/neoism) - Neo4j client for Golang\n  - [redigo](https://github.com/gomodule/redigo) - Redigo is a Go client for the Redis database.\n  - [redis](https://github.com/go-redis/redis) - Redis client for Golang\n  - [redis](https://github.com/hoisie/redis) - A simple, powerful Redis client for Go.\n  - [redis](https://github.com/bsm/redeo) - Redis-protocol compatible TCP servers/services.\n- Search and Analytic Databases\n  - [bleve](https://github.com/blevesearch/bleve) - A modern text indexing library for go.\n  - [elastic](https://github.com/olivere/elastic) - Elasticsearch client for Go.\n  - [elastigo](https://github.com/mattbaird/elastigo) - A Elasticsearch client library.\n  - [goes](https://github.com/belogik/goes) - A library to interact with Elasticsearch.\n  - [skizze](https://github.com/seiflotfy/skizze) - A probabilistic data-structures service and storage.","slug":"优秀的 Go存储开源项目和库","published":1,"updated":"2019-05-21T14:36:37.809Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubp90014amum7o72n2g8","content":"<h1 id=\"优秀的-Go存储开源项目和库\"><a href=\"#优秀的-Go存储开源项目和库\" class=\"headerlink\" title=\"优秀的 Go存储开源项目和库\"></a>优秀的 Go存储开源项目和库</h1><p>A curated list of awesome Go storage projects and libraries. Inspired by <a href=\"https://github.com/avelino/awesome-go\" target=\"_blank\" rel=\"noopener\">awesome-go</a>.</p>\n<h3 id=\"Contributing\"><a href=\"#Contributing\" class=\"headerlink\" title=\"Contributing\"></a>Contributing</h3><p>Please take a quick gander at the <a href=\"https://github.com/gostor/awesome-go-storage/blob/master/CONTRIBUTING.md\" target=\"_blank\" rel=\"noopener\">contribution guidelines</a> first. Thanks to all <a href=\"https://github.com/gostor/awesome-go-storage/graphs/contributors\" target=\"_blank\" rel=\"noopener\">contributors</a>; you rock!</p>\n<h4 id=\"If-you-see-a-package-or-project-here-that-is-no-longer-maintained-or-is-not-a-good-fit-please-submit-a-pull-request-to-improve-this-file-Thank-you\"><a href=\"#If-you-see-a-package-or-project-here-that-is-no-longer-maintained-or-is-not-a-good-fit-please-submit-a-pull-request-to-improve-this-file-Thank-you\" class=\"headerlink\" title=\"If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!\"></a><em>If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!</em></h4><h3 id=\"Contents\"><a href=\"#Contents\" class=\"headerlink\" title=\"Contents\"></a>Contents</h3><ul>\n<li>Awesome Go Storage<ul>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#storage-server\" target=\"_blank\" rel=\"noopener\">Storage Server</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#key-value-store\" target=\"_blank\" rel=\"noopener\">Key-Value Store</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#file-system\" target=\"_blank\" rel=\"noopener\">File System</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#database\" target=\"_blank\" rel=\"noopener\">Database</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#database-drivers\" target=\"_blank\" rel=\"noopener\">Database Drivers</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Storage-Server\"><a href=\"#Storage-Server\" class=\"headerlink\" title=\"Storage Server\"></a>Storage Server</h2><p><em>Storage Servers implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/minio/minio\" target=\"_blank\" rel=\"noopener\">minio</a> - Minio is an open source object storage server compatible with Amazon S3 APIs.</li>\n<li><a href=\"https://github.com/ncw/rclone\" target=\"_blank\" rel=\"noopener\">rclone</a> - “rsync for cloud storage” - Google Drive, Amazon Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Cloudfile…</li>\n<li><a href=\"https://github.com/perkeep/perkeep\" target=\"_blank\" rel=\"noopener\">perkeep</a> - Perkeep is your personal storage system for life: a way of storing, syncing, sharing, modelling and backing up content.</li>\n<li><a href=\"https://github.com/s3git/s3git\" target=\"_blank\" rel=\"noopener\">s3git</a> - Git for Cloud Storage. Distributed Version Control for Data.</li>\n<li><a href=\"https://github.com/storj/storj\" target=\"_blank\" rel=\"noopener\">storj</a> - Decentralized cloud object storage that is affordable, easy to use, private, and secure.</li>\n<li><a href=\"https://github.com/rook/rook\" target=\"_blank\" rel=\"noopener\">rook</a> - Open, Cloud Native, and Universal Distributed Storage.</li>\n<li><a href=\"https://github.com/rancher/longhorn\" target=\"_blank\" rel=\"noopener\">longhorn</a> Longhorn is an open source persistent block storage server delivered via containers.</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"Key-Value-Store\"><a href=\"#Key-Value-Store\" class=\"headerlink\" title=\"Key-Value Store\"></a>Key-Value Store</h2><p><em>Key-Value Store implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/prologic/bitcask\" target=\"_blank\" rel=\"noopener\">Bitcask</a> - Bitcask is an embeddable, persistent and fast key-value (KV) database written in pure Go with predictable read/write performance, low latency and high throughput thanks to the bitcask on-disk layout (LSM+WAL).</li>\n<li><a href=\"https://github.com/dgraph-io/badger\" target=\"_blank\" rel=\"noopener\">BadgerDB</a> - BadgerDB is an embeddable, persistent, simple and fast key-value (KV) database written in pure Go. It’s meant to be a performant alternative to non-Go-based key-value stores like RocksDB.</li>\n<li><a href=\"https://github.com/dcoker/biscuit\" target=\"_blank\" rel=\"noopener\">biscuit</a> - Biscuit is a multi-region HA key-value store for your AWS infrastructure secrets.</li>\n<li><a href=\"https://github.com/hashicorp/consul\" target=\"_blank\" rel=\"noopener\">consul</a> - Distributed consistent replicated key-value store for service discovery and configuration.</li>\n<li><a href=\"https://github.com/peterbourgon/diskv\" target=\"_blank\" rel=\"noopener\">diskv</a> - A disk-backed key-value store.</li>\n<li><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a> - Distributed reliable key-value store for the most critical data of a distributed system.</li>\n<li><a href=\"https://github.com/patrickmn/go-cache\" target=\"_blank\" rel=\"noopener\">go-cache</a> - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.</li>\n</ul>\n<h2 id=\"File-System\"><a href=\"#File-System\" class=\"headerlink\" title=\"File System\"></a>File System</h2><p><em>File Systems implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/git-lfs/git-lfs\" target=\"_blank\" rel=\"noopener\">git-lfs</a> - Git extension for versioning large files.</li>\n<li><a href=\"https://github.com/chrislusf/seaweedfs\" target=\"_blank\" rel=\"noopener\">seaweedfs</a> - SeaweedFS is a simple and highly scalable distributed file system for small files.</li>\n<li><a href=\"https://github.com/fsnotify/fsnotify\" target=\"_blank\" rel=\"noopener\">fsnotify</a> - Cross-platform file system notifications for Go.</li>\n<li><a href=\"https://github.com/kahing/goofys\" target=\"_blank\" rel=\"noopener\">goofys</a> - A high-performance, POSIX-ish Amazon S3 file system written in Go.</li>\n<li><a href=\"https://github.com/coreos/go-systemd\" target=\"_blank\" rel=\"noopener\">go-systemd</a> - Go bindings to systemd socket activation, journal, D-Bus, and unit files.</li>\n<li><a href=\"https://github.com/GoogleCloudPlatform/gcsfuse\" target=\"_blank\" rel=\"noopener\">gcsfuse</a> - A user-space file system for interacting with Google Cloud Storage.</li>\n<li><a href=\"https://github.com/ovh/svfs\" target=\"_blank\" rel=\"noopener\">svfs</a> - A virtual file system over Openstack Swift built upon fuse.</li>\n<li><a href=\"https://github.com/spf13/afero\" target=\"_blank\" rel=\"noopener\">afero</a> - A FileSystem Abstraction System for Go</li>\n</ul>\n<h2 id=\"Database\"><a href=\"#Database\" class=\"headerlink\" title=\"Database\"></a>Database</h2><p><em>Databases implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/allegro/bigcache\" target=\"_blank\" rel=\"noopener\">BigCache</a> - Efficient key/value cache for gigabytes of data.</li>\n<li><a href=\"https://github.com/boltdb/bolt\" target=\"_blank\" rel=\"noopener\">bolt</a> - A low-level key/value database for Go. This original version by Ben Johnson has been marked as unmaintained and forked by <a href=\"https://github.com/coreos/bbolt\" target=\"_blank\" rel=\"noopener\">CoreOS bbolt</a>.</li>\n<li><a href=\"https://github.com/tidwall/buntdb\" target=\"_blank\" rel=\"noopener\">buntdb</a> - A fast, embeddable, in-memory key/value database for Go with custom indexing and spatial support.</li>\n<li><a href=\"https://github.com/muesli/cache2go\" target=\"_blank\" rel=\"noopener\">cache2go</a> - An in-memory key:value cache which supports automatic invalidation based on timeouts.</li>\n<li><a href=\"https://github.com/cockroachdb/cockroach\" target=\"_blank\" rel=\"noopener\">cockroach</a> - A Scalable, Geo-Replicated, Transactional Datastore</li>\n<li><a href=\"https://github.com/codingsince1985/couchcache\" target=\"_blank\" rel=\"noopener\">couchcache</a> - A RESTful caching micro-service backed by Couchbase server.</li>\n<li><a href=\"https://github.com/CovenantSQL/CovenantSQL\" target=\"_blank\" rel=\"noopener\">CovenantSQL</a> - A SQL Database with Blockchain features.</li>\n<li><a href=\"https://github.com/dgraph-io/dgraph\" target=\"_blank\" rel=\"noopener\">dgraph</a> - Scalable, Distributed, Low Latency, High Throughput Graph Database.</li>\n<li><a href=\"https://github.com/peterbourgon/diskv\" target=\"_blank\" rel=\"noopener\">diskv</a> - A home-grown disk-backed key-value store.</li>\n<li><a href=\"https://github.com/krotik/eliasdb\" target=\"_blank\" rel=\"noopener\">eliasdb</a> - Dependency-free, transactional graph database with REST API, phrase search and SQL-like query language.</li>\n<li><a href=\"https://github.com/emitter-io/emitter\" target=\"_blank\" rel=\"noopener\">emitter</a> - Scalable, low-latency, distributed &amp; secure pub/sub database with time-series message storage, suitable for IoT, gaming, apps and real-time web.</li>\n<li><a href=\"https://github.com/couchbase/goforestdb\" target=\"_blank\" rel=\"noopener\">forestdb</a> - Go bindings for ForestDB.</li>\n<li><a href=\"https://github.com/bluele/gcache\" target=\"_blank\" rel=\"noopener\">GCache</a> - Cache library with support for expirable Cache, LFU, LRU and ARC.</li>\n<li><a href=\"https://github.com/melihmucuk/geocache\" target=\"_blank\" rel=\"noopener\">geocache</a> - An in-memory cache that is suitable for geolocation based applications.</li>\n<li><a href=\"https://github.com/pmylund/go-cache\" target=\"_blank\" rel=\"noopener\">go-cache</a> - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.</li>\n<li><a href=\"https://github.com/syndtr/goleveldb\" target=\"_blank\" rel=\"noopener\">goleveldb</a> - An implementation of the <a href=\"https://github.com/google/leveldb\" target=\"_blank\" rel=\"noopener\">LevelDB</a> key/value database in the Go.</li>\n<li><a href=\"https://github.com/golang/groupcache\" target=\"_blank\" rel=\"noopener\">groupcache</a> - Groupcache is a caching and cache-filling library, intended as a replacement for memcached in many cases.</li>\n<li><a href=\"https://github.com/influxdb/influxdb\" target=\"_blank\" rel=\"noopener\">influxdb</a> - Scalable datastore for metrics, events, and real-time analytics</li>\n<li><a href=\"https://github.com/siddontang/ledisdb\" target=\"_blank\" rel=\"noopener\">ledisdb</a> - Ledisdb is a high performance NoSQL like Redis based on LevelDB.</li>\n<li><a href=\"https://github.com/jmhodges/levigo\" target=\"_blank\" rel=\"noopener\">levigo</a> - Levigo is a Go wrapper for LevelDB.</li>\n<li><a href=\"https://github.com/couchbase/moss\" target=\"_blank\" rel=\"noopener\">moss</a> - Moss is a simple LSM key-value storage engine written in 100% Go.</li>\n<li><a href=\"https://github.com/attic-labs/noms\" target=\"_blank\" rel=\"noopener\">noms</a> - The versioned, forkable, syncable database.</li>\n<li><a href=\"https://github.com/fern4lvarez/piladb\" target=\"_blank\" rel=\"noopener\">piladb</a> - Lightweight RESTful database engine based on stack data structures.</li>\n<li><a href=\"https://github.com/nuveo/prest\" target=\"_blank\" rel=\"noopener\">pREST</a> - Serve a RESTful API from any PostgreSQL database.</li>\n<li><a href=\"https://github.com/prometheus/prometheus\" target=\"_blank\" rel=\"noopener\">prometheus</a> - Monitoring system and time series database.</li>\n<li><a href=\"https://github.com/rqlite/rqlite\" target=\"_blank\" rel=\"noopener\">rqlite</a> - The lightweight, distributed, relational database built on SQLite.</li>\n<li><a href=\"https://github.com/nanobox-io/golang-scribble\" target=\"_blank\" rel=\"noopener\">scribble</a> - A tiny flat file JSON store.</li>\n<li><a href=\"https://github.com/pingcap/tidb\" target=\"_blank\" rel=\"noopener\">tidb</a> - TiDB is a distributed SQL database. Inspired by the design of Google F1.</li>\n<li><a href=\"https://github.com/HouzuoGuo/tiedot\" target=\"_blank\" rel=\"noopener\">tiedot</a> - Your NoSQL database powered by Golang.</li>\n<li><a href=\"https://github.com/tidwall/tile38\" target=\"_blank\" rel=\"noopener\">Tile38</a> - A geolocation DB with spatial index and realtime geofencing.</li>\n</ul>\n<p><em>Database schema migration.</em></p>\n<ul>\n<li><a href=\"https://github.com/GuiaBolso/darwin\" target=\"_blank\" rel=\"noopener\">darwin</a> - Database schema evolution library for Go</li>\n<li><a href=\"https://github.com/steinbacher/goose\" target=\"_blank\" rel=\"noopener\">goose</a> - Database migration tool. You can manage your database’s evolution by creating incremental SQL or Go scripts.</li>\n<li><a href=\"https://github.com/go-gormigrate/gormigrate\" target=\"_blank\" rel=\"noopener\">gormigrate</a> - Database schema migration helper for Gorm ORM.</li>\n<li><a href=\"https://github.com/mattes/migrate\" target=\"_blank\" rel=\"noopener\">migrate</a> - Database migration handling in Golang support MySQL, PostgreSQL, Cassandra, and SQLite.</li>\n<li><a href=\"https://github.com/pravasan/pravasan\" target=\"_blank\" rel=\"noopener\">pravasan</a> - Simple Migration tool - currently for MySQL but planning to support soon for Postgres, SQLite, MongoDB, etc.,</li>\n<li><a href=\"https://github.com/markbates/pop/tree/master/soda\" target=\"_blank\" rel=\"noopener\">soda</a> - Database migration, creation, ORM, etc… for MySQL, PostgreSQL, and SQLite.</li>\n<li><a href=\"https://github.com/rubenv/sql-migrate\" target=\"_blank\" rel=\"noopener\">sql-migrate</a> - Database migration tool. Allows embedding migrations into the application using go-bindata.</li>\n</ul>\n<p><em>Database tools.</em></p>\n<ul>\n<li><a href=\"https://github.com/siddontang/go-mysql\" target=\"_blank\" rel=\"noopener\">go-mysql</a> - A go toolset to handle MySQL protocol and replication.</li>\n<li><a href=\"https://github.com/siddontang/go-mysql-elasticsearch\" target=\"_blank\" rel=\"noopener\">go-mysql-elasticsearch</a> - Sync your MySQL data into Elasticsearch automatically.</li>\n<li><a href=\"https://github.com/flike/kingshard\" target=\"_blank\" rel=\"noopener\">kingshard</a> - kingshard is a high performance proxy for MySQL powered by Golang.</li>\n<li><a href=\"https://github.com/2tvenom/myreplication\" target=\"_blank\" rel=\"noopener\">myreplication</a> - MySql binary log replication listener. Support statement and row based replication.</li>\n<li><a href=\"https://github.com/outbrain/orchestrator\" target=\"_blank\" rel=\"noopener\">orchestrator</a> - MySQL replication topology manager &amp; visualizer</li>\n<li><a href=\"https://github.com/sosedoff/pgweb\" target=\"_blank\" rel=\"noopener\">pgweb</a> - A web-based PostgreSQL database browser</li>\n<li><a href=\"https://github.com/youtube/vitess\" target=\"_blank\" rel=\"noopener\">vitess</a> - vitess provides servers and tools which facilitate scaling of MySQL databases for large scale web services.</li>\n<li><a href=\"https://github.com/xo/usql\" target=\"_blank\" rel=\"noopener\">usql</a> - universal command-line interface for SQL databases</li>\n</ul>\n<p><em>SQL query builder, libraries for building and using SQL.</em></p>\n<ul>\n<li><a href=\"https://github.com/mgutz/dat\" target=\"_blank\" rel=\"noopener\">dat</a> - Go Postgres Data Access Toolkit</li>\n<li><a href=\"https://github.com/gchaincl/dotsql\" target=\"_blank\" rel=\"noopener\">Dotsql</a> - Go library that helps you keep sql files in one place and use it with ease.</li>\n<li><a href=\"https://github.com/doug-martin/goqu\" target=\"_blank\" rel=\"noopener\">goqu</a> - An idiomatic SQL builder and query library.</li>\n<li><a href=\"https://github.com/galeone/igor\" target=\"_blank\" rel=\"noopener\">igor</a> - Abstraction layer for PostgreSQL that supports advanced functionality and uses gorm-like syntax.</li>\n<li><a href=\"https://github.com/go-ozzo/ozzo-dbx\" target=\"_blank\" rel=\"noopener\">ozzo-dbx</a> - Powerful data retrieval methods as well as DB-agnostic query building capabilities.</li>\n<li><a href=\"https://github.com/variadico/scaneo\" target=\"_blank\" rel=\"noopener\">scaneo</a> - Generate Go code to convert database rows into arbitrary structs.</li>\n<li><a href=\"https://github.com/elgris/sqrl\" target=\"_blank\" rel=\"noopener\">sqrl</a> - SQL query builder, fork of Squirrel with improved performance.</li>\n<li><a href=\"https://github.com/Masterminds/squirrel\" target=\"_blank\" rel=\"noopener\">Squirrel</a> - Go library that helps you build SQL queries.</li>\n<li><a href=\"https://github.com/knq/xo\" target=\"_blank\" rel=\"noopener\">xo</a> - Generate idiomatic Go code for databases based on existing schema definitions or custom queries supporting PostgreSQL, MySQL, SQLite, Oracle, and Microsoft SQL Server.</li>\n</ul>\n<h2 id=\"Database-Drivers\"><a href=\"#Database-Drivers\" class=\"headerlink\" title=\"Database Drivers\"></a>Database Drivers</h2><p><em>Libraries for connecting and operating databases.</em></p>\n<ul>\n<li>Relational Databases<ul>\n<li><a href=\"https://github.com/viant/bgc\" target=\"_blank\" rel=\"noopener\">bgc</a> - Datastore Connectivity for BigQuery for go.</li>\n<li><a href=\"https://github.com/nakagami/firebirdsql\" target=\"_blank\" rel=\"noopener\">firebirdsql</a> - Firebird RDBMS SQL driver for Go</li>\n<li><a href=\"https://github.com/mattn/go-adodb\" target=\"_blank\" rel=\"noopener\">go-adodb</a> - Microsoft ActiveX Object DataBase driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/rounds/go-bqstreamer\" target=\"_blank\" rel=\"noopener\">go-bqstreamer</a> - BigQuery fast and concurrent stream insert.</li>\n<li><a href=\"https://github.com/denisenkom/go-mssqldb\" target=\"_blank\" rel=\"noopener\">go-mssqldb</a> - Microsoft MSSQL driver in go language.</li>\n<li><a href=\"https://github.com/mattn/go-oci8\" target=\"_blank\" rel=\"noopener\">go-oci8</a> - Oracle driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/go-sql-driver/mysql\" target=\"_blank\" rel=\"noopener\">go-sql-driver/mysql</a> - MySQL driver for Go.</li>\n<li><a href=\"https://github.com/mattn/go-sqlite3\" target=\"_blank\" rel=\"noopener\">go-sqlite3</a> - SQLite3 driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/minus5/gofreetds\" target=\"_blank\" rel=\"noopener\">gofreetds</a> Microsoft MSSQL driver. Go wrapper over <a href=\"http://www.freetds.org/\" target=\"_blank\" rel=\"noopener\">FreeTDS</a>.</li>\n<li><a href=\"https://github.com/jackc/pgx\" target=\"_blank\" rel=\"noopener\">pgx</a> - PostgreSQL driver supporting features beyond those exposed by database/sql.</li>\n<li><a href=\"https://github.com/lib/pq\" target=\"_blank\" rel=\"noopener\">pq</a> - Pure Go Postgres driver for database/sql.</li>\n</ul>\n</li>\n<li>NoSQL Databases<ul>\n<li><a href=\"https://github.com/aerospike/aerospike-client-go\" target=\"_blank\" rel=\"noopener\">aerospike-client-go</a> - Aerospike client in Go language.</li>\n<li><a href=\"https://github.com/solher/arangolite\" target=\"_blank\" rel=\"noopener\">arangolite</a> - Lightweight golang driver for ArangoDB.</li>\n<li><a href=\"https://github.com/viant/asc\" target=\"_blank\" rel=\"noopener\">asc</a> - Datastore Connectivity for Aerospike for go.</li>\n<li><a href=\"https://github.com/google/cayley\" target=\"_blank\" rel=\"noopener\">cayley</a> - A graph database with support for multiple backends.</li>\n<li><a href=\"https://github.com/viant/dsc\" target=\"_blank\" rel=\"noopener\">dsc</a> - Datastore connectivity for SQL, NoSQL, structured files.</li>\n<li><a href=\"https://github.com/underarmour/dynago\" target=\"_blank\" rel=\"noopener\">dynago</a> - Dynago is a principle of least surprise client for DynamoDB</li>\n<li><a href=\"https://github.com/couchbase/go-couchbase\" target=\"_blank\" rel=\"noopener\">go-couchbase</a> - Couchbase client in Go</li>\n<li><a href=\"https://github.com/fjl/go-couchdb\" target=\"_blank\" rel=\"noopener\">go-couchdb</a> - Yet another CouchDB HTTP API wrapper for Go</li>\n<li><a href=\"https://github.com/couchbase/gocb\" target=\"_blank\" rel=\"noopener\">gocb</a> - Official Couchbase Go SDK</li>\n<li><a href=\"http://gocql.github.io/\" target=\"_blank\" rel=\"noopener\">gocql</a> - A Go language driver for Apache Cassandra.</li>\n<li><a href=\"https://github.com/bradfitz/gomemcache/\" target=\"_blank\" rel=\"noopener\">gomemcache</a> - memcache client library for the Go programming language.</li>\n<li><a href=\"https://github.com/GoRethink/gorethink\" target=\"_blank\" rel=\"noopener\">gorethink</a> - Go language driver for RethinkDB</li>\n<li><a href=\"https://github.com/zegl/goriak\" target=\"_blank\" rel=\"noopener\">goriak</a> - Go language driver for Riak KV</li>\n<li><a href=\"https://godoc.org/labix.org/v2/mgo\" target=\"_blank\" rel=\"noopener\">mgo</a> - MongoDB driver for the Go language that implements a rich and well tested selection of features under a very simple API following standard Go idioms.</li>\n<li><a href=\"https://github.com/cihangir/neo4j\" target=\"_blank\" rel=\"noopener\">neo4j</a> - Neo4j Rest API Bindings for Golang</li>\n<li><a href=\"https://github.com/davemeehan/Neo4j-GO\" target=\"_blank\" rel=\"noopener\">Neo4j-GO</a> - Neo4j REST Client in golang.</li>\n<li><a href=\"https://github.com/jmcvetta/neoism\" target=\"_blank\" rel=\"noopener\">neoism</a> - Neo4j client for Golang</li>\n<li><a href=\"https://github.com/gomodule/redigo\" target=\"_blank\" rel=\"noopener\">redigo</a> - Redigo is a Go client for the Redis database.</li>\n<li><a href=\"https://github.com/go-redis/redis\" target=\"_blank\" rel=\"noopener\">redis</a> - Redis client for Golang</li>\n<li><a href=\"https://github.com/hoisie/redis\" target=\"_blank\" rel=\"noopener\">redis</a> - A simple, powerful Redis client for Go.</li>\n<li><a href=\"https://github.com/bsm/redeo\" target=\"_blank\" rel=\"noopener\">redis</a> - Redis-protocol compatible TCP servers/services.</li>\n</ul>\n</li>\n<li>Search and Analytic Databases<ul>\n<li><a href=\"https://github.com/blevesearch/bleve\" target=\"_blank\" rel=\"noopener\">bleve</a> - A modern text indexing library for go.</li>\n<li><a href=\"https://github.com/olivere/elastic\" target=\"_blank\" rel=\"noopener\">elastic</a> - Elasticsearch client for Go.</li>\n<li><a href=\"https://github.com/mattbaird/elastigo\" target=\"_blank\" rel=\"noopener\">elastigo</a> - A Elasticsearch client library.</li>\n<li><a href=\"https://github.com/belogik/goes\" target=\"_blank\" rel=\"noopener\">goes</a> - A library to interact with Elasticsearch.</li>\n<li><a href=\"https://github.com/seiflotfy/skizze\" target=\"_blank\" rel=\"noopener\">skizze</a> - A probabilistic data-structures service and storage.</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"优秀的-Go存储开源项目和库\"><a href=\"#优秀的-Go存储开源项目和库\" class=\"headerlink\" title=\"优秀的 Go存储开源项目和库\"></a>优秀的 Go存储开源项目和库</h1><p>A curated list of awesome Go storage projects and libraries. Inspired by <a href=\"https://github.com/avelino/awesome-go\" target=\"_blank\" rel=\"noopener\">awesome-go</a>.</p>\n<h3 id=\"Contributing\"><a href=\"#Contributing\" class=\"headerlink\" title=\"Contributing\"></a>Contributing</h3><p>Please take a quick gander at the <a href=\"https://github.com/gostor/awesome-go-storage/blob/master/CONTRIBUTING.md\" target=\"_blank\" rel=\"noopener\">contribution guidelines</a> first. Thanks to all <a href=\"https://github.com/gostor/awesome-go-storage/graphs/contributors\" target=\"_blank\" rel=\"noopener\">contributors</a>; you rock!</p>\n<h4 id=\"If-you-see-a-package-or-project-here-that-is-no-longer-maintained-or-is-not-a-good-fit-please-submit-a-pull-request-to-improve-this-file-Thank-you\"><a href=\"#If-you-see-a-package-or-project-here-that-is-no-longer-maintained-or-is-not-a-good-fit-please-submit-a-pull-request-to-improve-this-file-Thank-you\" class=\"headerlink\" title=\"If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!\"></a><em>If you see a package or project here that is no longer maintained or is not a good fit, please submit a pull request to improve this file. Thank you!</em></h4><h3 id=\"Contents\"><a href=\"#Contents\" class=\"headerlink\" title=\"Contents\"></a>Contents</h3><ul>\n<li>Awesome Go Storage<ul>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#storage-server\" target=\"_blank\" rel=\"noopener\">Storage Server</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#key-value-store\" target=\"_blank\" rel=\"noopener\">Key-Value Store</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#file-system\" target=\"_blank\" rel=\"noopener\">File System</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#database\" target=\"_blank\" rel=\"noopener\">Database</a></li>\n<li><a href=\"https://github.com/gostor/awesome-go-storage#database-drivers\" target=\"_blank\" rel=\"noopener\">Database Drivers</a></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Storage-Server\"><a href=\"#Storage-Server\" class=\"headerlink\" title=\"Storage Server\"></a>Storage Server</h2><p><em>Storage Servers implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/minio/minio\" target=\"_blank\" rel=\"noopener\">minio</a> - Minio is an open source object storage server compatible with Amazon S3 APIs.</li>\n<li><a href=\"https://github.com/ncw/rclone\" target=\"_blank\" rel=\"noopener\">rclone</a> - “rsync for cloud storage” - Google Drive, Amazon Drive, S3, Dropbox, Backblaze B2, One Drive, Swift, Hubic, Cloudfile…</li>\n<li><a href=\"https://github.com/perkeep/perkeep\" target=\"_blank\" rel=\"noopener\">perkeep</a> - Perkeep is your personal storage system for life: a way of storing, syncing, sharing, modelling and backing up content.</li>\n<li><a href=\"https://github.com/s3git/s3git\" target=\"_blank\" rel=\"noopener\">s3git</a> - Git for Cloud Storage. Distributed Version Control for Data.</li>\n<li><a href=\"https://github.com/storj/storj\" target=\"_blank\" rel=\"noopener\">storj</a> - Decentralized cloud object storage that is affordable, easy to use, private, and secure.</li>\n<li><a href=\"https://github.com/rook/rook\" target=\"_blank\" rel=\"noopener\">rook</a> - Open, Cloud Native, and Universal Distributed Storage.</li>\n<li><a href=\"https://github.com/rancher/longhorn\" target=\"_blank\" rel=\"noopener\">longhorn</a> Longhorn is an open source persistent block storage server delivered via containers.</li>\n</ul>","more":"<h2 id=\"Key-Value-Store\"><a href=\"#Key-Value-Store\" class=\"headerlink\" title=\"Key-Value Store\"></a>Key-Value Store</h2><p><em>Key-Value Store implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/prologic/bitcask\" target=\"_blank\" rel=\"noopener\">Bitcask</a> - Bitcask is an embeddable, persistent and fast key-value (KV) database written in pure Go with predictable read/write performance, low latency and high throughput thanks to the bitcask on-disk layout (LSM+WAL).</li>\n<li><a href=\"https://github.com/dgraph-io/badger\" target=\"_blank\" rel=\"noopener\">BadgerDB</a> - BadgerDB is an embeddable, persistent, simple and fast key-value (KV) database written in pure Go. It’s meant to be a performant alternative to non-Go-based key-value stores like RocksDB.</li>\n<li><a href=\"https://github.com/dcoker/biscuit\" target=\"_blank\" rel=\"noopener\">biscuit</a> - Biscuit is a multi-region HA key-value store for your AWS infrastructure secrets.</li>\n<li><a href=\"https://github.com/hashicorp/consul\" target=\"_blank\" rel=\"noopener\">consul</a> - Distributed consistent replicated key-value store for service discovery and configuration.</li>\n<li><a href=\"https://github.com/peterbourgon/diskv\" target=\"_blank\" rel=\"noopener\">diskv</a> - A disk-backed key-value store.</li>\n<li><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a> - Distributed reliable key-value store for the most critical data of a distributed system.</li>\n<li><a href=\"https://github.com/patrickmn/go-cache\" target=\"_blank\" rel=\"noopener\">go-cache</a> - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.</li>\n</ul>\n<h2 id=\"File-System\"><a href=\"#File-System\" class=\"headerlink\" title=\"File System\"></a>File System</h2><p><em>File Systems implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/git-lfs/git-lfs\" target=\"_blank\" rel=\"noopener\">git-lfs</a> - Git extension for versioning large files.</li>\n<li><a href=\"https://github.com/chrislusf/seaweedfs\" target=\"_blank\" rel=\"noopener\">seaweedfs</a> - SeaweedFS is a simple and highly scalable distributed file system for small files.</li>\n<li><a href=\"https://github.com/fsnotify/fsnotify\" target=\"_blank\" rel=\"noopener\">fsnotify</a> - Cross-platform file system notifications for Go.</li>\n<li><a href=\"https://github.com/kahing/goofys\" target=\"_blank\" rel=\"noopener\">goofys</a> - A high-performance, POSIX-ish Amazon S3 file system written in Go.</li>\n<li><a href=\"https://github.com/coreos/go-systemd\" target=\"_blank\" rel=\"noopener\">go-systemd</a> - Go bindings to systemd socket activation, journal, D-Bus, and unit files.</li>\n<li><a href=\"https://github.com/GoogleCloudPlatform/gcsfuse\" target=\"_blank\" rel=\"noopener\">gcsfuse</a> - A user-space file system for interacting with Google Cloud Storage.</li>\n<li><a href=\"https://github.com/ovh/svfs\" target=\"_blank\" rel=\"noopener\">svfs</a> - A virtual file system over Openstack Swift built upon fuse.</li>\n<li><a href=\"https://github.com/spf13/afero\" target=\"_blank\" rel=\"noopener\">afero</a> - A FileSystem Abstraction System for Go</li>\n</ul>\n<h2 id=\"Database\"><a href=\"#Database\" class=\"headerlink\" title=\"Database\"></a>Database</h2><p><em>Databases implemented in Go.</em></p>\n<ul>\n<li><a href=\"https://github.com/allegro/bigcache\" target=\"_blank\" rel=\"noopener\">BigCache</a> - Efficient key/value cache for gigabytes of data.</li>\n<li><a href=\"https://github.com/boltdb/bolt\" target=\"_blank\" rel=\"noopener\">bolt</a> - A low-level key/value database for Go. This original version by Ben Johnson has been marked as unmaintained and forked by <a href=\"https://github.com/coreos/bbolt\" target=\"_blank\" rel=\"noopener\">CoreOS bbolt</a>.</li>\n<li><a href=\"https://github.com/tidwall/buntdb\" target=\"_blank\" rel=\"noopener\">buntdb</a> - A fast, embeddable, in-memory key/value database for Go with custom indexing and spatial support.</li>\n<li><a href=\"https://github.com/muesli/cache2go\" target=\"_blank\" rel=\"noopener\">cache2go</a> - An in-memory key:value cache which supports automatic invalidation based on timeouts.</li>\n<li><a href=\"https://github.com/cockroachdb/cockroach\" target=\"_blank\" rel=\"noopener\">cockroach</a> - A Scalable, Geo-Replicated, Transactional Datastore</li>\n<li><a href=\"https://github.com/codingsince1985/couchcache\" target=\"_blank\" rel=\"noopener\">couchcache</a> - A RESTful caching micro-service backed by Couchbase server.</li>\n<li><a href=\"https://github.com/CovenantSQL/CovenantSQL\" target=\"_blank\" rel=\"noopener\">CovenantSQL</a> - A SQL Database with Blockchain features.</li>\n<li><a href=\"https://github.com/dgraph-io/dgraph\" target=\"_blank\" rel=\"noopener\">dgraph</a> - Scalable, Distributed, Low Latency, High Throughput Graph Database.</li>\n<li><a href=\"https://github.com/peterbourgon/diskv\" target=\"_blank\" rel=\"noopener\">diskv</a> - A home-grown disk-backed key-value store.</li>\n<li><a href=\"https://github.com/krotik/eliasdb\" target=\"_blank\" rel=\"noopener\">eliasdb</a> - Dependency-free, transactional graph database with REST API, phrase search and SQL-like query language.</li>\n<li><a href=\"https://github.com/emitter-io/emitter\" target=\"_blank\" rel=\"noopener\">emitter</a> - Scalable, low-latency, distributed &amp; secure pub/sub database with time-series message storage, suitable for IoT, gaming, apps and real-time web.</li>\n<li><a href=\"https://github.com/couchbase/goforestdb\" target=\"_blank\" rel=\"noopener\">forestdb</a> - Go bindings for ForestDB.</li>\n<li><a href=\"https://github.com/bluele/gcache\" target=\"_blank\" rel=\"noopener\">GCache</a> - Cache library with support for expirable Cache, LFU, LRU and ARC.</li>\n<li><a href=\"https://github.com/melihmucuk/geocache\" target=\"_blank\" rel=\"noopener\">geocache</a> - An in-memory cache that is suitable for geolocation based applications.</li>\n<li><a href=\"https://github.com/pmylund/go-cache\" target=\"_blank\" rel=\"noopener\">go-cache</a> - An in-memory key:value store/cache (similar to Memcached) library for Go, suitable for single-machine applications.</li>\n<li><a href=\"https://github.com/syndtr/goleveldb\" target=\"_blank\" rel=\"noopener\">goleveldb</a> - An implementation of the <a href=\"https://github.com/google/leveldb\" target=\"_blank\" rel=\"noopener\">LevelDB</a> key/value database in the Go.</li>\n<li><a href=\"https://github.com/golang/groupcache\" target=\"_blank\" rel=\"noopener\">groupcache</a> - Groupcache is a caching and cache-filling library, intended as a replacement for memcached in many cases.</li>\n<li><a href=\"https://github.com/influxdb/influxdb\" target=\"_blank\" rel=\"noopener\">influxdb</a> - Scalable datastore for metrics, events, and real-time analytics</li>\n<li><a href=\"https://github.com/siddontang/ledisdb\" target=\"_blank\" rel=\"noopener\">ledisdb</a> - Ledisdb is a high performance NoSQL like Redis based on LevelDB.</li>\n<li><a href=\"https://github.com/jmhodges/levigo\" target=\"_blank\" rel=\"noopener\">levigo</a> - Levigo is a Go wrapper for LevelDB.</li>\n<li><a href=\"https://github.com/couchbase/moss\" target=\"_blank\" rel=\"noopener\">moss</a> - Moss is a simple LSM key-value storage engine written in 100% Go.</li>\n<li><a href=\"https://github.com/attic-labs/noms\" target=\"_blank\" rel=\"noopener\">noms</a> - The versioned, forkable, syncable database.</li>\n<li><a href=\"https://github.com/fern4lvarez/piladb\" target=\"_blank\" rel=\"noopener\">piladb</a> - Lightweight RESTful database engine based on stack data structures.</li>\n<li><a href=\"https://github.com/nuveo/prest\" target=\"_blank\" rel=\"noopener\">pREST</a> - Serve a RESTful API from any PostgreSQL database.</li>\n<li><a href=\"https://github.com/prometheus/prometheus\" target=\"_blank\" rel=\"noopener\">prometheus</a> - Monitoring system and time series database.</li>\n<li><a href=\"https://github.com/rqlite/rqlite\" target=\"_blank\" rel=\"noopener\">rqlite</a> - The lightweight, distributed, relational database built on SQLite.</li>\n<li><a href=\"https://github.com/nanobox-io/golang-scribble\" target=\"_blank\" rel=\"noopener\">scribble</a> - A tiny flat file JSON store.</li>\n<li><a href=\"https://github.com/pingcap/tidb\" target=\"_blank\" rel=\"noopener\">tidb</a> - TiDB is a distributed SQL database. Inspired by the design of Google F1.</li>\n<li><a href=\"https://github.com/HouzuoGuo/tiedot\" target=\"_blank\" rel=\"noopener\">tiedot</a> - Your NoSQL database powered by Golang.</li>\n<li><a href=\"https://github.com/tidwall/tile38\" target=\"_blank\" rel=\"noopener\">Tile38</a> - A geolocation DB with spatial index and realtime geofencing.</li>\n</ul>\n<p><em>Database schema migration.</em></p>\n<ul>\n<li><a href=\"https://github.com/GuiaBolso/darwin\" target=\"_blank\" rel=\"noopener\">darwin</a> - Database schema evolution library for Go</li>\n<li><a href=\"https://github.com/steinbacher/goose\" target=\"_blank\" rel=\"noopener\">goose</a> - Database migration tool. You can manage your database’s evolution by creating incremental SQL or Go scripts.</li>\n<li><a href=\"https://github.com/go-gormigrate/gormigrate\" target=\"_blank\" rel=\"noopener\">gormigrate</a> - Database schema migration helper for Gorm ORM.</li>\n<li><a href=\"https://github.com/mattes/migrate\" target=\"_blank\" rel=\"noopener\">migrate</a> - Database migration handling in Golang support MySQL, PostgreSQL, Cassandra, and SQLite.</li>\n<li><a href=\"https://github.com/pravasan/pravasan\" target=\"_blank\" rel=\"noopener\">pravasan</a> - Simple Migration tool - currently for MySQL but planning to support soon for Postgres, SQLite, MongoDB, etc.,</li>\n<li><a href=\"https://github.com/markbates/pop/tree/master/soda\" target=\"_blank\" rel=\"noopener\">soda</a> - Database migration, creation, ORM, etc… for MySQL, PostgreSQL, and SQLite.</li>\n<li><a href=\"https://github.com/rubenv/sql-migrate\" target=\"_blank\" rel=\"noopener\">sql-migrate</a> - Database migration tool. Allows embedding migrations into the application using go-bindata.</li>\n</ul>\n<p><em>Database tools.</em></p>\n<ul>\n<li><a href=\"https://github.com/siddontang/go-mysql\" target=\"_blank\" rel=\"noopener\">go-mysql</a> - A go toolset to handle MySQL protocol and replication.</li>\n<li><a href=\"https://github.com/siddontang/go-mysql-elasticsearch\" target=\"_blank\" rel=\"noopener\">go-mysql-elasticsearch</a> - Sync your MySQL data into Elasticsearch automatically.</li>\n<li><a href=\"https://github.com/flike/kingshard\" target=\"_blank\" rel=\"noopener\">kingshard</a> - kingshard is a high performance proxy for MySQL powered by Golang.</li>\n<li><a href=\"https://github.com/2tvenom/myreplication\" target=\"_blank\" rel=\"noopener\">myreplication</a> - MySql binary log replication listener. Support statement and row based replication.</li>\n<li><a href=\"https://github.com/outbrain/orchestrator\" target=\"_blank\" rel=\"noopener\">orchestrator</a> - MySQL replication topology manager &amp; visualizer</li>\n<li><a href=\"https://github.com/sosedoff/pgweb\" target=\"_blank\" rel=\"noopener\">pgweb</a> - A web-based PostgreSQL database browser</li>\n<li><a href=\"https://github.com/youtube/vitess\" target=\"_blank\" rel=\"noopener\">vitess</a> - vitess provides servers and tools which facilitate scaling of MySQL databases for large scale web services.</li>\n<li><a href=\"https://github.com/xo/usql\" target=\"_blank\" rel=\"noopener\">usql</a> - universal command-line interface for SQL databases</li>\n</ul>\n<p><em>SQL query builder, libraries for building and using SQL.</em></p>\n<ul>\n<li><a href=\"https://github.com/mgutz/dat\" target=\"_blank\" rel=\"noopener\">dat</a> - Go Postgres Data Access Toolkit</li>\n<li><a href=\"https://github.com/gchaincl/dotsql\" target=\"_blank\" rel=\"noopener\">Dotsql</a> - Go library that helps you keep sql files in one place and use it with ease.</li>\n<li><a href=\"https://github.com/doug-martin/goqu\" target=\"_blank\" rel=\"noopener\">goqu</a> - An idiomatic SQL builder and query library.</li>\n<li><a href=\"https://github.com/galeone/igor\" target=\"_blank\" rel=\"noopener\">igor</a> - Abstraction layer for PostgreSQL that supports advanced functionality and uses gorm-like syntax.</li>\n<li><a href=\"https://github.com/go-ozzo/ozzo-dbx\" target=\"_blank\" rel=\"noopener\">ozzo-dbx</a> - Powerful data retrieval methods as well as DB-agnostic query building capabilities.</li>\n<li><a href=\"https://github.com/variadico/scaneo\" target=\"_blank\" rel=\"noopener\">scaneo</a> - Generate Go code to convert database rows into arbitrary structs.</li>\n<li><a href=\"https://github.com/elgris/sqrl\" target=\"_blank\" rel=\"noopener\">sqrl</a> - SQL query builder, fork of Squirrel with improved performance.</li>\n<li><a href=\"https://github.com/Masterminds/squirrel\" target=\"_blank\" rel=\"noopener\">Squirrel</a> - Go library that helps you build SQL queries.</li>\n<li><a href=\"https://github.com/knq/xo\" target=\"_blank\" rel=\"noopener\">xo</a> - Generate idiomatic Go code for databases based on existing schema definitions or custom queries supporting PostgreSQL, MySQL, SQLite, Oracle, and Microsoft SQL Server.</li>\n</ul>\n<h2 id=\"Database-Drivers\"><a href=\"#Database-Drivers\" class=\"headerlink\" title=\"Database Drivers\"></a>Database Drivers</h2><p><em>Libraries for connecting and operating databases.</em></p>\n<ul>\n<li>Relational Databases<ul>\n<li><a href=\"https://github.com/viant/bgc\" target=\"_blank\" rel=\"noopener\">bgc</a> - Datastore Connectivity for BigQuery for go.</li>\n<li><a href=\"https://github.com/nakagami/firebirdsql\" target=\"_blank\" rel=\"noopener\">firebirdsql</a> - Firebird RDBMS SQL driver for Go</li>\n<li><a href=\"https://github.com/mattn/go-adodb\" target=\"_blank\" rel=\"noopener\">go-adodb</a> - Microsoft ActiveX Object DataBase driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/rounds/go-bqstreamer\" target=\"_blank\" rel=\"noopener\">go-bqstreamer</a> - BigQuery fast and concurrent stream insert.</li>\n<li><a href=\"https://github.com/denisenkom/go-mssqldb\" target=\"_blank\" rel=\"noopener\">go-mssqldb</a> - Microsoft MSSQL driver in go language.</li>\n<li><a href=\"https://github.com/mattn/go-oci8\" target=\"_blank\" rel=\"noopener\">go-oci8</a> - Oracle driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/go-sql-driver/mysql\" target=\"_blank\" rel=\"noopener\">go-sql-driver/mysql</a> - MySQL driver for Go.</li>\n<li><a href=\"https://github.com/mattn/go-sqlite3\" target=\"_blank\" rel=\"noopener\">go-sqlite3</a> - SQLite3 driver for go that using database/sql.</li>\n<li><a href=\"https://github.com/minus5/gofreetds\" target=\"_blank\" rel=\"noopener\">gofreetds</a> Microsoft MSSQL driver. Go wrapper over <a href=\"http://www.freetds.org/\" target=\"_blank\" rel=\"noopener\">FreeTDS</a>.</li>\n<li><a href=\"https://github.com/jackc/pgx\" target=\"_blank\" rel=\"noopener\">pgx</a> - PostgreSQL driver supporting features beyond those exposed by database/sql.</li>\n<li><a href=\"https://github.com/lib/pq\" target=\"_blank\" rel=\"noopener\">pq</a> - Pure Go Postgres driver for database/sql.</li>\n</ul>\n</li>\n<li>NoSQL Databases<ul>\n<li><a href=\"https://github.com/aerospike/aerospike-client-go\" target=\"_blank\" rel=\"noopener\">aerospike-client-go</a> - Aerospike client in Go language.</li>\n<li><a href=\"https://github.com/solher/arangolite\" target=\"_blank\" rel=\"noopener\">arangolite</a> - Lightweight golang driver for ArangoDB.</li>\n<li><a href=\"https://github.com/viant/asc\" target=\"_blank\" rel=\"noopener\">asc</a> - Datastore Connectivity for Aerospike for go.</li>\n<li><a href=\"https://github.com/google/cayley\" target=\"_blank\" rel=\"noopener\">cayley</a> - A graph database with support for multiple backends.</li>\n<li><a href=\"https://github.com/viant/dsc\" target=\"_blank\" rel=\"noopener\">dsc</a> - Datastore connectivity for SQL, NoSQL, structured files.</li>\n<li><a href=\"https://github.com/underarmour/dynago\" target=\"_blank\" rel=\"noopener\">dynago</a> - Dynago is a principle of least surprise client for DynamoDB</li>\n<li><a href=\"https://github.com/couchbase/go-couchbase\" target=\"_blank\" rel=\"noopener\">go-couchbase</a> - Couchbase client in Go</li>\n<li><a href=\"https://github.com/fjl/go-couchdb\" target=\"_blank\" rel=\"noopener\">go-couchdb</a> - Yet another CouchDB HTTP API wrapper for Go</li>\n<li><a href=\"https://github.com/couchbase/gocb\" target=\"_blank\" rel=\"noopener\">gocb</a> - Official Couchbase Go SDK</li>\n<li><a href=\"http://gocql.github.io/\" target=\"_blank\" rel=\"noopener\">gocql</a> - A Go language driver for Apache Cassandra.</li>\n<li><a href=\"https://github.com/bradfitz/gomemcache/\" target=\"_blank\" rel=\"noopener\">gomemcache</a> - memcache client library for the Go programming language.</li>\n<li><a href=\"https://github.com/GoRethink/gorethink\" target=\"_blank\" rel=\"noopener\">gorethink</a> - Go language driver for RethinkDB</li>\n<li><a href=\"https://github.com/zegl/goriak\" target=\"_blank\" rel=\"noopener\">goriak</a> - Go language driver for Riak KV</li>\n<li><a href=\"https://godoc.org/labix.org/v2/mgo\" target=\"_blank\" rel=\"noopener\">mgo</a> - MongoDB driver for the Go language that implements a rich and well tested selection of features under a very simple API following standard Go idioms.</li>\n<li><a href=\"https://github.com/cihangir/neo4j\" target=\"_blank\" rel=\"noopener\">neo4j</a> - Neo4j Rest API Bindings for Golang</li>\n<li><a href=\"https://github.com/davemeehan/Neo4j-GO\" target=\"_blank\" rel=\"noopener\">Neo4j-GO</a> - Neo4j REST Client in golang.</li>\n<li><a href=\"https://github.com/jmcvetta/neoism\" target=\"_blank\" rel=\"noopener\">neoism</a> - Neo4j client for Golang</li>\n<li><a href=\"https://github.com/gomodule/redigo\" target=\"_blank\" rel=\"noopener\">redigo</a> - Redigo is a Go client for the Redis database.</li>\n<li><a href=\"https://github.com/go-redis/redis\" target=\"_blank\" rel=\"noopener\">redis</a> - Redis client for Golang</li>\n<li><a href=\"https://github.com/hoisie/redis\" target=\"_blank\" rel=\"noopener\">redis</a> - A simple, powerful Redis client for Go.</li>\n<li><a href=\"https://github.com/bsm/redeo\" target=\"_blank\" rel=\"noopener\">redis</a> - Redis-protocol compatible TCP servers/services.</li>\n</ul>\n</li>\n<li>Search and Analytic Databases<ul>\n<li><a href=\"https://github.com/blevesearch/bleve\" target=\"_blank\" rel=\"noopener\">bleve</a> - A modern text indexing library for go.</li>\n<li><a href=\"https://github.com/olivere/elastic\" target=\"_blank\" rel=\"noopener\">elastic</a> - Elasticsearch client for Go.</li>\n<li><a href=\"https://github.com/mattbaird/elastigo\" target=\"_blank\" rel=\"noopener\">elastigo</a> - A Elasticsearch client library.</li>\n<li><a href=\"https://github.com/belogik/goes\" target=\"_blank\" rel=\"noopener\">goes</a> - A library to interact with Elasticsearch.</li>\n<li><a href=\"https://github.com/seiflotfy/skizze\" target=\"_blank\" rel=\"noopener\">skizze</a> - A probabilistic data-structures service and storage.</li>\n</ul>\n</li>\n</ul>"},{"title":"kafka高可用设计(上)","date":"2019-02-14T15:01:22.000Z","_content":"\n本文转发自[**技术世界**](http://www.jasongj.com/)，[原文链接](http://www.jasongj.com/2015/04/24/KafkaColumn2)　<http://www.jasongj.com/2015/04/24/KafkaColumn2>\n\n# 摘要\n\n　　Kafka在0.8以前的版本中，并不提供High Availablity机制，一旦一个或多个Broker宕机，则宕机期间其上所有Partition都无法继续提供服务。若该Broker永远不能再恢复，亦或磁盘故障，则其上数据将丢失。而Kafka的设计目标之一即是提供数据持久化，同时对于分布式系统来说，尤其当集群规模上升到一定程度后，一台或者多台机器宕机的可能性大大提高，对于Failover机制的需求非常高。因此，Kafka从0.8开始提供High Availability机制。本文从Data Replication和Leader Election两方面介绍了Kafka的HA机制。\n\n# Kafka为何需要High Available\n\n## 为何需要Replication\n\n　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。\n\n- 如果Producer使用同步模式则Producer会在尝试重新发送`message.send.max.retries`（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。\n- 如果Producer使用异步模式，则Producer会尝试重新发送`message.send.max.retries`（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。\n\n　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 　　\n\n## 为何需要Leader Election\n\n　　（本文所述Leader Election主要指Replica之间的Leader Election）\n　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replica中选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。\n　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 　　 　　\n\n# Kafka HA设计解析\n\n## 如何将所有Replica均匀分布到整个集群\n\n　　为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。\n　　Kafka分配Replica的算法如下：\n\n1. 将所有Broker（假设共n个Broker）和待分配的Partition排序\n2. 将第i个Partition分配到第（i mod n）个Broker上\n3. 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上\n\n<!--more-->\n\n## Data Replication\n\n　　Kafka的Data Replication需要解决如下问题：\n\n- 怎样Propagate消息\n- 在向Producer发送ACK前需要保证有多少个Replica已经收到该消息\n- 怎样处理某个Replica不工作的情况\n- 怎样处理Failed Replica恢复回来的情况\n\n### Propagate消息\n\n　　Producer在发布消息到某个Partition时，先通过Zookeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。\n为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。但考虑到这种场景非常少见，可以认为这种方式在性能和数据持久化上做了一个比较好的平衡。在将来的版本中，Kafka会考虑提供更高的持久性。\nConsumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。\nKafka Replication的数据流如下图所示\n[![Kafka Replication Data Flow](http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png)\n\n### ACK前需要保证有多少个备份\n\n　　和大部分分布式系统一样，Kafka处理失败需要明确定义一个Broker是否“活着”。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(这个通过Zookeeper的Heartbeat机制来实现)。二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。\n　　Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过`replica.lag.max.messages`配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过`replica.lag.time.max.ms`来配置，其默认值是10000）未向Leader发送fetch请求。。\n　　Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。\n　　需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过`request.required.acks`来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 　　\n\n### Leader Election算法\n\n　　上文说明了Kafka是如何做Replication的，另外一个很重要的问题是当Leader宕机了，怎样在Follower中选举出新的Leader。因为Follower可能落后许多或者crash了，所以必须确保选择“最新”的Follower作为新的Leader。一个基本的原则就是，如果Leader不在了，新的Leader必须拥有原来的Leader commit过的所有消息。这就需要作一个折衷，如果Leader在标明一条消息被commit前等待更多的Follower确认，那在它宕机之后就有更多的Follower可以作为新的Leader，但这也会造成吞吐率的下降。\n　　一种非常常用的Leader Election的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在[Zookeeper](http://zookeeper.apache.org/)这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于[majority-vote-based journal](http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1)，但是它的数据存储并没有使用这种方式。\n　　实际上，Leader Election算法非常多，比如Zookeeper的[Zab](http://web.stanford.edu/class/cs347/reading/zab.pdf), [Raft](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)和[Viewstamped Replication](http://pmg.csail.mit.edu/papers/vr-revisited.pdf)。而Kafka所使用的Leader Election算法更像微软的[PacificA](http://research.microsoft.com/apps/pubs/default.aspx?id=66814)算法。\n　　Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 　　\n\n### 如何处理所有Replica都不工作\n\n　　上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：\n\n- 等待ISR中的任一个Replica“活”过来，并且选它作为Leader\n- 选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader\n\n　　这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 　　\n\n### 如何选举Leader\n\n　　最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。\n　　但是该方法会有3个问题： 　　\n\n- split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致\n- herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整\n- Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。\n\n　　Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。\n\n## HA相关Zookeeper结构\n\n　　（本节所示Zookeeper结构中，实线框代表路径名是固定的，而虚线框代表路径名与业务相关）\n　　**admin** （该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除）\n[![Kafka Zookeeper Admin Structure](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png)\n\n　　`/admin/preferred_replica_election`数据结构\n\n```\n   Schema:\n{\n      \"fields\":[\n         {\n            \"name\":\"version\",\n            \"type\":\"int\",\n            \"doc\":\"version id\"\n         },\n         {\n            \"name\":\"partitions\",\n            \"type\":{\n               \"type\":\"array\",\n               \"items\":{\n                  \"fields\":[\n                     {\n                        \"name\":\"topic\",\n                        \"type\":\"string\",\n                        \"doc\":\"topic of the partition for which preferred replica election should be triggered\"\n                     },\n                     {\n                        \"name\":\"partition\",\n                        \"type\":\"int\",\n                        \"doc\":\"the partition for which preferred replica election should be triggered\"\n                     }\n                  ],\n               }\n               \"doc\":\"an array of partitions for which preferred replica election should be triggered\"\n            }\n         }\n      ]\n   }\n    \n   Example:     \n   {\n     \"version\": 1,\n     \"partitions\":\n        [\n           {\n               \"topic\": \"topic1\",\n               \"partition\": 8         \n           },\n           {\n               \"topic\": \"topic2\",\n               \"partition\": 16        \n           }\n        ]            \n   }\n```\n\n\n\n　　`/admin/reassign_partitions`用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。其数据结构如下\n\n```\n   Schema:\n{\n      \"fields\":[\n         {\n            \"name\":\"version\",\n            \"type\":\"int\",\n            \"doc\":\"version id\"\n         },\n         {\n            \"name\":\"partitions\",\n            \"type\":{\n               \"type\":\"array\",\n               \"items\":{\n                  \"fields\":[\n                     {\n                        \"name\":\"topic\",\n                        \"type\":\"string\",\n                        \"doc\":\"topic of the partition to be reassigned\"\n                     },\n                     {\n                        \"name\":\"partition\",\n                        \"type\":\"int\",\n                        \"doc\":\"the partition to be reassigned\"\n                     },\n                     {\n                        \"name\":\"replicas\",\n                        \"type\":\"array\",\n                        \"items\":\"int\",\n                        \"doc\":\"a list of replica ids\"\n                     }\n                  ],\n               }\n               \"doc\":\"an array of partitions to be reassigned to new replicas\"\n            }\n         }\n      ]\n   }\n\n   Example:\n   {\n     \"version\": 1,\n     \"partitions\":\n        [\n           {\n               \"topic\": \"topic3\",\n               \"partition\": 1,\n               \"replicas\": [1, 2, 3]\n           }\n        ]            \n   }\n```\n\n\n\n　　`/admin/delete_topics`数据结构\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"topics\",\n       \"type\": { \"type\": \"array\", \"items\": \"string\", \"doc\": \"an array of topics to be deleted\"}\n      } ]\n}\n \nExample:\n{\n  \"version\": 1,\n  \"topics\": [\"topic4\", \"topic5\"]\n}\n```\n\n\n\n　　**brokers**\n[![Kafka Zookeeper brokers structure](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png)\n\n　　broker（即`/brokers/ids/[brokerId]`）存储“活着”的Broker信息。数据结构如下\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"host\", \"type\": \"string\", \"doc\": \"ip address or host name of the broker\"},\n      {\"name\": \"port\", \"type\": \"int\", \"doc\": \"port of the broker\"},\n      {\"name\": \"jmx_port\", \"type\": \"int\", \"doc\": \"port for jmx\"}\n    ]\n}\n \nExample:\n{\n    \"jmx_port\":-1,\n    \"host\":\"node1\",\n    \"version\":1,\n    \"port\":9092\n}\n```\n\n\n\n　　topic注册信息（`/brokers/topics/[topic]`），存储该Topic的所有Partition的所有Replica所在的Broker id，第一个Replica即为Preferred Replica，对一个给定的Partition，它在同一个Broker上最多只有一个Replica,因此Broker id可作为Replica id。数据结构如下\n\n```\nSchema:\n{ \"fields\" :\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"partitions\",\n       \"type\": {\"type\": \"map\",\n                \"values\": {\"type\": \"array\", \"items\": \"int\", \"doc\": \"a list of replica ids\"},\n                \"doc\": \"a map from partition id to replica list\"},\n      }\n    ]\n}\nExample:\n{\n    \"version\":1,\n    \"partitions\":\n        {\"12\":[6],\n        \"8\":[2],\n        \"4\":[6],\n        \"11\":[5],\n        \"9\":[3],\n        \"5\":[7],\n        \"10\":[4],\n        \"6\":[8],\n        \"1\":[3],\n        \"0\":[2],\n        \"2\":[4],\n        \"7\":[1],\n        \"3\":[5]}\n}\n```\n\n\n\n　　partition state（`/brokers/topics/[topic]/partitions/[partitionId]/state`） 结构如下\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"isr\",\n       \"type\": {\"type\": \"array\",\n                \"items\": \"int\",\n                \"doc\": \"an array of the id of replicas in isr\"}\n      },\n      {\"name\": \"leader\", \"type\": \"int\", \"doc\": \"id of the leader replica\"},\n      {\"name\": \"controller_epoch\", \"type\": \"int\", \"doc\": \"epoch of the controller that last updated the leader and isr info\"},\n      {\"name\": \"leader_epoch\", \"type\": \"int\", \"doc\": \"epoch of the leader\"}\n    ]\n}\n \nExample:\n{\n    \"controller_epoch\":29,\n    \"leader\":2,\n    \"version\":1,\n    \"leader_epoch\":48,\n    \"isr\":[2]\n}\n```\n\n\n\n　　**controller**\n　　`/controller -> int (broker id of the controller)`存储当前controller的信息\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"brokerid\", \"type\": \"int\", \"doc\": \"broker id of the controller\"}\n    ]\n}\nExample:\n{\n    \"version\":1,\n　　\"brokerid\":8\n}\n```\n\n\n\n　　`/controller_epoch -> int (epoch)`直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。 　　 　　\n\n## broker failover过程简介\n\n1. Controller在Zookeeper注册Watch，一旦有Broker宕机（这是用宕机代表任何让系统认为其die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的znode会自动被删除，Zookeeper会fire Controller注册的watch，Controller读取最新的幸存的Broker\n2. Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition\n3. 对set_p中的每一个Partition\n   　　3.1 从`/brokers/topics/[topic]/partitions/[partition]/state`读取该Partition当前的ISR\n   　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。\n   　　　3.3 将新的Leader，ISR和新的`leader_epoch`及`controller_epoch`写入`/brokers/topics/[topic]/partitions/[partition]/state`。注意，该操作只有其version在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1\n4. 直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。\n   　　Broker failover顺序图如下所示。\n   [![broker failover sequence diagram ](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)","source":"_posts/kafka 高可用设计(上).md","raw":"---\ntitle: kafka高可用设计(上)\ndate: 2019-02-14 23:01:22\ntags: kafka\n---\n\n本文转发自[**技术世界**](http://www.jasongj.com/)，[原文链接](http://www.jasongj.com/2015/04/24/KafkaColumn2)　<http://www.jasongj.com/2015/04/24/KafkaColumn2>\n\n# 摘要\n\n　　Kafka在0.8以前的版本中，并不提供High Availablity机制，一旦一个或多个Broker宕机，则宕机期间其上所有Partition都无法继续提供服务。若该Broker永远不能再恢复，亦或磁盘故障，则其上数据将丢失。而Kafka的设计目标之一即是提供数据持久化，同时对于分布式系统来说，尤其当集群规模上升到一定程度后，一台或者多台机器宕机的可能性大大提高，对于Failover机制的需求非常高。因此，Kafka从0.8开始提供High Availability机制。本文从Data Replication和Leader Election两方面介绍了Kafka的HA机制。\n\n# Kafka为何需要High Available\n\n## 为何需要Replication\n\n　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。\n\n- 如果Producer使用同步模式则Producer会在尝试重新发送`message.send.max.retries`（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。\n- 如果Producer使用异步模式，则Producer会尝试重新发送`message.send.max.retries`（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。\n\n　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 　　\n\n## 为何需要Leader Election\n\n　　（本文所述Leader Election主要指Replica之间的Leader Election）\n　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replica中选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。\n　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 　　 　　\n\n# Kafka HA设计解析\n\n## 如何将所有Replica均匀分布到整个集群\n\n　　为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。\n　　Kafka分配Replica的算法如下：\n\n1. 将所有Broker（假设共n个Broker）和待分配的Partition排序\n2. 将第i个Partition分配到第（i mod n）个Broker上\n3. 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上\n\n<!--more-->\n\n## Data Replication\n\n　　Kafka的Data Replication需要解决如下问题：\n\n- 怎样Propagate消息\n- 在向Producer发送ACK前需要保证有多少个Replica已经收到该消息\n- 怎样处理某个Replica不工作的情况\n- 怎样处理Failed Replica恢复回来的情况\n\n### Propagate消息\n\n　　Producer在发布消息到某个Partition时，先通过Zookeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。\n为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。但考虑到这种场景非常少见，可以认为这种方式在性能和数据持久化上做了一个比较好的平衡。在将来的版本中，Kafka会考虑提供更高的持久性。\nConsumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。\nKafka Replication的数据流如下图所示\n[![Kafka Replication Data Flow](http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png)\n\n### ACK前需要保证有多少个备份\n\n　　和大部分分布式系统一样，Kafka处理失败需要明确定义一个Broker是否“活着”。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(这个通过Zookeeper的Heartbeat机制来实现)。二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。\n　　Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过`replica.lag.max.messages`配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过`replica.lag.time.max.ms`来配置，其默认值是10000）未向Leader发送fetch请求。。\n　　Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。\n　　需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过`request.required.acks`来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 　　\n\n### Leader Election算法\n\n　　上文说明了Kafka是如何做Replication的，另外一个很重要的问题是当Leader宕机了，怎样在Follower中选举出新的Leader。因为Follower可能落后许多或者crash了，所以必须确保选择“最新”的Follower作为新的Leader。一个基本的原则就是，如果Leader不在了，新的Leader必须拥有原来的Leader commit过的所有消息。这就需要作一个折衷，如果Leader在标明一条消息被commit前等待更多的Follower确认，那在它宕机之后就有更多的Follower可以作为新的Leader，但这也会造成吞吐率的下降。\n　　一种非常常用的Leader Election的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在[Zookeeper](http://zookeeper.apache.org/)这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于[majority-vote-based journal](http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1)，但是它的数据存储并没有使用这种方式。\n　　实际上，Leader Election算法非常多，比如Zookeeper的[Zab](http://web.stanford.edu/class/cs347/reading/zab.pdf), [Raft](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)和[Viewstamped Replication](http://pmg.csail.mit.edu/papers/vr-revisited.pdf)。而Kafka所使用的Leader Election算法更像微软的[PacificA](http://research.microsoft.com/apps/pubs/default.aspx?id=66814)算法。\n　　Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 　　\n\n### 如何处理所有Replica都不工作\n\n　　上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：\n\n- 等待ISR中的任一个Replica“活”过来，并且选它作为Leader\n- 选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader\n\n　　这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 　　\n\n### 如何选举Leader\n\n　　最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。\n　　但是该方法会有3个问题： 　　\n\n- split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致\n- herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整\n- Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。\n\n　　Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。\n\n## HA相关Zookeeper结构\n\n　　（本节所示Zookeeper结构中，实线框代表路径名是固定的，而虚线框代表路径名与业务相关）\n　　**admin** （该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除）\n[![Kafka Zookeeper Admin Structure](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png)\n\n　　`/admin/preferred_replica_election`数据结构\n\n```\n   Schema:\n{\n      \"fields\":[\n         {\n            \"name\":\"version\",\n            \"type\":\"int\",\n            \"doc\":\"version id\"\n         },\n         {\n            \"name\":\"partitions\",\n            \"type\":{\n               \"type\":\"array\",\n               \"items\":{\n                  \"fields\":[\n                     {\n                        \"name\":\"topic\",\n                        \"type\":\"string\",\n                        \"doc\":\"topic of the partition for which preferred replica election should be triggered\"\n                     },\n                     {\n                        \"name\":\"partition\",\n                        \"type\":\"int\",\n                        \"doc\":\"the partition for which preferred replica election should be triggered\"\n                     }\n                  ],\n               }\n               \"doc\":\"an array of partitions for which preferred replica election should be triggered\"\n            }\n         }\n      ]\n   }\n    \n   Example:     \n   {\n     \"version\": 1,\n     \"partitions\":\n        [\n           {\n               \"topic\": \"topic1\",\n               \"partition\": 8         \n           },\n           {\n               \"topic\": \"topic2\",\n               \"partition\": 16        \n           }\n        ]            \n   }\n```\n\n\n\n　　`/admin/reassign_partitions`用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。其数据结构如下\n\n```\n   Schema:\n{\n      \"fields\":[\n         {\n            \"name\":\"version\",\n            \"type\":\"int\",\n            \"doc\":\"version id\"\n         },\n         {\n            \"name\":\"partitions\",\n            \"type\":{\n               \"type\":\"array\",\n               \"items\":{\n                  \"fields\":[\n                     {\n                        \"name\":\"topic\",\n                        \"type\":\"string\",\n                        \"doc\":\"topic of the partition to be reassigned\"\n                     },\n                     {\n                        \"name\":\"partition\",\n                        \"type\":\"int\",\n                        \"doc\":\"the partition to be reassigned\"\n                     },\n                     {\n                        \"name\":\"replicas\",\n                        \"type\":\"array\",\n                        \"items\":\"int\",\n                        \"doc\":\"a list of replica ids\"\n                     }\n                  ],\n               }\n               \"doc\":\"an array of partitions to be reassigned to new replicas\"\n            }\n         }\n      ]\n   }\n\n   Example:\n   {\n     \"version\": 1,\n     \"partitions\":\n        [\n           {\n               \"topic\": \"topic3\",\n               \"partition\": 1,\n               \"replicas\": [1, 2, 3]\n           }\n        ]            \n   }\n```\n\n\n\n　　`/admin/delete_topics`数据结构\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"topics\",\n       \"type\": { \"type\": \"array\", \"items\": \"string\", \"doc\": \"an array of topics to be deleted\"}\n      } ]\n}\n \nExample:\n{\n  \"version\": 1,\n  \"topics\": [\"topic4\", \"topic5\"]\n}\n```\n\n\n\n　　**brokers**\n[![Kafka Zookeeper brokers structure](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png)\n\n　　broker（即`/brokers/ids/[brokerId]`）存储“活着”的Broker信息。数据结构如下\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"host\", \"type\": \"string\", \"doc\": \"ip address or host name of the broker\"},\n      {\"name\": \"port\", \"type\": \"int\", \"doc\": \"port of the broker\"},\n      {\"name\": \"jmx_port\", \"type\": \"int\", \"doc\": \"port for jmx\"}\n    ]\n}\n \nExample:\n{\n    \"jmx_port\":-1,\n    \"host\":\"node1\",\n    \"version\":1,\n    \"port\":9092\n}\n```\n\n\n\n　　topic注册信息（`/brokers/topics/[topic]`），存储该Topic的所有Partition的所有Replica所在的Broker id，第一个Replica即为Preferred Replica，对一个给定的Partition，它在同一个Broker上最多只有一个Replica,因此Broker id可作为Replica id。数据结构如下\n\n```\nSchema:\n{ \"fields\" :\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"partitions\",\n       \"type\": {\"type\": \"map\",\n                \"values\": {\"type\": \"array\", \"items\": \"int\", \"doc\": \"a list of replica ids\"},\n                \"doc\": \"a map from partition id to replica list\"},\n      }\n    ]\n}\nExample:\n{\n    \"version\":1,\n    \"partitions\":\n        {\"12\":[6],\n        \"8\":[2],\n        \"4\":[6],\n        \"11\":[5],\n        \"9\":[3],\n        \"5\":[7],\n        \"10\":[4],\n        \"6\":[8],\n        \"1\":[3],\n        \"0\":[2],\n        \"2\":[4],\n        \"7\":[1],\n        \"3\":[5]}\n}\n```\n\n\n\n　　partition state（`/brokers/topics/[topic]/partitions/[partitionId]/state`） 结构如下\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"isr\",\n       \"type\": {\"type\": \"array\",\n                \"items\": \"int\",\n                \"doc\": \"an array of the id of replicas in isr\"}\n      },\n      {\"name\": \"leader\", \"type\": \"int\", \"doc\": \"id of the leader replica\"},\n      {\"name\": \"controller_epoch\", \"type\": \"int\", \"doc\": \"epoch of the controller that last updated the leader and isr info\"},\n      {\"name\": \"leader_epoch\", \"type\": \"int\", \"doc\": \"epoch of the leader\"}\n    ]\n}\n \nExample:\n{\n    \"controller_epoch\":29,\n    \"leader\":2,\n    \"version\":1,\n    \"leader_epoch\":48,\n    \"isr\":[2]\n}\n```\n\n\n\n　　**controller**\n　　`/controller -> int (broker id of the controller)`存储当前controller的信息\n\n```\nSchema:\n{ \"fields\":\n    [ {\"name\": \"version\", \"type\": \"int\", \"doc\": \"version id\"},\n      {\"name\": \"brokerid\", \"type\": \"int\", \"doc\": \"broker id of the controller\"}\n    ]\n}\nExample:\n{\n    \"version\":1,\n　　\"brokerid\":8\n}\n```\n\n\n\n　　`/controller_epoch -> int (epoch)`直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。 　　 　　\n\n## broker failover过程简介\n\n1. Controller在Zookeeper注册Watch，一旦有Broker宕机（这是用宕机代表任何让系统认为其die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的znode会自动被删除，Zookeeper会fire Controller注册的watch，Controller读取最新的幸存的Broker\n2. Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition\n3. 对set_p中的每一个Partition\n   　　3.1 从`/brokers/topics/[topic]/partitions/[partition]/state`读取该Partition当前的ISR\n   　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。\n   　　　3.3 将新的Leader，ISR和新的`leader_epoch`及`controller_epoch`写入`/brokers/topics/[topic]/partitions/[partition]/state`。注意，该操作只有其version在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1\n4. 直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。\n   　　Broker failover顺序图如下所示。\n   [![broker failover sequence diagram ](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)","slug":"kafka 高可用设计(上)","published":1,"updated":"2019-02-14T15:32:37.494Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubq90019amum4pazjjv9","content":"<p>本文转发自<a href=\"http://www.jasongj.com/\" target=\"_blank\" rel=\"noopener\"><strong>技术世界</strong></a>，<a href=\"http://www.jasongj.com/2015/04/24/KafkaColumn2\" target=\"_blank\" rel=\"noopener\">原文链接</a>　<a href=\"http://www.jasongj.com/2015/04/24/KafkaColumn2\" target=\"_blank\" rel=\"noopener\">http://www.jasongj.com/2015/04/24/KafkaColumn2</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>　　Kafka在0.8以前的版本中，并不提供High Availablity机制，一旦一个或多个Broker宕机，则宕机期间其上所有Partition都无法继续提供服务。若该Broker永远不能再恢复，亦或磁盘故障，则其上数据将丢失。而Kafka的设计目标之一即是提供数据持久化，同时对于分布式系统来说，尤其当集群规模上升到一定程度后，一台或者多台机器宕机的可能性大大提高，对于Failover机制的需求非常高。因此，Kafka从0.8开始提供High Availability机制。本文从Data Replication和Leader Election两方面介绍了Kafka的HA机制。</p>\n<h1 id=\"Kafka为何需要High-Available\"><a href=\"#Kafka为何需要High-Available\" class=\"headerlink\" title=\"Kafka为何需要High Available\"></a>Kafka为何需要High Available</h1><h2 id=\"为何需要Replication\"><a href=\"#为何需要Replication\" class=\"headerlink\" title=\"为何需要Replication\"></a>为何需要Replication</h2><p>　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。</p>\n<ul>\n<li>如果Producer使用同步模式则Producer会在尝试重新发送<code>message.send.max.retries</code>（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。</li>\n<li>如果Producer使用异步模式，则Producer会尝试重新发送<code>message.send.max.retries</code>（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。</li>\n</ul>\n<p>　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 　　</p>\n<h2 id=\"为何需要Leader-Election\"><a href=\"#为何需要Leader-Election\" class=\"headerlink\" title=\"为何需要Leader Election\"></a>为何需要Leader Election</h2><p>　　（本文所述Leader Election主要指Replica之间的Leader Election）<br>　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replica中选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。<br>　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 　　 　　</p>\n<h1 id=\"Kafka-HA设计解析\"><a href=\"#Kafka-HA设计解析\" class=\"headerlink\" title=\"Kafka HA设计解析\"></a>Kafka HA设计解析</h1><h2 id=\"如何将所有Replica均匀分布到整个集群\"><a href=\"#如何将所有Replica均匀分布到整个集群\" class=\"headerlink\" title=\"如何将所有Replica均匀分布到整个集群\"></a>如何将所有Replica均匀分布到整个集群</h2><p>　　为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。<br>　　Kafka分配Replica的算法如下：</p>\n<ol>\n<li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>\n<li>将第i个Partition分配到第（i mod n）个Broker上</li>\n<li>将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上</li>\n</ol>\n<a id=\"more\"></a>\n<h2 id=\"Data-Replication\"><a href=\"#Data-Replication\" class=\"headerlink\" title=\"Data Replication\"></a>Data Replication</h2><p>　　Kafka的Data Replication需要解决如下问题：</p>\n<ul>\n<li>怎样Propagate消息</li>\n<li>在向Producer发送ACK前需要保证有多少个Replica已经收到该消息</li>\n<li>怎样处理某个Replica不工作的情况</li>\n<li>怎样处理Failed Replica恢复回来的情况</li>\n</ul>\n<h3 id=\"Propagate消息\"><a href=\"#Propagate消息\" class=\"headerlink\" title=\"Propagate消息\"></a>Propagate消息</h3><p>　　Producer在发布消息到某个Partition时，先通过Zookeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。<br>为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。但考虑到这种场景非常少见，可以认为这种方式在性能和数据持久化上做了一个比较好的平衡。在将来的版本中，Kafka会考虑提供更高的持久性。<br>Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。<br>Kafka Replication的数据流如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png\" alt=\"Kafka Replication Data Flow\"></a></p>\n<h3 id=\"ACK前需要保证有多少个备份\"><a href=\"#ACK前需要保证有多少个备份\" class=\"headerlink\" title=\"ACK前需要保证有多少个备份\"></a>ACK前需要保证有多少个备份</h3><p>　　和大部分分布式系统一样，Kafka处理失败需要明确定义一个Broker是否“活着”。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(这个通过Zookeeper的Heartbeat机制来实现)。二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。<br>　　Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过<code>replica.lag.max.messages</code>配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过<code>replica.lag.time.max.ms</code>来配置，其默认值是10000）未向Leader发送fetch请求。。<br>　　Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。<br>　　需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过<code>request.required.acks</code>来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 　　</p>\n<h3 id=\"Leader-Election算法\"><a href=\"#Leader-Election算法\" class=\"headerlink\" title=\"Leader Election算法\"></a>Leader Election算法</h3><p>　　上文说明了Kafka是如何做Replication的，另外一个很重要的问题是当Leader宕机了，怎样在Follower中选举出新的Leader。因为Follower可能落后许多或者crash了，所以必须确保选择“最新”的Follower作为新的Leader。一个基本的原则就是，如果Leader不在了，新的Leader必须拥有原来的Leader commit过的所有消息。这就需要作一个折衷，如果Leader在标明一条消息被commit前等待更多的Follower确认，那在它宕机之后就有更多的Follower可以作为新的Leader，但这也会造成吞吐率的下降。<br>　　一种非常常用的Leader Election的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在<a href=\"http://zookeeper.apache.org/\" target=\"_blank\" rel=\"noopener\">Zookeeper</a>这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于<a href=\"http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1\" target=\"_blank\" rel=\"noopener\">majority-vote-based journal</a>，但是它的数据存储并没有使用这种方式。<br>　　实际上，Leader Election算法非常多，比如Zookeeper的<a href=\"http://web.stanford.edu/class/cs347/reading/zab.pdf\" target=\"_blank\" rel=\"noopener\">Zab</a>, <a href=\"https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a>和<a href=\"http://pmg.csail.mit.edu/papers/vr-revisited.pdf\" target=\"_blank\" rel=\"noopener\">Viewstamped Replication</a>。而Kafka所使用的Leader Election算法更像微软的<a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=66814\" target=\"_blank\" rel=\"noopener\">PacificA</a>算法。<br>　　Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 　　</p>\n<h3 id=\"如何处理所有Replica都不工作\"><a href=\"#如何处理所有Replica都不工作\" class=\"headerlink\" title=\"如何处理所有Replica都不工作\"></a>如何处理所有Replica都不工作</h3><p>　　上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：</p>\n<ul>\n<li>等待ISR中的任一个Replica“活”过来，并且选它作为Leader</li>\n<li>选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader</li>\n</ul>\n<p>　　这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 　　</p>\n<h3 id=\"如何选举Leader\"><a href=\"#如何选举Leader\" class=\"headerlink\" title=\"如何选举Leader\"></a>如何选举Leader</h3><p>　　最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。<br>　　但是该方法会有3个问题： 　　</p>\n<ul>\n<li>split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致</li>\n<li>herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</li>\n<li>Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。</li>\n</ul>\n<p>　　Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。</p>\n<h2 id=\"HA相关Zookeeper结构\"><a href=\"#HA相关Zookeeper结构\" class=\"headerlink\" title=\"HA相关Zookeeper结构\"></a>HA相关Zookeeper结构</h2><p>　　（本节所示Zookeeper结构中，实线框代表路径名是固定的，而虚线框代表路径名与业务相关）<br>　　<strong>admin</strong> （该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除）<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png\" alt=\"Kafka Zookeeper Admin Structure\"></a></p>\n<p>　　<code>/admin/preferred_replica_election</code>数据结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   Schema:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;fields&quot;:[</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;version&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">            &quot;doc&quot;:&quot;version id&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;partitions&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&#123;</span><br><span class=\"line\">               &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">               &quot;items&quot;:&#123;</span><br><span class=\"line\">                  &quot;fields&quot;:[</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;topic&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;string&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;topic of the partition for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;partition&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;the partition for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">                     &#125;</span><br><span class=\"line\">                  ],</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">               &quot;doc&quot;:&quot;an array of partitions for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">   Example:     </span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;version&quot;: 1,</span><br><span class=\"line\">     &quot;partitions&quot;:</span><br><span class=\"line\">        [</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic1&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 8         </span><br><span class=\"line\">           &#125;,</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic2&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 16        </span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">        ]            </span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/admin/reassign_partitions</code>用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。其数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   Schema:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;fields&quot;:[</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;version&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">            &quot;doc&quot;:&quot;version id&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;partitions&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&#123;</span><br><span class=\"line\">               &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">               &quot;items&quot;:&#123;</span><br><span class=\"line\">                  &quot;fields&quot;:[</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;topic&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;string&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;topic of the partition to be reassigned&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;partition&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;the partition to be reassigned&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;replicas&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">                        &quot;items&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;a list of replica ids&quot;</span><br><span class=\"line\">                     &#125;</span><br><span class=\"line\">                  ],</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">               &quot;doc&quot;:&quot;an array of partitions to be reassigned to new replicas&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   Example:</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;version&quot;: 1,</span><br><span class=\"line\">     &quot;partitions&quot;:</span><br><span class=\"line\">        [</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic3&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 1,</span><br><span class=\"line\">               &quot;replicas&quot;: [1, 2, 3]</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">        ]            </span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/admin/delete_topics</code>数据结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;topics&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;an array of topics to be deleted&quot;&#125;</span><br><span class=\"line\">      &#125; ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;version&quot;: 1,</span><br><span class=\"line\">  &quot;topics&quot;: [&quot;topic4&quot;, &quot;topic5&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>brokers</strong><br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png\" alt=\"Kafka Zookeeper brokers structure\"></a></p>\n<p>　　broker（即<code>/brokers/ids/[brokerId]</code>）存储“活着”的Broker信息。数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;host&quot;, &quot;type&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;ip address or host name of the broker&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;port&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;port of the broker&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;jmx_port&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;port for jmx&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;jmx_port&quot;:-1,</span><br><span class=\"line\">    &quot;host&quot;:&quot;node1&quot;,</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;port&quot;:9092</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　topic注册信息（<code>/brokers/topics/[topic]</code>），存储该Topic的所有Partition的所有Replica所在的Broker id，第一个Replica即为Preferred Replica，对一个给定的Partition，它在同一个Broker上最多只有一个Replica,因此Broker id可作为Replica id。数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot; :</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;partitions&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123;&quot;type&quot;: &quot;map&quot;,</span><br><span class=\"line\">                &quot;values&quot;: &#123;&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;a list of replica ids&quot;&#125;,</span><br><span class=\"line\">                &quot;doc&quot;: &quot;a map from partition id to replica list&quot;&#125;,</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;partitions&quot;:</span><br><span class=\"line\">        &#123;&quot;12&quot;:[6],</span><br><span class=\"line\">        &quot;8&quot;:[2],</span><br><span class=\"line\">        &quot;4&quot;:[6],</span><br><span class=\"line\">        &quot;11&quot;:[5],</span><br><span class=\"line\">        &quot;9&quot;:[3],</span><br><span class=\"line\">        &quot;5&quot;:[7],</span><br><span class=\"line\">        &quot;10&quot;:[4],</span><br><span class=\"line\">        &quot;6&quot;:[8],</span><br><span class=\"line\">        &quot;1&quot;:[3],</span><br><span class=\"line\">        &quot;0&quot;:[2],</span><br><span class=\"line\">        &quot;2&quot;:[4],</span><br><span class=\"line\">        &quot;7&quot;:[1],</span><br><span class=\"line\">        &quot;3&quot;:[5]&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　partition state（<code>/brokers/topics/[topic]/partitions/[partitionId]/state</code>） 结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;isr&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123;&quot;type&quot;: &quot;array&quot;,</span><br><span class=\"line\">                &quot;items&quot;: &quot;int&quot;,</span><br><span class=\"line\">                &quot;doc&quot;: &quot;an array of the id of replicas in isr&quot;&#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;leader&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;id of the leader replica&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;controller_epoch&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;epoch of the controller that last updated the leader and isr info&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;leader_epoch&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;epoch of the leader&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;controller_epoch&quot;:29,</span><br><span class=\"line\">    &quot;leader&quot;:2,</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;leader_epoch&quot;:48,</span><br><span class=\"line\">    &quot;isr&quot;:[2]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>controller</strong><br>　　<code>/controller -&gt; int (broker id of the controller)</code>存储当前controller的信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;brokerid&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;broker id of the controller&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">　　&quot;brokerid&quot;:8</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/controller_epoch -&gt; int (epoch)</code>直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。 　　 　　</p>\n<h2 id=\"broker-failover过程简介\"><a href=\"#broker-failover过程简介\" class=\"headerlink\" title=\"broker failover过程简介\"></a>broker failover过程简介</h2><ol>\n<li>Controller在Zookeeper注册Watch，一旦有Broker宕机（这是用宕机代表任何让系统认为其die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的znode会自动被删除，Zookeeper会fire Controller注册的watch，Controller读取最新的幸存的Broker</li>\n<li>Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition</li>\n<li>对set_p中的每一个Partition<br>　　3.1 从<code>/brokers/topics/[topic]/partitions/[partition]/state</code>读取该Partition当前的ISR<br>　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。<br>　　　3.3 将新的Leader，ISR和新的<code>leader_epoch</code>及<code>controller_epoch</code>写入<code>/brokers/topics/[topic]/partitions/[partition]/state</code>。注意，该操作只有其version在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1</li>\n<li>直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。<br>　　Broker failover顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" alt=\"broker failover sequence diagram \"></a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>本文转发自<a href=\"http://www.jasongj.com/\" target=\"_blank\" rel=\"noopener\"><strong>技术世界</strong></a>，<a href=\"http://www.jasongj.com/2015/04/24/KafkaColumn2\" target=\"_blank\" rel=\"noopener\">原文链接</a>　<a href=\"http://www.jasongj.com/2015/04/24/KafkaColumn2\" target=\"_blank\" rel=\"noopener\">http://www.jasongj.com/2015/04/24/KafkaColumn2</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>　　Kafka在0.8以前的版本中，并不提供High Availablity机制，一旦一个或多个Broker宕机，则宕机期间其上所有Partition都无法继续提供服务。若该Broker永远不能再恢复，亦或磁盘故障，则其上数据将丢失。而Kafka的设计目标之一即是提供数据持久化，同时对于分布式系统来说，尤其当集群规模上升到一定程度后，一台或者多台机器宕机的可能性大大提高，对于Failover机制的需求非常高。因此，Kafka从0.8开始提供High Availability机制。本文从Data Replication和Leader Election两方面介绍了Kafka的HA机制。</p>\n<h1 id=\"Kafka为何需要High-Available\"><a href=\"#Kafka为何需要High-Available\" class=\"headerlink\" title=\"Kafka为何需要High Available\"></a>Kafka为何需要High Available</h1><h2 id=\"为何需要Replication\"><a href=\"#为何需要Replication\" class=\"headerlink\" title=\"为何需要Replication\"></a>为何需要Replication</h2><p>　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。</p>\n<ul>\n<li>如果Producer使用同步模式则Producer会在尝试重新发送<code>message.send.max.retries</code>（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。</li>\n<li>如果Producer使用异步模式，则Producer会尝试重新发送<code>message.send.max.retries</code>（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。</li>\n</ul>\n<p>　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 　　</p>\n<h2 id=\"为何需要Leader-Election\"><a href=\"#为何需要Leader-Election\" class=\"headerlink\" title=\"为何需要Leader Election\"></a>为何需要Leader Election</h2><p>　　（本文所述Leader Election主要指Replica之间的Leader Election）<br>　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replica中选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。<br>　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 　　 　　</p>\n<h1 id=\"Kafka-HA设计解析\"><a href=\"#Kafka-HA设计解析\" class=\"headerlink\" title=\"Kafka HA设计解析\"></a>Kafka HA设计解析</h1><h2 id=\"如何将所有Replica均匀分布到整个集群\"><a href=\"#如何将所有Replica均匀分布到整个集群\" class=\"headerlink\" title=\"如何将所有Replica均匀分布到整个集群\"></a>如何将所有Replica均匀分布到整个集群</h2><p>　　为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。<br>　　Kafka分配Replica的算法如下：</p>\n<ol>\n<li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>\n<li>将第i个Partition分配到第（i mod n）个Broker上</li>\n<li>将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上</li>\n</ol>","more":"<h2 id=\"Data-Replication\"><a href=\"#Data-Replication\" class=\"headerlink\" title=\"Data Replication\"></a>Data Replication</h2><p>　　Kafka的Data Replication需要解决如下问题：</p>\n<ul>\n<li>怎样Propagate消息</li>\n<li>在向Producer发送ACK前需要保证有多少个Replica已经收到该消息</li>\n<li>怎样处理某个Replica不工作的情况</li>\n<li>怎样处理Failed Replica恢复回来的情况</li>\n</ul>\n<h3 id=\"Propagate消息\"><a href=\"#Propagate消息\" class=\"headerlink\" title=\"Propagate消息\"></a>Propagate消息</h3><p>　　Producer在发布消息到某个Partition时，先通过Zookeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。<br>为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。但考虑到这种场景非常少见，可以认为这种方式在性能和数据持久化上做了一个比较好的平衡。在将来的版本中，Kafka会考虑提供更高的持久性。<br>Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。<br>Kafka Replication的数据流如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/Replication.png\" alt=\"Kafka Replication Data Flow\"></a></p>\n<h3 id=\"ACK前需要保证有多少个备份\"><a href=\"#ACK前需要保证有多少个备份\" class=\"headerlink\" title=\"ACK前需要保证有多少个备份\"></a>ACK前需要保证有多少个备份</h3><p>　　和大部分分布式系统一样，Kafka处理失败需要明确定义一个Broker是否“活着”。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(这个通过Zookeeper的Heartbeat机制来实现)。二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。<br>　　Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过<code>replica.lag.max.messages</code>配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过<code>replica.lag.time.max.ms</code>来配置，其默认值是10000）未向Leader发送fetch请求。。<br>　　Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。<br>　　需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过<code>request.required.acks</code>来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 　　</p>\n<h3 id=\"Leader-Election算法\"><a href=\"#Leader-Election算法\" class=\"headerlink\" title=\"Leader Election算法\"></a>Leader Election算法</h3><p>　　上文说明了Kafka是如何做Replication的，另外一个很重要的问题是当Leader宕机了，怎样在Follower中选举出新的Leader。因为Follower可能落后许多或者crash了，所以必须确保选择“最新”的Follower作为新的Leader。一个基本的原则就是，如果Leader不在了，新的Leader必须拥有原来的Leader commit过的所有消息。这就需要作一个折衷，如果Leader在标明一条消息被commit前等待更多的Follower确认，那在它宕机之后就有更多的Follower可以作为新的Leader，但这也会造成吞吐率的下降。<br>　　一种非常常用的Leader Election的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在<a href=\"http://zookeeper.apache.org/\" target=\"_blank\" rel=\"noopener\">Zookeeper</a>这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于<a href=\"http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1\" target=\"_blank\" rel=\"noopener\">majority-vote-based journal</a>，但是它的数据存储并没有使用这种方式。<br>　　实际上，Leader Election算法非常多，比如Zookeeper的<a href=\"http://web.stanford.edu/class/cs347/reading/zab.pdf\" target=\"_blank\" rel=\"noopener\">Zab</a>, <a href=\"https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft</a>和<a href=\"http://pmg.csail.mit.edu/papers/vr-revisited.pdf\" target=\"_blank\" rel=\"noopener\">Viewstamped Replication</a>。而Kafka所使用的Leader Election算法更像微软的<a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=66814\" target=\"_blank\" rel=\"noopener\">PacificA</a>算法。<br>　　Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 　　</p>\n<h3 id=\"如何处理所有Replica都不工作\"><a href=\"#如何处理所有Replica都不工作\" class=\"headerlink\" title=\"如何处理所有Replica都不工作\"></a>如何处理所有Replica都不工作</h3><p>　　上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：</p>\n<ul>\n<li>等待ISR中的任一个Replica“活”过来，并且选它作为Leader</li>\n<li>选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader</li>\n</ul>\n<p>　　这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 　　</p>\n<h3 id=\"如何选举Leader\"><a href=\"#如何选举Leader\" class=\"headerlink\" title=\"如何选举Leader\"></a>如何选举Leader</h3><p>　　最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。<br>　　但是该方法会有3个问题： 　　</p>\n<ul>\n<li>split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致</li>\n<li>herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</li>\n<li>Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。</li>\n</ul>\n<p>　　Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。</p>\n<h2 id=\"HA相关Zookeeper结构\"><a href=\"#HA相关Zookeeper结构\" class=\"headerlink\" title=\"HA相关Zookeeper结构\"></a>HA相关Zookeeper结构</h2><p>　　（本节所示Zookeeper结构中，实线框代表路径名是固定的，而虚线框代表路径名与业务相关）<br>　　<strong>admin</strong> （该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除）<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_admin.png\" alt=\"Kafka Zookeeper Admin Structure\"></a></p>\n<p>　　<code>/admin/preferred_replica_election</code>数据结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   Schema:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;fields&quot;:[</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;version&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">            &quot;doc&quot;:&quot;version id&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;partitions&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&#123;</span><br><span class=\"line\">               &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">               &quot;items&quot;:&#123;</span><br><span class=\"line\">                  &quot;fields&quot;:[</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;topic&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;string&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;topic of the partition for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;partition&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;the partition for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">                     &#125;</span><br><span class=\"line\">                  ],</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">               &quot;doc&quot;:&quot;an array of partitions for which preferred replica election should be triggered&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">   Example:     </span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;version&quot;: 1,</span><br><span class=\"line\">     &quot;partitions&quot;:</span><br><span class=\"line\">        [</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic1&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 8         </span><br><span class=\"line\">           &#125;,</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic2&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 16        </span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">        ]            </span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/admin/reassign_partitions</code>用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。其数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   Schema:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;fields&quot;:[</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;version&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">            &quot;doc&quot;:&quot;version id&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &#123;</span><br><span class=\"line\">            &quot;name&quot;:&quot;partitions&quot;,</span><br><span class=\"line\">            &quot;type&quot;:&#123;</span><br><span class=\"line\">               &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">               &quot;items&quot;:&#123;</span><br><span class=\"line\">                  &quot;fields&quot;:[</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;topic&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;string&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;topic of the partition to be reassigned&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;partition&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;the partition to be reassigned&quot;</span><br><span class=\"line\">                     &#125;,</span><br><span class=\"line\">                     &#123;</span><br><span class=\"line\">                        &quot;name&quot;:&quot;replicas&quot;,</span><br><span class=\"line\">                        &quot;type&quot;:&quot;array&quot;,</span><br><span class=\"line\">                        &quot;items&quot;:&quot;int&quot;,</span><br><span class=\"line\">                        &quot;doc&quot;:&quot;a list of replica ids&quot;</span><br><span class=\"line\">                     &#125;</span><br><span class=\"line\">                  ],</span><br><span class=\"line\">               &#125;</span><br><span class=\"line\">               &quot;doc&quot;:&quot;an array of partitions to be reassigned to new replicas&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   Example:</span><br><span class=\"line\">   &#123;</span><br><span class=\"line\">     &quot;version&quot;: 1,</span><br><span class=\"line\">     &quot;partitions&quot;:</span><br><span class=\"line\">        [</span><br><span class=\"line\">           &#123;</span><br><span class=\"line\">               &quot;topic&quot;: &quot;topic3&quot;,</span><br><span class=\"line\">               &quot;partition&quot;: 1,</span><br><span class=\"line\">               &quot;replicas&quot;: [1, 2, 3]</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">        ]            </span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/admin/delete_topics</code>数据结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;topics&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;an array of topics to be deleted&quot;&#125;</span><br><span class=\"line\">      &#125; ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;version&quot;: 1,</span><br><span class=\"line\">  &quot;topics&quot;: [&quot;topic4&quot;, &quot;topic5&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>brokers</strong><br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_zookeeper_brokers.png\" alt=\"Kafka Zookeeper brokers structure\"></a></p>\n<p>　　broker（即<code>/brokers/ids/[brokerId]</code>）存储“活着”的Broker信息。数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;host&quot;, &quot;type&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;ip address or host name of the broker&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;port&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;port of the broker&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;jmx_port&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;port for jmx&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;jmx_port&quot;:-1,</span><br><span class=\"line\">    &quot;host&quot;:&quot;node1&quot;,</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;port&quot;:9092</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　topic注册信息（<code>/brokers/topics/[topic]</code>），存储该Topic的所有Partition的所有Replica所在的Broker id，第一个Replica即为Preferred Replica，对一个给定的Partition，它在同一个Broker上最多只有一个Replica,因此Broker id可作为Replica id。数据结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot; :</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;partitions&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123;&quot;type&quot;: &quot;map&quot;,</span><br><span class=\"line\">                &quot;values&quot;: &#123;&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;a list of replica ids&quot;&#125;,</span><br><span class=\"line\">                &quot;doc&quot;: &quot;a map from partition id to replica list&quot;&#125;,</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;partitions&quot;:</span><br><span class=\"line\">        &#123;&quot;12&quot;:[6],</span><br><span class=\"line\">        &quot;8&quot;:[2],</span><br><span class=\"line\">        &quot;4&quot;:[6],</span><br><span class=\"line\">        &quot;11&quot;:[5],</span><br><span class=\"line\">        &quot;9&quot;:[3],</span><br><span class=\"line\">        &quot;5&quot;:[7],</span><br><span class=\"line\">        &quot;10&quot;:[4],</span><br><span class=\"line\">        &quot;6&quot;:[8],</span><br><span class=\"line\">        &quot;1&quot;:[3],</span><br><span class=\"line\">        &quot;0&quot;:[2],</span><br><span class=\"line\">        &quot;2&quot;:[4],</span><br><span class=\"line\">        &quot;7&quot;:[1],</span><br><span class=\"line\">        &quot;3&quot;:[5]&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　partition state（<code>/brokers/topics/[topic]/partitions/[partitionId]/state</code>） 结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;isr&quot;,</span><br><span class=\"line\">       &quot;type&quot;: &#123;&quot;type&quot;: &quot;array&quot;,</span><br><span class=\"line\">                &quot;items&quot;: &quot;int&quot;,</span><br><span class=\"line\">                &quot;doc&quot;: &quot;an array of the id of replicas in isr&quot;&#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;leader&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;id of the leader replica&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;controller_epoch&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;epoch of the controller that last updated the leader and isr info&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;leader_epoch&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;epoch of the leader&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;controller_epoch&quot;:29,</span><br><span class=\"line\">    &quot;leader&quot;:2,</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">    &quot;leader_epoch&quot;:48,</span><br><span class=\"line\">    &quot;isr&quot;:[2]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<strong>controller</strong><br>　　<code>/controller -&gt; int (broker id of the controller)</code>存储当前controller的信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Schema:</span><br><span class=\"line\">&#123; &quot;fields&quot;:</span><br><span class=\"line\">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class=\"line\">      &#123;&quot;name&quot;: &quot;brokerid&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;broker id of the controller&quot;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Example:</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;version&quot;:1,</span><br><span class=\"line\">　　&quot;brokerid&quot;:8</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>　　<code>/controller_epoch -&gt; int (epoch)</code>直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。 　　 　　</p>\n<h2 id=\"broker-failover过程简介\"><a href=\"#broker-failover过程简介\" class=\"headerlink\" title=\"broker failover过程简介\"></a>broker failover过程简介</h2><ol>\n<li>Controller在Zookeeper注册Watch，一旦有Broker宕机（这是用宕机代表任何让系统认为其die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的znode会自动被删除，Zookeeper会fire Controller注册的watch，Controller读取最新的幸存的Broker</li>\n<li>Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition</li>\n<li>对set_p中的每一个Partition<br>　　3.1 从<code>/brokers/topics/[topic]/partitions/[partition]/state</code>读取该Partition当前的ISR<br>　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。<br>　　　3.3 将新的Leader，ISR和新的<code>leader_epoch</code>及<code>controller_epoch</code>写入<code>/brokers/topics/[topic]/partitions/[partition]/state</code>。注意，该操作只有其version在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1</li>\n<li>直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。<br>　　Broker failover顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" alt=\"broker failover sequence diagram \"></a></li>\n</ol>"},{"title":"nginx互斥锁的实现","date":"2018-03-16T03:15:22.000Z","_content":"\n# nginx互斥锁的实现\n\n发表于 2017-06-22 |\n\nnginx 基于原子操作、信号量以及文件锁实现了一个简单高效的互斥锁，当多个 worker 进程之间需要互斥操作时都会用到。下面来看下 nginx 是如何实现它的。\n\n## 原子操作\n\n在实现互斥锁时用到了原子操作，先来了解一下 nginx 下提供的两个原子操作相关的函数：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old, ngx_atomic_uint_t set)\n\nstatic ngx_inline ngx_atomic_int_t\nngx_atomic_fetch_add(ngx_atomic_t *value, ngx_atomic_int_t add)\n```\n\n第一个函数是一个 `CAS` 操作，首先它比较 `lock` 地址处的变量是否等于 `old`， 如果相等，就把 `lock` 地址处的变量设为 `set` 变返回成功，否则返回失败。注意上述过程是作为一个原子一起进行的，不会被打断。 用代码可以描述如下：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old, ngx_atomic_uint_t set)\n{\n    if (*lock == old) {\n        *lock = set;\n        return 1;\n    }\n\n    return 0;\n}\n```\n\n第二个函数是读取 `value` 地址处的变量，并将其与 `add` 相加的结果再写入 `*lock`，然后返回原来 `*lock` 的值，这些操作也是作为一个整体完成的，不会被打断。用代码可描述如下：\n\n```c\nstatic ngx_inline ngx_atomic_int_t\nngx_atomic_fetch_add(ngx_atomic_t *value, ngx_atomic_int_t add)\n{\n    ngx_atomic_int_t  old;\n\n    old = *value;\n    *value += add;\n\n    return old;\n}\n```\n\n<!--more -->\n\nnginx 在实现这两个函数时会首先判断有没有支持原子操作的库，如果有，则直接使用库提供的原子操作实现，如果没有，则会使用汇编语言自己实现。下面以 `x86` 平台下实现 ngx_atomic_cmp_set 的汇编实现方式，实现主要使用了 `cmpxchgq` 指令，代码如下：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old,\n    ngx_atomic_uint_t set)\n{\n    u_char  res;\n\n    __asm__ volatile ( //volatile 关键字告诉编译器不要对下面的指令循序进行调整与优化。\n\n         NGX_SMP_LOCK  //这是一个宏，如果是当前是单cpu，则展开为空，如果是多cpu，则展开为lock;，它会锁住内存地址进行排他访问\n    \"    cmpxchgl  %3, %1;   \" //进行 cas 操作\n    \"    sete      %0;       \" //将操作结果写到 res 变量\n\n    : \"=a\" (res)  // 输出部分\n    : \"m\" (*lock), \"a\" (old), \"r\" (set) // 输入部分\n    : \"cc\", \"memory\" // 破坏描述部分，表示修改了哪些寄存器和内存，提醒编译器优化时要注意。\n    );\n\n    return res;\n}\n```\n\n上面的代码采用 gcc 嵌入汇编方式来进行编写，了解了 `cmpxchgq` 指令后还是比较容易理解的。\n\n## 锁结构体\n\n首先 nginx 使用 `ngx_shmtx_lock` 结构体表示锁，它的各个成员变量如下：\n\n```c\ntypedef struct {\n#if (NGX_HAVE_ATOMIC_OPS) \n    ngx_atomic_t  *lock;\n#if (NGX_HAVE_POSIX_SEM)\n    ngx_atomic_t  *wait;\n    ngx_uint_t     semaphore;\n    sem_t          sem;\n#endif\n#else  // 不支持原子变量，使用文件锁，效率稍低。\n    ngx_fd_t       fd;\n    u_char        *name;\n#endif\n    ngx_uint_t     spin; //获取锁时尝试的自旋次数，使用原子操作实现锁时才有意义\n} ngx_shmtx_t;\n```\n\n上面的结构体定义使用了两个宏：`NGX_HAVE_ATOMIC_OPS` 与 `NGX_HAVE_POSIX_SEM`，分别用来代表操作系统是否支持原子变量操作与信号量。根据这两个宏的取值，可以有3种不同的互斥锁实现：\n\n1. 不支持原子操作。\n2. 支持原子操作，但不支持信号量\n3. 支持原子操作，也支持信号量\n\n第1种情况最简单，会直接使用文件锁来实现互斥锁，这时该结构体只有 `fd` 、 `name`和 `spin` 三个字段，但 `spin` 字段是不起作用的。对于2和3两种情况 nginx 均会使用原子变量操作来实现一个自旋锁，其中 `spin` 表示自旋次数。它们两个的区别是：在支持信号量的情况下，如果自旋次数达到了上限而进程还未获取到锁，则进程会在信号量上阻塞等待，进入睡眠状态。不支持信号量的情况，则不会有这样的操作，而是通过调度器直接 「让出」cpu。 下面对这三种情况下锁的实现分别进行介绍。\n\n## 基于文件锁实现的锁\n\n### 锁的创建\n\n首先通过下面的函数创建一个锁：\n\n```c\n/*\n  mtx：要创建的锁\n  addr：使用文件锁实现互斥锁时不会用到该变量\n  name：文件锁使用的文件\n*/\nngx_int_t\nngx_shmtx_create(ngx_shmtx_t *mtx, ngx_shmtx_sh_t *addr, u_char *name)\n{\n\n    if (mtx->name) { // mtx->name不为NULL，说明它之前已经创建过锁\n\n        if (ngx_strcmp(name, mtx->name) == 0) { // 之前创建过锁，且与这次创建锁的文件相同，则不需要创建，直接返回\n            mtx->name = name;\n            return NGX_OK;\n        }\n        // 销毁之前创建到锁，其实就是关闭之前创建锁时打开的文件。\n        ngx_shmtx_destroy(mtx);\n    }\n\n    //打开文件\n    mtx->fd = ngx_open_file(name, NGX_FILE_RDWR, NGX_FILE_CREATE_OR_OPEN,\n                            NGX_FILE_DEFAULT_ACCESS);\n    \n    //打开文件失败，打印日志，然后返回\n    if (mtx->fd == NGX_INVALID_FILE) {\n        ngx_log_error(NGX_LOG_EMERG, ngx_cycle->log, ngx_errno,\n                      ngx_open_file_n \" \\\"%s\\\" failed\", name);\n        return NGX_ERROR;\n    }\n    \n    //使用锁时只需要该文件在内核中的inode信息，所以将该文件删掉\n    if (ngx_delete_file(name) == NGX_FILE_ERROR) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      ngx_delete_file_n \" \\\"%s\\\" failed\", name);\n    }\n\n    mtx->name = name;\n\n    return NGX_OK;\n}\n```\n\n### 阻塞锁的获取\n\n当进程需要进行阻塞加锁时，通过下面的函数进行：\n\n```c\n/*\n 通过文件锁的方式实现互斥锁，如果该文件锁正被其他进程占有，则会导致进程阻塞。\n*/\nvoid\nngx_shmtx_lock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    //通过获取文件锁来进行加锁。\n    err = ngx_lock_fd(mtx->fd);\n\n    if (err == 0) {\n        return;\n    }\n\n    ngx_log_abort(err, ngx_lock_fd_n \" %s failed\", mtx->name);\n}\n```\n\n上面函数主要的操作就是通过 `ngx_lock_fd` 来获取锁，它的实现如下：\n\n```c\nngx_err_t\nngx_lock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //设置文件锁的类型为写锁，即互斥锁\n    fl.l_type = F_WRLCK;\n    fl.l_whence = SEEK_SET;\n\n    //设置操作为F_SETLKW，表示获取不到文件锁时，会阻塞直到可以获取\n    if (fcntl(fd, F_SETLKW, &fl) == -1) {\n        return ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n它主要是通过 `fcntl` 函数来获取文件锁。\n\n### 非阻塞锁的获取\n\n上面获取锁的方式是阻塞式的，在获取不到锁时进程会阻塞，但有时候我们并不希望这样，而是不能获取锁时直接返回，nginx 通过这么函数来非阻塞的获取锁：\n\n```c\nngx_uint_t\nngx_shmtx_trylock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    // 与上面的阻塞版本比较，最主要的变化是将 ngx_lock_fd 函数换成了 ngx_trylock_fd\n    err = ngx_trylock_fd(mtx->fd);\n\n    // 获取锁成功，返回1\n    if (err == 0) {\n        return 1;\n    }\n\n    // 获取锁失败，如果错误码是 NGX_EAGAIN，表示文件锁正被其他进程占用，返回0\n    if (err == NGX_EAGAIN) {\n        return 0;\n    }\n\n#if __osf__ /* Tru64 UNIX */\n\n    if (err == NGX_EACCES) {\n        return 0;\n    }\n\n#endif\n\n    // 其他错误都不应该发生，打印错误日志\n    ngx_log_abort(err, ngx_trylock_fd_n \" %s failed\", mtx->name);\n\n    return 0;\n}\n```\n\n可以看到与阻塞版本相比，非阻塞版本最主要的变化 `ngx_lock_fd` 换成了 `ngx_trylock_fd`, 它的实现如下：\n\n```c\nngx_err_t\nngx_trylock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //锁的类型同样是写锁\n    fl.l_type = F_WRLCK;\n    fl.l_whence = SEEK_SET;\n\n    //操作变成了 F_SETLK, 该操作在获取不到锁时会直接返回，而不会阻塞进程。\n    if (fcntl(fd, F_SETLK, &fl) == -1) {\n        return ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n### 锁的释放\n\n上面说了如何加锁，接下来看一下如何释放锁，逻辑比较简单，直接放代码：\n\n```c\nvoid\nngx_shmtx_unlock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    //调用 ngx_unlock_fd函数释放锁\n    err = ngx_unlock_fd(mtx->fd);\n\n    if (err == 0) {\n        return;\n    }\n\n    ngx_log_abort(err, ngx_unlock_fd_n \" %s failed\", mtx->name);\n}\n```\n\n```c\nngx_err_t\nngx_unlock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //锁的类型为 F_UNLCK, 表示释放锁\n    fl.l_type = F_UNLCK;\n    fl.l_whence = SEEK_SET;\n\n    if (fcntl(fd, F_SETLK, &fl) == -1) {\n        return  ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n## 基于原子操作实现锁\n\n上面谈到了在不支持原子操作时，nginx 如何使用文件锁来实现互斥锁。现在操作系统一般都支持原子操作，用它实现互斥锁效率会较文件锁的方式更高，这也是 nginx 默认选用该种方式实现锁的原因，下面看一下它是如何实现的。\n\n### 锁的创建\n\n与上面一样，我们还是先看是如何创建锁的：\n\n```c\n/*\n mtx： 要创建的锁\n addr：创建锁时，内部用到的原子变量\n name：没有意义，只有上\n*/\nngx_int_t\nngx_shmtx_create(ngx_shmtx_t *mtx, ngx_shmtx_sh_t *addr, u_char *name)\n{\n    // 保存原子变量的地址，由于锁时多个进程之间共享的，那么原子变量一般在共享内存进行分配\n    // 上面的addr就表示在共享内存中分配的内存地址，至于共享内存的分配下次再说\n    mtx->lock = &addr->lock;\n\n    // 在不支持信号量时，spin只表示锁的自旋次数，那么该值为0或负数表示不进行自旋，直接让出cpu，\n    // 当支持信号量时，它为-1表示，不要使用信号量将进程置于睡眠状态，这对 nginx 的性能至关重要。\n    if (mtx->spin == (ngx_uint_t) -1) {\n        return NGX_OK;\n    }\n    // 默认自旋次数是2048\n    mtx->spin = 2048;\n\n    // 支持信号量，继续执行下面代码，主要是信号量的初始化。\n#if (NGX_HAVE_POSIX_SEM)\n\n    mtx->wait = &addr->wait;\n\n    //初始化信号量，第二个参数1表示，信号量使用在多进程环境中，第三个参数0表示信号量的初始值\n    //当信号量的值小于等于0时，尝试等待信号量会阻塞\n    //当信号量大于0时，尝试等待信号量会成功，并把信号量的值减一。\n    if (sem_init(&mtx->sem, 1, 0) == -1) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      \"sem_init() failed\");\n    } else {\n        mtx->semaphore = 1;\n    }\n\n#endif\n\n    return NGX_OK;\n}\n```\n\n该函数的 `addr` 指针变量指向进行原子操作用到的原子变量，它的类型如下：\n\n```c\ntypedef struct {\n    // 通过对该变量进行原子操作来进行锁的获取与释放\n    ngx_atomic_t   lock;\n#if (NGX_HAVE_POSIX_SEM)\n    // 支持信号量时才会有该成语变量，表示当前在在变量上等待的进程数目。\n    ngx_atomic_t   wait;\n#endif\n} ngx_shmtx_sh_t;\n```\n\n由于锁是多个进程之间共享的， 所以 `addr` 指向的内存都是在共享内存进行分配的。\n\n### 阻塞锁的获取\n\n与文件锁实现的互斥锁一样，依然有阻塞和非阻塞类型，下面首先来看下阻塞锁的实现，相比于文件锁实现的方式要复杂很多:\n\n```c\nvoid\nngx_shmtx_lock(ngx_shmtx_t *mtx)\n{\n    ngx_uint_t         i, n;\n\n    ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0, \"shmtx lock\");\n\n    for ( ;; ) {\n\n        //尝试获取锁，如果*mtx->lock为0，表示锁未被其他进程占有，\n        //这时调用ngx_atomic_cmp_set这个原子操作尝试将*mtx->lock设置为进程id，如果设置成功，则表示加锁成功，否则失败。\n        //注意：由于在多进程环境下执行，*mtx->lock == 0 为真时，并不能确保ngx_atomic_cmp_set函数执行成功\n        if (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid)) {\n            return;\n        }\n\n        // 获取锁失败了，这时候判断cpu的数目，如果数目大于1，则先自旋一段时间，然后再让出cpu\n        // 如果cpu数目为1，则没必要进行自旋了，应该直接让出cpu给其他进程执行。\n        if (ngx_ncpu > 1) {\n\n            for (n = 1; n < mtx->spin; n <<= 1) {\n\n                for (i = 0; i < n; i++) {\n                    // ngx_cpu_pause函数并不是真的将程序暂停，而是为了提升循环等待时的性能，并且可以降低系统功耗。\n                    // 实现它时往往是一个指令： `__asm__`(\"pause\")\n                    ngx_cpu_pause();\n                }\n                \n                // 再次尝试获取锁\n                if (*mtx->lock == 0\n                    && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid))\n                {\n                    return;\n                }\n            }\n        }\n\n        // 如果支持信号量，会执行到这里\n#if (NGX_HAVE_POSIX_SEM)\n        // 上面自旋次数已经达到，依然没有获取锁，将进程在信号量上挂起，等待其他进程释放锁后再唤醒。\n        if (mtx->semaphore) { // 使用信号量进行阻塞，即上面设置创建锁时，mtx的spin成员变量的值不是-1\n            \n            // 当前在该信号量上等待的进程数目加一\n            (void) ngx_atomic_fetch_add(mtx->wait, 1);\n\n            // 尝试获取一次锁，如果获取成功，将等待的进程数目减一，然后返回\n            if (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid)) {\n                (void) ngx_atomic_fetch_add(mtx->wait, -1);\n                return;\n            }\n\n            ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                           \"shmtx wait %uA\", *mtx->wait);\n\n            //  在信号量上进行等待\n            while (sem_wait(&mtx->sem) == -1) {\n                ngx_err_t  err;\n\n                err = ngx_errno;\n\n                if (err != NGX_EINTR) {\n                    ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, err,\n                                  \"sem_wait() failed while waiting on shmtx\");\n                    break;\n                }\n            }\n\n            ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                           \"shmtx awoke\");\n            // 执行到此，肯定是其他进程释放锁了，所以继续回到循环的开始，尝试再次获取锁，注意它并不会执行下面的386行\n            continue;\n        }\n\n#endif\n        // 在没有获取到锁，且不使用信号量时，会执行到这里，他一般通过 sched_yield 函数实现，让调度器暂时将进程切出，让其他进程执行。\n        // 在其它进程执行后有可能释放锁，那么下次调度到本进程时，则有可能获取成功。\n        ngx_sched_yield();\n    }\n}\n```\n\n上面代码的实现流程通过注释已经描述的很清楚了，再强调一点就是，使用信号量与否的区别就在于获取不到锁时进行的操作不同，如果使用信号量，则会在信号量上阻塞，进程进入睡眠状态。而不使用信号量，则是暂时「让出」cpu，进程并不会进入睡眠状态，这会减少内核态与用户态度切换带来的开销，所以往往性能更好，因此在 nginx 中使用锁时一般不使用信号量，比如负载均衡均衡锁的初始化方式如下：\n\n```c\nngx_accept_mutex.spin = (ngx_uint_t) -1;\n```\n\n将spin值设为-1，表示不使用信号量。\n\n### 非阻塞锁的获取\n\n非阻塞锁的代码就比较简单了，因为是非阻塞的，所以在获取不到锁时不需要考虑进程是否需要睡眠，也就不需要使用信号量，实现如下：\n\n```c\nngx_uint_t\nngx_shmtx_trylock(ngx_shmtx_t *mtx)\n{\n    return (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid));\n}\n```\n\n### 锁的释放\n\n释放锁时，主要操作是将原子变量设为0，如果使用信号量，则可能还需要唤醒在信号量上等候的进程：\n\n```c\nvoid\nngx_shmtx_unlock(ngx_shmtx_t *mtx)\n{\n    if (mtx->spin != (ngx_uint_t) -1) {\n        ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0, \"shmtx unlock\");\n    }\n\n    //释放锁，将原子变量设为0，同时唤醒在信号量上等待的进程\n    if (ngx_atomic_cmp_set(mtx->lock, ngx_pid, 0)) {\n        ngx_shmtx_wakeup(mtx);\n    }\n}\n```\n\n其中 `ngx_shmtx_wakeup` 的实现如下：\n\n```c\nstatic void\nngx_shmtx_wakeup(ngx_shmtx_t *mtx)\n{\n// 如果不支持信号量，那么该函数为空，啥也不做\n#if (NGX_HAVE_POSIX_SEM)\n    ngx_atomic_uint_t  wait;\n\n    // 如果没有使用信号量，直接返回\n    if (!mtx->semaphore) {\n        return;\n    }\n\n    // 将在信号量上等待的进程数减1，因为是多进程环境，ngx_atomic_cmp_set不一定能一次成功，所以需要循环调用\n    for ( ;; ) {\n\n        wait = *mtx->wait;\n\n        // wait 小于等于0，说明当前没有进程在信号量上睡眠\n        if ((ngx_atomic_int_t) wait <= 0) {\n            return;\n        }\n\n        if (ngx_atomic_cmp_set(mtx->wait, wait, wait - 1)) {\n            break;\n        }\n    }\n\n    ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                   \"shmtx wake %uA\", wait);\n\n    // 将信号量的值加1\n    if (sem_post(&mtx->sem) == -1) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      \"sem_post() failed while wake shmtx\");\n    }\n\n#endif\n}\n```\n\n## 锁的销毁\n\n因为锁的销毁代码比较简单，就不分开进行说明了。对于基于文件锁实现的互斥锁在销毁时需要关闭打开的文件。对于基于原子变量实现的锁，如果支持信号量，则需要销毁创建的信号量，代码分别入下：\n\n基于文件锁实现的锁的销毁：\n\n```c\nvoid\nngx_shmtx_destroy(ngx_shmtx_t *mtx)\n{\n    if (ngx_close_file(mtx->fd) == NGX_FILE_ERROR) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      ngx_close_file_n \" \\\"%s\\\" failed\", mtx->name);\n    }\n}\n```\n\n基于原子操作实现的锁的销毁：\n\n```c\nvoid\nngx_shmtx_destroy(ngx_shmtx_t *mtx)\n{\n#if (NGX_HAVE_POSIX_SEM)\n\n    if (mtx->semaphore) {\n        if (sem_destroy(&mtx->sem) == -1) {\n            ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                          \"sem_destroy() failed\");\n        }\n    }\n\n#endif\n}\n```\n","source":"_posts/nginx互斥锁的实现.md","raw":"---\ntitle: nginx互斥锁的实现\ndate: 2018-03-16 11:15:22\ntags: nginx\n---\n\n# nginx互斥锁的实现\n\n发表于 2017-06-22 |\n\nnginx 基于原子操作、信号量以及文件锁实现了一个简单高效的互斥锁，当多个 worker 进程之间需要互斥操作时都会用到。下面来看下 nginx 是如何实现它的。\n\n## 原子操作\n\n在实现互斥锁时用到了原子操作，先来了解一下 nginx 下提供的两个原子操作相关的函数：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old, ngx_atomic_uint_t set)\n\nstatic ngx_inline ngx_atomic_int_t\nngx_atomic_fetch_add(ngx_atomic_t *value, ngx_atomic_int_t add)\n```\n\n第一个函数是一个 `CAS` 操作，首先它比较 `lock` 地址处的变量是否等于 `old`， 如果相等，就把 `lock` 地址处的变量设为 `set` 变返回成功，否则返回失败。注意上述过程是作为一个原子一起进行的，不会被打断。 用代码可以描述如下：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old, ngx_atomic_uint_t set)\n{\n    if (*lock == old) {\n        *lock = set;\n        return 1;\n    }\n\n    return 0;\n}\n```\n\n第二个函数是读取 `value` 地址处的变量，并将其与 `add` 相加的结果再写入 `*lock`，然后返回原来 `*lock` 的值，这些操作也是作为一个整体完成的，不会被打断。用代码可描述如下：\n\n```c\nstatic ngx_inline ngx_atomic_int_t\nngx_atomic_fetch_add(ngx_atomic_t *value, ngx_atomic_int_t add)\n{\n    ngx_atomic_int_t  old;\n\n    old = *value;\n    *value += add;\n\n    return old;\n}\n```\n\n<!--more -->\n\nnginx 在实现这两个函数时会首先判断有没有支持原子操作的库，如果有，则直接使用库提供的原子操作实现，如果没有，则会使用汇编语言自己实现。下面以 `x86` 平台下实现 ngx_atomic_cmp_set 的汇编实现方式，实现主要使用了 `cmpxchgq` 指令，代码如下：\n\n```c\nstatic ngx_inline ngx_atomic_uint_t\nngx_atomic_cmp_set(ngx_atomic_t *lock, ngx_atomic_uint_t old,\n    ngx_atomic_uint_t set)\n{\n    u_char  res;\n\n    __asm__ volatile ( //volatile 关键字告诉编译器不要对下面的指令循序进行调整与优化。\n\n         NGX_SMP_LOCK  //这是一个宏，如果是当前是单cpu，则展开为空，如果是多cpu，则展开为lock;，它会锁住内存地址进行排他访问\n    \"    cmpxchgl  %3, %1;   \" //进行 cas 操作\n    \"    sete      %0;       \" //将操作结果写到 res 变量\n\n    : \"=a\" (res)  // 输出部分\n    : \"m\" (*lock), \"a\" (old), \"r\" (set) // 输入部分\n    : \"cc\", \"memory\" // 破坏描述部分，表示修改了哪些寄存器和内存，提醒编译器优化时要注意。\n    );\n\n    return res;\n}\n```\n\n上面的代码采用 gcc 嵌入汇编方式来进行编写，了解了 `cmpxchgq` 指令后还是比较容易理解的。\n\n## 锁结构体\n\n首先 nginx 使用 `ngx_shmtx_lock` 结构体表示锁，它的各个成员变量如下：\n\n```c\ntypedef struct {\n#if (NGX_HAVE_ATOMIC_OPS) \n    ngx_atomic_t  *lock;\n#if (NGX_HAVE_POSIX_SEM)\n    ngx_atomic_t  *wait;\n    ngx_uint_t     semaphore;\n    sem_t          sem;\n#endif\n#else  // 不支持原子变量，使用文件锁，效率稍低。\n    ngx_fd_t       fd;\n    u_char        *name;\n#endif\n    ngx_uint_t     spin; //获取锁时尝试的自旋次数，使用原子操作实现锁时才有意义\n} ngx_shmtx_t;\n```\n\n上面的结构体定义使用了两个宏：`NGX_HAVE_ATOMIC_OPS` 与 `NGX_HAVE_POSIX_SEM`，分别用来代表操作系统是否支持原子变量操作与信号量。根据这两个宏的取值，可以有3种不同的互斥锁实现：\n\n1. 不支持原子操作。\n2. 支持原子操作，但不支持信号量\n3. 支持原子操作，也支持信号量\n\n第1种情况最简单，会直接使用文件锁来实现互斥锁，这时该结构体只有 `fd` 、 `name`和 `spin` 三个字段，但 `spin` 字段是不起作用的。对于2和3两种情况 nginx 均会使用原子变量操作来实现一个自旋锁，其中 `spin` 表示自旋次数。它们两个的区别是：在支持信号量的情况下，如果自旋次数达到了上限而进程还未获取到锁，则进程会在信号量上阻塞等待，进入睡眠状态。不支持信号量的情况，则不会有这样的操作，而是通过调度器直接 「让出」cpu。 下面对这三种情况下锁的实现分别进行介绍。\n\n## 基于文件锁实现的锁\n\n### 锁的创建\n\n首先通过下面的函数创建一个锁：\n\n```c\n/*\n  mtx：要创建的锁\n  addr：使用文件锁实现互斥锁时不会用到该变量\n  name：文件锁使用的文件\n*/\nngx_int_t\nngx_shmtx_create(ngx_shmtx_t *mtx, ngx_shmtx_sh_t *addr, u_char *name)\n{\n\n    if (mtx->name) { // mtx->name不为NULL，说明它之前已经创建过锁\n\n        if (ngx_strcmp(name, mtx->name) == 0) { // 之前创建过锁，且与这次创建锁的文件相同，则不需要创建，直接返回\n            mtx->name = name;\n            return NGX_OK;\n        }\n        // 销毁之前创建到锁，其实就是关闭之前创建锁时打开的文件。\n        ngx_shmtx_destroy(mtx);\n    }\n\n    //打开文件\n    mtx->fd = ngx_open_file(name, NGX_FILE_RDWR, NGX_FILE_CREATE_OR_OPEN,\n                            NGX_FILE_DEFAULT_ACCESS);\n    \n    //打开文件失败，打印日志，然后返回\n    if (mtx->fd == NGX_INVALID_FILE) {\n        ngx_log_error(NGX_LOG_EMERG, ngx_cycle->log, ngx_errno,\n                      ngx_open_file_n \" \\\"%s\\\" failed\", name);\n        return NGX_ERROR;\n    }\n    \n    //使用锁时只需要该文件在内核中的inode信息，所以将该文件删掉\n    if (ngx_delete_file(name) == NGX_FILE_ERROR) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      ngx_delete_file_n \" \\\"%s\\\" failed\", name);\n    }\n\n    mtx->name = name;\n\n    return NGX_OK;\n}\n```\n\n### 阻塞锁的获取\n\n当进程需要进行阻塞加锁时，通过下面的函数进行：\n\n```c\n/*\n 通过文件锁的方式实现互斥锁，如果该文件锁正被其他进程占有，则会导致进程阻塞。\n*/\nvoid\nngx_shmtx_lock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    //通过获取文件锁来进行加锁。\n    err = ngx_lock_fd(mtx->fd);\n\n    if (err == 0) {\n        return;\n    }\n\n    ngx_log_abort(err, ngx_lock_fd_n \" %s failed\", mtx->name);\n}\n```\n\n上面函数主要的操作就是通过 `ngx_lock_fd` 来获取锁，它的实现如下：\n\n```c\nngx_err_t\nngx_lock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //设置文件锁的类型为写锁，即互斥锁\n    fl.l_type = F_WRLCK;\n    fl.l_whence = SEEK_SET;\n\n    //设置操作为F_SETLKW，表示获取不到文件锁时，会阻塞直到可以获取\n    if (fcntl(fd, F_SETLKW, &fl) == -1) {\n        return ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n它主要是通过 `fcntl` 函数来获取文件锁。\n\n### 非阻塞锁的获取\n\n上面获取锁的方式是阻塞式的，在获取不到锁时进程会阻塞，但有时候我们并不希望这样，而是不能获取锁时直接返回，nginx 通过这么函数来非阻塞的获取锁：\n\n```c\nngx_uint_t\nngx_shmtx_trylock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    // 与上面的阻塞版本比较，最主要的变化是将 ngx_lock_fd 函数换成了 ngx_trylock_fd\n    err = ngx_trylock_fd(mtx->fd);\n\n    // 获取锁成功，返回1\n    if (err == 0) {\n        return 1;\n    }\n\n    // 获取锁失败，如果错误码是 NGX_EAGAIN，表示文件锁正被其他进程占用，返回0\n    if (err == NGX_EAGAIN) {\n        return 0;\n    }\n\n#if __osf__ /* Tru64 UNIX */\n\n    if (err == NGX_EACCES) {\n        return 0;\n    }\n\n#endif\n\n    // 其他错误都不应该发生，打印错误日志\n    ngx_log_abort(err, ngx_trylock_fd_n \" %s failed\", mtx->name);\n\n    return 0;\n}\n```\n\n可以看到与阻塞版本相比，非阻塞版本最主要的变化 `ngx_lock_fd` 换成了 `ngx_trylock_fd`, 它的实现如下：\n\n```c\nngx_err_t\nngx_trylock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //锁的类型同样是写锁\n    fl.l_type = F_WRLCK;\n    fl.l_whence = SEEK_SET;\n\n    //操作变成了 F_SETLK, 该操作在获取不到锁时会直接返回，而不会阻塞进程。\n    if (fcntl(fd, F_SETLK, &fl) == -1) {\n        return ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n### 锁的释放\n\n上面说了如何加锁，接下来看一下如何释放锁，逻辑比较简单，直接放代码：\n\n```c\nvoid\nngx_shmtx_unlock(ngx_shmtx_t *mtx)\n{\n    ngx_err_t  err;\n\n    //调用 ngx_unlock_fd函数释放锁\n    err = ngx_unlock_fd(mtx->fd);\n\n    if (err == 0) {\n        return;\n    }\n\n    ngx_log_abort(err, ngx_unlock_fd_n \" %s failed\", mtx->name);\n}\n```\n\n```c\nngx_err_t\nngx_unlock_fd(ngx_fd_t fd)\n{\n    struct flock  fl;\n\n    ngx_memzero(&fl, sizeof(struct flock));\n\n    //锁的类型为 F_UNLCK, 表示释放锁\n    fl.l_type = F_UNLCK;\n    fl.l_whence = SEEK_SET;\n\n    if (fcntl(fd, F_SETLK, &fl) == -1) {\n        return  ngx_errno;\n    }\n\n    return 0;\n}\n```\n\n## 基于原子操作实现锁\n\n上面谈到了在不支持原子操作时，nginx 如何使用文件锁来实现互斥锁。现在操作系统一般都支持原子操作，用它实现互斥锁效率会较文件锁的方式更高，这也是 nginx 默认选用该种方式实现锁的原因，下面看一下它是如何实现的。\n\n### 锁的创建\n\n与上面一样，我们还是先看是如何创建锁的：\n\n```c\n/*\n mtx： 要创建的锁\n addr：创建锁时，内部用到的原子变量\n name：没有意义，只有上\n*/\nngx_int_t\nngx_shmtx_create(ngx_shmtx_t *mtx, ngx_shmtx_sh_t *addr, u_char *name)\n{\n    // 保存原子变量的地址，由于锁时多个进程之间共享的，那么原子变量一般在共享内存进行分配\n    // 上面的addr就表示在共享内存中分配的内存地址，至于共享内存的分配下次再说\n    mtx->lock = &addr->lock;\n\n    // 在不支持信号量时，spin只表示锁的自旋次数，那么该值为0或负数表示不进行自旋，直接让出cpu，\n    // 当支持信号量时，它为-1表示，不要使用信号量将进程置于睡眠状态，这对 nginx 的性能至关重要。\n    if (mtx->spin == (ngx_uint_t) -1) {\n        return NGX_OK;\n    }\n    // 默认自旋次数是2048\n    mtx->spin = 2048;\n\n    // 支持信号量，继续执行下面代码，主要是信号量的初始化。\n#if (NGX_HAVE_POSIX_SEM)\n\n    mtx->wait = &addr->wait;\n\n    //初始化信号量，第二个参数1表示，信号量使用在多进程环境中，第三个参数0表示信号量的初始值\n    //当信号量的值小于等于0时，尝试等待信号量会阻塞\n    //当信号量大于0时，尝试等待信号量会成功，并把信号量的值减一。\n    if (sem_init(&mtx->sem, 1, 0) == -1) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      \"sem_init() failed\");\n    } else {\n        mtx->semaphore = 1;\n    }\n\n#endif\n\n    return NGX_OK;\n}\n```\n\n该函数的 `addr` 指针变量指向进行原子操作用到的原子变量，它的类型如下：\n\n```c\ntypedef struct {\n    // 通过对该变量进行原子操作来进行锁的获取与释放\n    ngx_atomic_t   lock;\n#if (NGX_HAVE_POSIX_SEM)\n    // 支持信号量时才会有该成语变量，表示当前在在变量上等待的进程数目。\n    ngx_atomic_t   wait;\n#endif\n} ngx_shmtx_sh_t;\n```\n\n由于锁是多个进程之间共享的， 所以 `addr` 指向的内存都是在共享内存进行分配的。\n\n### 阻塞锁的获取\n\n与文件锁实现的互斥锁一样，依然有阻塞和非阻塞类型，下面首先来看下阻塞锁的实现，相比于文件锁实现的方式要复杂很多:\n\n```c\nvoid\nngx_shmtx_lock(ngx_shmtx_t *mtx)\n{\n    ngx_uint_t         i, n;\n\n    ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0, \"shmtx lock\");\n\n    for ( ;; ) {\n\n        //尝试获取锁，如果*mtx->lock为0，表示锁未被其他进程占有，\n        //这时调用ngx_atomic_cmp_set这个原子操作尝试将*mtx->lock设置为进程id，如果设置成功，则表示加锁成功，否则失败。\n        //注意：由于在多进程环境下执行，*mtx->lock == 0 为真时，并不能确保ngx_atomic_cmp_set函数执行成功\n        if (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid)) {\n            return;\n        }\n\n        // 获取锁失败了，这时候判断cpu的数目，如果数目大于1，则先自旋一段时间，然后再让出cpu\n        // 如果cpu数目为1，则没必要进行自旋了，应该直接让出cpu给其他进程执行。\n        if (ngx_ncpu > 1) {\n\n            for (n = 1; n < mtx->spin; n <<= 1) {\n\n                for (i = 0; i < n; i++) {\n                    // ngx_cpu_pause函数并不是真的将程序暂停，而是为了提升循环等待时的性能，并且可以降低系统功耗。\n                    // 实现它时往往是一个指令： `__asm__`(\"pause\")\n                    ngx_cpu_pause();\n                }\n                \n                // 再次尝试获取锁\n                if (*mtx->lock == 0\n                    && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid))\n                {\n                    return;\n                }\n            }\n        }\n\n        // 如果支持信号量，会执行到这里\n#if (NGX_HAVE_POSIX_SEM)\n        // 上面自旋次数已经达到，依然没有获取锁，将进程在信号量上挂起，等待其他进程释放锁后再唤醒。\n        if (mtx->semaphore) { // 使用信号量进行阻塞，即上面设置创建锁时，mtx的spin成员变量的值不是-1\n            \n            // 当前在该信号量上等待的进程数目加一\n            (void) ngx_atomic_fetch_add(mtx->wait, 1);\n\n            // 尝试获取一次锁，如果获取成功，将等待的进程数目减一，然后返回\n            if (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid)) {\n                (void) ngx_atomic_fetch_add(mtx->wait, -1);\n                return;\n            }\n\n            ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                           \"shmtx wait %uA\", *mtx->wait);\n\n            //  在信号量上进行等待\n            while (sem_wait(&mtx->sem) == -1) {\n                ngx_err_t  err;\n\n                err = ngx_errno;\n\n                if (err != NGX_EINTR) {\n                    ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, err,\n                                  \"sem_wait() failed while waiting on shmtx\");\n                    break;\n                }\n            }\n\n            ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                           \"shmtx awoke\");\n            // 执行到此，肯定是其他进程释放锁了，所以继续回到循环的开始，尝试再次获取锁，注意它并不会执行下面的386行\n            continue;\n        }\n\n#endif\n        // 在没有获取到锁，且不使用信号量时，会执行到这里，他一般通过 sched_yield 函数实现，让调度器暂时将进程切出，让其他进程执行。\n        // 在其它进程执行后有可能释放锁，那么下次调度到本进程时，则有可能获取成功。\n        ngx_sched_yield();\n    }\n}\n```\n\n上面代码的实现流程通过注释已经描述的很清楚了，再强调一点就是，使用信号量与否的区别就在于获取不到锁时进行的操作不同，如果使用信号量，则会在信号量上阻塞，进程进入睡眠状态。而不使用信号量，则是暂时「让出」cpu，进程并不会进入睡眠状态，这会减少内核态与用户态度切换带来的开销，所以往往性能更好，因此在 nginx 中使用锁时一般不使用信号量，比如负载均衡均衡锁的初始化方式如下：\n\n```c\nngx_accept_mutex.spin = (ngx_uint_t) -1;\n```\n\n将spin值设为-1，表示不使用信号量。\n\n### 非阻塞锁的获取\n\n非阻塞锁的代码就比较简单了，因为是非阻塞的，所以在获取不到锁时不需要考虑进程是否需要睡眠，也就不需要使用信号量，实现如下：\n\n```c\nngx_uint_t\nngx_shmtx_trylock(ngx_shmtx_t *mtx)\n{\n    return (*mtx->lock == 0 && ngx_atomic_cmp_set(mtx->lock, 0, ngx_pid));\n}\n```\n\n### 锁的释放\n\n释放锁时，主要操作是将原子变量设为0，如果使用信号量，则可能还需要唤醒在信号量上等候的进程：\n\n```c\nvoid\nngx_shmtx_unlock(ngx_shmtx_t *mtx)\n{\n    if (mtx->spin != (ngx_uint_t) -1) {\n        ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0, \"shmtx unlock\");\n    }\n\n    //释放锁，将原子变量设为0，同时唤醒在信号量上等待的进程\n    if (ngx_atomic_cmp_set(mtx->lock, ngx_pid, 0)) {\n        ngx_shmtx_wakeup(mtx);\n    }\n}\n```\n\n其中 `ngx_shmtx_wakeup` 的实现如下：\n\n```c\nstatic void\nngx_shmtx_wakeup(ngx_shmtx_t *mtx)\n{\n// 如果不支持信号量，那么该函数为空，啥也不做\n#if (NGX_HAVE_POSIX_SEM)\n    ngx_atomic_uint_t  wait;\n\n    // 如果没有使用信号量，直接返回\n    if (!mtx->semaphore) {\n        return;\n    }\n\n    // 将在信号量上等待的进程数减1，因为是多进程环境，ngx_atomic_cmp_set不一定能一次成功，所以需要循环调用\n    for ( ;; ) {\n\n        wait = *mtx->wait;\n\n        // wait 小于等于0，说明当前没有进程在信号量上睡眠\n        if ((ngx_atomic_int_t) wait <= 0) {\n            return;\n        }\n\n        if (ngx_atomic_cmp_set(mtx->wait, wait, wait - 1)) {\n            break;\n        }\n    }\n\n    ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle->log, 0,\n                   \"shmtx wake %uA\", wait);\n\n    // 将信号量的值加1\n    if (sem_post(&mtx->sem) == -1) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      \"sem_post() failed while wake shmtx\");\n    }\n\n#endif\n}\n```\n\n## 锁的销毁\n\n因为锁的销毁代码比较简单，就不分开进行说明了。对于基于文件锁实现的互斥锁在销毁时需要关闭打开的文件。对于基于原子变量实现的锁，如果支持信号量，则需要销毁创建的信号量，代码分别入下：\n\n基于文件锁实现的锁的销毁：\n\n```c\nvoid\nngx_shmtx_destroy(ngx_shmtx_t *mtx)\n{\n    if (ngx_close_file(mtx->fd) == NGX_FILE_ERROR) {\n        ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                      ngx_close_file_n \" \\\"%s\\\" failed\", mtx->name);\n    }\n}\n```\n\n基于原子操作实现的锁的销毁：\n\n```c\nvoid\nngx_shmtx_destroy(ngx_shmtx_t *mtx)\n{\n#if (NGX_HAVE_POSIX_SEM)\n\n    if (mtx->semaphore) {\n        if (sem_destroy(&mtx->sem) == -1) {\n            ngx_log_error(NGX_LOG_ALERT, ngx_cycle->log, ngx_errno,\n                          \"sem_destroy() failed\");\n        }\n    }\n\n#endif\n}\n```\n","slug":"nginx互斥锁的实现","published":1,"updated":"2018-08-29T15:39:24.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubq9001aamumia3zpx9n","content":"<h1 id=\"nginx互斥锁的实现\"><a href=\"#nginx互斥锁的实现\" class=\"headerlink\" title=\"nginx互斥锁的实现\"></a>nginx互斥锁的实现</h1><p>发表于 2017-06-22 |</p>\n<p>nginx 基于原子操作、信号量以及文件锁实现了一个简单高效的互斥锁，当多个 worker 进程之间需要互斥操作时都会用到。下面来看下 nginx 是如何实现它的。</p>\n<h2 id=\"原子操作\"><a href=\"#原子操作\" class=\"headerlink\" title=\"原子操作\"></a>原子操作</h2><p>在实现互斥锁时用到了原子操作，先来了解一下 nginx 下提供的两个原子操作相关的函数：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old, <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_int_t</span></span><br><span class=\"line\">ngx_atomic_fetch_add(<span class=\"keyword\">ngx_atomic_t</span> *value, <span class=\"keyword\">ngx_atomic_int_t</span> add)</span><br></pre></td></tr></table></figure>\n<p>第一个函数是一个 <code>CAS</code> 操作，首先它比较 <code>lock</code> 地址处的变量是否等于 <code>old</code>， 如果相等，就把 <code>lock</code> 地址处的变量设为 <code>set</code> 变返回成功，否则返回失败。注意上述过程是作为一个原子一起进行的，不会被打断。 用代码可以描述如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old, <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (*lock == old) &#123;</span><br><span class=\"line\">        *lock = <span class=\"built_in\">set</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>第二个函数是读取 <code>value</code> 地址处的变量，并将其与 <code>add</code> 相加的结果再写入 <code>*lock</code>，然后返回原来 <code>*lock</code> 的值，这些操作也是作为一个整体完成的，不会被打断。用代码可描述如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_int_t</span></span><br><span class=\"line\">ngx_atomic_fetch_add(<span class=\"keyword\">ngx_atomic_t</span> *value, <span class=\"keyword\">ngx_atomic_int_t</span> add)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_int_t</span>  old;</span><br><span class=\"line\"></span><br><span class=\"line\">    old = *value;</span><br><span class=\"line\">    *value += add;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> old;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<p>nginx 在实现这两个函数时会首先判断有没有支持原子操作的库，如果有，则直接使用库提供的原子操作实现，如果没有，则会使用汇编语言自己实现。下面以 <code>x86</code> 平台下实现 ngx_atomic_cmp_set 的汇编实现方式，实现主要使用了 <code>cmpxchgq</code> 指令，代码如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old,</span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    u_char  res;</span><br><span class=\"line\"></span><br><span class=\"line\">    __<span class=\"function\">asm__ <span class=\"title\">volatile</span> <span class=\"params\">( <span class=\"comment\">//volatile 关键字告诉编译器不要对下面的指令循序进行调整与优化。</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">         NGX_SMP_LOCK  <span class=\"comment\">//这是一个宏，如果是当前是单cpu，则展开为空，如果是多cpu，则展开为lock;，它会锁住内存地址进行排他访问</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"string\">\"    cmpxchgl  %3, %1;   \"</span> <span class=\"comment\">//进行 cas 操作</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"string\">\"    sete      %0;       \"</span> <span class=\"comment\">//将操作结果写到 res 变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    : <span class=\"string\">\"=a\"</span> (res)</span>  <span class=\"comment\">// 输出部分</span></span></span><br><span class=\"line\">    : \"m\" (*lock), \"a\" (old), \"r\" (set) // 输入部分</span><br><span class=\"line\">    : <span class=\"string\">\"cc\"</span>, <span class=\"string\">\"memory\"</span> <span class=\"comment\">// 破坏描述部分，表示修改了哪些寄存器和内存，提醒编译器优化时要注意。</span></span><br><span class=\"line\">    );</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码采用 gcc 嵌入汇编方式来进行编写，了解了 <code>cmpxchgq</code> 指令后还是比较容易理解的。</p>\n<h2 id=\"锁结构体\"><a href=\"#锁结构体\" class=\"headerlink\" title=\"锁结构体\"></a>锁结构体</h2><p>首先 nginx 使用 <code>ngx_shmtx_lock</code> 结构体表示锁，它的各个成员变量如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_ATOMIC_OPS) </span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>  *lock;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>  *wait;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>     semaphore;</span><br><span class=\"line\">    <span class=\"keyword\">sem_t</span>          sem;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span>  <span class=\"comment\">// 不支持原子变量，使用文件锁，效率稍低。</span></span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_fd_t</span>       fd;</span><br><span class=\"line\">    u_char        *name;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>     spin; <span class=\"comment\">//获取锁时尝试的自旋次数，使用原子操作实现锁时才有意义</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">ngx_shmtx_t</span>;</span><br></pre></td></tr></table></figure>\n<p>上面的结构体定义使用了两个宏：<code>NGX_HAVE_ATOMIC_OPS</code> 与 <code>NGX_HAVE_POSIX_SEM</code>，分别用来代表操作系统是否支持原子变量操作与信号量。根据这两个宏的取值，可以有3种不同的互斥锁实现：</p>\n<ol>\n<li>不支持原子操作。</li>\n<li>支持原子操作，但不支持信号量</li>\n<li>支持原子操作，也支持信号量</li>\n</ol>\n<p>第1种情况最简单，会直接使用文件锁来实现互斥锁，这时该结构体只有 <code>fd</code> 、 <code>name</code>和 <code>spin</code> 三个字段，但 <code>spin</code> 字段是不起作用的。对于2和3两种情况 nginx 均会使用原子变量操作来实现一个自旋锁，其中 <code>spin</code> 表示自旋次数。它们两个的区别是：在支持信号量的情况下，如果自旋次数达到了上限而进程还未获取到锁，则进程会在信号量上阻塞等待，进入睡眠状态。不支持信号量的情况，则不会有这样的操作，而是通过调度器直接 「让出」cpu。 下面对这三种情况下锁的实现分别进行介绍。</p>\n<h2 id=\"基于文件锁实现的锁\"><a href=\"#基于文件锁实现的锁\" class=\"headerlink\" title=\"基于文件锁实现的锁\"></a>基于文件锁实现的锁</h2><h3 id=\"锁的创建\"><a href=\"#锁的创建\" class=\"headerlink\" title=\"锁的创建\"></a>锁的创建</h3><p>首先通过下面的函数创建一个锁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">  mtx：要创建的锁</span></span><br><span class=\"line\"><span class=\"comment\">  addr：使用文件锁实现互斥锁时不会用到该变量</span></span><br><span class=\"line\"><span class=\"comment\">  name：文件锁使用的文件</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_shmtx_create(<span class=\"keyword\">ngx_shmtx_t</span> *mtx, <span class=\"keyword\">ngx_shmtx_sh_t</span> *addr, u_char *name)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;name) &#123; <span class=\"comment\">// mtx-&gt;name不为NULL，说明它之前已经创建过锁</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_strcmp(name, mtx-&gt;name) == <span class=\"number\">0</span>) &#123; <span class=\"comment\">// 之前创建过锁，且与这次创建锁的文件相同，则不需要创建，直接返回</span></span><br><span class=\"line\">            mtx-&gt;name = name;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 销毁之前创建到锁，其实就是关闭之前创建锁时打开的文件。</span></span><br><span class=\"line\">        ngx_shmtx_destroy(mtx);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//打开文件</span></span><br><span class=\"line\">    mtx-&gt;fd = ngx_open_file(name, NGX_FILE_RDWR, NGX_FILE_CREATE_OR_OPEN,</span><br><span class=\"line\">                            NGX_FILE_DEFAULT_ACCESS);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//打开文件失败，打印日志，然后返回</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;fd == NGX_INVALID_FILE) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_EMERG, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_open_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, name);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> NGX_ERROR;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//使用锁时只需要该文件在内核中的inode信息，所以将该文件删掉</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_delete_file(name) == NGX_FILE_ERROR) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_delete_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    mtx-&gt;name = name;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"阻塞锁的获取\"><a href=\"#阻塞锁的获取\" class=\"headerlink\" title=\"阻塞锁的获取\"></a>阻塞锁的获取</h3><p>当进程需要进行阻塞加锁时，通过下面的函数进行：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> 通过文件锁的方式实现互斥锁，如果该文件锁正被其他进程占有，则会导致进程阻塞。</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_lock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//通过获取文件锁来进行加锁。</span></span><br><span class=\"line\">    err = ngx_lock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_abort(err, ngx_lock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面函数主要的操作就是通过 <code>ngx_lock_fd</code> 来获取锁，它的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_lock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//设置文件锁的类型为写锁，即互斥锁</span></span><br><span class=\"line\">    fl.l_type = F_WRLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//设置操作为F_SETLKW，表示获取不到文件锁时，会阻塞直到可以获取</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLKW, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>它主要是通过 <code>fcntl</code> 函数来获取文件锁。</p>\n<h3 id=\"非阻塞锁的获取\"><a href=\"#非阻塞锁的获取\" class=\"headerlink\" title=\"非阻塞锁的获取\"></a>非阻塞锁的获取</h3><p>上面获取锁的方式是阻塞式的，在获取不到锁时进程会阻塞，但有时候我们并不希望这样，而是不能获取锁时直接返回，nginx 通过这么函数来非阻塞的获取锁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_uint_t</span></span><br><span class=\"line\">ngx_shmtx_trylock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 与上面的阻塞版本比较，最主要的变化是将 ngx_lock_fd 函数换成了 ngx_trylock_fd</span></span><br><span class=\"line\">    err = ngx_trylock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 获取锁成功，返回1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 获取锁失败，如果错误码是 NGX_EAGAIN，表示文件锁正被其他进程占用，返回0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == NGX_EAGAIN) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> __osf__ <span class=\"comment\">/* Tru64 UNIX */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == NGX_EACCES) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 其他错误都不应该发生，打印错误日志</span></span><br><span class=\"line\">    ngx_log_abort(err, ngx_trylock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到与阻塞版本相比，非阻塞版本最主要的变化 <code>ngx_lock_fd</code> 换成了 <code>ngx_trylock_fd</code>, 它的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_trylock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//锁的类型同样是写锁</span></span><br><span class=\"line\">    fl.l_type = F_WRLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//操作变成了 F_SETLK, 该操作在获取不到锁时会直接返回，而不会阻塞进程。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLK, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"锁的释放\"><a href=\"#锁的释放\" class=\"headerlink\" title=\"锁的释放\"></a>锁的释放</h3><p>上面说了如何加锁，接下来看一下如何释放锁，逻辑比较简单，直接放代码：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_unlock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//调用 ngx_unlock_fd函数释放锁</span></span><br><span class=\"line\">    err = ngx_unlock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_abort(err, ngx_unlock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_unlock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//锁的类型为 F_UNLCK, 表示释放锁</span></span><br><span class=\"line\">    fl.l_type = F_UNLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLK, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>  ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"基于原子操作实现锁\"><a href=\"#基于原子操作实现锁\" class=\"headerlink\" title=\"基于原子操作实现锁\"></a>基于原子操作实现锁</h2><p>上面谈到了在不支持原子操作时，nginx 如何使用文件锁来实现互斥锁。现在操作系统一般都支持原子操作，用它实现互斥锁效率会较文件锁的方式更高，这也是 nginx 默认选用该种方式实现锁的原因，下面看一下它是如何实现的。</p>\n<h3 id=\"锁的创建-1\"><a href=\"#锁的创建-1\" class=\"headerlink\" title=\"锁的创建\"></a>锁的创建</h3><p>与上面一样，我们还是先看是如何创建锁的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> mtx： 要创建的锁</span></span><br><span class=\"line\"><span class=\"comment\"> addr：创建锁时，内部用到的原子变量</span></span><br><span class=\"line\"><span class=\"comment\"> name：没有意义，只有上</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_shmtx_create(<span class=\"keyword\">ngx_shmtx_t</span> *mtx, <span class=\"keyword\">ngx_shmtx_sh_t</span> *addr, u_char *name)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 保存原子变量的地址，由于锁时多个进程之间共享的，那么原子变量一般在共享内存进行分配</span></span><br><span class=\"line\">    <span class=\"comment\">// 上面的addr就表示在共享内存中分配的内存地址，至于共享内存的分配下次再说</span></span><br><span class=\"line\">    mtx-&gt;lock = &amp;addr-&gt;lock;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 在不支持信号量时，spin只表示锁的自旋次数，那么该值为0或负数表示不进行自旋，直接让出cpu，</span></span><br><span class=\"line\">    <span class=\"comment\">// 当支持信号量时，它为-1表示，不要使用信号量将进程置于睡眠状态，这对 nginx 的性能至关重要。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;spin == (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 默认自旋次数是2048</span></span><br><span class=\"line\">    mtx-&gt;spin = <span class=\"number\">2048</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 支持信号量，继续执行下面代码，主要是信号量的初始化。</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    mtx-&gt;wait = &amp;addr-&gt;wait;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//初始化信号量，第二个参数1表示，信号量使用在多进程环境中，第三个参数0表示信号量的初始值</span></span><br><span class=\"line\">    <span class=\"comment\">//当信号量的值小于等于0时，尝试等待信号量会阻塞</span></span><br><span class=\"line\">    <span class=\"comment\">//当信号量大于0时，尝试等待信号量会成功，并把信号量的值减一。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (sem_init(&amp;mtx-&gt;sem, <span class=\"number\">1</span>, <span class=\"number\">0</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      <span class=\"string\">\"sem_init() failed\"</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        mtx-&gt;semaphore = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该函数的 <code>addr</code> 指针变量指向进行原子操作用到的原子变量，它的类型如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">    <span class=\"comment\">// 通过对该变量进行原子操作来进行锁的获取与释放</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>   lock;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"comment\">// 支持信号量时才会有该成语变量，表示当前在在变量上等待的进程数目。</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>   wait;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125; <span class=\"keyword\">ngx_shmtx_sh_t</span>;</span><br></pre></td></tr></table></figure>\n<p>由于锁是多个进程之间共享的， 所以 <code>addr</code> 指向的内存都是在共享内存进行分配的。</p>\n<h3 id=\"阻塞锁的获取-1\"><a href=\"#阻塞锁的获取-1\" class=\"headerlink\" title=\"阻塞锁的获取\"></a>阻塞锁的获取</h3><p>与文件锁实现的互斥锁一样，依然有阻塞和非阻塞类型，下面首先来看下阻塞锁的实现，相比于文件锁实现的方式要复杂很多:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_lock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>         i, n;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>, <span class=\"string\">\"shmtx lock\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( ;; ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//尝试获取锁，如果*mtx-&gt;lock为0，表示锁未被其他进程占有，</span></span><br><span class=\"line\">        <span class=\"comment\">//这时调用ngx_atomic_cmp_set这个原子操作尝试将*mtx-&gt;lock设置为进程id，如果设置成功，则表示加锁成功，否则失败。</span></span><br><span class=\"line\">        <span class=\"comment\">//注意：由于在多进程环境下执行，*mtx-&gt;lock == 0 为真时，并不能确保ngx_atomic_cmp_set函数执行成功</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 获取锁失败了，这时候判断cpu的数目，如果数目大于1，则先自旋一段时间，然后再让出cpu</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果cpu数目为1，则没必要进行自旋了，应该直接让出cpu给其他进程执行。</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_ncpu &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (n = <span class=\"number\">1</span>; n &lt; mtx-&gt;spin; n &lt;&lt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; n; i++) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// ngx_cpu_pause函数并不是真的将程序暂停，而是为了提升循环等待时的性能，并且可以降低系统功耗。</span></span><br><span class=\"line\">                    <span class=\"comment\">// 实现它时往往是一个指令： `__asm__`(\"pause\")</span></span><br><span class=\"line\">                    ngx_cpu_pause();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                </span><br><span class=\"line\">                <span class=\"comment\">// 再次尝试获取锁</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span></span><br><span class=\"line\">                    &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid))</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 如果支持信号量，会执行到这里</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">        <span class=\"comment\">// 上面自旋次数已经达到，依然没有获取锁，将进程在信号量上挂起，等待其他进程释放锁后再唤醒。</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (mtx-&gt;semaphore) &#123; <span class=\"comment\">// 使用信号量进行阻塞，即上面设置创建锁时，mtx的spin成员变量的值不是-1</span></span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\">// 当前在该信号量上等待的进程数目加一</span></span><br><span class=\"line\">            (<span class=\"keyword\">void</span>) ngx_atomic_fetch_add(mtx-&gt;wait, <span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 尝试获取一次锁，如果获取成功，将等待的进程数目减一，然后返回</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid)) &#123;</span><br><span class=\"line\">                (<span class=\"keyword\">void</span>) ngx_atomic_fetch_add(mtx-&gt;wait, <span class=\"number\">-1</span>);</span><br><span class=\"line\">                <span class=\"keyword\">return</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                           <span class=\"string\">\"shmtx wait %uA\"</span>, *mtx-&gt;wait);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">//  在信号量上进行等待</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> (sem_wait(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">                err = ngx_errno;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (err != NGX_EINTR) &#123;</span><br><span class=\"line\">                    ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, err,</span><br><span class=\"line\">                                  <span class=\"string\">\"sem_wait() failed while waiting on shmtx\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                           <span class=\"string\">\"shmtx awoke\"</span>);</span><br><span class=\"line\">            <span class=\"comment\">// 执行到此，肯定是其他进程释放锁了，所以继续回到循环的开始，尝试再次获取锁，注意它并不会执行下面的386行</span></span><br><span class=\"line\">            <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">        <span class=\"comment\">// 在没有获取到锁，且不使用信号量时，会执行到这里，他一般通过 sched_yield 函数实现，让调度器暂时将进程切出，让其他进程执行。</span></span><br><span class=\"line\">        <span class=\"comment\">// 在其它进程执行后有可能释放锁，那么下次调度到本进程时，则有可能获取成功。</span></span><br><span class=\"line\">        ngx_sched_yield();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码的实现流程通过注释已经描述的很清楚了，再强调一点就是，使用信号量与否的区别就在于获取不到锁时进行的操作不同，如果使用信号量，则会在信号量上阻塞，进程进入睡眠状态。而不使用信号量，则是暂时「让出」cpu，进程并不会进入睡眠状态，这会减少内核态与用户态度切换带来的开销，所以往往性能更好，因此在 nginx 中使用锁时一般不使用信号量，比如负载均衡均衡锁的初始化方式如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ngx_accept_mutex.spin = (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>;</span><br></pre></td></tr></table></figure>\n<p>将spin值设为-1，表示不使用信号量。</p>\n<h3 id=\"非阻塞锁的获取-1\"><a href=\"#非阻塞锁的获取-1\" class=\"headerlink\" title=\"非阻塞锁的获取\"></a>非阻塞锁的获取</h3><p>非阻塞锁的代码就比较简单了，因为是非阻塞的，所以在获取不到锁时不需要考虑进程是否需要睡眠，也就不需要使用信号量，实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_uint_t</span></span><br><span class=\"line\">ngx_shmtx_trylock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"锁的释放-1\"><a href=\"#锁的释放-1\" class=\"headerlink\" title=\"锁的释放\"></a>锁的释放</h3><p>释放锁时，主要操作是将原子变量设为0，如果使用信号量，则可能还需要唤醒在信号量上等候的进程：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_unlock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;spin != (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>, <span class=\"string\">\"shmtx unlock\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//释放锁，将原子变量设为0，同时唤醒在信号量上等待的进程</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_atomic_cmp_set(mtx-&gt;lock, ngx_pid, <span class=\"number\">0</span>)) &#123;</span><br><span class=\"line\">        ngx_shmtx_wakeup(mtx);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 <code>ngx_shmtx_wakeup</code> 的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_wakeup(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"comment\">// 如果不支持信号量，那么该函数为空，啥也不做</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_uint_t</span>  wait;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果没有使用信号量，直接返回</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!mtx-&gt;semaphore) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将在信号量上等待的进程数减1，因为是多进程环境，ngx_atomic_cmp_set不一定能一次成功，所以需要循环调用</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( ;; ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        wait = *mtx-&gt;wait;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// wait 小于等于0，说明当前没有进程在信号量上睡眠</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((<span class=\"keyword\">ngx_atomic_int_t</span>) wait &lt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_atomic_cmp_set(mtx-&gt;wait, wait, wait - <span class=\"number\">1</span>)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                   <span class=\"string\">\"shmtx wake %uA\"</span>, wait);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将信号量的值加1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (sem_post(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      <span class=\"string\">\"sem_post() failed while wake shmtx\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"锁的销毁\"><a href=\"#锁的销毁\" class=\"headerlink\" title=\"锁的销毁\"></a>锁的销毁</h2><p>因为锁的销毁代码比较简单，就不分开进行说明了。对于基于文件锁实现的互斥锁在销毁时需要关闭打开的文件。对于基于原子变量实现的锁，如果支持信号量，则需要销毁创建的信号量，代码分别入下：</p>\n<p>基于文件锁实现的锁的销毁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_destroy(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_close_file(mtx-&gt;fd) == NGX_FILE_ERROR) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_close_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>基于原子操作实现的锁的销毁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_destroy(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;semaphore) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sem_destroy(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">            ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                          <span class=\"string\">\"sem_destroy() failed\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"nginx互斥锁的实现\"><a href=\"#nginx互斥锁的实现\" class=\"headerlink\" title=\"nginx互斥锁的实现\"></a>nginx互斥锁的实现</h1><p>发表于 2017-06-22 |</p>\n<p>nginx 基于原子操作、信号量以及文件锁实现了一个简单高效的互斥锁，当多个 worker 进程之间需要互斥操作时都会用到。下面来看下 nginx 是如何实现它的。</p>\n<h2 id=\"原子操作\"><a href=\"#原子操作\" class=\"headerlink\" title=\"原子操作\"></a>原子操作</h2><p>在实现互斥锁时用到了原子操作，先来了解一下 nginx 下提供的两个原子操作相关的函数：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old, <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_int_t</span></span><br><span class=\"line\">ngx_atomic_fetch_add(<span class=\"keyword\">ngx_atomic_t</span> *value, <span class=\"keyword\">ngx_atomic_int_t</span> add)</span><br></pre></td></tr></table></figure>\n<p>第一个函数是一个 <code>CAS</code> 操作，首先它比较 <code>lock</code> 地址处的变量是否等于 <code>old</code>， 如果相等，就把 <code>lock</code> 地址处的变量设为 <code>set</code> 变返回成功，否则返回失败。注意上述过程是作为一个原子一起进行的，不会被打断。 用代码可以描述如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old, <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (*lock == old) &#123;</span><br><span class=\"line\">        *lock = <span class=\"built_in\">set</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>第二个函数是读取 <code>value</code> 地址处的变量，并将其与 <code>add</code> 相加的结果再写入 <code>*lock</code>，然后返回原来 <code>*lock</code> 的值，这些操作也是作为一个整体完成的，不会被打断。用代码可描述如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_int_t</span></span><br><span class=\"line\">ngx_atomic_fetch_add(<span class=\"keyword\">ngx_atomic_t</span> *value, <span class=\"keyword\">ngx_atomic_int_t</span> add)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_int_t</span>  old;</span><br><span class=\"line\"></span><br><span class=\"line\">    old = *value;</span><br><span class=\"line\">    *value += add;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> old;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"<p>nginx 在实现这两个函数时会首先判断有没有支持原子操作的库，如果有，则直接使用库提供的原子操作实现，如果没有，则会使用汇编语言自己实现。下面以 <code>x86</code> 平台下实现 ngx_atomic_cmp_set 的汇编实现方式，实现主要使用了 <code>cmpxchgq</code> 指令，代码如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> ngx_inline <span class=\"keyword\">ngx_atomic_uint_t</span></span><br><span class=\"line\">ngx_atomic_cmp_set(<span class=\"keyword\">ngx_atomic_t</span> *lock, <span class=\"keyword\">ngx_atomic_uint_t</span> old,</span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_uint_t</span> <span class=\"built_in\">set</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    u_char  res;</span><br><span class=\"line\"></span><br><span class=\"line\">    __<span class=\"function\">asm__ <span class=\"title\">volatile</span> <span class=\"params\">( <span class=\"comment\">//volatile 关键字告诉编译器不要对下面的指令循序进行调整与优化。</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">         NGX_SMP_LOCK  <span class=\"comment\">//这是一个宏，如果是当前是单cpu，则展开为空，如果是多cpu，则展开为lock;，它会锁住内存地址进行排他访问</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"string\">\"    cmpxchgl  %3, %1;   \"</span> <span class=\"comment\">//进行 cas 操作</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"string\">\"    sete      %0;       \"</span> <span class=\"comment\">//将操作结果写到 res 变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    : <span class=\"string\">\"=a\"</span> (res)</span>  <span class=\"comment\">// 输出部分</span></span></span><br><span class=\"line\">    : \"m\" (*lock), \"a\" (old), \"r\" (set) // 输入部分</span><br><span class=\"line\">    : <span class=\"string\">\"cc\"</span>, <span class=\"string\">\"memory\"</span> <span class=\"comment\">// 破坏描述部分，表示修改了哪些寄存器和内存，提醒编译器优化时要注意。</span></span><br><span class=\"line\">    );</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码采用 gcc 嵌入汇编方式来进行编写，了解了 <code>cmpxchgq</code> 指令后还是比较容易理解的。</p>\n<h2 id=\"锁结构体\"><a href=\"#锁结构体\" class=\"headerlink\" title=\"锁结构体\"></a>锁结构体</h2><p>首先 nginx 使用 <code>ngx_shmtx_lock</code> 结构体表示锁，它的各个成员变量如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_ATOMIC_OPS) </span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>  *lock;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>  *wait;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>     semaphore;</span><br><span class=\"line\">    <span class=\"keyword\">sem_t</span>          sem;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">else</span>  <span class=\"comment\">// 不支持原子变量，使用文件锁，效率稍低。</span></span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_fd_t</span>       fd;</span><br><span class=\"line\">    u_char        *name;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>     spin; <span class=\"comment\">//获取锁时尝试的自旋次数，使用原子操作实现锁时才有意义</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">ngx_shmtx_t</span>;</span><br></pre></td></tr></table></figure>\n<p>上面的结构体定义使用了两个宏：<code>NGX_HAVE_ATOMIC_OPS</code> 与 <code>NGX_HAVE_POSIX_SEM</code>，分别用来代表操作系统是否支持原子变量操作与信号量。根据这两个宏的取值，可以有3种不同的互斥锁实现：</p>\n<ol>\n<li>不支持原子操作。</li>\n<li>支持原子操作，但不支持信号量</li>\n<li>支持原子操作，也支持信号量</li>\n</ol>\n<p>第1种情况最简单，会直接使用文件锁来实现互斥锁，这时该结构体只有 <code>fd</code> 、 <code>name</code>和 <code>spin</code> 三个字段，但 <code>spin</code> 字段是不起作用的。对于2和3两种情况 nginx 均会使用原子变量操作来实现一个自旋锁，其中 <code>spin</code> 表示自旋次数。它们两个的区别是：在支持信号量的情况下，如果自旋次数达到了上限而进程还未获取到锁，则进程会在信号量上阻塞等待，进入睡眠状态。不支持信号量的情况，则不会有这样的操作，而是通过调度器直接 「让出」cpu。 下面对这三种情况下锁的实现分别进行介绍。</p>\n<h2 id=\"基于文件锁实现的锁\"><a href=\"#基于文件锁实现的锁\" class=\"headerlink\" title=\"基于文件锁实现的锁\"></a>基于文件锁实现的锁</h2><h3 id=\"锁的创建\"><a href=\"#锁的创建\" class=\"headerlink\" title=\"锁的创建\"></a>锁的创建</h3><p>首先通过下面的函数创建一个锁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">  mtx：要创建的锁</span></span><br><span class=\"line\"><span class=\"comment\">  addr：使用文件锁实现互斥锁时不会用到该变量</span></span><br><span class=\"line\"><span class=\"comment\">  name：文件锁使用的文件</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_shmtx_create(<span class=\"keyword\">ngx_shmtx_t</span> *mtx, <span class=\"keyword\">ngx_shmtx_sh_t</span> *addr, u_char *name)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;name) &#123; <span class=\"comment\">// mtx-&gt;name不为NULL，说明它之前已经创建过锁</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_strcmp(name, mtx-&gt;name) == <span class=\"number\">0</span>) &#123; <span class=\"comment\">// 之前创建过锁，且与这次创建锁的文件相同，则不需要创建，直接返回</span></span><br><span class=\"line\">            mtx-&gt;name = name;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 销毁之前创建到锁，其实就是关闭之前创建锁时打开的文件。</span></span><br><span class=\"line\">        ngx_shmtx_destroy(mtx);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//打开文件</span></span><br><span class=\"line\">    mtx-&gt;fd = ngx_open_file(name, NGX_FILE_RDWR, NGX_FILE_CREATE_OR_OPEN,</span><br><span class=\"line\">                            NGX_FILE_DEFAULT_ACCESS);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//打开文件失败，打印日志，然后返回</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;fd == NGX_INVALID_FILE) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_EMERG, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_open_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, name);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> NGX_ERROR;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//使用锁时只需要该文件在内核中的inode信息，所以将该文件删掉</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_delete_file(name) == NGX_FILE_ERROR) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_delete_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    mtx-&gt;name = name;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"阻塞锁的获取\"><a href=\"#阻塞锁的获取\" class=\"headerlink\" title=\"阻塞锁的获取\"></a>阻塞锁的获取</h3><p>当进程需要进行阻塞加锁时，通过下面的函数进行：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> 通过文件锁的方式实现互斥锁，如果该文件锁正被其他进程占有，则会导致进程阻塞。</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_lock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//通过获取文件锁来进行加锁。</span></span><br><span class=\"line\">    err = ngx_lock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_abort(err, ngx_lock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面函数主要的操作就是通过 <code>ngx_lock_fd</code> 来获取锁，它的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_lock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//设置文件锁的类型为写锁，即互斥锁</span></span><br><span class=\"line\">    fl.l_type = F_WRLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//设置操作为F_SETLKW，表示获取不到文件锁时，会阻塞直到可以获取</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLKW, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>它主要是通过 <code>fcntl</code> 函数来获取文件锁。</p>\n<h3 id=\"非阻塞锁的获取\"><a href=\"#非阻塞锁的获取\" class=\"headerlink\" title=\"非阻塞锁的获取\"></a>非阻塞锁的获取</h3><p>上面获取锁的方式是阻塞式的，在获取不到锁时进程会阻塞，但有时候我们并不希望这样，而是不能获取锁时直接返回，nginx 通过这么函数来非阻塞的获取锁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_uint_t</span></span><br><span class=\"line\">ngx_shmtx_trylock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 与上面的阻塞版本比较，最主要的变化是将 ngx_lock_fd 函数换成了 ngx_trylock_fd</span></span><br><span class=\"line\">    err = ngx_trylock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 获取锁成功，返回1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 获取锁失败，如果错误码是 NGX_EAGAIN，表示文件锁正被其他进程占用，返回0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == NGX_EAGAIN) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> __osf__ <span class=\"comment\">/* Tru64 UNIX */</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == NGX_EACCES) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 其他错误都不应该发生，打印错误日志</span></span><br><span class=\"line\">    ngx_log_abort(err, ngx_trylock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到与阻塞版本相比，非阻塞版本最主要的变化 <code>ngx_lock_fd</code> 换成了 <code>ngx_trylock_fd</code>, 它的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_trylock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//锁的类型同样是写锁</span></span><br><span class=\"line\">    fl.l_type = F_WRLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//操作变成了 F_SETLK, 该操作在获取不到锁时会直接返回，而不会阻塞进程。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLK, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"锁的释放\"><a href=\"#锁的释放\" class=\"headerlink\" title=\"锁的释放\"></a>锁的释放</h3><p>上面说了如何加锁，接下来看一下如何释放锁，逻辑比较简单，直接放代码：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_unlock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//调用 ngx_unlock_fd函数释放锁</span></span><br><span class=\"line\">    err = ngx_unlock_fd(mtx-&gt;fd);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (err == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_abort(err, ngx_unlock_fd_n <span class=\"string\">\" %s failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_err_t</span></span><br><span class=\"line\">ngx_unlock_fd(<span class=\"keyword\">ngx_fd_t</span> fd)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">flock</span>  <span class=\"title\">fl</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_memzero(&amp;fl, <span class=\"keyword\">sizeof</span>(struct flock));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//锁的类型为 F_UNLCK, 表示释放锁</span></span><br><span class=\"line\">    fl.l_type = F_UNLCK;</span><br><span class=\"line\">    fl.l_whence = SEEK_SET;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (fcntl(fd, F_SETLK, &amp;fl) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>  ngx_errno;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"基于原子操作实现锁\"><a href=\"#基于原子操作实现锁\" class=\"headerlink\" title=\"基于原子操作实现锁\"></a>基于原子操作实现锁</h2><p>上面谈到了在不支持原子操作时，nginx 如何使用文件锁来实现互斥锁。现在操作系统一般都支持原子操作，用它实现互斥锁效率会较文件锁的方式更高，这也是 nginx 默认选用该种方式实现锁的原因，下面看一下它是如何实现的。</p>\n<h3 id=\"锁的创建-1\"><a href=\"#锁的创建-1\" class=\"headerlink\" title=\"锁的创建\"></a>锁的创建</h3><p>与上面一样，我们还是先看是如何创建锁的：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\"> mtx： 要创建的锁</span></span><br><span class=\"line\"><span class=\"comment\"> addr：创建锁时，内部用到的原子变量</span></span><br><span class=\"line\"><span class=\"comment\"> name：没有意义，只有上</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"keyword\">ngx_int_t</span></span><br><span class=\"line\">ngx_shmtx_create(<span class=\"keyword\">ngx_shmtx_t</span> *mtx, <span class=\"keyword\">ngx_shmtx_sh_t</span> *addr, u_char *name)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 保存原子变量的地址，由于锁时多个进程之间共享的，那么原子变量一般在共享内存进行分配</span></span><br><span class=\"line\">    <span class=\"comment\">// 上面的addr就表示在共享内存中分配的内存地址，至于共享内存的分配下次再说</span></span><br><span class=\"line\">    mtx-&gt;lock = &amp;addr-&gt;lock;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 在不支持信号量时，spin只表示锁的自旋次数，那么该值为0或负数表示不进行自旋，直接让出cpu，</span></span><br><span class=\"line\">    <span class=\"comment\">// 当支持信号量时，它为-1表示，不要使用信号量将进程置于睡眠状态，这对 nginx 的性能至关重要。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;spin == (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 默认自旋次数是2048</span></span><br><span class=\"line\">    mtx-&gt;spin = <span class=\"number\">2048</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 支持信号量，继续执行下面代码，主要是信号量的初始化。</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    mtx-&gt;wait = &amp;addr-&gt;wait;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//初始化信号量，第二个参数1表示，信号量使用在多进程环境中，第三个参数0表示信号量的初始值</span></span><br><span class=\"line\">    <span class=\"comment\">//当信号量的值小于等于0时，尝试等待信号量会阻塞</span></span><br><span class=\"line\">    <span class=\"comment\">//当信号量大于0时，尝试等待信号量会成功，并把信号量的值减一。</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (sem_init(&amp;mtx-&gt;sem, <span class=\"number\">1</span>, <span class=\"number\">0</span>) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      <span class=\"string\">\"sem_init() failed\"</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        mtx-&gt;semaphore = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> NGX_OK;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该函数的 <code>addr</code> 指针变量指向进行原子操作用到的原子变量，它的类型如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">    <span class=\"comment\">// 通过对该变量进行原子操作来进行锁的获取与释放</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>   lock;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"comment\">// 支持信号量时才会有该成语变量，表示当前在在变量上等待的进程数目。</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_t</span>   wait;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125; <span class=\"keyword\">ngx_shmtx_sh_t</span>;</span><br></pre></td></tr></table></figure>\n<p>由于锁是多个进程之间共享的， 所以 <code>addr</code> 指向的内存都是在共享内存进行分配的。</p>\n<h3 id=\"阻塞锁的获取-1\"><a href=\"#阻塞锁的获取-1\" class=\"headerlink\" title=\"阻塞锁的获取\"></a>阻塞锁的获取</h3><p>与文件锁实现的互斥锁一样，依然有阻塞和非阻塞类型，下面首先来看下阻塞锁的实现，相比于文件锁实现的方式要复杂很多:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_lock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">ngx_uint_t</span>         i, n;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>, <span class=\"string\">\"shmtx lock\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( ;; ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//尝试获取锁，如果*mtx-&gt;lock为0，表示锁未被其他进程占有，</span></span><br><span class=\"line\">        <span class=\"comment\">//这时调用ngx_atomic_cmp_set这个原子操作尝试将*mtx-&gt;lock设置为进程id，如果设置成功，则表示加锁成功，否则失败。</span></span><br><span class=\"line\">        <span class=\"comment\">//注意：由于在多进程环境下执行，*mtx-&gt;lock == 0 为真时，并不能确保ngx_atomic_cmp_set函数执行成功</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 获取锁失败了，这时候判断cpu的数目，如果数目大于1，则先自旋一段时间，然后再让出cpu</span></span><br><span class=\"line\">        <span class=\"comment\">// 如果cpu数目为1，则没必要进行自旋了，应该直接让出cpu给其他进程执行。</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_ncpu &gt; <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (n = <span class=\"number\">1</span>; n &lt; mtx-&gt;spin; n &lt;&lt;= <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; n; i++) &#123;</span><br><span class=\"line\">                    <span class=\"comment\">// ngx_cpu_pause函数并不是真的将程序暂停，而是为了提升循环等待时的性能，并且可以降低系统功耗。</span></span><br><span class=\"line\">                    <span class=\"comment\">// 实现它时往往是一个指令： `__asm__`(\"pause\")</span></span><br><span class=\"line\">                    ngx_cpu_pause();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                </span><br><span class=\"line\">                <span class=\"comment\">// 再次尝试获取锁</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span></span><br><span class=\"line\">                    &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid))</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 如果支持信号量，会执行到这里</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">        <span class=\"comment\">// 上面自旋次数已经达到，依然没有获取锁，将进程在信号量上挂起，等待其他进程释放锁后再唤醒。</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (mtx-&gt;semaphore) &#123; <span class=\"comment\">// 使用信号量进行阻塞，即上面设置创建锁时，mtx的spin成员变量的值不是-1</span></span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\">// 当前在该信号量上等待的进程数目加一</span></span><br><span class=\"line\">            (<span class=\"keyword\">void</span>) ngx_atomic_fetch_add(mtx-&gt;wait, <span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">// 尝试获取一次锁，如果获取成功，将等待的进程数目减一，然后返回</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid)) &#123;</span><br><span class=\"line\">                (<span class=\"keyword\">void</span>) ngx_atomic_fetch_add(mtx-&gt;wait, <span class=\"number\">-1</span>);</span><br><span class=\"line\">                <span class=\"keyword\">return</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                           <span class=\"string\">\"shmtx wait %uA\"</span>, *mtx-&gt;wait);</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\">//  在信号量上进行等待</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> (sem_wait(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">ngx_err_t</span>  err;</span><br><span class=\"line\"></span><br><span class=\"line\">                err = ngx_errno;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (err != NGX_EINTR) &#123;</span><br><span class=\"line\">                    ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, err,</span><br><span class=\"line\">                                  <span class=\"string\">\"sem_wait() failed while waiting on shmtx\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                           <span class=\"string\">\"shmtx awoke\"</span>);</span><br><span class=\"line\">            <span class=\"comment\">// 执行到此，肯定是其他进程释放锁了，所以继续回到循环的开始，尝试再次获取锁，注意它并不会执行下面的386行</span></span><br><span class=\"line\">            <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">        <span class=\"comment\">// 在没有获取到锁，且不使用信号量时，会执行到这里，他一般通过 sched_yield 函数实现，让调度器暂时将进程切出，让其他进程执行。</span></span><br><span class=\"line\">        <span class=\"comment\">// 在其它进程执行后有可能释放锁，那么下次调度到本进程时，则有可能获取成功。</span></span><br><span class=\"line\">        ngx_sched_yield();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面代码的实现流程通过注释已经描述的很清楚了，再强调一点就是，使用信号量与否的区别就在于获取不到锁时进行的操作不同，如果使用信号量，则会在信号量上阻塞，进程进入睡眠状态。而不使用信号量，则是暂时「让出」cpu，进程并不会进入睡眠状态，这会减少内核态与用户态度切换带来的开销，所以往往性能更好，因此在 nginx 中使用锁时一般不使用信号量，比如负载均衡均衡锁的初始化方式如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ngx_accept_mutex.spin = (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>;</span><br></pre></td></tr></table></figure>\n<p>将spin值设为-1，表示不使用信号量。</p>\n<h3 id=\"非阻塞锁的获取-1\"><a href=\"#非阻塞锁的获取-1\" class=\"headerlink\" title=\"非阻塞锁的获取\"></a>非阻塞锁的获取</h3><p>非阻塞锁的代码就比较简单了，因为是非阻塞的，所以在获取不到锁时不需要考虑进程是否需要睡眠，也就不需要使用信号量，实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ngx_uint_t</span></span><br><span class=\"line\">ngx_shmtx_trylock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (*mtx-&gt;lock == <span class=\"number\">0</span> &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, <span class=\"number\">0</span>, ngx_pid));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"锁的释放-1\"><a href=\"#锁的释放-1\" class=\"headerlink\" title=\"锁的释放\"></a>锁的释放</h3><p>释放锁时，主要操作是将原子变量设为0，如果使用信号量，则可能还需要唤醒在信号量上等候的进程：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_unlock(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;spin != (<span class=\"keyword\">ngx_uint_t</span>) <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>, <span class=\"string\">\"shmtx unlock\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//释放锁，将原子变量设为0，同时唤醒在信号量上等待的进程</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_atomic_cmp_set(mtx-&gt;lock, ngx_pid, <span class=\"number\">0</span>)) &#123;</span><br><span class=\"line\">        ngx_shmtx_wakeup(mtx);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 <code>ngx_shmtx_wakeup</code> 的实现如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_wakeup(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"comment\">// 如果不支持信号量，那么该函数为空，啥也不做</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\">    <span class=\"keyword\">ngx_atomic_uint_t</span>  wait;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果没有使用信号量，直接返回</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!mtx-&gt;semaphore) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将在信号量上等待的进程数减1，因为是多进程环境，ngx_atomic_cmp_set不一定能一次成功，所以需要循环调用</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> ( ;; ) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        wait = *mtx-&gt;wait;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// wait 小于等于0，说明当前没有进程在信号量上睡眠</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((<span class=\"keyword\">ngx_atomic_int_t</span>) wait &lt;= <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ngx_atomic_cmp_set(mtx-&gt;wait, wait, wait - <span class=\"number\">1</span>)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ngx_log_debug1(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;<span class=\"built_in\">log</span>, <span class=\"number\">0</span>,</span><br><span class=\"line\">                   <span class=\"string\">\"shmtx wake %uA\"</span>, wait);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将信号量的值加1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (sem_post(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      <span class=\"string\">\"sem_post() failed while wake shmtx\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"锁的销毁\"><a href=\"#锁的销毁\" class=\"headerlink\" title=\"锁的销毁\"></a>锁的销毁</h2><p>因为锁的销毁代码比较简单，就不分开进行说明了。对于基于文件锁实现的互斥锁在销毁时需要关闭打开的文件。对于基于原子变量实现的锁，如果支持信号量，则需要销毁创建的信号量，代码分别入下：</p>\n<p>基于文件锁实现的锁的销毁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_destroy(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ngx_close_file(mtx-&gt;fd) == NGX_FILE_ERROR) &#123;</span><br><span class=\"line\">        ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                      ngx_close_file_n <span class=\"string\">\" \\\"%s\\\" failed\"</span>, mtx-&gt;name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>基于原子操作实现的锁的销毁：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">void</span></span><br><span class=\"line\">ngx_shmtx_destroy(<span class=\"keyword\">ngx_shmtx_t</span> *mtx)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">if</span> (NGX_HAVE_POSIX_SEM)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (mtx-&gt;semaphore) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sem_destroy(&amp;mtx-&gt;sem) == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">            ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;<span class=\"built_in\">log</span>, ngx_errno,</span><br><span class=\"line\">                          <span class=\"string\">\"sem_destroy() failed\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">endif</span></span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"分布式锁的实现","date":"2018-04-02T10:25:01.000Z","_content":"\n[原文地址](http://www.weizijun.cn/2016/03/17/%E8%81%8A%E4%B8%80%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E8%AE%BE%E8%AE%A1/)\n\n## 起因\n\n前段时间，看到redis作者发布的一篇文章[《Is Redlock safe?》](http://antirez.com/news/101)，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章[《How to do distributed locking》](http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。\n\n在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。\n\n<!--more-->\n\n## 数据库锁表\n\n我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下：\n\n```sql\nCREATE TABLE `lockedOrder` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主码',\n  `type` tinyint(8) unsigned NOT NULL DEFAULT '0' COMMENT '操作类别',\n  `order_id` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的order_id',\n  `memo` varchar(1024) NOT NULL DEFAULT '',\n  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `uidx_order_id` (`order_id`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的订单';\n```\n\norder_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了`UNIQUE KEY`，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单：\n\n```python\ndef lock ：\n    exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)\n    if result == true :\n        return true\n    else :\n        return false\n\ndef unlock ：\n    exec sql: delete from lockedOrder where order_id='order_id'\n```\n\n读者可以发现，这个锁从功能上有几个问题：\n\n- 数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。\n\n- 这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。\n\n  ```python\n  def clear_timeout_lock :\n      exec sql : delete from lockedOrder where update_time <  ADDTIME(NOW(),'-00:02:00')\n  ```\n\n  这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。\n\n- 这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下：\n\n  ```python\n  def lock ：\n      exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)\n      if result == true :\n          return true\n      else :\n          exec sql : select id from lockedOrder where order_id='order_id' and memo = 'TradeCenterId'\n          if count > 0 :\n              return true\n          else \n              return false\n  ```\n\n  在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。\n\n可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。\n\n## 缓存锁\n\n后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。\n\nredis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。\n\n仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。\n\n缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。\n\n## 分布式缓存锁—Redlock\n\nredis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。\n\nRedlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。\n\n算法的步骤如下：\n\n- 1、客户端获取当前时间，以毫秒为单位。\n- 2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。\n- 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。\n- 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。\n- 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。\n\n使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。\n\n### 分布式专家质疑Redlock\n\n介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。\n\n该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。\n\n如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。\n\n如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。\n\n现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。\n\n![Alt text](http://www.weizijun.cn/images/lock_unsafe-lock.png)\n\n专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。\n\n![Alt text](http://www.weizijun.cn/images/lock_fencing-tokens.png)\n\n然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。\n\n- 1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。\n- 2、节点C的时钟不准确，导致锁超时。\n- 3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。\n- 4、这样client 1和client 2都获得了锁。\n\n总结专家关于Redlock不可用的两点：\n\n- 1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。\n- 2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。\n\n所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。\n\n### redis作者解疑Redlock\n\nredis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。\n\n> I asked for an analysis in the original Redlock specification here: <http://redis.io/topics/distlock>. So thank you Martin. However I don’t agree with the analysis.\n\nredis作者关于使用token解决锁超时问题可以概括成下面五点：\n\n- 观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。\n- 观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。\n- 观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。\n- 观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。\n- 观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。\n\n专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。\n\n### 再次分析Redlock\n\n看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。\n\n第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。\n\n第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。\n\n由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。\n\n- 首先必须部署5个节点才能让Redlock的可靠性更强。\n- 然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。\n- 然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。\n- 如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。\n- 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。\n\n分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。\n\n## 更好的分布式锁—zookeeper\n\n提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。\n\nzookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。\n\n- zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。\n- zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。\n\nzookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。\n\n如果要实现tryLock，那么竞争失败就直接返回false即可。\n\nzookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤\n\n![Alt text](http://www.weizijun.cn/images/lock_zookeeper.png)\n\n需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，[《跟着实例学习ZooKeeper的用法： 分布式锁》](http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2)文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。\n\n## 总结\n\n文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。\n\n参考资料：\n\n- [1][《Distributed locks with Redis》](http://redis.io/topics/distlock)\n- [2][《Is Redlock safe?》](http://antirez.com/news/101)\n- [3][《How to do distributed locking》](http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n- [4][《跟着实例学习ZooKeeper的用法： 分布式锁》](http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2)\n- [5]《从Paxos到Zookeeper:分布式一致性原理与实践》\n\n","source":"_posts/分布式锁的设计.md","raw":"---\ntitle: 分布式锁的实现\ndate: 2018-04-02 18:25:01\ntags: 分布式\n---\n\n[原文地址](http://www.weizijun.cn/2016/03/17/%E8%81%8A%E4%B8%80%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E8%AE%BE%E8%AE%A1/)\n\n## 起因\n\n前段时间，看到redis作者发布的一篇文章[《Is Redlock safe?》](http://antirez.com/news/101)，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章[《How to do distributed locking》](http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。\n\n在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。\n\n<!--more-->\n\n## 数据库锁表\n\n我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下：\n\n```sql\nCREATE TABLE `lockedOrder` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主码',\n  `type` tinyint(8) unsigned NOT NULL DEFAULT '0' COMMENT '操作类别',\n  `order_id` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的order_id',\n  `memo` varchar(1024) NOT NULL DEFAULT '',\n  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `uidx_order_id` (`order_id`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的订单';\n```\n\norder_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了`UNIQUE KEY`，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单：\n\n```python\ndef lock ：\n    exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)\n    if result == true :\n        return true\n    else :\n        return false\n\ndef unlock ：\n    exec sql: delete from lockedOrder where order_id='order_id'\n```\n\n读者可以发现，这个锁从功能上有几个问题：\n\n- 数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。\n\n- 这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。\n\n  ```python\n  def clear_timeout_lock :\n      exec sql : delete from lockedOrder where update_time <  ADDTIME(NOW(),'-00:02:00')\n  ```\n\n  这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。\n\n- 这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下：\n\n  ```python\n  def lock ：\n      exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)\n      if result == true :\n          return true\n      else :\n          exec sql : select id from lockedOrder where order_id='order_id' and memo = 'TradeCenterId'\n          if count > 0 :\n              return true\n          else \n              return false\n  ```\n\n  在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。\n\n可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。\n\n## 缓存锁\n\n后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。\n\nredis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。\n\n仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。\n\n缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。\n\n## 分布式缓存锁—Redlock\n\nredis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。\n\nRedlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。\n\n算法的步骤如下：\n\n- 1、客户端获取当前时间，以毫秒为单位。\n- 2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。\n- 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。\n- 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。\n- 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。\n\n使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。\n\n### 分布式专家质疑Redlock\n\n介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。\n\n该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。\n\n如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。\n\n如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。\n\n现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。\n\n![Alt text](http://www.weizijun.cn/images/lock_unsafe-lock.png)\n\n专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。\n\n![Alt text](http://www.weizijun.cn/images/lock_fencing-tokens.png)\n\n然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。\n\n- 1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。\n- 2、节点C的时钟不准确，导致锁超时。\n- 3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。\n- 4、这样client 1和client 2都获得了锁。\n\n总结专家关于Redlock不可用的两点：\n\n- 1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。\n- 2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。\n\n所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。\n\n### redis作者解疑Redlock\n\nredis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。\n\n> I asked for an analysis in the original Redlock specification here: <http://redis.io/topics/distlock>. So thank you Martin. However I don’t agree with the analysis.\n\nredis作者关于使用token解决锁超时问题可以概括成下面五点：\n\n- 观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。\n- 观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。\n- 观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。\n- 观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。\n- 观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。\n\n专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。\n\n### 再次分析Redlock\n\n看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。\n\n第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。\n\n第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。\n\n由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。\n\n- 首先必须部署5个节点才能让Redlock的可靠性更强。\n- 然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。\n- 然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。\n- 如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。\n- 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。\n\n分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。\n\n## 更好的分布式锁—zookeeper\n\n提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。\n\nzookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。\n\n- zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。\n- zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。\n\nzookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。\n\n如果要实现tryLock，那么竞争失败就直接返回false即可。\n\nzookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤\n\n![Alt text](http://www.weizijun.cn/images/lock_zookeeper.png)\n\n需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，[《跟着实例学习ZooKeeper的用法： 分布式锁》](http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2)文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。\n\n## 总结\n\n文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。\n\n参考资料：\n\n- [1][《Distributed locks with Redis》](http://redis.io/topics/distlock)\n- [2][《Is Redlock safe?》](http://antirez.com/news/101)\n- [3][《How to do distributed locking》](http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n- [4][《跟着实例学习ZooKeeper的用法： 分布式锁》](http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2)\n- [5]《从Paxos到Zookeeper:分布式一致性原理与实践》\n\n","slug":"分布式锁的设计","published":1,"updated":"2018-08-29T15:39:24.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubqa001camumlrbmu6k4","content":"<p><a href=\"http://www.weizijun.cn/2016/03/17/%E8%81%8A%E4%B8%80%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E8%AE%BE%E8%AE%A1/\" target=\"_blank\" rel=\"noopener\">原文地址</a></p>\n<h2 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h2><p>前段时间，看到redis作者发布的一篇文章<a href=\"http://antirez.com/news/101\" target=\"_blank\" rel=\"noopener\">《Is Redlock safe?》</a>，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章<a href=\"http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\" target=\"_blank\" rel=\"noopener\">《How to do distributed locking》</a>，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。</p>\n<p>在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。</p>\n<a id=\"more\"></a>\n<h2 id=\"数据库锁表\"><a href=\"#数据库锁表\" class=\"headerlink\" title=\"数据库锁表\"></a>数据库锁表</h2><p>我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> <span class=\"string\">`lockedOrder`</span> (</span><br><span class=\"line\">  <span class=\"string\">`id`</span> <span class=\"built_in\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">COMMENT</span> <span class=\"string\">'主码'</span>,</span><br><span class=\"line\">  <span class=\"string\">`type`</span> tinyint(<span class=\"number\">8</span>) <span class=\"keyword\">unsigned</span> <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">'0'</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'操作类别'</span>,</span><br><span class=\"line\">  <span class=\"string\">`order_id`</span> <span class=\"built_in\">varchar</span>(<span class=\"number\">64</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">''</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'锁定的order_id'</span>,</span><br><span class=\"line\">  <span class=\"string\">`memo`</span> <span class=\"built_in\">varchar</span>(<span class=\"number\">1024</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">''</span>,</span><br><span class=\"line\">  <span class=\"string\">`update_time`</span> <span class=\"keyword\">timestamp</span> <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">CURRENT_TIMESTAMP</span> <span class=\"keyword\">ON</span> <span class=\"keyword\">UPDATE</span> <span class=\"keyword\">CURRENT_TIMESTAMP</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'保存数据时间，自动生成'</span>,</span><br><span class=\"line\">  PRIMARY <span class=\"keyword\">KEY</span> (<span class=\"string\">`id`</span>),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> <span class=\"keyword\">KEY</span> <span class=\"string\">`uidx_order_id`</span> (<span class=\"string\">`order_id`</span>) <span class=\"keyword\">USING</span> BTREE</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span>=<span class=\"keyword\">InnoDB</span> <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">CHARSET</span>=utf8 <span class=\"keyword\">COMMENT</span>=<span class=\"string\">'锁定中的订单'</span>;</span><br></pre></td></tr></table></figure>\n<p>order_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了<code>UNIQUE KEY</code>，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> result == true :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> true</span><br><span class=\"line\">    <span class=\"keyword\">else</span> :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> false</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unlock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> delete <span class=\"keyword\">from</span> lockedOrder where order_id=<span class=\"string\">'order_id'</span></span><br></pre></td></tr></table></figure>\n<p>读者可以发现，这个锁从功能上有几个问题：</p>\n<ul>\n<li><p>数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。</p>\n</li>\n<li><p>这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clear_timeout_lock</span> :</span></span><br><span class=\"line\">    <span class=\"keyword\">exec</span> sql : delete <span class=\"keyword\">from</span> lockedOrder where update_time &lt;  ADDTIME(NOW(),<span class=\"string\">'-00:02:00'</span>)</span><br></pre></td></tr></table></figure>\n<p>这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。</p>\n</li>\n<li><p>这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> result == true :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> true</span><br><span class=\"line\">    <span class=\"keyword\">else</span> :</span><br><span class=\"line\">        <span class=\"keyword\">exec</span> sql : select id <span class=\"keyword\">from</span> lockedOrder where order_id=<span class=\"string\">'order_id'</span> <span class=\"keyword\">and</span> memo = <span class=\"string\">'TradeCenterId'</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> count &gt; <span class=\"number\">0</span> :</span><br><span class=\"line\">            <span class=\"keyword\">return</span> true</span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">            <span class=\"keyword\">return</span> false</span><br></pre></td></tr></table></figure>\n<p>在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。</p>\n</li>\n</ul>\n<p>可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。</p>\n<h2 id=\"缓存锁\"><a href=\"#缓存锁\" class=\"headerlink\" title=\"缓存锁\"></a>缓存锁</h2><p>后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。</p>\n<p>redis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。</p>\n<p>仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。</p>\n<p>缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。</p>\n<h2 id=\"分布式缓存锁—Redlock\"><a href=\"#分布式缓存锁—Redlock\" class=\"headerlink\" title=\"分布式缓存锁—Redlock\"></a>分布式缓存锁—Redlock</h2><p>redis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。</p>\n<p>Redlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。</p>\n<p>算法的步骤如下：</p>\n<ul>\n<li>1、客户端获取当前时间，以毫秒为单位。</li>\n<li>2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。</li>\n<li>3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。</li>\n<li>4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。</li>\n<li>5、如果客户端获取锁失败了，客户端会依次删除所有的锁。</li>\n</ul>\n<p>使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。</p>\n<h3 id=\"分布式专家质疑Redlock\"><a href=\"#分布式专家质疑Redlock\" class=\"headerlink\" title=\"分布式专家质疑Redlock\"></a>分布式专家质疑Redlock</h3><p>介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。</p>\n<p>该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。</p>\n<p>如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。</p>\n<p>如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。</p>\n<p>现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_unsafe-lock.png\" alt=\"Alt text\"></p>\n<p>专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_fencing-tokens.png\" alt=\"Alt text\"></p>\n<p>然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。</p>\n<ul>\n<li>1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。</li>\n<li>2、节点C的时钟不准确，导致锁超时。</li>\n<li>3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。</li>\n<li>4、这样client 1和client 2都获得了锁。</li>\n</ul>\n<p>总结专家关于Redlock不可用的两点：</p>\n<ul>\n<li>1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。</li>\n<li>2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。</li>\n</ul>\n<p>所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。</p>\n<h3 id=\"redis作者解疑Redlock\"><a href=\"#redis作者解疑Redlock\" class=\"headerlink\" title=\"redis作者解疑Redlock\"></a>redis作者解疑Redlock</h3><p>redis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。</p>\n<blockquote>\n<p>I asked for an analysis in the original Redlock specification here: <a href=\"http://redis.io/topics/distlock\" target=\"_blank\" rel=\"noopener\">http://redis.io/topics/distlock</a>. So thank you Martin. However I don’t agree with the analysis.</p>\n</blockquote>\n<p>redis作者关于使用token解决锁超时问题可以概括成下面五点：</p>\n<ul>\n<li>观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。</li>\n<li>观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。</li>\n<li>观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。</li>\n<li>观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。</li>\n<li>观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。</li>\n</ul>\n<p>专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。</p>\n<h3 id=\"再次分析Redlock\"><a href=\"#再次分析Redlock\" class=\"headerlink\" title=\"再次分析Redlock\"></a>再次分析Redlock</h3><p>看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。</p>\n<p>第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。</p>\n<p>第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。</p>\n<p>由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。</p>\n<ul>\n<li>首先必须部署5个节点才能让Redlock的可靠性更强。</li>\n<li>然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。</li>\n<li>然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。</li>\n<li>如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。</li>\n<li>如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。</li>\n</ul>\n<p>分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。</p>\n<h2 id=\"更好的分布式锁—zookeeper\"><a href=\"#更好的分布式锁—zookeeper\" class=\"headerlink\" title=\"更好的分布式锁—zookeeper\"></a>更好的分布式锁—zookeeper</h2><p>提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。</p>\n<p>zookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。</p>\n<ul>\n<li>zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。</li>\n<li>zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。</li>\n</ul>\n<p>zookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。</p>\n<p>如果要实现tryLock，那么竞争失败就直接返回false即可。</p>\n<p>zookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_zookeeper.png\" alt=\"Alt text\"></p>\n<p>需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，<a href=\"http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2\" target=\"_blank\" rel=\"noopener\">《跟着实例学习ZooKeeper的用法： 分布式锁》</a>文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。</p>\n<p>参考资料：</p>\n<ul>\n<li>[1]<a href=\"http://redis.io/topics/distlock\" target=\"_blank\" rel=\"noopener\">《Distributed locks with Redis》</a></li>\n<li>[2]<a href=\"http://antirez.com/news/101\" target=\"_blank\" rel=\"noopener\">《Is Redlock safe?》</a></li>\n<li>[3]<a href=\"http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\" target=\"_blank\" rel=\"noopener\">《How to do distributed locking》</a></li>\n<li>[4]<a href=\"http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2\" target=\"_blank\" rel=\"noopener\">《跟着实例学习ZooKeeper的用法： 分布式锁》</a></li>\n<li>[5]《从Paxos到Zookeeper:分布式一致性原理与实践》</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://www.weizijun.cn/2016/03/17/%E8%81%8A%E4%B8%80%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E8%AE%BE%E8%AE%A1/\" target=\"_blank\" rel=\"noopener\">原文地址</a></p>\n<h2 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h2><p>前段时间，看到redis作者发布的一篇文章<a href=\"http://antirez.com/news/101\" target=\"_blank\" rel=\"noopener\">《Is Redlock safe?》</a>，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章<a href=\"http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\" target=\"_blank\" rel=\"noopener\">《How to do distributed locking》</a>，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。</p>\n<p>在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。</p>","more":"<h2 id=\"数据库锁表\"><a href=\"#数据库锁表\" class=\"headerlink\" title=\"数据库锁表\"></a>数据库锁表</h2><p>我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> <span class=\"string\">`lockedOrder`</span> (</span><br><span class=\"line\">  <span class=\"string\">`id`</span> <span class=\"built_in\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> AUTO_INCREMENT <span class=\"keyword\">COMMENT</span> <span class=\"string\">'主码'</span>,</span><br><span class=\"line\">  <span class=\"string\">`type`</span> tinyint(<span class=\"number\">8</span>) <span class=\"keyword\">unsigned</span> <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">'0'</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'操作类别'</span>,</span><br><span class=\"line\">  <span class=\"string\">`order_id`</span> <span class=\"built_in\">varchar</span>(<span class=\"number\">64</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">''</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'锁定的order_id'</span>,</span><br><span class=\"line\">  <span class=\"string\">`memo`</span> <span class=\"built_in\">varchar</span>(<span class=\"number\">1024</span>) <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">''</span>,</span><br><span class=\"line\">  <span class=\"string\">`update_time`</span> <span class=\"keyword\">timestamp</span> <span class=\"keyword\">NOT</span> <span class=\"literal\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">CURRENT_TIMESTAMP</span> <span class=\"keyword\">ON</span> <span class=\"keyword\">UPDATE</span> <span class=\"keyword\">CURRENT_TIMESTAMP</span> <span class=\"keyword\">COMMENT</span> <span class=\"string\">'保存数据时间，自动生成'</span>,</span><br><span class=\"line\">  PRIMARY <span class=\"keyword\">KEY</span> (<span class=\"string\">`id`</span>),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> <span class=\"keyword\">KEY</span> <span class=\"string\">`uidx_order_id`</span> (<span class=\"string\">`order_id`</span>) <span class=\"keyword\">USING</span> BTREE</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span>=<span class=\"keyword\">InnoDB</span> <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">CHARSET</span>=utf8 <span class=\"keyword\">COMMENT</span>=<span class=\"string\">'锁定中的订单'</span>;</span><br></pre></td></tr></table></figure>\n<p>order_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了<code>UNIQUE KEY</code>，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> result == true :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> true</span><br><span class=\"line\">    <span class=\"keyword\">else</span> :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> false</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unlock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> delete <span class=\"keyword\">from</span> lockedOrder where order_id=<span class=\"string\">'order_id'</span></span><br></pre></td></tr></table></figure>\n<p>读者可以发现，这个锁从功能上有几个问题：</p>\n<ul>\n<li><p>数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。</p>\n</li>\n<li><p>这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clear_timeout_lock</span> :</span></span><br><span class=\"line\">    <span class=\"keyword\">exec</span> sql : delete <span class=\"keyword\">from</span> lockedOrder where update_time &lt;  ADDTIME(NOW(),<span class=\"string\">'-00:02:00'</span>)</span><br></pre></td></tr></table></figure>\n<p>这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。</p>\n</li>\n<li><p>这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lock</span> ：</span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">exec</span> <span class=\"title\">sql</span>:</span> insert into lockedOrder(type,order_id,memo) values (type,order_id,memo)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> result == true :</span><br><span class=\"line\">        <span class=\"keyword\">return</span> true</span><br><span class=\"line\">    <span class=\"keyword\">else</span> :</span><br><span class=\"line\">        <span class=\"keyword\">exec</span> sql : select id <span class=\"keyword\">from</span> lockedOrder where order_id=<span class=\"string\">'order_id'</span> <span class=\"keyword\">and</span> memo = <span class=\"string\">'TradeCenterId'</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> count &gt; <span class=\"number\">0</span> :</span><br><span class=\"line\">            <span class=\"keyword\">return</span> true</span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">            <span class=\"keyword\">return</span> false</span><br></pre></td></tr></table></figure>\n<p>在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。</p>\n</li>\n</ul>\n<p>可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。</p>\n<h2 id=\"缓存锁\"><a href=\"#缓存锁\" class=\"headerlink\" title=\"缓存锁\"></a>缓存锁</h2><p>后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。</p>\n<p>redis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。</p>\n<p>仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。</p>\n<p>缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。</p>\n<h2 id=\"分布式缓存锁—Redlock\"><a href=\"#分布式缓存锁—Redlock\" class=\"headerlink\" title=\"分布式缓存锁—Redlock\"></a>分布式缓存锁—Redlock</h2><p>redis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。</p>\n<p>Redlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。</p>\n<p>算法的步骤如下：</p>\n<ul>\n<li>1、客户端获取当前时间，以毫秒为单位。</li>\n<li>2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。</li>\n<li>3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。</li>\n<li>4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。</li>\n<li>5、如果客户端获取锁失败了，客户端会依次删除所有的锁。</li>\n</ul>\n<p>使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。</p>\n<h3 id=\"分布式专家质疑Redlock\"><a href=\"#分布式专家质疑Redlock\" class=\"headerlink\" title=\"分布式专家质疑Redlock\"></a>分布式专家质疑Redlock</h3><p>介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。</p>\n<p>该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。</p>\n<p>如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。</p>\n<p>如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。</p>\n<p>现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_unsafe-lock.png\" alt=\"Alt text\"></p>\n<p>专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_fencing-tokens.png\" alt=\"Alt text\"></p>\n<p>然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。</p>\n<ul>\n<li>1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。</li>\n<li>2、节点C的时钟不准确，导致锁超时。</li>\n<li>3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。</li>\n<li>4、这样client 1和client 2都获得了锁。</li>\n</ul>\n<p>总结专家关于Redlock不可用的两点：</p>\n<ul>\n<li>1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。</li>\n<li>2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。</li>\n</ul>\n<p>所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。</p>\n<h3 id=\"redis作者解疑Redlock\"><a href=\"#redis作者解疑Redlock\" class=\"headerlink\" title=\"redis作者解疑Redlock\"></a>redis作者解疑Redlock</h3><p>redis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。</p>\n<blockquote>\n<p>I asked for an analysis in the original Redlock specification here: <a href=\"http://redis.io/topics/distlock\" target=\"_blank\" rel=\"noopener\">http://redis.io/topics/distlock</a>. So thank you Martin. However I don’t agree with the analysis.</p>\n</blockquote>\n<p>redis作者关于使用token解决锁超时问题可以概括成下面五点：</p>\n<ul>\n<li>观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。</li>\n<li>观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。</li>\n<li>观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。</li>\n<li>观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。</li>\n<li>观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。</li>\n</ul>\n<p>专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。</p>\n<h3 id=\"再次分析Redlock\"><a href=\"#再次分析Redlock\" class=\"headerlink\" title=\"再次分析Redlock\"></a>再次分析Redlock</h3><p>看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。</p>\n<p>第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。</p>\n<p>第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。</p>\n<p>由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。</p>\n<ul>\n<li>首先必须部署5个节点才能让Redlock的可靠性更强。</li>\n<li>然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。</li>\n<li>然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。</li>\n<li>如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。</li>\n<li>如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。</li>\n</ul>\n<p>分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。</p>\n<h2 id=\"更好的分布式锁—zookeeper\"><a href=\"#更好的分布式锁—zookeeper\" class=\"headerlink\" title=\"更好的分布式锁—zookeeper\"></a>更好的分布式锁—zookeeper</h2><p>提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。</p>\n<p>zookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。</p>\n<ul>\n<li>zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。</li>\n<li>zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。</li>\n</ul>\n<p>zookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。</p>\n<p>如果要实现tryLock，那么竞争失败就直接返回false即可。</p>\n<p>zookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤</p>\n<p><img src=\"http://www.weizijun.cn/images/lock_zookeeper.png\" alt=\"Alt text\"></p>\n<p>需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，<a href=\"http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2\" target=\"_blank\" rel=\"noopener\">《跟着实例学习ZooKeeper的用法： 分布式锁》</a>文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。</p>\n<p>参考资料：</p>\n<ul>\n<li>[1]<a href=\"http://redis.io/topics/distlock\" target=\"_blank\" rel=\"noopener\">《Distributed locks with Redis》</a></li>\n<li>[2]<a href=\"http://antirez.com/news/101\" target=\"_blank\" rel=\"noopener\">《Is Redlock safe?》</a></li>\n<li>[3]<a href=\"http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\" target=\"_blank\" rel=\"noopener\">《How to do distributed locking》</a></li>\n<li>[4]<a href=\"http://colobu.com/2014/12/12/zookeeper-recipes-by-example-2\" target=\"_blank\" rel=\"noopener\">《跟着实例学习ZooKeeper的用法： 分布式锁》</a></li>\n<li>[5]《从Paxos到Zookeeper:分布式一致性原理与实践》</li>\n</ul>"},{"title":"Golang垃圾回收机制","date":"2018-02-15T04:11:22.000Z","_content":"\n转载自：(http://legendtkl.com/2017/04/28/golang-gc/)\n\n## 1.Golang GC 发展\n\nGolang 从第一个版本以来，GC 一直是大家诟病最多的。但是每一个版本的发布基本都伴随着 GC 的改进。下面列出一些比较重要的改动。\n\n- v1.1 STW\n- v1.3 Mark STW, Sweep 并行\n- v1.5 三色标记法\n- v1.8 hybrid write barrier\n\n## 2. GC 算法简介\n\n这一小节介绍三种经典的 GC 算法：引用计数（reference counting）、标记-清扫（mark & sweep）、节点复制（Copying Garbage Collection），分代收集（Generational Garbage Collection）。\n\n### 2.1 引用计数\n\n引用计数的思想非常简单：每个单元维护一个域，保存其它单元指向它的引用数量（类似有向图的入度）。当引用数量为 0 时，将其回收。引用计数是渐进式的，能够将内存管理的开销分布到整个程序之中。C++ 的 share_ptr 使用的就是引用计算方法。\n\n引用计数算法实现一般是把所有的单元放在一个单元池里，比如类似 free list。这样所有的单元就被串起来了，就可以进行引用计数了。新分配的单元计数值被设置为 1（注意不是 0，因为申请一般都说 ptr = new object 这种）。每次有一个指针被设为指向该单元时，该单元的计数值加 1；而每次删除某个指向它的指针时，它的计数值减 1。当其引用计数为 0 的时候，该单元会被进行回收。虽然这里说的比较简单，实现的时候还是有很多细节需要考虑，比如删除某个单元的时候，那么它指向的所有单元都需要对引用计数减 1。那么如果这个时候，发现其中某个指向的单元的引用计数又为 0，那么是递归的进行还是采用其他的策略呢？递归处理的话会导致系统颠簸。关于这些细节这里就不讨论了，可以参考文章后面的给的参考资料。\n\n<!-- more -->\n\n##### 优点\n\n1. 渐进式。内存管理与用户程序的执行交织在一起，将 GC 的代价分散到整个程序。不像标记-清扫算法需要 STW (Stop The World，GC 的时候挂起用户程序)。\n2. 算法易于实现。\n3. 内存单元能够很快被回收。相比于其他垃圾回收算法，堆被耗尽或者达到某个阈值才会进行垃圾回收。\n\n##### 缺点\n\n1. 原始的引用计数不能处理循环引用。大概这是被诟病最多的缺点了。不过针对这个问题，也除了很多解决方案，比如强引用等。\n2. 维护引用计数降低运行效率。内存单元的更新删除等都需要维护相关的内存单元的引用计数，相比于一些追踪式的垃圾回收算法并不需要这些代价。\n3. 单元池 free list 实现的话不是 cache-friendly 的，这样会导致频繁的 cache miss，降低程序运行效率。\n\n### 2.2 标记-清扫\n\n标记-清扫算法是第一种自动内存管理，基于追踪的垃圾收集算法。算法思想在 70 年代就提出了，是一种非常古老的算法。内存单元并不会在变成垃圾立刻回收，而是保持不可达状态，直到到达某个阈值或者固定时间长度。这个时候系统会挂起用户程序，也就是 STW，转而执行垃圾回收程序。垃圾回收程序对所有的存活单元进行一次全局遍历确定哪些单元可以回收。算法分两个部分：标记（mark）和清扫（sweep）。标记阶段表明所有的存活单元，清扫阶段将垃圾单元回收。可视化可以参考下图。\n\n[![img](https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif)](https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif)\n\n标记-清扫算法的优点也就是基于追踪的垃圾回收算法具有的优点：避免了引用计数算法的缺点（不能处理循环引用，需要维护指针）。缺点也很明显，需要 STW。\n\n##### 三色标记算法\n\n三色标记算法是对标记阶段的改进，原理如下：\n\n1. 起初所有对象都是白色。\n2. 从根出发扫描所有可达对象，标记为灰色，放入待处理队列。\n3. 从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。\n4. 重复 3，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。\n\n可视化如下。\n[![img](https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif)](https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif)\n\n三色标记的一个明显好处是能够让用户程序和 mark 并发的进行，具体可以参考论文：《On-the-fly garbage collection: an exercise in cooperation.》。Golang 的 GC 实现也是基于这篇论文，后面再具体说明。\n\n### 2.3 节点复制\n\n节点复制也是基于追踪的算法。其将整个堆等分为两个半区（semi-space），一个包含现有数据，另一个包含已被废弃的数据。节点复制式垃圾收集从切换（flip）两个半区的角色开始，然后收集器在老的半区，也就是 Fromspace 中遍历存活的数据结构，在第一次访问某个单元时把它复制到新半区，也就是 Tospace 中去。在 Fromspace 中所有存活单元都被访问过之后，收集器在 Tospace 中建立一个存活数据结构的副本，用户程序可以重新开始运行了。\n\n##### 优点\n\n1. 所有存活的数据结构都缩并地排列在 Tospace 的底部，这样就不会存在内存碎片的问题。\n2. 获取新内存可以简单地通过递增自由空间指针来实现。\n\n##### 缺点\n\n1. 内存得不到充分利用，总有一半的内存空间处于浪费状态。\n\n### 2.4 分代收集\n\n基于追踪的垃圾回收算法（标记-清扫、节点复制）一个主要问题是在生命周期较长的对象上浪费时间（长生命周期的对象是不需要频繁扫描的）。同时，内存分配存在这么一个事实 “most object die young”。基于这两点，分代垃圾回收算法将对象按生命周期长短存放到堆上的两个（或者更多）区域，这些区域就是分代（generation）。对于新生代的区域的垃圾回收频率要明显高于老年代区域。\n\n分配对象的时候从新生代里面分配，如果后面发现对象的生命周期较长，则将其移到老年代，这个过程叫做 promote。随着不断 promote，最后新生代的大小在整个堆的占用比例不会特别大。收集的时候集中主要精力在新生代就会相对来说效率更高，STW 时间也会更短。\n\n##### 优点\n\n1. 性能更优。\n\n##### 缺点\n\n1. 实现复杂\n\n## 3. Golang GC\n\n### 3.1 Overview\n\n在说 Golang 的具体垃圾回收流程时，我们先来看一下几个基本的问题。\n\n##### 1. 何时触发 GC\n\n在堆上分配大于 32K byte 对象的时候进行检测此时是否满足垃圾回收条件，如果满足则进行垃圾回收。\n\n```go\nfunc mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {\n    ...\n    shouldhelpgc := false\n    // 分配的对象小于 32K byte\n    if size <= maxSmallSize {\n        ...\n    } else {\n        shouldhelpgc = true\n        ...\n    }\n    ...\n    // gcShouldStart() 函数进行触发条件检测\n    if shouldhelpgc && gcShouldStart(false) {\n        // gcStart() 函数进行垃圾回收\n        gcStart(gcBackgroundMode, false)\n    }\n}\n```\n\n上面是自动垃圾回收，还有一种是主动垃圾回收，通过调用 runtime.GC()，这是阻塞式的。\n\n```go\n// GC runs a garbage collection and blocks the caller until the\n// garbage collection is complete. It may also block the entire\n// program.\nfunc GC() {\n    gcStart(gcForceBlockMode, false)\n}\n```\n\n##### 2. GC 触发条件\n\n触发条件主要关注下面代码中的中间部分：`forceTrigger || memstats.heap_live >= memstats.gc_trigger` 。forceTrigger 是 forceGC 的标志；后面半句的意思是当前堆上的活跃对象大于我们初始化时候设置的 GC 触发阈值。在 malloc 以及 free 的时候 heap_live 会一直进行更新，这里就不再展开了。\n\n```go\n// gcShouldStart returns true if the exit condition for the _GCoff\n// phase has been met. The exit condition should be tested when\n// allocating.\n//\n// If forceTrigger is true, it ignores the current heap size, but\n// checks all other conditions. In general this should be false.\nfunc gcShouldStart(forceTrigger bool) bool {\n    return gcphase == _GCoff && (forceTrigger || memstats.heap_live >= memstats.gc_trigger) && memstats.enablegc && panicking == 0 && gcpercent >= 0\n}\n\n//初始化的时候设置 GC 的触发阈值\nfunc gcinit() {\n    _ = setGCPercent(readgogc())\n    memstats.gc_trigger = heapminimum\n    ...\n}\n// 启动的时候通过 GOGC 传递百分比 x\n// 触发阈值等于 x * defaultHeapMinimum (defaultHeapMinimum 默认是 4M)\nfunc readgogc() int32 {\n    p := gogetenv(\"GOGC\")\n    if p == \"off\" {\n        return -1\n    }\n    if n, ok := atoi32(p); ok {\n        return n\n    }\n    return 100\n}\n```\n\n##### 3. 垃圾回收的主要流程\n\n三色标记法，主要流程如下：\n\n- 所有对象最开始都是白色。\n- 从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。\n- 遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。\n- 处理完灰色对象队列，执行清扫工作。\n\n详细的过程如下图所示，具体可参考 [9]。\n[![img](http://legendtkl.com/img/uploads/2017/gc.png)](http://legendtkl.com/img/uploads/2017/gc.png)\n\n关于上图有几点需要说明的是。\n\n1. 首先从 root 开始遍历，root 包括全局指针和 goroutine 栈上的指针。\n2. mark 有两个过程。\n   1. 从 root 开始遍历，标记为灰色。遍历灰色队列。\n   2. re-scan 全局指针和栈。因为 mark 和用户程序是并行的，所以在过程 1 的时候可能会有新的对象分配，这个时候就需要通过写屏障（write barrier）记录下来。re-scan 再完成检查一下。\n3. Stop The World 有两个过程。\n   1. 第一个是 GC 将要开始的时候，这个时候主要是一些准备工作，比如 enable write barrier。\n   2. 第二个过程就是上面提到的 re-scan 过程。如果这个时候没有 stw，那么 mark 将无休止。\n\n另外针对上图各个阶段对应 GCPhase 如下：\n\n- Off: _GCoff\n- Stack scan ~ Mark: _GCmark\n- Mark termination: _GCmarktermination\n\n### 3.2 写屏障 (write barrier)\n\n关于 write barrier，完全可以另外写成一篇文章，所以这里只简单介绍一下，这篇文章的重点还是 Golang 的 GC。垃圾回收中的 write barrier 可以理解为编译器在写操作时特意插入的一段代码，对应的还有 read barrier。\n\n为什么需要 write barrier，很简单，对于和用户程序并发运行的垃圾回收算法，用户程序会一直修改内存，所以需要记录下来。\n\nGolang 1.7 之前的 write barrier 使用的经典的 Dijkstra-style insertion write barrier [Dijkstra ‘78]， STW 的主要耗时就在 stack re-scan 的过程。自 1.8 之后采用一种混合的 write barrier 方式 （Yuasa-style deletion write barrier [Yuasa ‘90] 和 Dijkstra-style insertion write barrier [Dijkstra ‘78]）来避免 re-scan。具体的可以参考 [17503-eliminate-rescan](https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md)。\n\n### 3.3 标记\n\n下面的源码还是基于 go1.8rc3。这个版本的 GC 代码相比之前改动还是挺大的，我们下面尽量只关注主流程。垃圾回收的代码主要集中在函数 `gcStart()` 中。\n\n```go\n// gcStart 是 GC 的入口函数，根据 gcMode 做处理。\n// 1. gcMode == gcBackgroundMode（后台运行，也就是并行）, _GCoff -> _GCmark\n// 2. 否则 GCoff -> _GCmarktermination，这个时候就是主动 GC \nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n}\n```\n\n##### 1. STW phase 1\n\n在 GC 开始之前的准备工作。\n\n```go\nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n    //在后台启动 mark worker \n    if mode == gcBackgroundMode {\n        gcBgMarkStartWorkers()\n    }\n    ...\n    // Stop The World\n    systemstack(stopTheWorldWithSema)\n    ...\n    if mode == gcBackgroundMode {\n        // GC 开始前的准备工作\n\n        //处理设置 GCPhase，setGCPhase 还会 enable write barrier\n        setGCPhase(_GCmark)\n      \t\n        gcBgMarkPrepare() // Must happen before assist enable.\n        gcMarkRootPrepare()\n\n        // Mark all active tinyalloc blocks. Since we're\n        // allocating from these, they need to be black like\n        // other allocations. The alternative is to blacken\n        // the tiny block on every allocation from it, which\n        // would slow down the tiny allocator.\n        gcMarkTinyAllocs()\n      \t\n        // Start The World\n        systemstack(startTheWorldWithSema)\n    } else {\n        ...\n    }\n}\n```\n\n##### 2. Mark\n\nMark 阶段是并行的运行，通过在后台一直运行 mark worker 来实现。\n\n```go\nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n    //在后台启动 mark worker \n    if mode == gcBackgroundMode {\n        gcBgMarkStartWorkers()\n    }\n}\n\nfunc gcBgMarkStartWorkers() {\n    // Background marking is performed by per-P G's. Ensure that\n    // each P has a background GC G.\n    for _, p := range &allp {\n        if p == nil || p.status == _Pdead {\n            break\n        }\n        if p.gcBgMarkWorker == 0 {\n            go gcBgMarkWorker(p)\n            notetsleepg(&work.bgMarkReady, -1)\n            noteclear(&work.bgMarkReady)\n        }\n    }\n}\n// gcBgMarkWorker 是一直在后台运行的，大部分时候是休眠状态，通过 gcController 来调度\nfunc gcBgMarkWorker(_p_ *p) {\n    for {\n        // 将当前 goroutine 休眠，直到满足某些条件\n        gopark(...)\n        ...\n        // mark 过程\n        systemstack(func() {\n        // Mark our goroutine preemptible so its stack\n        // can be scanned. This lets two mark workers\n        // scan each other (otherwise, they would\n        // deadlock). We must not modify anything on\n        // the G stack. However, stack shrinking is\n        // disabled for mark workers, so it is safe to\n        // read from the G stack.\n        casgstatus(gp, _Grunning, _Gwaiting)\n        switch _p_.gcMarkWorkerMode {\n        default:\n            throw(\"gcBgMarkWorker: unexpected gcMarkWorkerMode\")\n        case gcMarkWorkerDedicatedMode:\n            gcDrain(&_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)\n        case gcMarkWorkerFractionalMode:\n            gcDrain(&_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)\n        case gcMarkWorkerIdleMode:\n            gcDrain(&_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)\n        }\n        casgstatus(gp, _Gwaiting, _Grunning)\n        })\n        ...\n    }\n}\n```\n\nMark 阶段的标记代码主要在函数 gcDrain() 中实现。\n\n```\n// gcDrain scans roots and objects in work buffers, blackening grey\n// objects until all roots and work buffers have been drained.\nfunc gcDrain(gcw *gcWork, flags gcDrainFlags) {\n    ...\t\n    // Drain root marking jobs.\n    if work.markrootNext < work.markrootJobs {\n        for !(preemptible && gp.preempt) {\n            job := atomic.Xadd(&work.markrootNext, +1) - 1\n            if job >= work.markrootJobs {\n                break\n            }\n            markroot(gcw, job)\n            if idle && pollWork() {\n                goto done\n            }\n        }\n    }\n  \t\n    // 处理 heap 标记\n    // Drain heap marking jobs.\n    for !(preemptible && gp.preempt) {\n        ...\n        //从灰色列队中取出对象\n        var b uintptr\n        if blocking {\n            b = gcw.get()\n        } else {\n            b = gcw.tryGetFast()\n            if b == 0 {\n                b = gcw.tryGet()\n            }\n        }\n        if b == 0 {\n            // work barrier reached or tryGet failed.\n            break\n        }\n        //扫描灰色对象的引用对象，标记为灰色，入灰色队列\n        scanobject(b, gcw)\n    }\n}\n```\n\n##### 3. Mark termination (STW phase 2)\n\nmark termination 阶段会 stop the world。函数实现在 `gcMarkTermination()`。1.8 版本已经不会再对 goroutine stack 进行 re-scan 了。细节有点多，这里不细说了。\n\n```go\nfunc gcMarkTermination() {\n    // World is stopped.\n    // Run gc on the g0 stack. We do this so that the g stack\n    // we're currently running on will no longer change. Cuts\n    // the root set down a bit (g0 stacks are not scanned, and\n    // we don't need to scan gc's internal state).  We also\n    // need to switch to g0 so we can shrink the stack.\n    systemstack(func() {\n        gcMark(startTime)\n        // Must return immediately.\n        // The outer function's stack may have moved\n        // during gcMark (it shrinks stacks, including the\n        // outer function's stack), so we must not refer\n        // to any of its variables. Return back to the\n        // non-system stack to pick up the new addresses\n        // before continuing.\n    })\n    ...\n}\n```\n\n### 3.4 清扫\n\n清扫相对来说就简单很多了。\n\n```go\nfunc gcSweep(mode gcMode) {\n    ...\n    //阻塞式\n    if !_ConcurrentSweep || mode == gcForceBlockMode {\n        // Special case synchronous sweep.\n        ...\n        // Sweep all spans eagerly.\n        for sweepone() != ^uintptr(0) {\n            sweep.npausesweep++\n        }\n        // Do an additional mProf_GC, because all 'free' events are now real as well.\n        mProf_GC()\n        mProf_GC()\n        return\n    }\n  \t\n    // 并行式\n    // Background sweep.\n    lock(&sweep.lock)\n    if sweep.parked {\n        sweep.parked = false\n        ready(sweep.g, 0, true)\n    }\n    unlock(&sweep.lock)\n}\n```\n\n对于并行式清扫，在 GC 初始化的时候就会启动 `bgsweep()`，然后在后台一直循环。\n\n```go\nfunc bgsweep(c chan int) {\n    sweep.g = getg()\n\n    lock(&sweep.lock)\n    sweep.parked = true\n    c <- 1\n    goparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n\n    for {\n        for gosweepone() != ^uintptr(0) {\n            sweep.nbgsweep++\n            Gosched()\n        }\n        lock(&sweep.lock)\n        if !gosweepdone() {\n            // This can happen if a GC runs between\n            // gosweepone returning ^0 above\n            // and the lock being acquired.\n            unlock(&sweep.lock)\n            continue\n        }\n        sweep.parked = true\n        goparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n    }\n}\n\nfunc gosweepone() uintptr {\n    var ret uintptr\n    systemstack(func() {\n        ret = sweepone()\n    })\n    return ret\n}\n```\n\n不管是阻塞式还是并行式，都是通过 `sweepone()`函数来做清扫工作的。如果对于上篇文章 [Golang 内存管理](http://legendtkl.com/2017/04/02/golang-alloc/) 熟悉的话，这个地方就很好理解。内存管理都是基于 span 的，mheap_ 是一个全局的变量，所有分配的对象都会记录在 mheap_ 中。在标记的时候，我们只要找到对对象对应的 span 进行标记，清扫的时候扫描 span，没有标记的 span 就可以回收了。\n\n```go\n// sweeps one span\n// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep\nfunc sweepone() uintptr {\n    ...\n    for {\n        s := mheap_.sweepSpans[1-sg/2%2].pop()\n        ...\n        if !s.sweep(false) {\n            // Span is still in-use, so this returned no\n            // pages to the heap and the span needs to\n            // move to the swept in-use list.\n            npages = 0\n        }\n    }\n}\n\n// Sweep frees or collects finalizers for blocks not marked in the mark phase.\n// It clears the mark bits in preparation for the next GC round.\n// Returns true if the span was returned to heap.\n// If preserve=true, don't return it to heap nor relink in MCentral lists;\n// caller takes care of it.\nfunc (s *mspan) sweep(preserve bool) bool {\n    ...\n}\n```\n\n### 3.5 其他\n\n##### 1. gcWork\n\n这里介绍一下任务队列，或者说灰色对象管理。每个 P 上都有一个 gcw 用来管理灰色对象（get 和 put），gcw 的结构就是 gcWork。gcWork 中的核心是 wbuf1 和 wbuf2，里面存储就是灰色对象，或者说是 work（下面就全部统一叫做 work）。\n\n```go\ntype p struct {\n    ...\n    gcw gcWork\n}\n\ntype gcWork struct {\n    // wbuf1 and wbuf2 are the primary and secondary work buffers.\n    wbuf1, wbuf2 wbufptr\n  \n    // Bytes marked (blackened) on this gcWork. This is aggregated\n    // into work.bytesMarked by dispose.\n    bytesMarked uint64\n\n    // Scan work performed on this gcWork. This is aggregated into\n    // gcController by dispose and may also be flushed by callers.\n    scanWork int64\n}\n```\n\n既然每个 P 上有一个 work buffer，那么是不是还有一个全局的 work list 呢？是的。通过在每个 P 上绑定一个 work buffer 的好处和 cache 一样，不需要加锁。\n\n```go\nvar work struct {\n    full  uint64                   // lock-free list of full blocks workbuf\n    empty uint64                   // lock-free list of empty blocks workbuf\n    pad0  [sys.CacheLineSize]uint8 // prevents false-sharing between full/empty and nproc/nwait\n    ...\n}\n```\n\n那么为什么使用两个 work buffer （wbuf1 和 wbuf2）呢？我下面举个例子。比如我现在要 get 一个 work 出来，先从 wbuf1 中取，wbuf1 为空的话则与 wbuf2 swap 再 get。在其他时间将 work buffer 中的 full 或者 empty buffer 移到 global 的 work 中。这样的好处在于，在 get 的时候去全局的 work 里面取（多个 goroutine 去取会有竞争）。这里有趣的是 global 的 work list 是 lock-free 的，通过原子操作 cas 等实现。下面列举几个函数看一下 gcWrok。\n\n初始化。\n\n```go\nfunc (w *gcWork) init() {\n    w.wbuf1 = wbufptrOf(getempty())\n    wbuf2 := trygetfull()\n    if wbuf2 == nil {\n        wbuf2 = getempty()\n    }\n    w.wbuf2 = wbufptrOf(wbuf2)\n}\n```\n\nput。\n\n```go\n// put enqueues a pointer for the garbage collector to trace.\n// obj must point to the beginning of a heap object or an oblet.\nfunc (w *gcWork) put(obj uintptr) {\n    wbuf := w.wbuf1.ptr()\n    if wbuf == nil {\n        w.init()\n        wbuf = w.wbuf1.ptr()\n        // wbuf is empty at this point.\n    } else if wbuf.nobj == len(wbuf.obj) {\n        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n        wbuf = w.wbuf1.ptr()\n        if wbuf.nobj == len(wbuf.obj) {\n            putfull(wbuf)\n            wbuf = getempty()\n            w.wbuf1 = wbufptrOf(wbuf)\n            flushed = true\n        }\n    }\n\n    wbuf.obj[wbuf.nobj] = obj\n    wbuf.nobj++\n}\n```\n\nget。\n\n```go\n// get dequeues a pointer for the garbage collector to trace, blocking\n// if necessary to ensure all pointers from all queues and caches have\n// been retrieved.  get returns 0 if there are no pointers remaining.\n//go:nowritebarrier\nfunc (w *gcWork) get() uintptr {\n    wbuf := w.wbuf1.ptr()\n    if wbuf == nil {\n        w.init()\n        wbuf = w.wbuf1.ptr()\n        // wbuf is empty at this point.\n    }\n    if wbuf.nobj == 0 {\n        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n        wbuf = w.wbuf1.ptr()\n        if wbuf.nobj == 0 {\n            owbuf := wbuf\n            wbuf = getfull()\n            if wbuf == nil {\n                return 0\n            }\n            putempty(owbuf)\n            w.wbuf1 = wbufptrOf(wbuf)\n        }\n    }\n\n    // TODO: This might be a good place to add prefetch code\n\n    wbuf.nobj--\n    return wbuf.obj[wbuf.nobj]\n}\n```\n\n##### 2. forcegc\n\n我们上面讲了两种 GC 触发方式：自动检测和用户主动调用。除此之后 Golang 本身还会对运行状态进行监控，如果超过两分钟没有 GC，则触发 GC。监控函数是 `sysmon()`，在主 goroutine 中启动。\n\n```go\n// The main goroutine\nfunc main() {\n    ...\n    systemstack(func() {\n      \tnewm(sysmon, nil)\n    })\n}\n// Always runs without a P, so write barriers are not allowed.\nfunc sysmon() {\n    ...\n    for {\n        now := nanotime()\n        unixnow := unixnanotime()\n      \t\n        lastgc := int64(atomic.Load64(&memstats.last_gc))\n        if gcphase == _GCoff && lastgc != 0 && unixnow-lastgc > forcegcperiod && atomic.Load(&forcegc.idle) != 0 {\n            lock(&forcegc.lock)\n            forcegc.idle = 0\n            forcegc.g.schedlink = 0\n            injectglist(forcegc.g)\t// 将 forcegc goroutine 加入 runnable queue\n            unlock(&forcegc.lock)\n        }\n    }\n}\n\nvar forcegcperiod int64 = 2 * 60 *1e9\t//两分钟\n```\n\n## 4.参考资料\n\n1. 《Go 语言学习笔记》\n2. [《垃圾收集》 - 豆瓣](https://book.douban.com/subject/1157908/)\n3. [Tracing Garbage Collection - wikipedia](https://en.wikipedia.org/wiki/Tracing_garbage_collection#Na.C3.AFve_mark-and-sweep)\n4. 《On-the-fly garbage collection: an exercise in cooperation.》 — Edsger W. Dijkstra, Leslie Lamport, A. J. Martin\n5. [Garbage Collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science))\n6. [Tracing Garbage Collection](https://en.wikipedia.org/wiki/Tracing_garbage_collection)\n7. [Copying Garbage Collection – youtube](https://www.youtube.com/watch?v=P1rU_9IB414)\n8. [Generational Garbage Collection – youtube](https://www.youtube.com/watch?v=pJHISaOW6Vc)\n9. [golang gc talk](https://talks.golang.org/2015/go-gc.pdf)\n10. [17503-eliminate-rescan](https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md)\n","source":"_posts/Golang垃圾回收剖析.md","raw":"---\ntitle: Golang垃圾回收机制\ndate: 2018-02-15 12:11:22\ntags: go\n---\n\n转载自：(http://legendtkl.com/2017/04/28/golang-gc/)\n\n## 1.Golang GC 发展\n\nGolang 从第一个版本以来，GC 一直是大家诟病最多的。但是每一个版本的发布基本都伴随着 GC 的改进。下面列出一些比较重要的改动。\n\n- v1.1 STW\n- v1.3 Mark STW, Sweep 并行\n- v1.5 三色标记法\n- v1.8 hybrid write barrier\n\n## 2. GC 算法简介\n\n这一小节介绍三种经典的 GC 算法：引用计数（reference counting）、标记-清扫（mark & sweep）、节点复制（Copying Garbage Collection），分代收集（Generational Garbage Collection）。\n\n### 2.1 引用计数\n\n引用计数的思想非常简单：每个单元维护一个域，保存其它单元指向它的引用数量（类似有向图的入度）。当引用数量为 0 时，将其回收。引用计数是渐进式的，能够将内存管理的开销分布到整个程序之中。C++ 的 share_ptr 使用的就是引用计算方法。\n\n引用计数算法实现一般是把所有的单元放在一个单元池里，比如类似 free list。这样所有的单元就被串起来了，就可以进行引用计数了。新分配的单元计数值被设置为 1（注意不是 0，因为申请一般都说 ptr = new object 这种）。每次有一个指针被设为指向该单元时，该单元的计数值加 1；而每次删除某个指向它的指针时，它的计数值减 1。当其引用计数为 0 的时候，该单元会被进行回收。虽然这里说的比较简单，实现的时候还是有很多细节需要考虑，比如删除某个单元的时候，那么它指向的所有单元都需要对引用计数减 1。那么如果这个时候，发现其中某个指向的单元的引用计数又为 0，那么是递归的进行还是采用其他的策略呢？递归处理的话会导致系统颠簸。关于这些细节这里就不讨论了，可以参考文章后面的给的参考资料。\n\n<!-- more -->\n\n##### 优点\n\n1. 渐进式。内存管理与用户程序的执行交织在一起，将 GC 的代价分散到整个程序。不像标记-清扫算法需要 STW (Stop The World，GC 的时候挂起用户程序)。\n2. 算法易于实现。\n3. 内存单元能够很快被回收。相比于其他垃圾回收算法，堆被耗尽或者达到某个阈值才会进行垃圾回收。\n\n##### 缺点\n\n1. 原始的引用计数不能处理循环引用。大概这是被诟病最多的缺点了。不过针对这个问题，也除了很多解决方案，比如强引用等。\n2. 维护引用计数降低运行效率。内存单元的更新删除等都需要维护相关的内存单元的引用计数，相比于一些追踪式的垃圾回收算法并不需要这些代价。\n3. 单元池 free list 实现的话不是 cache-friendly 的，这样会导致频繁的 cache miss，降低程序运行效率。\n\n### 2.2 标记-清扫\n\n标记-清扫算法是第一种自动内存管理，基于追踪的垃圾收集算法。算法思想在 70 年代就提出了，是一种非常古老的算法。内存单元并不会在变成垃圾立刻回收，而是保持不可达状态，直到到达某个阈值或者固定时间长度。这个时候系统会挂起用户程序，也就是 STW，转而执行垃圾回收程序。垃圾回收程序对所有的存活单元进行一次全局遍历确定哪些单元可以回收。算法分两个部分：标记（mark）和清扫（sweep）。标记阶段表明所有的存活单元，清扫阶段将垃圾单元回收。可视化可以参考下图。\n\n[![img](https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif)](https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif)\n\n标记-清扫算法的优点也就是基于追踪的垃圾回收算法具有的优点：避免了引用计数算法的缺点（不能处理循环引用，需要维护指针）。缺点也很明显，需要 STW。\n\n##### 三色标记算法\n\n三色标记算法是对标记阶段的改进，原理如下：\n\n1. 起初所有对象都是白色。\n2. 从根出发扫描所有可达对象，标记为灰色，放入待处理队列。\n3. 从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。\n4. 重复 3，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。\n\n可视化如下。\n[![img](https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif)](https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif)\n\n三色标记的一个明显好处是能够让用户程序和 mark 并发的进行，具体可以参考论文：《On-the-fly garbage collection: an exercise in cooperation.》。Golang 的 GC 实现也是基于这篇论文，后面再具体说明。\n\n### 2.3 节点复制\n\n节点复制也是基于追踪的算法。其将整个堆等分为两个半区（semi-space），一个包含现有数据，另一个包含已被废弃的数据。节点复制式垃圾收集从切换（flip）两个半区的角色开始，然后收集器在老的半区，也就是 Fromspace 中遍历存活的数据结构，在第一次访问某个单元时把它复制到新半区，也就是 Tospace 中去。在 Fromspace 中所有存活单元都被访问过之后，收集器在 Tospace 中建立一个存活数据结构的副本，用户程序可以重新开始运行了。\n\n##### 优点\n\n1. 所有存活的数据结构都缩并地排列在 Tospace 的底部，这样就不会存在内存碎片的问题。\n2. 获取新内存可以简单地通过递增自由空间指针来实现。\n\n##### 缺点\n\n1. 内存得不到充分利用，总有一半的内存空间处于浪费状态。\n\n### 2.4 分代收集\n\n基于追踪的垃圾回收算法（标记-清扫、节点复制）一个主要问题是在生命周期较长的对象上浪费时间（长生命周期的对象是不需要频繁扫描的）。同时，内存分配存在这么一个事实 “most object die young”。基于这两点，分代垃圾回收算法将对象按生命周期长短存放到堆上的两个（或者更多）区域，这些区域就是分代（generation）。对于新生代的区域的垃圾回收频率要明显高于老年代区域。\n\n分配对象的时候从新生代里面分配，如果后面发现对象的生命周期较长，则将其移到老年代，这个过程叫做 promote。随着不断 promote，最后新生代的大小在整个堆的占用比例不会特别大。收集的时候集中主要精力在新生代就会相对来说效率更高，STW 时间也会更短。\n\n##### 优点\n\n1. 性能更优。\n\n##### 缺点\n\n1. 实现复杂\n\n## 3. Golang GC\n\n### 3.1 Overview\n\n在说 Golang 的具体垃圾回收流程时，我们先来看一下几个基本的问题。\n\n##### 1. 何时触发 GC\n\n在堆上分配大于 32K byte 对象的时候进行检测此时是否满足垃圾回收条件，如果满足则进行垃圾回收。\n\n```go\nfunc mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {\n    ...\n    shouldhelpgc := false\n    // 分配的对象小于 32K byte\n    if size <= maxSmallSize {\n        ...\n    } else {\n        shouldhelpgc = true\n        ...\n    }\n    ...\n    // gcShouldStart() 函数进行触发条件检测\n    if shouldhelpgc && gcShouldStart(false) {\n        // gcStart() 函数进行垃圾回收\n        gcStart(gcBackgroundMode, false)\n    }\n}\n```\n\n上面是自动垃圾回收，还有一种是主动垃圾回收，通过调用 runtime.GC()，这是阻塞式的。\n\n```go\n// GC runs a garbage collection and blocks the caller until the\n// garbage collection is complete. It may also block the entire\n// program.\nfunc GC() {\n    gcStart(gcForceBlockMode, false)\n}\n```\n\n##### 2. GC 触发条件\n\n触发条件主要关注下面代码中的中间部分：`forceTrigger || memstats.heap_live >= memstats.gc_trigger` 。forceTrigger 是 forceGC 的标志；后面半句的意思是当前堆上的活跃对象大于我们初始化时候设置的 GC 触发阈值。在 malloc 以及 free 的时候 heap_live 会一直进行更新，这里就不再展开了。\n\n```go\n// gcShouldStart returns true if the exit condition for the _GCoff\n// phase has been met. The exit condition should be tested when\n// allocating.\n//\n// If forceTrigger is true, it ignores the current heap size, but\n// checks all other conditions. In general this should be false.\nfunc gcShouldStart(forceTrigger bool) bool {\n    return gcphase == _GCoff && (forceTrigger || memstats.heap_live >= memstats.gc_trigger) && memstats.enablegc && panicking == 0 && gcpercent >= 0\n}\n\n//初始化的时候设置 GC 的触发阈值\nfunc gcinit() {\n    _ = setGCPercent(readgogc())\n    memstats.gc_trigger = heapminimum\n    ...\n}\n// 启动的时候通过 GOGC 传递百分比 x\n// 触发阈值等于 x * defaultHeapMinimum (defaultHeapMinimum 默认是 4M)\nfunc readgogc() int32 {\n    p := gogetenv(\"GOGC\")\n    if p == \"off\" {\n        return -1\n    }\n    if n, ok := atoi32(p); ok {\n        return n\n    }\n    return 100\n}\n```\n\n##### 3. 垃圾回收的主要流程\n\n三色标记法，主要流程如下：\n\n- 所有对象最开始都是白色。\n- 从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。\n- 遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。\n- 处理完灰色对象队列，执行清扫工作。\n\n详细的过程如下图所示，具体可参考 [9]。\n[![img](http://legendtkl.com/img/uploads/2017/gc.png)](http://legendtkl.com/img/uploads/2017/gc.png)\n\n关于上图有几点需要说明的是。\n\n1. 首先从 root 开始遍历，root 包括全局指针和 goroutine 栈上的指针。\n2. mark 有两个过程。\n   1. 从 root 开始遍历，标记为灰色。遍历灰色队列。\n   2. re-scan 全局指针和栈。因为 mark 和用户程序是并行的，所以在过程 1 的时候可能会有新的对象分配，这个时候就需要通过写屏障（write barrier）记录下来。re-scan 再完成检查一下。\n3. Stop The World 有两个过程。\n   1. 第一个是 GC 将要开始的时候，这个时候主要是一些准备工作，比如 enable write barrier。\n   2. 第二个过程就是上面提到的 re-scan 过程。如果这个时候没有 stw，那么 mark 将无休止。\n\n另外针对上图各个阶段对应 GCPhase 如下：\n\n- Off: _GCoff\n- Stack scan ~ Mark: _GCmark\n- Mark termination: _GCmarktermination\n\n### 3.2 写屏障 (write barrier)\n\n关于 write barrier，完全可以另外写成一篇文章，所以这里只简单介绍一下，这篇文章的重点还是 Golang 的 GC。垃圾回收中的 write barrier 可以理解为编译器在写操作时特意插入的一段代码，对应的还有 read barrier。\n\n为什么需要 write barrier，很简单，对于和用户程序并发运行的垃圾回收算法，用户程序会一直修改内存，所以需要记录下来。\n\nGolang 1.7 之前的 write barrier 使用的经典的 Dijkstra-style insertion write barrier [Dijkstra ‘78]， STW 的主要耗时就在 stack re-scan 的过程。自 1.8 之后采用一种混合的 write barrier 方式 （Yuasa-style deletion write barrier [Yuasa ‘90] 和 Dijkstra-style insertion write barrier [Dijkstra ‘78]）来避免 re-scan。具体的可以参考 [17503-eliminate-rescan](https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md)。\n\n### 3.3 标记\n\n下面的源码还是基于 go1.8rc3。这个版本的 GC 代码相比之前改动还是挺大的，我们下面尽量只关注主流程。垃圾回收的代码主要集中在函数 `gcStart()` 中。\n\n```go\n// gcStart 是 GC 的入口函数，根据 gcMode 做处理。\n// 1. gcMode == gcBackgroundMode（后台运行，也就是并行）, _GCoff -> _GCmark\n// 2. 否则 GCoff -> _GCmarktermination，这个时候就是主动 GC \nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n}\n```\n\n##### 1. STW phase 1\n\n在 GC 开始之前的准备工作。\n\n```go\nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n    //在后台启动 mark worker \n    if mode == gcBackgroundMode {\n        gcBgMarkStartWorkers()\n    }\n    ...\n    // Stop The World\n    systemstack(stopTheWorldWithSema)\n    ...\n    if mode == gcBackgroundMode {\n        // GC 开始前的准备工作\n\n        //处理设置 GCPhase，setGCPhase 还会 enable write barrier\n        setGCPhase(_GCmark)\n      \t\n        gcBgMarkPrepare() // Must happen before assist enable.\n        gcMarkRootPrepare()\n\n        // Mark all active tinyalloc blocks. Since we're\n        // allocating from these, they need to be black like\n        // other allocations. The alternative is to blacken\n        // the tiny block on every allocation from it, which\n        // would slow down the tiny allocator.\n        gcMarkTinyAllocs()\n      \t\n        // Start The World\n        systemstack(startTheWorldWithSema)\n    } else {\n        ...\n    }\n}\n```\n\n##### 2. Mark\n\nMark 阶段是并行的运行，通过在后台一直运行 mark worker 来实现。\n\n```go\nfunc gcStart(mode gcMode, forceTrigger bool) {\n    ...\n    //在后台启动 mark worker \n    if mode == gcBackgroundMode {\n        gcBgMarkStartWorkers()\n    }\n}\n\nfunc gcBgMarkStartWorkers() {\n    // Background marking is performed by per-P G's. Ensure that\n    // each P has a background GC G.\n    for _, p := range &allp {\n        if p == nil || p.status == _Pdead {\n            break\n        }\n        if p.gcBgMarkWorker == 0 {\n            go gcBgMarkWorker(p)\n            notetsleepg(&work.bgMarkReady, -1)\n            noteclear(&work.bgMarkReady)\n        }\n    }\n}\n// gcBgMarkWorker 是一直在后台运行的，大部分时候是休眠状态，通过 gcController 来调度\nfunc gcBgMarkWorker(_p_ *p) {\n    for {\n        // 将当前 goroutine 休眠，直到满足某些条件\n        gopark(...)\n        ...\n        // mark 过程\n        systemstack(func() {\n        // Mark our goroutine preemptible so its stack\n        // can be scanned. This lets two mark workers\n        // scan each other (otherwise, they would\n        // deadlock). We must not modify anything on\n        // the G stack. However, stack shrinking is\n        // disabled for mark workers, so it is safe to\n        // read from the G stack.\n        casgstatus(gp, _Grunning, _Gwaiting)\n        switch _p_.gcMarkWorkerMode {\n        default:\n            throw(\"gcBgMarkWorker: unexpected gcMarkWorkerMode\")\n        case gcMarkWorkerDedicatedMode:\n            gcDrain(&_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)\n        case gcMarkWorkerFractionalMode:\n            gcDrain(&_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)\n        case gcMarkWorkerIdleMode:\n            gcDrain(&_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)\n        }\n        casgstatus(gp, _Gwaiting, _Grunning)\n        })\n        ...\n    }\n}\n```\n\nMark 阶段的标记代码主要在函数 gcDrain() 中实现。\n\n```\n// gcDrain scans roots and objects in work buffers, blackening grey\n// objects until all roots and work buffers have been drained.\nfunc gcDrain(gcw *gcWork, flags gcDrainFlags) {\n    ...\t\n    // Drain root marking jobs.\n    if work.markrootNext < work.markrootJobs {\n        for !(preemptible && gp.preempt) {\n            job := atomic.Xadd(&work.markrootNext, +1) - 1\n            if job >= work.markrootJobs {\n                break\n            }\n            markroot(gcw, job)\n            if idle && pollWork() {\n                goto done\n            }\n        }\n    }\n  \t\n    // 处理 heap 标记\n    // Drain heap marking jobs.\n    for !(preemptible && gp.preempt) {\n        ...\n        //从灰色列队中取出对象\n        var b uintptr\n        if blocking {\n            b = gcw.get()\n        } else {\n            b = gcw.tryGetFast()\n            if b == 0 {\n                b = gcw.tryGet()\n            }\n        }\n        if b == 0 {\n            // work barrier reached or tryGet failed.\n            break\n        }\n        //扫描灰色对象的引用对象，标记为灰色，入灰色队列\n        scanobject(b, gcw)\n    }\n}\n```\n\n##### 3. Mark termination (STW phase 2)\n\nmark termination 阶段会 stop the world。函数实现在 `gcMarkTermination()`。1.8 版本已经不会再对 goroutine stack 进行 re-scan 了。细节有点多，这里不细说了。\n\n```go\nfunc gcMarkTermination() {\n    // World is stopped.\n    // Run gc on the g0 stack. We do this so that the g stack\n    // we're currently running on will no longer change. Cuts\n    // the root set down a bit (g0 stacks are not scanned, and\n    // we don't need to scan gc's internal state).  We also\n    // need to switch to g0 so we can shrink the stack.\n    systemstack(func() {\n        gcMark(startTime)\n        // Must return immediately.\n        // The outer function's stack may have moved\n        // during gcMark (it shrinks stacks, including the\n        // outer function's stack), so we must not refer\n        // to any of its variables. Return back to the\n        // non-system stack to pick up the new addresses\n        // before continuing.\n    })\n    ...\n}\n```\n\n### 3.4 清扫\n\n清扫相对来说就简单很多了。\n\n```go\nfunc gcSweep(mode gcMode) {\n    ...\n    //阻塞式\n    if !_ConcurrentSweep || mode == gcForceBlockMode {\n        // Special case synchronous sweep.\n        ...\n        // Sweep all spans eagerly.\n        for sweepone() != ^uintptr(0) {\n            sweep.npausesweep++\n        }\n        // Do an additional mProf_GC, because all 'free' events are now real as well.\n        mProf_GC()\n        mProf_GC()\n        return\n    }\n  \t\n    // 并行式\n    // Background sweep.\n    lock(&sweep.lock)\n    if sweep.parked {\n        sweep.parked = false\n        ready(sweep.g, 0, true)\n    }\n    unlock(&sweep.lock)\n}\n```\n\n对于并行式清扫，在 GC 初始化的时候就会启动 `bgsweep()`，然后在后台一直循环。\n\n```go\nfunc bgsweep(c chan int) {\n    sweep.g = getg()\n\n    lock(&sweep.lock)\n    sweep.parked = true\n    c <- 1\n    goparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n\n    for {\n        for gosweepone() != ^uintptr(0) {\n            sweep.nbgsweep++\n            Gosched()\n        }\n        lock(&sweep.lock)\n        if !gosweepdone() {\n            // This can happen if a GC runs between\n            // gosweepone returning ^0 above\n            // and the lock being acquired.\n            unlock(&sweep.lock)\n            continue\n        }\n        sweep.parked = true\n        goparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n    }\n}\n\nfunc gosweepone() uintptr {\n    var ret uintptr\n    systemstack(func() {\n        ret = sweepone()\n    })\n    return ret\n}\n```\n\n不管是阻塞式还是并行式，都是通过 `sweepone()`函数来做清扫工作的。如果对于上篇文章 [Golang 内存管理](http://legendtkl.com/2017/04/02/golang-alloc/) 熟悉的话，这个地方就很好理解。内存管理都是基于 span 的，mheap_ 是一个全局的变量，所有分配的对象都会记录在 mheap_ 中。在标记的时候，我们只要找到对对象对应的 span 进行标记，清扫的时候扫描 span，没有标记的 span 就可以回收了。\n\n```go\n// sweeps one span\n// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep\nfunc sweepone() uintptr {\n    ...\n    for {\n        s := mheap_.sweepSpans[1-sg/2%2].pop()\n        ...\n        if !s.sweep(false) {\n            // Span is still in-use, so this returned no\n            // pages to the heap and the span needs to\n            // move to the swept in-use list.\n            npages = 0\n        }\n    }\n}\n\n// Sweep frees or collects finalizers for blocks not marked in the mark phase.\n// It clears the mark bits in preparation for the next GC round.\n// Returns true if the span was returned to heap.\n// If preserve=true, don't return it to heap nor relink in MCentral lists;\n// caller takes care of it.\nfunc (s *mspan) sweep(preserve bool) bool {\n    ...\n}\n```\n\n### 3.5 其他\n\n##### 1. gcWork\n\n这里介绍一下任务队列，或者说灰色对象管理。每个 P 上都有一个 gcw 用来管理灰色对象（get 和 put），gcw 的结构就是 gcWork。gcWork 中的核心是 wbuf1 和 wbuf2，里面存储就是灰色对象，或者说是 work（下面就全部统一叫做 work）。\n\n```go\ntype p struct {\n    ...\n    gcw gcWork\n}\n\ntype gcWork struct {\n    // wbuf1 and wbuf2 are the primary and secondary work buffers.\n    wbuf1, wbuf2 wbufptr\n  \n    // Bytes marked (blackened) on this gcWork. This is aggregated\n    // into work.bytesMarked by dispose.\n    bytesMarked uint64\n\n    // Scan work performed on this gcWork. This is aggregated into\n    // gcController by dispose and may also be flushed by callers.\n    scanWork int64\n}\n```\n\n既然每个 P 上有一个 work buffer，那么是不是还有一个全局的 work list 呢？是的。通过在每个 P 上绑定一个 work buffer 的好处和 cache 一样，不需要加锁。\n\n```go\nvar work struct {\n    full  uint64                   // lock-free list of full blocks workbuf\n    empty uint64                   // lock-free list of empty blocks workbuf\n    pad0  [sys.CacheLineSize]uint8 // prevents false-sharing between full/empty and nproc/nwait\n    ...\n}\n```\n\n那么为什么使用两个 work buffer （wbuf1 和 wbuf2）呢？我下面举个例子。比如我现在要 get 一个 work 出来，先从 wbuf1 中取，wbuf1 为空的话则与 wbuf2 swap 再 get。在其他时间将 work buffer 中的 full 或者 empty buffer 移到 global 的 work 中。这样的好处在于，在 get 的时候去全局的 work 里面取（多个 goroutine 去取会有竞争）。这里有趣的是 global 的 work list 是 lock-free 的，通过原子操作 cas 等实现。下面列举几个函数看一下 gcWrok。\n\n初始化。\n\n```go\nfunc (w *gcWork) init() {\n    w.wbuf1 = wbufptrOf(getempty())\n    wbuf2 := trygetfull()\n    if wbuf2 == nil {\n        wbuf2 = getempty()\n    }\n    w.wbuf2 = wbufptrOf(wbuf2)\n}\n```\n\nput。\n\n```go\n// put enqueues a pointer for the garbage collector to trace.\n// obj must point to the beginning of a heap object or an oblet.\nfunc (w *gcWork) put(obj uintptr) {\n    wbuf := w.wbuf1.ptr()\n    if wbuf == nil {\n        w.init()\n        wbuf = w.wbuf1.ptr()\n        // wbuf is empty at this point.\n    } else if wbuf.nobj == len(wbuf.obj) {\n        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n        wbuf = w.wbuf1.ptr()\n        if wbuf.nobj == len(wbuf.obj) {\n            putfull(wbuf)\n            wbuf = getempty()\n            w.wbuf1 = wbufptrOf(wbuf)\n            flushed = true\n        }\n    }\n\n    wbuf.obj[wbuf.nobj] = obj\n    wbuf.nobj++\n}\n```\n\nget。\n\n```go\n// get dequeues a pointer for the garbage collector to trace, blocking\n// if necessary to ensure all pointers from all queues and caches have\n// been retrieved.  get returns 0 if there are no pointers remaining.\n//go:nowritebarrier\nfunc (w *gcWork) get() uintptr {\n    wbuf := w.wbuf1.ptr()\n    if wbuf == nil {\n        w.init()\n        wbuf = w.wbuf1.ptr()\n        // wbuf is empty at this point.\n    }\n    if wbuf.nobj == 0 {\n        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n        wbuf = w.wbuf1.ptr()\n        if wbuf.nobj == 0 {\n            owbuf := wbuf\n            wbuf = getfull()\n            if wbuf == nil {\n                return 0\n            }\n            putempty(owbuf)\n            w.wbuf1 = wbufptrOf(wbuf)\n        }\n    }\n\n    // TODO: This might be a good place to add prefetch code\n\n    wbuf.nobj--\n    return wbuf.obj[wbuf.nobj]\n}\n```\n\n##### 2. forcegc\n\n我们上面讲了两种 GC 触发方式：自动检测和用户主动调用。除此之后 Golang 本身还会对运行状态进行监控，如果超过两分钟没有 GC，则触发 GC。监控函数是 `sysmon()`，在主 goroutine 中启动。\n\n```go\n// The main goroutine\nfunc main() {\n    ...\n    systemstack(func() {\n      \tnewm(sysmon, nil)\n    })\n}\n// Always runs without a P, so write barriers are not allowed.\nfunc sysmon() {\n    ...\n    for {\n        now := nanotime()\n        unixnow := unixnanotime()\n      \t\n        lastgc := int64(atomic.Load64(&memstats.last_gc))\n        if gcphase == _GCoff && lastgc != 0 && unixnow-lastgc > forcegcperiod && atomic.Load(&forcegc.idle) != 0 {\n            lock(&forcegc.lock)\n            forcegc.idle = 0\n            forcegc.g.schedlink = 0\n            injectglist(forcegc.g)\t// 将 forcegc goroutine 加入 runnable queue\n            unlock(&forcegc.lock)\n        }\n    }\n}\n\nvar forcegcperiod int64 = 2 * 60 *1e9\t//两分钟\n```\n\n## 4.参考资料\n\n1. 《Go 语言学习笔记》\n2. [《垃圾收集》 - 豆瓣](https://book.douban.com/subject/1157908/)\n3. [Tracing Garbage Collection - wikipedia](https://en.wikipedia.org/wiki/Tracing_garbage_collection#Na.C3.AFve_mark-and-sweep)\n4. 《On-the-fly garbage collection: an exercise in cooperation.》 — Edsger W. Dijkstra, Leslie Lamport, A. J. Martin\n5. [Garbage Collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science))\n6. [Tracing Garbage Collection](https://en.wikipedia.org/wiki/Tracing_garbage_collection)\n7. [Copying Garbage Collection – youtube](https://www.youtube.com/watch?v=P1rU_9IB414)\n8. [Generational Garbage Collection – youtube](https://www.youtube.com/watch?v=pJHISaOW6Vc)\n9. [golang gc talk](https://talks.golang.org/2015/go-gc.pdf)\n10. [17503-eliminate-rescan](https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md)\n","slug":"Golang垃圾回收剖析","published":1,"updated":"2018-08-29T15:39:24.650Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubr8001gamumv7qurlyd","content":"<p>转载自：(<a href=\"http://legendtkl.com/2017/04/28/golang-gc/\" target=\"_blank\" rel=\"noopener\">http://legendtkl.com/2017/04/28/golang-gc/</a>)</p>\n<h2 id=\"1-Golang-GC-发展\"><a href=\"#1-Golang-GC-发展\" class=\"headerlink\" title=\"1.Golang GC 发展\"></a>1.Golang GC 发展</h2><p>Golang 从第一个版本以来，GC 一直是大家诟病最多的。但是每一个版本的发布基本都伴随着 GC 的改进。下面列出一些比较重要的改动。</p>\n<ul>\n<li>v1.1 STW</li>\n<li>v1.3 Mark STW, Sweep 并行</li>\n<li>v1.5 三色标记法</li>\n<li>v1.8 hybrid write barrier</li>\n</ul>\n<h2 id=\"2-GC-算法简介\"><a href=\"#2-GC-算法简介\" class=\"headerlink\" title=\"2. GC 算法简介\"></a>2. GC 算法简介</h2><p>这一小节介绍三种经典的 GC 算法：引用计数（reference counting）、标记-清扫（mark &amp; sweep）、节点复制（Copying Garbage Collection），分代收集（Generational Garbage Collection）。</p>\n<h3 id=\"2-1-引用计数\"><a href=\"#2-1-引用计数\" class=\"headerlink\" title=\"2.1 引用计数\"></a>2.1 引用计数</h3><p>引用计数的思想非常简单：每个单元维护一个域，保存其它单元指向它的引用数量（类似有向图的入度）。当引用数量为 0 时，将其回收。引用计数是渐进式的，能够将内存管理的开销分布到整个程序之中。C++ 的 share_ptr 使用的就是引用计算方法。</p>\n<p>引用计数算法实现一般是把所有的单元放在一个单元池里，比如类似 free list。这样所有的单元就被串起来了，就可以进行引用计数了。新分配的单元计数值被设置为 1（注意不是 0，因为申请一般都说 ptr = new object 这种）。每次有一个指针被设为指向该单元时，该单元的计数值加 1；而每次删除某个指向它的指针时，它的计数值减 1。当其引用计数为 0 的时候，该单元会被进行回收。虽然这里说的比较简单，实现的时候还是有很多细节需要考虑，比如删除某个单元的时候，那么它指向的所有单元都需要对引用计数减 1。那么如果这个时候，发现其中某个指向的单元的引用计数又为 0，那么是递归的进行还是采用其他的策略呢？递归处理的话会导致系统颠簸。关于这些细节这里就不讨论了，可以参考文章后面的给的参考资料。</p>\n<a id=\"more\"></a>\n<h5 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>渐进式。内存管理与用户程序的执行交织在一起，将 GC 的代价分散到整个程序。不像标记-清扫算法需要 STW (Stop The World，GC 的时候挂起用户程序)。</li>\n<li>算法易于实现。</li>\n<li>内存单元能够很快被回收。相比于其他垃圾回收算法，堆被耗尽或者达到某个阈值才会进行垃圾回收。</li>\n</ol>\n<h5 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>原始的引用计数不能处理循环引用。大概这是被诟病最多的缺点了。不过针对这个问题，也除了很多解决方案，比如强引用等。</li>\n<li>维护引用计数降低运行效率。内存单元的更新删除等都需要维护相关的内存单元的引用计数，相比于一些追踪式的垃圾回收算法并不需要这些代价。</li>\n<li>单元池 free list 实现的话不是 cache-friendly 的，这样会导致频繁的 cache miss，降低程序运行效率。</li>\n</ol>\n<h3 id=\"2-2-标记-清扫\"><a href=\"#2-2-标记-清扫\" class=\"headerlink\" title=\"2.2 标记-清扫\"></a>2.2 标记-清扫</h3><p>标记-清扫算法是第一种自动内存管理，基于追踪的垃圾收集算法。算法思想在 70 年代就提出了，是一种非常古老的算法。内存单元并不会在变成垃圾立刻回收，而是保持不可达状态，直到到达某个阈值或者固定时间长度。这个时候系统会挂起用户程序，也就是 STW，转而执行垃圾回收程序。垃圾回收程序对所有的存活单元进行一次全局遍历确定哪些单元可以回收。算法分两个部分：标记（mark）和清扫（sweep）。标记阶段表明所有的存活单元，清扫阶段将垃圾单元回收。可视化可以参考下图。</p>\n<p><a href=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif\" target=\"_blank\" rel=\"noopener\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif\" alt=\"img\"></a></p>\n<p>标记-清扫算法的优点也就是基于追踪的垃圾回收算法具有的优点：避免了引用计数算法的缺点（不能处理循环引用，需要维护指针）。缺点也很明显，需要 STW。</p>\n<h5 id=\"三色标记算法\"><a href=\"#三色标记算法\" class=\"headerlink\" title=\"三色标记算法\"></a>三色标记算法</h5><p>三色标记算法是对标记阶段的改进，原理如下：</p>\n<ol>\n<li>起初所有对象都是白色。</li>\n<li>从根出发扫描所有可达对象，标记为灰色，放入待处理队列。</li>\n<li>从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。</li>\n<li>重复 3，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。</li>\n</ol>\n<p>可视化如下。<br><a href=\"https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif\" target=\"_blank\" rel=\"noopener\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif\" alt=\"img\"></a></p>\n<p>三色标记的一个明显好处是能够让用户程序和 mark 并发的进行，具体可以参考论文：《On-the-fly garbage collection: an exercise in cooperation.》。Golang 的 GC 实现也是基于这篇论文，后面再具体说明。</p>\n<h3 id=\"2-3-节点复制\"><a href=\"#2-3-节点复制\" class=\"headerlink\" title=\"2.3 节点复制\"></a>2.3 节点复制</h3><p>节点复制也是基于追踪的算法。其将整个堆等分为两个半区（semi-space），一个包含现有数据，另一个包含已被废弃的数据。节点复制式垃圾收集从切换（flip）两个半区的角色开始，然后收集器在老的半区，也就是 Fromspace 中遍历存活的数据结构，在第一次访问某个单元时把它复制到新半区，也就是 Tospace 中去。在 Fromspace 中所有存活单元都被访问过之后，收集器在 Tospace 中建立一个存活数据结构的副本，用户程序可以重新开始运行了。</p>\n<h5 id=\"优点-1\"><a href=\"#优点-1\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>所有存活的数据结构都缩并地排列在 Tospace 的底部，这样就不会存在内存碎片的问题。</li>\n<li>获取新内存可以简单地通过递增自由空间指针来实现。</li>\n</ol>\n<h5 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>内存得不到充分利用，总有一半的内存空间处于浪费状态。</li>\n</ol>\n<h3 id=\"2-4-分代收集\"><a href=\"#2-4-分代收集\" class=\"headerlink\" title=\"2.4 分代收集\"></a>2.4 分代收集</h3><p>基于追踪的垃圾回收算法（标记-清扫、节点复制）一个主要问题是在生命周期较长的对象上浪费时间（长生命周期的对象是不需要频繁扫描的）。同时，内存分配存在这么一个事实 “most object die young”。基于这两点，分代垃圾回收算法将对象按生命周期长短存放到堆上的两个（或者更多）区域，这些区域就是分代（generation）。对于新生代的区域的垃圾回收频率要明显高于老年代区域。</p>\n<p>分配对象的时候从新生代里面分配，如果后面发现对象的生命周期较长，则将其移到老年代，这个过程叫做 promote。随着不断 promote，最后新生代的大小在整个堆的占用比例不会特别大。收集的时候集中主要精力在新生代就会相对来说效率更高，STW 时间也会更短。</p>\n<h5 id=\"优点-2\"><a href=\"#优点-2\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>性能更优。</li>\n</ol>\n<h5 id=\"缺点-2\"><a href=\"#缺点-2\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>实现复杂</li>\n</ol>\n<h2 id=\"3-Golang-GC\"><a href=\"#3-Golang-GC\" class=\"headerlink\" title=\"3. Golang GC\"></a>3. Golang GC</h2><h3 id=\"3-1-Overview\"><a href=\"#3-1-Overview\" class=\"headerlink\" title=\"3.1 Overview\"></a>3.1 Overview</h3><p>在说 Golang 的具体垃圾回收流程时，我们先来看一下几个基本的问题。</p>\n<h5 id=\"1-何时触发-GC\"><a href=\"#1-何时触发-GC\" class=\"headerlink\" title=\"1. 何时触发 GC\"></a>1. 何时触发 GC</h5><p>在堆上分配大于 32K byte 对象的时候进行检测此时是否满足垃圾回收条件，如果满足则进行垃圾回收。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">mallocgc</span><span class=\"params\">(size <span class=\"keyword\">uintptr</span>, typ *_type, needzero <span class=\"keyword\">bool</span>)</span> <span class=\"title\">unsafe</span>.<span class=\"title\">Pointer</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    shouldhelpgc := <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"comment\">// 分配的对象小于 32K byte</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> size &lt;= maxSmallSize &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        shouldhelpgc = <span class=\"literal\">true</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// gcShouldStart() 函数进行触发条件检测</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> shouldhelpgc &amp;&amp; gcShouldStart(<span class=\"literal\">false</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// gcStart() 函数进行垃圾回收</span></span><br><span class=\"line\">        gcStart(gcBackgroundMode, <span class=\"literal\">false</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面是自动垃圾回收，还有一种是主动垃圾回收，通过调用 runtime.GC()，这是阻塞式的。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// GC runs a garbage collection and blocks the caller until the</span></span><br><span class=\"line\"><span class=\"comment\">// garbage collection is complete. It may also block the entire</span></span><br><span class=\"line\"><span class=\"comment\">// program.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">GC</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    gcStart(gcForceBlockMode, <span class=\"literal\">false</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-GC-触发条件\"><a href=\"#2-GC-触发条件\" class=\"headerlink\" title=\"2. GC 触发条件\"></a>2. GC 触发条件</h5><p>触发条件主要关注下面代码中的中间部分：<code>forceTrigger || memstats.heap_live &gt;= memstats.gc_trigger</code> 。forceTrigger 是 forceGC 的标志；后面半句的意思是当前堆上的活跃对象大于我们初始化时候设置的 GC 触发阈值。在 malloc 以及 free 的时候 heap_live 会一直进行更新，这里就不再展开了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// gcShouldStart returns true if the exit condition for the _GCoff</span></span><br><span class=\"line\"><span class=\"comment\">// phase has been met. The exit condition should be tested when</span></span><br><span class=\"line\"><span class=\"comment\">// allocating.</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// If forceTrigger is true, it ignores the current heap size, but</span></span><br><span class=\"line\"><span class=\"comment\">// checks all other conditions. In general this should be false.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcShouldStart</span><span class=\"params\">(forceTrigger <span class=\"keyword\">bool</span>)</span> <span class=\"title\">bool</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> gcphase == _GCoff &amp;&amp; (forceTrigger || memstats.heap_live &gt;= memstats.gc_trigger) &amp;&amp; memstats.enablegc &amp;&amp; panicking == <span class=\"number\">0</span> &amp;&amp; gcpercent &gt;= <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化的时候设置 GC 的触发阈值</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcinit</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    _ = setGCPercent(readgogc())</span><br><span class=\"line\">    memstats.gc_trigger = heapminimum</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 启动的时候通过 GOGC 传递百分比 x</span></span><br><span class=\"line\"><span class=\"comment\">// 触发阈值等于 x * defaultHeapMinimum (defaultHeapMinimum 默认是 4M)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">readgogc</span><span class=\"params\">()</span> <span class=\"title\">int32</span></span> &#123;</span><br><span class=\"line\">    p := gogetenv(<span class=\"string\">\"GOGC\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == <span class=\"string\">\"off\"</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> n, ok := atoi32(p); ok &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> n</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">100</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-垃圾回收的主要流程\"><a href=\"#3-垃圾回收的主要流程\" class=\"headerlink\" title=\"3. 垃圾回收的主要流程\"></a>3. 垃圾回收的主要流程</h5><p>三色标记法，主要流程如下：</p>\n<ul>\n<li>所有对象最开始都是白色。</li>\n<li>从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。</li>\n<li>遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。</li>\n<li>处理完灰色对象队列，执行清扫工作。</li>\n</ul>\n<p>详细的过程如下图所示，具体可参考 [9]。<br><a href=\"http://legendtkl.com/img/uploads/2017/gc.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://legendtkl.com/img/uploads/2017/gc.png\" alt=\"img\"></a></p>\n<p>关于上图有几点需要说明的是。</p>\n<ol>\n<li>首先从 root 开始遍历，root 包括全局指针和 goroutine 栈上的指针。</li>\n<li>mark 有两个过程。<ol>\n<li>从 root 开始遍历，标记为灰色。遍历灰色队列。</li>\n<li>re-scan 全局指针和栈。因为 mark 和用户程序是并行的，所以在过程 1 的时候可能会有新的对象分配，这个时候就需要通过写屏障（write barrier）记录下来。re-scan 再完成检查一下。</li>\n</ol>\n</li>\n<li>Stop The World 有两个过程。<ol>\n<li>第一个是 GC 将要开始的时候，这个时候主要是一些准备工作，比如 enable write barrier。</li>\n<li>第二个过程就是上面提到的 re-scan 过程。如果这个时候没有 stw，那么 mark 将无休止。</li>\n</ol>\n</li>\n</ol>\n<p>另外针对上图各个阶段对应 GCPhase 如下：</p>\n<ul>\n<li>Off: _GCoff</li>\n<li>Stack scan ~ Mark: _GCmark</li>\n<li>Mark termination: _GCmarktermination</li>\n</ul>\n<h3 id=\"3-2-写屏障-write-barrier\"><a href=\"#3-2-写屏障-write-barrier\" class=\"headerlink\" title=\"3.2 写屏障 (write barrier)\"></a>3.2 写屏障 (write barrier)</h3><p>关于 write barrier，完全可以另外写成一篇文章，所以这里只简单介绍一下，这篇文章的重点还是 Golang 的 GC。垃圾回收中的 write barrier 可以理解为编译器在写操作时特意插入的一段代码，对应的还有 read barrier。</p>\n<p>为什么需要 write barrier，很简单，对于和用户程序并发运行的垃圾回收算法，用户程序会一直修改内存，所以需要记录下来。</p>\n<p>Golang 1.7 之前的 write barrier 使用的经典的 Dijkstra-style insertion write barrier [Dijkstra ‘78]， STW 的主要耗时就在 stack re-scan 的过程。自 1.8 之后采用一种混合的 write barrier 方式 （Yuasa-style deletion write barrier [Yuasa ‘90] 和 Dijkstra-style insertion write barrier [Dijkstra ‘78]）来避免 re-scan。具体的可以参考 <a href=\"https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\" target=\"_blank\" rel=\"noopener\">17503-eliminate-rescan</a>。</p>\n<h3 id=\"3-3-标记\"><a href=\"#3-3-标记\" class=\"headerlink\" title=\"3.3 标记\"></a>3.3 标记</h3><p>下面的源码还是基于 go1.8rc3。这个版本的 GC 代码相比之前改动还是挺大的，我们下面尽量只关注主流程。垃圾回收的代码主要集中在函数 <code>gcStart()</code> 中。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// gcStart 是 GC 的入口函数，根据 gcMode 做处理。</span></span><br><span class=\"line\"><span class=\"comment\">// 1. gcMode == gcBackgroundMode（后台运行，也就是并行）, _GCoff -&gt; _GCmark</span></span><br><span class=\"line\"><span class=\"comment\">// 2. 否则 GCoff -&gt; _GCmarktermination，这个时候就是主动 GC </span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-STW-phase-1\"><a href=\"#1-STW-phase-1\" class=\"headerlink\" title=\"1. STW phase 1\"></a>1. STW phase 1</h5><p>在 GC 开始之前的准备工作。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//在后台启动 mark worker </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        gcBgMarkStartWorkers()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// Stop The World</span></span><br><span class=\"line\">    systemstack(stopTheWorldWithSema)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        <span class=\"comment\">// GC 开始前的准备工作</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//处理设置 GCPhase，setGCPhase 还会 enable write barrier</span></span><br><span class=\"line\">        setGCPhase(_GCmark)</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        gcBgMarkPrepare() <span class=\"comment\">// Must happen before assist enable.</span></span><br><span class=\"line\">        gcMarkRootPrepare()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Mark all active tinyalloc blocks. Since we're</span></span><br><span class=\"line\">        <span class=\"comment\">// allocating from these, they need to be black like</span></span><br><span class=\"line\">        <span class=\"comment\">// other allocations. The alternative is to blacken</span></span><br><span class=\"line\">        <span class=\"comment\">// the tiny block on every allocation from it, which</span></span><br><span class=\"line\">        <span class=\"comment\">// would slow down the tiny allocator.</span></span><br><span class=\"line\">        gcMarkTinyAllocs()</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        <span class=\"comment\">// Start The World</span></span><br><span class=\"line\">        systemstack(startTheWorldWithSema)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-Mark\"><a href=\"#2-Mark\" class=\"headerlink\" title=\"2. Mark\"></a>2. Mark</h5><p>Mark 阶段是并行的运行，通过在后台一直运行 mark worker 来实现。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//在后台启动 mark worker </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        gcBgMarkStartWorkers()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcBgMarkStartWorkers</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Background marking is performed by per-P G's. Ensure that</span></span><br><span class=\"line\">    <span class=\"comment\">// each P has a background GC G.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _, p := <span class=\"keyword\">range</span> &amp;allp &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> p == <span class=\"literal\">nil</span> || p.status == _Pdead &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> p.gcBgMarkWorker == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">go</span> gcBgMarkWorker(p)</span><br><span class=\"line\">            notetsleepg(&amp;work.bgMarkReady, <span class=\"number\">-1</span>)</span><br><span class=\"line\">            noteclear(&amp;work.bgMarkReady)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// gcBgMarkWorker 是一直在后台运行的，大部分时候是休眠状态，通过 gcController 来调度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcBgMarkWorker</span><span class=\"params\">(_p_ *p)</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 将当前 goroutine 休眠，直到满足某些条件</span></span><br><span class=\"line\">        gopark(...)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// mark 过程</span></span><br><span class=\"line\">        systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Mark our goroutine preemptible so its stack</span></span><br><span class=\"line\">        <span class=\"comment\">// can be scanned. This lets two mark workers</span></span><br><span class=\"line\">        <span class=\"comment\">// scan each other (otherwise, they would</span></span><br><span class=\"line\">        <span class=\"comment\">// deadlock). We must not modify anything on</span></span><br><span class=\"line\">        <span class=\"comment\">// the G stack. However, stack shrinking is</span></span><br><span class=\"line\">        <span class=\"comment\">// disabled for mark workers, so it is safe to</span></span><br><span class=\"line\">        <span class=\"comment\">// read from the G stack.</span></span><br><span class=\"line\">        casgstatus(gp, _Grunning, _Gwaiting)</span><br><span class=\"line\">        <span class=\"keyword\">switch</span> _p_.gcMarkWorkerMode &#123;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            throw(<span class=\"string\">\"gcBgMarkWorker: unexpected gcMarkWorkerMode\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerDedicatedMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerFractionalMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerIdleMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        casgstatus(gp, _Gwaiting, _Grunning)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Mark 阶段的标记代码主要在函数 gcDrain() 中实现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// gcDrain scans roots and objects in work buffers, blackening grey</span><br><span class=\"line\">// objects until all roots and work buffers have been drained.</span><br><span class=\"line\">func gcDrain(gcw *gcWork, flags gcDrainFlags) &#123;</span><br><span class=\"line\">    ...\t</span><br><span class=\"line\">    // Drain root marking jobs.</span><br><span class=\"line\">    if work.markrootNext &lt; work.markrootJobs &#123;</span><br><span class=\"line\">        for !(preemptible &amp;&amp; gp.preempt) &#123;</span><br><span class=\"line\">            job := atomic.Xadd(&amp;work.markrootNext, +1) - 1</span><br><span class=\"line\">            if job &gt;= work.markrootJobs &#123;</span><br><span class=\"line\">                break</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            markroot(gcw, job)</span><br><span class=\"line\">            if idle &amp;&amp; pollWork() &#123;</span><br><span class=\"line\">                goto done</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  \t</span><br><span class=\"line\">    // 处理 heap 标记</span><br><span class=\"line\">    // Drain heap marking jobs.</span><br><span class=\"line\">    for !(preemptible &amp;&amp; gp.preempt) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        //从灰色列队中取出对象</span><br><span class=\"line\">        var b uintptr</span><br><span class=\"line\">        if blocking &#123;</span><br><span class=\"line\">            b = gcw.get()</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            b = gcw.tryGetFast()</span><br><span class=\"line\">            if b == 0 &#123;</span><br><span class=\"line\">                b = gcw.tryGet()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if b == 0 &#123;</span><br><span class=\"line\">            // work barrier reached or tryGet failed.</span><br><span class=\"line\">            break</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        //扫描灰色对象的引用对象，标记为灰色，入灰色队列</span><br><span class=\"line\">        scanobject(b, gcw)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-Mark-termination-STW-phase-2\"><a href=\"#3-Mark-termination-STW-phase-2\" class=\"headerlink\" title=\"3. Mark termination (STW phase 2)\"></a>3. Mark termination (STW phase 2)</h5><p>mark termination 阶段会 stop the world。函数实现在 <code>gcMarkTermination()</code>。1.8 版本已经不会再对 goroutine stack 进行 re-scan 了。细节有点多，这里不细说了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcMarkTermination</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// World is stopped.</span></span><br><span class=\"line\">    <span class=\"comment\">// Run gc on the g0 stack. We do this so that the g stack</span></span><br><span class=\"line\">    <span class=\"comment\">// we're currently running on will no longer change. Cuts</span></span><br><span class=\"line\">    <span class=\"comment\">// the root set down a bit (g0 stacks are not scanned, and</span></span><br><span class=\"line\">    <span class=\"comment\">// we don't need to scan gc's internal state).  We also</span></span><br><span class=\"line\">    <span class=\"comment\">// need to switch to g0 so we can shrink the stack.</span></span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        gcMark(startTime)</span><br><span class=\"line\">        <span class=\"comment\">// Must return immediately.</span></span><br><span class=\"line\">        <span class=\"comment\">// The outer function's stack may have moved</span></span><br><span class=\"line\">        <span class=\"comment\">// during gcMark (it shrinks stacks, including the</span></span><br><span class=\"line\">        <span class=\"comment\">// outer function's stack), so we must not refer</span></span><br><span class=\"line\">        <span class=\"comment\">// to any of its variables. Return back to the</span></span><br><span class=\"line\">        <span class=\"comment\">// non-system stack to pick up the new addresses</span></span><br><span class=\"line\">        <span class=\"comment\">// before continuing.</span></span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-4-清扫\"><a href=\"#3-4-清扫\" class=\"headerlink\" title=\"3.4 清扫\"></a>3.4 清扫</h3><p>清扫相对来说就简单很多了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcSweep</span><span class=\"params\">(mode gcMode)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//阻塞式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> !_ConcurrentSweep || mode == gcForceBlockMode &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Special case synchronous sweep.</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// Sweep all spans eagerly.</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> sweepone() != ^<span class=\"keyword\">uintptr</span>(<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            sweep.npausesweep++</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// Do an additional mProf_GC, because all 'free' events are now real as well.</span></span><br><span class=\"line\">        mProf_GC()</span><br><span class=\"line\">        mProf_GC()</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  \t</span><br><span class=\"line\">    <span class=\"comment\">// 并行式</span></span><br><span class=\"line\">    <span class=\"comment\">// Background sweep.</span></span><br><span class=\"line\">    lock(&amp;sweep.lock)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> sweep.parked &#123;</span><br><span class=\"line\">        sweep.parked = <span class=\"literal\">false</span></span><br><span class=\"line\">        ready(sweep.g, <span class=\"number\">0</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    unlock(&amp;sweep.lock)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于并行式清扫，在 GC 初始化的时候就会启动 <code>bgsweep()</code>，然后在后台一直循环。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">bgsweep</span><span class=\"params\">(c <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">    sweep.g = getg()</span><br><span class=\"line\"></span><br><span class=\"line\">    lock(&amp;sweep.lock)</span><br><span class=\"line\">    sweep.parked = <span class=\"literal\">true</span></span><br><span class=\"line\">    c &lt;- <span class=\"number\">1</span></span><br><span class=\"line\">    goparkunlock(&amp;sweep.lock, <span class=\"string\">\"GC sweep wait\"</span>, traceEvGoBlock, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> gosweepone() != ^<span class=\"keyword\">uintptr</span>(<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            sweep.nbgsweep++</span><br><span class=\"line\">            Gosched()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        lock(&amp;sweep.lock)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> !gosweepdone() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// This can happen if a GC runs between</span></span><br><span class=\"line\">            <span class=\"comment\">// gosweepone returning ^0 above</span></span><br><span class=\"line\">            <span class=\"comment\">// and the lock being acquired.</span></span><br><span class=\"line\">            unlock(&amp;sweep.lock)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sweep.parked = <span class=\"literal\">true</span></span><br><span class=\"line\">        goparkunlock(&amp;sweep.lock, <span class=\"string\">\"GC sweep wait\"</span>, traceEvGoBlock, <span class=\"number\">1</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gosweepone</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> ret <span class=\"keyword\">uintptr</span></span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        ret = sweepone()</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>不管是阻塞式还是并行式，都是通过 <code>sweepone()</code>函数来做清扫工作的。如果对于上篇文章 <a href=\"http://legendtkl.com/2017/04/02/golang-alloc/\" target=\"_blank\" rel=\"noopener\">Golang 内存管理</a> 熟悉的话，这个地方就很好理解。内存管理都是基于 span 的，mheap<em> 是一个全局的变量，所有分配的对象都会记录在 mheap</em> 中。在标记的时候，我们只要找到对对象对应的 span 进行标记，清扫的时候扫描 span，没有标记的 span 就可以回收了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// sweeps one span</span></span><br><span class=\"line\"><span class=\"comment\">// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">sweepone</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        s := mheap_.sweepSpans[<span class=\"number\">1</span>-sg/<span class=\"number\">2</span>%<span class=\"number\">2</span>].pop()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"keyword\">if</span> !s.sweep(<span class=\"literal\">false</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Span is still in-use, so this returned no</span></span><br><span class=\"line\">            <span class=\"comment\">// pages to the heap and the span needs to</span></span><br><span class=\"line\">            <span class=\"comment\">// move to the swept in-use list.</span></span><br><span class=\"line\">            npages = <span class=\"number\">0</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Sweep frees or collects finalizers for blocks not marked in the mark phase.</span></span><br><span class=\"line\"><span class=\"comment\">// It clears the mark bits in preparation for the next GC round.</span></span><br><span class=\"line\"><span class=\"comment\">// Returns true if the span was returned to heap.</span></span><br><span class=\"line\"><span class=\"comment\">// If preserve=true, don't return it to heap nor relink in MCentral lists;</span></span><br><span class=\"line\"><span class=\"comment\">// caller takes care of it.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *mspan)</span> <span class=\"title\">sweep</span><span class=\"params\">(preserve <span class=\"keyword\">bool</span>)</span> <span class=\"title\">bool</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-5-其他\"><a href=\"#3-5-其他\" class=\"headerlink\" title=\"3.5 其他\"></a>3.5 其他</h3><h5 id=\"1-gcWork\"><a href=\"#1-gcWork\" class=\"headerlink\" title=\"1. gcWork\"></a>1. gcWork</h5><p>这里介绍一下任务队列，或者说灰色对象管理。每个 P 上都有一个 gcw 用来管理灰色对象（get 和 put），gcw 的结构就是 gcWork。gcWork 中的核心是 wbuf1 和 wbuf2，里面存储就是灰色对象，或者说是 work（下面就全部统一叫做 work）。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> p <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    gcw gcWork</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> gcWork <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// wbuf1 and wbuf2 are the primary and secondary work buffers.</span></span><br><span class=\"line\">    wbuf1, wbuf2 wbufptr</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Bytes marked (blackened) on this gcWork. This is aggregated</span></span><br><span class=\"line\">    <span class=\"comment\">// into work.bytesMarked by dispose.</span></span><br><span class=\"line\">    bytesMarked <span class=\"keyword\">uint64</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Scan work performed on this gcWork. This is aggregated into</span></span><br><span class=\"line\">    <span class=\"comment\">// gcController by dispose and may also be flushed by callers.</span></span><br><span class=\"line\">    scanWork <span class=\"keyword\">int64</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>既然每个 P 上有一个 work buffer，那么是不是还有一个全局的 work list 呢？是的。通过在每个 P 上绑定一个 work buffer 的好处和 cache 一样，不需要加锁。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> work <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    full  <span class=\"keyword\">uint64</span>                   <span class=\"comment\">// lock-free list of full blocks workbuf</span></span><br><span class=\"line\">    empty <span class=\"keyword\">uint64</span>                   <span class=\"comment\">// lock-free list of empty blocks workbuf</span></span><br><span class=\"line\">    pad0  [sys.CacheLineSize]<span class=\"keyword\">uint8</span> <span class=\"comment\">// prevents false-sharing between full/empty and nproc/nwait</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>那么为什么使用两个 work buffer （wbuf1 和 wbuf2）呢？我下面举个例子。比如我现在要 get 一个 work 出来，先从 wbuf1 中取，wbuf1 为空的话则与 wbuf2 swap 再 get。在其他时间将 work buffer 中的 full 或者 empty buffer 移到 global 的 work 中。这样的好处在于，在 get 的时候去全局的 work 里面取（多个 goroutine 去取会有竞争）。这里有趣的是 global 的 work list 是 lock-free 的，通过原子操作 cas 等实现。下面列举几个函数看一下 gcWrok。</p>\n<p>初始化。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">init</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    w.wbuf1 = wbufptrOf(getempty())</span><br><span class=\"line\">    wbuf2 := trygetfull()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf2 == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        wbuf2 = getempty()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    w.wbuf2 = wbufptrOf(wbuf2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>put。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// put enqueues a pointer for the garbage collector to trace.</span></span><br><span class=\"line\"><span class=\"comment\">// obj must point to the beginning of a heap object or an oblet.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">put</span><span class=\"params\">(obj <span class=\"keyword\">uintptr</span>)</span></span> &#123;</span><br><span class=\"line\">    wbuf := w.wbuf1.ptr()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        w.init()</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"comment\">// wbuf is empty at this point.</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"built_in\">len</span>(wbuf.obj) &#123;</span><br><span class=\"line\">        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"built_in\">len</span>(wbuf.obj) &#123;</span><br><span class=\"line\">            putfull(wbuf)</span><br><span class=\"line\">            wbuf = getempty()</span><br><span class=\"line\">            w.wbuf1 = wbufptrOf(wbuf)</span><br><span class=\"line\">            flushed = <span class=\"literal\">true</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    wbuf.obj[wbuf.nobj] = obj</span><br><span class=\"line\">    wbuf.nobj++</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>get。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// get dequeues a pointer for the garbage collector to trace, blocking</span></span><br><span class=\"line\"><span class=\"comment\">// if necessary to ensure all pointers from all queues and caches have</span></span><br><span class=\"line\"><span class=\"comment\">// been retrieved.  get returns 0 if there are no pointers remaining.</span></span><br><span class=\"line\"><span class=\"comment\">//go:nowritebarrier</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">get</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    wbuf := w.wbuf1.ptr()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        w.init()</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"comment\">// wbuf is empty at this point.</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            owbuf := wbuf</span><br><span class=\"line\">            wbuf = getfull()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            putempty(owbuf)</span><br><span class=\"line\">            w.wbuf1 = wbufptrOf(wbuf)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> This might be a good place to add prefetch code</span></span><br><span class=\"line\"></span><br><span class=\"line\">    wbuf.nobj--</span><br><span class=\"line\">    <span class=\"keyword\">return</span> wbuf.obj[wbuf.nobj]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-forcegc\"><a href=\"#2-forcegc\" class=\"headerlink\" title=\"2. forcegc\"></a>2. forcegc</h5><p>我们上面讲了两种 GC 触发方式：自动检测和用户主动调用。除此之后 Golang 本身还会对运行状态进行监控，如果超过两分钟没有 GC，则触发 GC。监控函数是 <code>sysmon()</code>，在主 goroutine 中启动。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// The main goroutine</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">      \tnewm(sysmon, <span class=\"literal\">nil</span>)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// Always runs without a P, so write barriers are not allowed.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">sysmon</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        now := nanotime()</span><br><span class=\"line\">        unixnow := unixnanotime()</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        lastgc := <span class=\"keyword\">int64</span>(atomic.Load64(&amp;memstats.last_gc))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gcphase == _GCoff &amp;&amp; lastgc != <span class=\"number\">0</span> &amp;&amp; unixnow-lastgc &gt; forcegcperiod &amp;&amp; atomic.Load(&amp;forcegc.idle) != <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            lock(&amp;forcegc.lock)</span><br><span class=\"line\">            forcegc.idle = <span class=\"number\">0</span></span><br><span class=\"line\">            forcegc.g.schedlink = <span class=\"number\">0</span></span><br><span class=\"line\">            injectglist(forcegc.g)\t<span class=\"comment\">// 将 forcegc goroutine 加入 runnable queue</span></span><br><span class=\"line\">            unlock(&amp;forcegc.lock)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> forcegcperiod <span class=\"keyword\">int64</span> = <span class=\"number\">2</span> * <span class=\"number\">60</span> *<span class=\"number\">1e9</span>\t<span class=\"comment\">//两分钟</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-参考资料\"><a href=\"#4-参考资料\" class=\"headerlink\" title=\"4.参考资料\"></a>4.参考资料</h2><ol>\n<li>《Go 语言学习笔记》</li>\n<li><a href=\"https://book.douban.com/subject/1157908/\" target=\"_blank\" rel=\"noopener\">《垃圾收集》 - 豆瓣</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Tracing_garbage_collection#Na.C3.AFve_mark-and-sweep\" target=\"_blank\" rel=\"noopener\">Tracing Garbage Collection - wikipedia</a></li>\n<li>《On-the-fly garbage collection: an exercise in cooperation.》 — Edsger W. Dijkstra, Leslie Lamport, A. J. Martin</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\" target=\"_blank\" rel=\"noopener\">Garbage Collection</a>)</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Tracing_garbage_collection\" target=\"_blank\" rel=\"noopener\">Tracing Garbage Collection</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=P1rU_9IB414\" target=\"_blank\" rel=\"noopener\">Copying Garbage Collection – youtube</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=pJHISaOW6Vc\" target=\"_blank\" rel=\"noopener\">Generational Garbage Collection – youtube</a></li>\n<li><a href=\"https://talks.golang.org/2015/go-gc.pdf\" target=\"_blank\" rel=\"noopener\">golang gc talk</a></li>\n<li><a href=\"https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\" target=\"_blank\" rel=\"noopener\">17503-eliminate-rescan</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>转载自：(<a href=\"http://legendtkl.com/2017/04/28/golang-gc/\" target=\"_blank\" rel=\"noopener\">http://legendtkl.com/2017/04/28/golang-gc/</a>)</p>\n<h2 id=\"1-Golang-GC-发展\"><a href=\"#1-Golang-GC-发展\" class=\"headerlink\" title=\"1.Golang GC 发展\"></a>1.Golang GC 发展</h2><p>Golang 从第一个版本以来，GC 一直是大家诟病最多的。但是每一个版本的发布基本都伴随着 GC 的改进。下面列出一些比较重要的改动。</p>\n<ul>\n<li>v1.1 STW</li>\n<li>v1.3 Mark STW, Sweep 并行</li>\n<li>v1.5 三色标记法</li>\n<li>v1.8 hybrid write barrier</li>\n</ul>\n<h2 id=\"2-GC-算法简介\"><a href=\"#2-GC-算法简介\" class=\"headerlink\" title=\"2. GC 算法简介\"></a>2. GC 算法简介</h2><p>这一小节介绍三种经典的 GC 算法：引用计数（reference counting）、标记-清扫（mark &amp; sweep）、节点复制（Copying Garbage Collection），分代收集（Generational Garbage Collection）。</p>\n<h3 id=\"2-1-引用计数\"><a href=\"#2-1-引用计数\" class=\"headerlink\" title=\"2.1 引用计数\"></a>2.1 引用计数</h3><p>引用计数的思想非常简单：每个单元维护一个域，保存其它单元指向它的引用数量（类似有向图的入度）。当引用数量为 0 时，将其回收。引用计数是渐进式的，能够将内存管理的开销分布到整个程序之中。C++ 的 share_ptr 使用的就是引用计算方法。</p>\n<p>引用计数算法实现一般是把所有的单元放在一个单元池里，比如类似 free list。这样所有的单元就被串起来了，就可以进行引用计数了。新分配的单元计数值被设置为 1（注意不是 0，因为申请一般都说 ptr = new object 这种）。每次有一个指针被设为指向该单元时，该单元的计数值加 1；而每次删除某个指向它的指针时，它的计数值减 1。当其引用计数为 0 的时候，该单元会被进行回收。虽然这里说的比较简单，实现的时候还是有很多细节需要考虑，比如删除某个单元的时候，那么它指向的所有单元都需要对引用计数减 1。那么如果这个时候，发现其中某个指向的单元的引用计数又为 0，那么是递归的进行还是采用其他的策略呢？递归处理的话会导致系统颠簸。关于这些细节这里就不讨论了，可以参考文章后面的给的参考资料。</p>","more":"<h5 id=\"优点\"><a href=\"#优点\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>渐进式。内存管理与用户程序的执行交织在一起，将 GC 的代价分散到整个程序。不像标记-清扫算法需要 STW (Stop The World，GC 的时候挂起用户程序)。</li>\n<li>算法易于实现。</li>\n<li>内存单元能够很快被回收。相比于其他垃圾回收算法，堆被耗尽或者达到某个阈值才会进行垃圾回收。</li>\n</ol>\n<h5 id=\"缺点\"><a href=\"#缺点\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>原始的引用计数不能处理循环引用。大概这是被诟病最多的缺点了。不过针对这个问题，也除了很多解决方案，比如强引用等。</li>\n<li>维护引用计数降低运行效率。内存单元的更新删除等都需要维护相关的内存单元的引用计数，相比于一些追踪式的垃圾回收算法并不需要这些代价。</li>\n<li>单元池 free list 实现的话不是 cache-friendly 的，这样会导致频繁的 cache miss，降低程序运行效率。</li>\n</ol>\n<h3 id=\"2-2-标记-清扫\"><a href=\"#2-2-标记-清扫\" class=\"headerlink\" title=\"2.2 标记-清扫\"></a>2.2 标记-清扫</h3><p>标记-清扫算法是第一种自动内存管理，基于追踪的垃圾收集算法。算法思想在 70 年代就提出了，是一种非常古老的算法。内存单元并不会在变成垃圾立刻回收，而是保持不可达状态，直到到达某个阈值或者固定时间长度。这个时候系统会挂起用户程序，也就是 STW，转而执行垃圾回收程序。垃圾回收程序对所有的存活单元进行一次全局遍历确定哪些单元可以回收。算法分两个部分：标记（mark）和清扫（sweep）。标记阶段表明所有的存活单元，清扫阶段将垃圾单元回收。可视化可以参考下图。</p>\n<p><a href=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif\" target=\"_blank\" rel=\"noopener\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Animation_of_the_Naive_Mark_and_Sweep_Garbage_Collector_Algorithm.gif\" alt=\"img\"></a></p>\n<p>标记-清扫算法的优点也就是基于追踪的垃圾回收算法具有的优点：避免了引用计数算法的缺点（不能处理循环引用，需要维护指针）。缺点也很明显，需要 STW。</p>\n<h5 id=\"三色标记算法\"><a href=\"#三色标记算法\" class=\"headerlink\" title=\"三色标记算法\"></a>三色标记算法</h5><p>三色标记算法是对标记阶段的改进，原理如下：</p>\n<ol>\n<li>起初所有对象都是白色。</li>\n<li>从根出发扫描所有可达对象，标记为灰色，放入待处理队列。</li>\n<li>从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。</li>\n<li>重复 3，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。</li>\n</ol>\n<p>可视化如下。<br><a href=\"https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif\" target=\"_blank\" rel=\"noopener\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1d/Animation_of_tri-color_garbage_collection.gif\" alt=\"img\"></a></p>\n<p>三色标记的一个明显好处是能够让用户程序和 mark 并发的进行，具体可以参考论文：《On-the-fly garbage collection: an exercise in cooperation.》。Golang 的 GC 实现也是基于这篇论文，后面再具体说明。</p>\n<h3 id=\"2-3-节点复制\"><a href=\"#2-3-节点复制\" class=\"headerlink\" title=\"2.3 节点复制\"></a>2.3 节点复制</h3><p>节点复制也是基于追踪的算法。其将整个堆等分为两个半区（semi-space），一个包含现有数据，另一个包含已被废弃的数据。节点复制式垃圾收集从切换（flip）两个半区的角色开始，然后收集器在老的半区，也就是 Fromspace 中遍历存活的数据结构，在第一次访问某个单元时把它复制到新半区，也就是 Tospace 中去。在 Fromspace 中所有存活单元都被访问过之后，收集器在 Tospace 中建立一个存活数据结构的副本，用户程序可以重新开始运行了。</p>\n<h5 id=\"优点-1\"><a href=\"#优点-1\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>所有存活的数据结构都缩并地排列在 Tospace 的底部，这样就不会存在内存碎片的问题。</li>\n<li>获取新内存可以简单地通过递增自由空间指针来实现。</li>\n</ol>\n<h5 id=\"缺点-1\"><a href=\"#缺点-1\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>内存得不到充分利用，总有一半的内存空间处于浪费状态。</li>\n</ol>\n<h3 id=\"2-4-分代收集\"><a href=\"#2-4-分代收集\" class=\"headerlink\" title=\"2.4 分代收集\"></a>2.4 分代收集</h3><p>基于追踪的垃圾回收算法（标记-清扫、节点复制）一个主要问题是在生命周期较长的对象上浪费时间（长生命周期的对象是不需要频繁扫描的）。同时，内存分配存在这么一个事实 “most object die young”。基于这两点，分代垃圾回收算法将对象按生命周期长短存放到堆上的两个（或者更多）区域，这些区域就是分代（generation）。对于新生代的区域的垃圾回收频率要明显高于老年代区域。</p>\n<p>分配对象的时候从新生代里面分配，如果后面发现对象的生命周期较长，则将其移到老年代，这个过程叫做 promote。随着不断 promote，最后新生代的大小在整个堆的占用比例不会特别大。收集的时候集中主要精力在新生代就会相对来说效率更高，STW 时间也会更短。</p>\n<h5 id=\"优点-2\"><a href=\"#优点-2\" class=\"headerlink\" title=\"优点\"></a>优点</h5><ol>\n<li>性能更优。</li>\n</ol>\n<h5 id=\"缺点-2\"><a href=\"#缺点-2\" class=\"headerlink\" title=\"缺点\"></a>缺点</h5><ol>\n<li>实现复杂</li>\n</ol>\n<h2 id=\"3-Golang-GC\"><a href=\"#3-Golang-GC\" class=\"headerlink\" title=\"3. Golang GC\"></a>3. Golang GC</h2><h3 id=\"3-1-Overview\"><a href=\"#3-1-Overview\" class=\"headerlink\" title=\"3.1 Overview\"></a>3.1 Overview</h3><p>在说 Golang 的具体垃圾回收流程时，我们先来看一下几个基本的问题。</p>\n<h5 id=\"1-何时触发-GC\"><a href=\"#1-何时触发-GC\" class=\"headerlink\" title=\"1. 何时触发 GC\"></a>1. 何时触发 GC</h5><p>在堆上分配大于 32K byte 对象的时候进行检测此时是否满足垃圾回收条件，如果满足则进行垃圾回收。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">mallocgc</span><span class=\"params\">(size <span class=\"keyword\">uintptr</span>, typ *_type, needzero <span class=\"keyword\">bool</span>)</span> <span class=\"title\">unsafe</span>.<span class=\"title\">Pointer</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    shouldhelpgc := <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"comment\">// 分配的对象小于 32K byte</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> size &lt;= maxSmallSize &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        shouldhelpgc = <span class=\"literal\">true</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// gcShouldStart() 函数进行触发条件检测</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> shouldhelpgc &amp;&amp; gcShouldStart(<span class=\"literal\">false</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// gcStart() 函数进行垃圾回收</span></span><br><span class=\"line\">        gcStart(gcBackgroundMode, <span class=\"literal\">false</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面是自动垃圾回收，还有一种是主动垃圾回收，通过调用 runtime.GC()，这是阻塞式的。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// GC runs a garbage collection and blocks the caller until the</span></span><br><span class=\"line\"><span class=\"comment\">// garbage collection is complete. It may also block the entire</span></span><br><span class=\"line\"><span class=\"comment\">// program.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">GC</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    gcStart(gcForceBlockMode, <span class=\"literal\">false</span>)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-GC-触发条件\"><a href=\"#2-GC-触发条件\" class=\"headerlink\" title=\"2. GC 触发条件\"></a>2. GC 触发条件</h5><p>触发条件主要关注下面代码中的中间部分：<code>forceTrigger || memstats.heap_live &gt;= memstats.gc_trigger</code> 。forceTrigger 是 forceGC 的标志；后面半句的意思是当前堆上的活跃对象大于我们初始化时候设置的 GC 触发阈值。在 malloc 以及 free 的时候 heap_live 会一直进行更新，这里就不再展开了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// gcShouldStart returns true if the exit condition for the _GCoff</span></span><br><span class=\"line\"><span class=\"comment\">// phase has been met. The exit condition should be tested when</span></span><br><span class=\"line\"><span class=\"comment\">// allocating.</span></span><br><span class=\"line\"><span class=\"comment\">//</span></span><br><span class=\"line\"><span class=\"comment\">// If forceTrigger is true, it ignores the current heap size, but</span></span><br><span class=\"line\"><span class=\"comment\">// checks all other conditions. In general this should be false.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcShouldStart</span><span class=\"params\">(forceTrigger <span class=\"keyword\">bool</span>)</span> <span class=\"title\">bool</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> gcphase == _GCoff &amp;&amp; (forceTrigger || memstats.heap_live &gt;= memstats.gc_trigger) &amp;&amp; memstats.enablegc &amp;&amp; panicking == <span class=\"number\">0</span> &amp;&amp; gcpercent &gt;= <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//初始化的时候设置 GC 的触发阈值</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcinit</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    _ = setGCPercent(readgogc())</span><br><span class=\"line\">    memstats.gc_trigger = heapminimum</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 启动的时候通过 GOGC 传递百分比 x</span></span><br><span class=\"line\"><span class=\"comment\">// 触发阈值等于 x * defaultHeapMinimum (defaultHeapMinimum 默认是 4M)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">readgogc</span><span class=\"params\">()</span> <span class=\"title\">int32</span></span> &#123;</span><br><span class=\"line\">    p := gogetenv(<span class=\"string\">\"GOGC\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == <span class=\"string\">\"off\"</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> n, ok := atoi32(p); ok &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> n</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">100</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-垃圾回收的主要流程\"><a href=\"#3-垃圾回收的主要流程\" class=\"headerlink\" title=\"3. 垃圾回收的主要流程\"></a>3. 垃圾回收的主要流程</h5><p>三色标记法，主要流程如下：</p>\n<ul>\n<li>所有对象最开始都是白色。</li>\n<li>从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。</li>\n<li>遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。</li>\n<li>处理完灰色对象队列，执行清扫工作。</li>\n</ul>\n<p>详细的过程如下图所示，具体可参考 [9]。<br><a href=\"http://legendtkl.com/img/uploads/2017/gc.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://legendtkl.com/img/uploads/2017/gc.png\" alt=\"img\"></a></p>\n<p>关于上图有几点需要说明的是。</p>\n<ol>\n<li>首先从 root 开始遍历，root 包括全局指针和 goroutine 栈上的指针。</li>\n<li>mark 有两个过程。<ol>\n<li>从 root 开始遍历，标记为灰色。遍历灰色队列。</li>\n<li>re-scan 全局指针和栈。因为 mark 和用户程序是并行的，所以在过程 1 的时候可能会有新的对象分配，这个时候就需要通过写屏障（write barrier）记录下来。re-scan 再完成检查一下。</li>\n</ol>\n</li>\n<li>Stop The World 有两个过程。<ol>\n<li>第一个是 GC 将要开始的时候，这个时候主要是一些准备工作，比如 enable write barrier。</li>\n<li>第二个过程就是上面提到的 re-scan 过程。如果这个时候没有 stw，那么 mark 将无休止。</li>\n</ol>\n</li>\n</ol>\n<p>另外针对上图各个阶段对应 GCPhase 如下：</p>\n<ul>\n<li>Off: _GCoff</li>\n<li>Stack scan ~ Mark: _GCmark</li>\n<li>Mark termination: _GCmarktermination</li>\n</ul>\n<h3 id=\"3-2-写屏障-write-barrier\"><a href=\"#3-2-写屏障-write-barrier\" class=\"headerlink\" title=\"3.2 写屏障 (write barrier)\"></a>3.2 写屏障 (write barrier)</h3><p>关于 write barrier，完全可以另外写成一篇文章，所以这里只简单介绍一下，这篇文章的重点还是 Golang 的 GC。垃圾回收中的 write barrier 可以理解为编译器在写操作时特意插入的一段代码，对应的还有 read barrier。</p>\n<p>为什么需要 write barrier，很简单，对于和用户程序并发运行的垃圾回收算法，用户程序会一直修改内存，所以需要记录下来。</p>\n<p>Golang 1.7 之前的 write barrier 使用的经典的 Dijkstra-style insertion write barrier [Dijkstra ‘78]， STW 的主要耗时就在 stack re-scan 的过程。自 1.8 之后采用一种混合的 write barrier 方式 （Yuasa-style deletion write barrier [Yuasa ‘90] 和 Dijkstra-style insertion write barrier [Dijkstra ‘78]）来避免 re-scan。具体的可以参考 <a href=\"https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\" target=\"_blank\" rel=\"noopener\">17503-eliminate-rescan</a>。</p>\n<h3 id=\"3-3-标记\"><a href=\"#3-3-标记\" class=\"headerlink\" title=\"3.3 标记\"></a>3.3 标记</h3><p>下面的源码还是基于 go1.8rc3。这个版本的 GC 代码相比之前改动还是挺大的，我们下面尽量只关注主流程。垃圾回收的代码主要集中在函数 <code>gcStart()</code> 中。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// gcStart 是 GC 的入口函数，根据 gcMode 做处理。</span></span><br><span class=\"line\"><span class=\"comment\">// 1. gcMode == gcBackgroundMode（后台运行，也就是并行）, _GCoff -&gt; _GCmark</span></span><br><span class=\"line\"><span class=\"comment\">// 2. 否则 GCoff -&gt; _GCmarktermination，这个时候就是主动 GC </span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-STW-phase-1\"><a href=\"#1-STW-phase-1\" class=\"headerlink\" title=\"1. STW phase 1\"></a>1. STW phase 1</h5><p>在 GC 开始之前的准备工作。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//在后台启动 mark worker </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        gcBgMarkStartWorkers()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">// Stop The World</span></span><br><span class=\"line\">    systemstack(stopTheWorldWithSema)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        <span class=\"comment\">// GC 开始前的准备工作</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//处理设置 GCPhase，setGCPhase 还会 enable write barrier</span></span><br><span class=\"line\">        setGCPhase(_GCmark)</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        gcBgMarkPrepare() <span class=\"comment\">// Must happen before assist enable.</span></span><br><span class=\"line\">        gcMarkRootPrepare()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Mark all active tinyalloc blocks. Since we're</span></span><br><span class=\"line\">        <span class=\"comment\">// allocating from these, they need to be black like</span></span><br><span class=\"line\">        <span class=\"comment\">// other allocations. The alternative is to blacken</span></span><br><span class=\"line\">        <span class=\"comment\">// the tiny block on every allocation from it, which</span></span><br><span class=\"line\">        <span class=\"comment\">// would slow down the tiny allocator.</span></span><br><span class=\"line\">        gcMarkTinyAllocs()</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        <span class=\"comment\">// Start The World</span></span><br><span class=\"line\">        systemstack(startTheWorldWithSema)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-Mark\"><a href=\"#2-Mark\" class=\"headerlink\" title=\"2. Mark\"></a>2. Mark</h5><p>Mark 阶段是并行的运行，通过在后台一直运行 mark worker 来实现。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcStart</span><span class=\"params\">(mode gcMode, forceTrigger <span class=\"keyword\">bool</span>)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//在后台启动 mark worker </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mode == gcBackgroundMode &#123;</span><br><span class=\"line\">        gcBgMarkStartWorkers()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcBgMarkStartWorkers</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Background marking is performed by per-P G's. Ensure that</span></span><br><span class=\"line\">    <span class=\"comment\">// each P has a background GC G.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _, p := <span class=\"keyword\">range</span> &amp;allp &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> p == <span class=\"literal\">nil</span> || p.status == _Pdead &#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> p.gcBgMarkWorker == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">go</span> gcBgMarkWorker(p)</span><br><span class=\"line\">            notetsleepg(&amp;work.bgMarkReady, <span class=\"number\">-1</span>)</span><br><span class=\"line\">            noteclear(&amp;work.bgMarkReady)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// gcBgMarkWorker 是一直在后台运行的，大部分时候是休眠状态，通过 gcController 来调度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcBgMarkWorker</span><span class=\"params\">(_p_ *p)</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 将当前 goroutine 休眠，直到满足某些条件</span></span><br><span class=\"line\">        gopark(...)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// mark 过程</span></span><br><span class=\"line\">        systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Mark our goroutine preemptible so its stack</span></span><br><span class=\"line\">        <span class=\"comment\">// can be scanned. This lets two mark workers</span></span><br><span class=\"line\">        <span class=\"comment\">// scan each other (otherwise, they would</span></span><br><span class=\"line\">        <span class=\"comment\">// deadlock). We must not modify anything on</span></span><br><span class=\"line\">        <span class=\"comment\">// the G stack. However, stack shrinking is</span></span><br><span class=\"line\">        <span class=\"comment\">// disabled for mark workers, so it is safe to</span></span><br><span class=\"line\">        <span class=\"comment\">// read from the G stack.</span></span><br><span class=\"line\">        casgstatus(gp, _Grunning, _Gwaiting)</span><br><span class=\"line\">        <span class=\"keyword\">switch</span> _p_.gcMarkWorkerMode &#123;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            throw(<span class=\"string\">\"gcBgMarkWorker: unexpected gcMarkWorkerMode\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerDedicatedMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerFractionalMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)</span><br><span class=\"line\">        <span class=\"keyword\">case</span> gcMarkWorkerIdleMode:</span><br><span class=\"line\">            gcDrain(&amp;_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        casgstatus(gp, _Gwaiting, _Grunning)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Mark 阶段的标记代码主要在函数 gcDrain() 中实现。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// gcDrain scans roots and objects in work buffers, blackening grey</span><br><span class=\"line\">// objects until all roots and work buffers have been drained.</span><br><span class=\"line\">func gcDrain(gcw *gcWork, flags gcDrainFlags) &#123;</span><br><span class=\"line\">    ...\t</span><br><span class=\"line\">    // Drain root marking jobs.</span><br><span class=\"line\">    if work.markrootNext &lt; work.markrootJobs &#123;</span><br><span class=\"line\">        for !(preemptible &amp;&amp; gp.preempt) &#123;</span><br><span class=\"line\">            job := atomic.Xadd(&amp;work.markrootNext, +1) - 1</span><br><span class=\"line\">            if job &gt;= work.markrootJobs &#123;</span><br><span class=\"line\">                break</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            markroot(gcw, job)</span><br><span class=\"line\">            if idle &amp;&amp; pollWork() &#123;</span><br><span class=\"line\">                goto done</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  \t</span><br><span class=\"line\">    // 处理 heap 标记</span><br><span class=\"line\">    // Drain heap marking jobs.</span><br><span class=\"line\">    for !(preemptible &amp;&amp; gp.preempt) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        //从灰色列队中取出对象</span><br><span class=\"line\">        var b uintptr</span><br><span class=\"line\">        if blocking &#123;</span><br><span class=\"line\">            b = gcw.get()</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            b = gcw.tryGetFast()</span><br><span class=\"line\">            if b == 0 &#123;</span><br><span class=\"line\">                b = gcw.tryGet()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if b == 0 &#123;</span><br><span class=\"line\">            // work barrier reached or tryGet failed.</span><br><span class=\"line\">            break</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        //扫描灰色对象的引用对象，标记为灰色，入灰色队列</span><br><span class=\"line\">        scanobject(b, gcw)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3-Mark-termination-STW-phase-2\"><a href=\"#3-Mark-termination-STW-phase-2\" class=\"headerlink\" title=\"3. Mark termination (STW phase 2)\"></a>3. Mark termination (STW phase 2)</h5><p>mark termination 阶段会 stop the world。函数实现在 <code>gcMarkTermination()</code>。1.8 版本已经不会再对 goroutine stack 进行 re-scan 了。细节有点多，这里不细说了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcMarkTermination</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// World is stopped.</span></span><br><span class=\"line\">    <span class=\"comment\">// Run gc on the g0 stack. We do this so that the g stack</span></span><br><span class=\"line\">    <span class=\"comment\">// we're currently running on will no longer change. Cuts</span></span><br><span class=\"line\">    <span class=\"comment\">// the root set down a bit (g0 stacks are not scanned, and</span></span><br><span class=\"line\">    <span class=\"comment\">// we don't need to scan gc's internal state).  We also</span></span><br><span class=\"line\">    <span class=\"comment\">// need to switch to g0 so we can shrink the stack.</span></span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        gcMark(startTime)</span><br><span class=\"line\">        <span class=\"comment\">// Must return immediately.</span></span><br><span class=\"line\">        <span class=\"comment\">// The outer function's stack may have moved</span></span><br><span class=\"line\">        <span class=\"comment\">// during gcMark (it shrinks stacks, including the</span></span><br><span class=\"line\">        <span class=\"comment\">// outer function's stack), so we must not refer</span></span><br><span class=\"line\">        <span class=\"comment\">// to any of its variables. Return back to the</span></span><br><span class=\"line\">        <span class=\"comment\">// non-system stack to pick up the new addresses</span></span><br><span class=\"line\">        <span class=\"comment\">// before continuing.</span></span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-4-清扫\"><a href=\"#3-4-清扫\" class=\"headerlink\" title=\"3.4 清扫\"></a>3.4 清扫</h3><p>清扫相对来说就简单很多了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gcSweep</span><span class=\"params\">(mode gcMode)</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"comment\">//阻塞式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> !_ConcurrentSweep || mode == gcForceBlockMode &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Special case synchronous sweep.</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"comment\">// Sweep all spans eagerly.</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> sweepone() != ^<span class=\"keyword\">uintptr</span>(<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            sweep.npausesweep++</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// Do an additional mProf_GC, because all 'free' events are now real as well.</span></span><br><span class=\"line\">        mProf_GC()</span><br><span class=\"line\">        mProf_GC()</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  \t</span><br><span class=\"line\">    <span class=\"comment\">// 并行式</span></span><br><span class=\"line\">    <span class=\"comment\">// Background sweep.</span></span><br><span class=\"line\">    lock(&amp;sweep.lock)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> sweep.parked &#123;</span><br><span class=\"line\">        sweep.parked = <span class=\"literal\">false</span></span><br><span class=\"line\">        ready(sweep.g, <span class=\"number\">0</span>, <span class=\"literal\">true</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    unlock(&amp;sweep.lock)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于并行式清扫，在 GC 初始化的时候就会启动 <code>bgsweep()</code>，然后在后台一直循环。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">bgsweep</span><span class=\"params\">(c <span class=\"keyword\">chan</span> <span class=\"keyword\">int</span>)</span></span> &#123;</span><br><span class=\"line\">    sweep.g = getg()</span><br><span class=\"line\"></span><br><span class=\"line\">    lock(&amp;sweep.lock)</span><br><span class=\"line\">    sweep.parked = <span class=\"literal\">true</span></span><br><span class=\"line\">    c &lt;- <span class=\"number\">1</span></span><br><span class=\"line\">    goparkunlock(&amp;sweep.lock, <span class=\"string\">\"GC sweep wait\"</span>, traceEvGoBlock, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> gosweepone() != ^<span class=\"keyword\">uintptr</span>(<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            sweep.nbgsweep++</span><br><span class=\"line\">            Gosched()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        lock(&amp;sweep.lock)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> !gosweepdone() &#123;</span><br><span class=\"line\">            <span class=\"comment\">// This can happen if a GC runs between</span></span><br><span class=\"line\">            <span class=\"comment\">// gosweepone returning ^0 above</span></span><br><span class=\"line\">            <span class=\"comment\">// and the lock being acquired.</span></span><br><span class=\"line\">            unlock(&amp;sweep.lock)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sweep.parked = <span class=\"literal\">true</span></span><br><span class=\"line\">        goparkunlock(&amp;sweep.lock, <span class=\"string\">\"GC sweep wait\"</span>, traceEvGoBlock, <span class=\"number\">1</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">gosweepone</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> ret <span class=\"keyword\">uintptr</span></span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">        ret = sweepone()</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>不管是阻塞式还是并行式，都是通过 <code>sweepone()</code>函数来做清扫工作的。如果对于上篇文章 <a href=\"http://legendtkl.com/2017/04/02/golang-alloc/\" target=\"_blank\" rel=\"noopener\">Golang 内存管理</a> 熟悉的话，这个地方就很好理解。内存管理都是基于 span 的，mheap<em> 是一个全局的变量，所有分配的对象都会记录在 mheap</em> 中。在标记的时候，我们只要找到对对象对应的 span 进行标记，清扫的时候扫描 span，没有标记的 span 就可以回收了。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// sweeps one span</span></span><br><span class=\"line\"><span class=\"comment\">// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">sweepone</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        s := mheap_.sweepSpans[<span class=\"number\">1</span>-sg/<span class=\"number\">2</span>%<span class=\"number\">2</span>].pop()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        <span class=\"keyword\">if</span> !s.sweep(<span class=\"literal\">false</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Span is still in-use, so this returned no</span></span><br><span class=\"line\">            <span class=\"comment\">// pages to the heap and the span needs to</span></span><br><span class=\"line\">            <span class=\"comment\">// move to the swept in-use list.</span></span><br><span class=\"line\">            npages = <span class=\"number\">0</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Sweep frees or collects finalizers for blocks not marked in the mark phase.</span></span><br><span class=\"line\"><span class=\"comment\">// It clears the mark bits in preparation for the next GC round.</span></span><br><span class=\"line\"><span class=\"comment\">// Returns true if the span was returned to heap.</span></span><br><span class=\"line\"><span class=\"comment\">// If preserve=true, don't return it to heap nor relink in MCentral lists;</span></span><br><span class=\"line\"><span class=\"comment\">// caller takes care of it.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *mspan)</span> <span class=\"title\">sweep</span><span class=\"params\">(preserve <span class=\"keyword\">bool</span>)</span> <span class=\"title\">bool</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-5-其他\"><a href=\"#3-5-其他\" class=\"headerlink\" title=\"3.5 其他\"></a>3.5 其他</h3><h5 id=\"1-gcWork\"><a href=\"#1-gcWork\" class=\"headerlink\" title=\"1. gcWork\"></a>1. gcWork</h5><p>这里介绍一下任务队列，或者说灰色对象管理。每个 P 上都有一个 gcw 用来管理灰色对象（get 和 put），gcw 的结构就是 gcWork。gcWork 中的核心是 wbuf1 和 wbuf2，里面存储就是灰色对象，或者说是 work（下面就全部统一叫做 work）。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> p <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    gcw gcWork</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> gcWork <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// wbuf1 and wbuf2 are the primary and secondary work buffers.</span></span><br><span class=\"line\">    wbuf1, wbuf2 wbufptr</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Bytes marked (blackened) on this gcWork. This is aggregated</span></span><br><span class=\"line\">    <span class=\"comment\">// into work.bytesMarked by dispose.</span></span><br><span class=\"line\">    bytesMarked <span class=\"keyword\">uint64</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Scan work performed on this gcWork. This is aggregated into</span></span><br><span class=\"line\">    <span class=\"comment\">// gcController by dispose and may also be flushed by callers.</span></span><br><span class=\"line\">    scanWork <span class=\"keyword\">int64</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>既然每个 P 上有一个 work buffer，那么是不是还有一个全局的 work list 呢？是的。通过在每个 P 上绑定一个 work buffer 的好处和 cache 一样，不需要加锁。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> work <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    full  <span class=\"keyword\">uint64</span>                   <span class=\"comment\">// lock-free list of full blocks workbuf</span></span><br><span class=\"line\">    empty <span class=\"keyword\">uint64</span>                   <span class=\"comment\">// lock-free list of empty blocks workbuf</span></span><br><span class=\"line\">    pad0  [sys.CacheLineSize]<span class=\"keyword\">uint8</span> <span class=\"comment\">// prevents false-sharing between full/empty and nproc/nwait</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>那么为什么使用两个 work buffer （wbuf1 和 wbuf2）呢？我下面举个例子。比如我现在要 get 一个 work 出来，先从 wbuf1 中取，wbuf1 为空的话则与 wbuf2 swap 再 get。在其他时间将 work buffer 中的 full 或者 empty buffer 移到 global 的 work 中。这样的好处在于，在 get 的时候去全局的 work 里面取（多个 goroutine 去取会有竞争）。这里有趣的是 global 的 work list 是 lock-free 的，通过原子操作 cas 等实现。下面列举几个函数看一下 gcWrok。</p>\n<p>初始化。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">init</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    w.wbuf1 = wbufptrOf(getempty())</span><br><span class=\"line\">    wbuf2 := trygetfull()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf2 == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        wbuf2 = getempty()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    w.wbuf2 = wbufptrOf(wbuf2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>put。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// put enqueues a pointer for the garbage collector to trace.</span></span><br><span class=\"line\"><span class=\"comment\">// obj must point to the beginning of a heap object or an oblet.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">put</span><span class=\"params\">(obj <span class=\"keyword\">uintptr</span>)</span></span> &#123;</span><br><span class=\"line\">    wbuf := w.wbuf1.ptr()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        w.init()</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"comment\">// wbuf is empty at this point.</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"built_in\">len</span>(wbuf.obj) &#123;</span><br><span class=\"line\">        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"built_in\">len</span>(wbuf.obj) &#123;</span><br><span class=\"line\">            putfull(wbuf)</span><br><span class=\"line\">            wbuf = getempty()</span><br><span class=\"line\">            w.wbuf1 = wbufptrOf(wbuf)</span><br><span class=\"line\">            flushed = <span class=\"literal\">true</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    wbuf.obj[wbuf.nobj] = obj</span><br><span class=\"line\">    wbuf.nobj++</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>get。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// get dequeues a pointer for the garbage collector to trace, blocking</span></span><br><span class=\"line\"><span class=\"comment\">// if necessary to ensure all pointers from all queues and caches have</span></span><br><span class=\"line\"><span class=\"comment\">// been retrieved.  get returns 0 if there are no pointers remaining.</span></span><br><span class=\"line\"><span class=\"comment\">//go:nowritebarrier</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(w *gcWork)</span> <span class=\"title\">get</span><span class=\"params\">()</span> <span class=\"title\">uintptr</span></span> &#123;</span><br><span class=\"line\">    wbuf := w.wbuf1.ptr()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">        w.init()</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"comment\">// wbuf is empty at this point.</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">        w.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1</span><br><span class=\"line\">        wbuf = w.wbuf1.ptr()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> wbuf.nobj == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            owbuf := wbuf</span><br><span class=\"line\">            wbuf = getfull()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> wbuf == <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            putempty(owbuf)</span><br><span class=\"line\">            w.wbuf1 = wbufptrOf(wbuf)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> This might be a good place to add prefetch code</span></span><br><span class=\"line\"></span><br><span class=\"line\">    wbuf.nobj--</span><br><span class=\"line\">    <span class=\"keyword\">return</span> wbuf.obj[wbuf.nobj]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-forcegc\"><a href=\"#2-forcegc\" class=\"headerlink\" title=\"2. forcegc\"></a>2. forcegc</h5><p>我们上面讲了两种 GC 触发方式：自动检测和用户主动调用。除此之后 Golang 本身还会对运行状态进行监控，如果超过两分钟没有 GC，则触发 GC。监控函数是 <code>sysmon()</code>，在主 goroutine 中启动。</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// The main goroutine</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    systemstack(<span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">      \tnewm(sysmon, <span class=\"literal\">nil</span>)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// Always runs without a P, so write barriers are not allowed.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">sysmon</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">        now := nanotime()</span><br><span class=\"line\">        unixnow := unixnanotime()</span><br><span class=\"line\">      \t</span><br><span class=\"line\">        lastgc := <span class=\"keyword\">int64</span>(atomic.Load64(&amp;memstats.last_gc))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> gcphase == _GCoff &amp;&amp; lastgc != <span class=\"number\">0</span> &amp;&amp; unixnow-lastgc &gt; forcegcperiod &amp;&amp; atomic.Load(&amp;forcegc.idle) != <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            lock(&amp;forcegc.lock)</span><br><span class=\"line\">            forcegc.idle = <span class=\"number\">0</span></span><br><span class=\"line\">            forcegc.g.schedlink = <span class=\"number\">0</span></span><br><span class=\"line\">            injectglist(forcegc.g)\t<span class=\"comment\">// 将 forcegc goroutine 加入 runnable queue</span></span><br><span class=\"line\">            unlock(&amp;forcegc.lock)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> forcegcperiod <span class=\"keyword\">int64</span> = <span class=\"number\">2</span> * <span class=\"number\">60</span> *<span class=\"number\">1e9</span>\t<span class=\"comment\">//两分钟</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"4-参考资料\"><a href=\"#4-参考资料\" class=\"headerlink\" title=\"4.参考资料\"></a>4.参考资料</h2><ol>\n<li>《Go 语言学习笔记》</li>\n<li><a href=\"https://book.douban.com/subject/1157908/\" target=\"_blank\" rel=\"noopener\">《垃圾收集》 - 豆瓣</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Tracing_garbage_collection#Na.C3.AFve_mark-and-sweep\" target=\"_blank\" rel=\"noopener\">Tracing Garbage Collection - wikipedia</a></li>\n<li>《On-the-fly garbage collection: an exercise in cooperation.》 — Edsger W. Dijkstra, Leslie Lamport, A. J. Martin</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\" target=\"_blank\" rel=\"noopener\">Garbage Collection</a>)</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Tracing_garbage_collection\" target=\"_blank\" rel=\"noopener\">Tracing Garbage Collection</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=P1rU_9IB414\" target=\"_blank\" rel=\"noopener\">Copying Garbage Collection – youtube</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=pJHISaOW6Vc\" target=\"_blank\" rel=\"noopener\">Generational Garbage Collection – youtube</a></li>\n<li><a href=\"https://talks.golang.org/2015/go-gc.pdf\" target=\"_blank\" rel=\"noopener\">golang gc talk</a></li>\n<li><a href=\"https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\" target=\"_blank\" rel=\"noopener\">17503-eliminate-rescan</a></li>\n</ol>"},{"title":"kafka高可用设计(下)","date":"2019-02-14T15:01:22.000Z","_content":"\n\n本文转发自[**技术世界**](http://www.jasongj.com/)，[原文链接](http://www.jasongj.com/2015/06/08/KafkaColumn3)　<http://www.jasongj.com/2015/06/08/KafkaColumn3>\n\n本文在上篇文章 基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。\n\n# 摘要\n\n　　本文在上篇文章基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。\n\n# Broker Failover过程\n\n## Controller对Broker failure的处理过程\n\n1. Controller在Zookeeper的`/brokers/ids`节点上注册Watch。一旦有Broker宕机（本文用宕机代表任何让Kafka认为其Broker die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的Znode会自动被删除，Zookeeper会fire Controller注册的Watch，Controller即可获取最新的幸存的Broker列表。\n2. Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition。\n3. 对set_p中的每一个Partition：\n   　　3.1 从`/brokers/topics/[topic]/partitions/[partition]/state`读取该Partition当前的ISR。\n   　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。\n   　　3.3 将新的Leader，ISR和新的`leader_epoch`及`controller_epoch`写入`/brokers/topics/[topic]/partitions/[partition]/state`。注意，该操作只有Controller版本在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1。\n4. 直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。\n   　　Broker failover顺序图如下所示。\n   [![broker failover sequence diagram ](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)\n\n　　LeaderAndIsrRequest结构如下\n[![LeaderAndIsrRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png)\n\n　　LeaderAndIsrResponse结构如下\n[![LeaderAndIsrResponse](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png)\n\n<!--more-->\n\n## 创建/删除Topic\n\n1. Controller在Zookeeper的`/brokers/topics`节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。\n2. 对于删除Topic操作，Topic工具会将该Topic名字存于`/admin/delete_topics`。若`delete.topic.enable`为true，则Controller注册在`/admin/delete_topics`上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest；若为false则Controller不会在`/admin/delete_topics`上注册Watch，也就不会对该事件作出反应，此时Topic操作只被记录而不会被执行。\n3. 对于创建Topic操作，Controller从`/brokers/ids`读取当前所有可用的Broker列表，对于set_p中的每一个Partition：\n   　　3.1 从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）\n   　　3.2 将新的Leader和ISR写入`/brokers/topics/[topic]/partitions/[partition]`\n4. 直接通过RPC向相关的Broker发送LeaderAndISRRequest。\n   　　创建Topic顺序图如下所示。\n   [![create topic sequence diagram](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png)\n\n## Broker响应请求流程\n\n　　Broker通过`kafka.network.SocketServer`及相关模块接受各种请求并作出响应。整个网络通信模块基于Java NIO开发，并采用Reactor模式，其中包含1个Acceptor负责接受客户请求，N个Processor负责读写数据，M个Handler处理业务逻辑。\n　　Acceptor的主要职责是监听并接受客户端（请求发起方，包括但不限于Producer，Consumer，Controller，Admin Tool）的连接请求，并建立和客户端的数据传输通道，然后为该客户端指定一个Processor，至此它对该客户端该次请求的任务就结束了，它可以去响应下一个客户端的连接请求了。其核心代码如下。\n[![Kafka SocketServer Acceptor_run](http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png)\n　　\n　　Processor主要负责从客户端读取数据并将响应返回给客户端，它本身并不处理具体的业务逻辑，并且其内部维护了一个队列来保存分配给它的所有SocketChannel。Processor的run方法会循环从队列中取出新的SocketChannel并将其`SelectionKey.OP_READ`注册到selector上，然后循环处理已就绪的读（请求）和写（响应）。Processor读取完数据后，将其封装成Request对象并将其交给RequestChannel。\n　　RequestChannel是Processor和KafkaRequestHandler交换数据的地方，它包含一个队列requestQueue用来存放Processor加入的Request，KafkaRequestHandler会从里面取出Request来处理；同时它还包含一个respondQueue，用来存放KafkaRequestHandler处理完Request后返还给客户端的Response。\n　　Processor会通过processNewResponses方法依次将requestChannel中responseQueue保存的Response取出，并将对应的`SelectionKey.OP_WRITE`事件注册到selector上。当selector的select方法返回时，对检测到的可写通道，调用write方法将Response返回给客户端。\n　　KafkaRequestHandler循环从RequestChannel中取Request并交给`kafka.server.KafkaApis`处理具体的业务逻辑。\n\n## LeaderAndIsrRequest响应过程\n\n　　对于收到的LeaderAndIsrRequest，Broker主要通过ReplicaManager的becomeLeaderOrFollower处理，流程如下：\n\n1. 若请求中controllerEpoch小于当前最新的controllerEpoch，则直接返回ErrorMapping.StaleControllerEpochCode。\n2. 对于请求中partitionStateInfos中的每一个元素，即（(topic, partitionId), partitionStateInfo)：\n   　　2.1 若partitionStateInfo中的leader epoch大于当前ReplicManager中存储的(topic, partitionId)对应的partition的leader epoch，则：\n   　　　　2.1.1 若当前brokerid（或者说replica id）在partitionStateInfo中，则将该partition及partitionStateInfo存入一个名为partitionState的HashMap中\n   　　　　2.1.2否则说明该Broker不在该Partition分配的Replica list中，将该信息记录于log中\n   　　2.2否则将相应的Error code（ErrorMapping.StaleLeaderEpochCode）存入Response中\n3. 筛选出partitionState中Leader与当前Broker ID相等的所有记录存入partitionsTobeLeader中，其它记录存入partitionsToBeFollower中。\n4. 若partitionsTobeLeader不为空，则对其执行makeLeaders方。\n5. 若partitionsToBeFollower不为空，则对其执行makeFollowers方法。\n6. 若highwatermak线程还未启动，则将其启动，并将hwThreadInitialized设为true。\n7. 关闭所有Idle状态的Fetcher。\n\n　　LeaderAndIsrRequest处理过程如下图所示\n[![LeaderAndIsrRequest Flow Chart](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png)\n\n## Broker启动过程\n\n　　Broker启动后首先根据其ID在Zookeeper的`/brokers/ids`zonde下创建临时子节点（[Ephemeral node](http://zookeeper.apache.org/doc/trunk/zookeeperOver.html#Nodes+and+ephemeral+nodes)），创建成功后Controller的ReplicaStateMachine注册其上的Broker Change Watch会被fire，从而通过回调KafkaController.onBrokerStartup方法完成以下步骤：\n\n1. 向所有新启动的Broker发送UpdateMetadataRequest，其定义如下。\n   [![UpdateMetadataRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png)\n2. 将新启动的Broker上的所有Replica设置为OnlineReplica状态，同时这些Broker会为这些Partition启动high watermark线程。\n3. 通过partitionStateMachine触发OnlinePartitionStateChange。\n\n## Controller Failover\n\nController也需要Failover。每个Broker都会在Controller Path (`/controller`)上注册一个Watch。当前Controller失败时，对应的Controller Path会自动消失（因为它是Ephemeral Node），此时该Watch被fire，所有“活”着的Broker都会去竞选成为新的Controller（创建新的Controller Path），但是只会有一个竞选成功（这点由Zookeeper保证）。竞选成功者即为新的Leader，竞选失败者则重新在新的Controller Path上注册Watch。因为[Zookeeper的Watch是一次性的，被fire一次之后即失效](http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkWatches)，所以需要重新注册。\n\nBroker成功竞选为新Controller后会触发KafkaController.onControllerFailover方法，并在该方法中完成如下操作：\n\n1. 读取并增加Controller Epoch。\n2. 在ReassignedPartitions Path(`/admin/reassign_partitions`)上注册Watch。\n3. 在PreferredReplicaElection Path(`/admin/preferred_replica_election`)上注册Watch。\n4. 通过partitionStateMachine在Broker Topics Patch(`/brokers/topics`)上注册Watch。\n5. 若`delete.topic.enable`设置为true（默认值是false），则partitionStateMachine在Delete Topic Patch(`/admin/delete_topics`)上注册Watch。\n6. 通过replicaStateMachine在Broker Ids Patch(`/brokers/ids`)上注册Watch。\n7. 初始化ControllerContext对象，设置当前所有Topic，“活”着的Broker列表，所有Partition的Leader及ISR等。\n8. 启动replicaStateMachine和partitionStateMachine。\n9. 将brokerState状态设置为RunningAsController。\n10. 将每个Partition的Leadership信息发送给所有“活”着的Broker。\n11. 若`auto.leader.rebalance.enable`配置为true（默认值是true），则启动partition-rebalance线程。\n12. 若`delete.topic.enable`设置为true且Delete Topic Patch(`/admin/delete_topics`)中有值，则删除相应的Topic。\n\n## Partition重新分配\n\n　　管理工具发出重新分配Partition请求后，会将相应信息写到`/admin/reassign_partitions`上，而该操作会触发ReassignedPartitionsIsrChangeListener，从而通过执行回调函数KafkaController.onPartitionReassignment来完成以下操作：\n\n1. 将Zookeeper中的AR（Current Assigned Replicas）更新为OAR（Original list of replicas for partition） + RAR（Reassigned replicas）。\n2. 强制更新Zookeeper中的leader epoch，向AR中的每个Replica发送LeaderAndIsrRequest。\n3. 将RAR - OAR中的Replica设置为NewReplica状态。\n4. 等待直到RAR中所有的Replica都与其Leader同步。\n5. 将RAR中所有的Replica都设置为OnlineReplica状态。\n6. 将Cache中的AR设置为RAR。\n7. 若Leader不在RAR中，则从RAR中重新选举出一个新的Leader并发送LeaderAndIsrRequest。若新的Leader不是从RAR中选举而出，则还要增加Zookeeper中的leader epoch。\n8. 将OAR - RAR中的所有Replica设置为OfflineReplica状态，该过程包含两部分。第一，将Zookeeper上ISR中的OAR - RAR移除并向Leader发送LeaderAndIsrRequest从而通知这些Replica已经从ISR中移除；第二，向OAR - RAR中的Replica发送StopReplicaRequest从而停止不再分配给该Partition的Replica。\n9. 将OAR - RAR中的所有Replica设置为NonExistentReplica状态从而将其从磁盘上删除。\n10. 将Zookeeper中的AR设置为RAR。\n11. 删除`/admin/reassign_partition`。\n    　　\n    **注意**：最后一步才将Zookeeper中的AR更新，因为这是唯一一个持久存储AR的地方，如果Controller在这一步之前crash，新的Controller仍然能够继续完成该过程。\n    　　以下是Partition重新分配的案例，OAR = ｛1，2，3｝，RAR = ｛4，5，6｝，Partition重新分配过程中Zookeeper中的AR和Leader/ISR路径如下\n\n| AR            | leader/isr      | Step            |\n| ------------- | --------------- | --------------- |\n| {1,2,3}       | 1/{1,2,3}       | (initial state) |\n| {1,2,3,4,5,6} | 1/{1,2,3}       | (step 2)        |\n| {1,2,3,4,5,6} | 1/{1,2,3,4,5,6} | (step 4)        |\n| {1,2,3,4,5,6} | 4/{1,2,3,4,5,6} | (step 7)        |\n| {1,2,3,4,5,6} | 4/{4,5,6}       | (step 8)        |\n| {4,5,6}       | 4/{4,5,6}       | (step 10)       |\n\n## Follower从Leader Fetch数据\n\n　　Follower通过向Leader发送FetchRequest获取消息，FetchRequest结构如下\n[![FetchRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png)\n　　从FetchRequest的结构可以看出，每个Fetch请求都要指定最大等待时间和最小获取字节数，以及由TopicAndPartition和PartitionFetchInfo构成的Map。实际上，Follower从Leader数据和Consumer从Broker Fetch数据，都是通过FetchRequest请求完成，所以在FetchRequest结构中，其中一个字段是clientID，并且其默认值是ConsumerConfig.DefaultClientId。\n　　\n　　Leader收到Fetch请求后，Kafka通过KafkaApis.handleFetchRequest响应该请求，响应过程如下：\n\n1. replicaManager根据请求读出数据存入dataRead中。\n2. 如果该请求来自Follower则更新其相应的LEO（log end offset）以及相应Partition的High Watermark\n3. 根据dataRead算出可读消息长度（单位为字节）并存入bytesReadable中。\n4. 满足下面4个条件中的1个，则立即将相应的数据返回\n\n- Fetch请求不希望等待，即fetchRequest.macWait <= 0\n- Fetch请求不要求一定能取到消息，即fetchRequest.numPartitions <= 0，也即requestInfo为空\n- 有足够的数据可供返回，即bytesReadable >= fetchRequest.minBytes\n- 读取数据时发生异常\n\n1. 若不满足以上4个条件，FetchRequest将不会立即返回，并将该请求封装成DelayedFetch。检查该DeplayedFetch是否满足，若满足则返回请求，否则将该请求加入Watch列表\n\n　　Leader通过以FetchResponse的形式将消息返回给Follower，FetchResponse结构如下\n[![FetchResponse](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png)\n\n\\#Replication工具\n\n## Topic Tool\n\n　　`$KAFKA_HOME/bin/kafka-topics.sh`，该工具可用于创建、删除、修改、查看某个Topic，也可用于列出所有Topic。另外，该工具还可修改以下配置。\n\n```\nunclean.leader.election.enable\ndelete.retention.ms\nsegment.jitter.ms\nretention.ms\nflush.ms\nsegment.bytes\nflush.messages\nsegment.ms\nretention.bytes\ncleanup.policy\nsegment.index.bytes\nmin.cleanable.dirty.ratio\nmax.message.bytes\nfile.delete.delay.ms\nmin.insync.replicas\nindex.interval.bytes\n```\n\n\n\n## Replica Verification Tool\n\n　　`$KAFKA_HOME/bin/kafka-replica-verification.sh`，该工具用来验证所指定的一个或多个Topic下每个Partition对应的所有Replica是否都同步。可通过`topic-white-list`这一参数指定所需要验证的所有Topic，支持正则表达式。 　　\n\n## Preferred Replica Leader Election Tool\n\n**用途**\n　　有了Replication机制后，每个Partition可能有多个备份。某个Partition的Replica列表叫作AR（Assigned Replicas），AR中的第一个Replica即为“Preferred Replica”。创建一个新的Topic或者给已有Topic增加Partition时，Kafka保证Preferred Replica被均匀分布到集群中的所有Broker上。理想情况下，Preferred Replica会被选为Leader。以上两点保证了所有Partition的Leader被均匀分布到了集群当中，这一点非常重要，因为所有的读写操作都由Leader完成，若Leader分布过于集中，会造成集群负载不均衡。但是，随着集群的运行，该平衡可能会因为Broker的宕机而被打破，该工具就是用来帮助恢复Leader分配的平衡。\n　　事实上，每个Topic从失败中恢复过来后，它默认会被设置为Follower角色，除非某个Partition的Replica全部宕机，而当前Broker是该Partition的AR中第一个恢复回来的Replica。因此，某个Partition的Leader（Preferred Replica）宕机并恢复后，它很可能不再是该Partition的Leader，但仍然是Preferred Replica。\n　　\n**原理**\n\n1. 在Zookeeper上创建`/admin/preferred_replica_election`节点，并存入需要调整Preferred Replica的Partition信息。\n2. Controller一直Watch该节点，一旦该节点被创建，Controller会收到通知，并获取该内容。\n3. Controller读取Preferred Replica，如果发现该Replica当前并非是Leader并且它在该Partition的ISR中，Controller向该Replica发送LeaderAndIsrRequest，使该Replica成为Leader。如果该Replica当前并非是Leader，且不在ISR中，Controller为了保证没有数据丢失，并不会将其设置为Leader。 　\n\n**用法**\n　　`$KAFKA_HOME/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181`\n\n　　在包含8个Broker的Kafka集群上，创建1个名为topic1，replication-factor为3，Partition数为8的Topic，使用`$KAFKA_HOME/bin/kafka-topics.sh --describe --topic topic1 --zookeeper localhost:2181`命令查看其Partition/Replica分布。\n\n　　查询结果如下图所示，从图中可以看到，Kafka将所有Replica均匀分布到了整个集群，并且Leader也均匀分布。\n[![preferred_topic_test_1](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png)\n\n　　手动停止部分Broker，topic1的Partition/Replica分布如下图所示。从图中可以看到，由于Broker 1/2/4都被停止，Partition 0的Leader由原来的1变为3，Partition 1的Leader由原来的2变为5，Partition 2的Leader由原来的3变为6，Partition 3的Leader由原来的4变为7。\n[![preferred_topic_test_2](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png)　　\n　　\n　　再重新启动ID为1的Broker，topic1的Partition/Replica分布如下。可以看到，虽然Broker 1已经启动（Partition 0和Partition5的ISR中有1），但是1并不是任何一个Parititon的Leader，而Broker 5/6/7都是2个Partition的Leader，即Leader的分布不均衡——一个Broker最多是2个Partition的Leader，而最少是0个Partition的Leader。\n[![preferred_topic_test_3](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png)\n　　\n　　运行该工具后，topic1的Partition/Replica分布如下图所示。由图可见，除了Partition 1和Partition 3由于Broker 2和Broker 4还未启动，所以其Leader不是其Preferred Repliac外，其它所有Partition的Leader都是其Preferred Replica。同时，与运行该工具前相比，Leader的分配更均匀——一个Broker最多是2个Parittion的Leader，最少是1个Partition的Leader。\n[![preferred_topic_test_4](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png)\n　　\n　　启动Broker 2和Broker 4，Leader分布与上一步相比并未变化，如下图所示。\n[![preferred_topic_test_5](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png)\n\n　　再次运行该工具，所有Partition的Leader都由其Preferred Replica承担，Leader分布更均匀——每个Broker承担1个Partition的Leader角色。\n　　\n　　除了手动运行该工具使Leader分配均匀外，Kafka还提供了自动平衡Leader分配的功能，该功能可通过将`auto.leader.rebalance.enable`设置为true开启，它将周期性检查Leader分配是否平衡，若不平衡度超过一定阈值则自动由Controller尝试将各Partition的Leader设置为其Preferred Replica。检查周期由`leader.imbalance.check.interval.seconds`指定，不平衡度阈值由`leader.imbalance.per.broker.percentage`指定。 　　\n\n## Kafka Reassign Partitions Tool\n\n**用途**\n　　该工具的设计目标与Preferred Replica Leader Election Tool有些类似，都旨在促进Kafka集群的负载均衡。不同的是，Preferred Replica Leader Election只能在Partition的AR范围内调整其Leader，使Leader分布均匀，而该工具还可以调整Partition的AR。\n　　Follower需要从Leader Fetch数据以保持与Leader同步，所以仅仅保持Leader分布的平衡对整个集群的负载均衡来说是不够的。另外，生产环境下，随着负载的增大，可能需要给Kafka集群扩容。向Kafka集群中增加Broker非常简单方便，但是对于已有的Topic，并不会自动将其Partition迁移到新加入的Broker上，此时可用该工具达到此目的。某些场景下，实际负载可能远小于最初预期负载，此时可用该工具将分布在整个集群上的Partition重装分配到某些机器上，然后可以停止不需要的Broker从而实现节约资源的目的。\n　　需要说明的是，该工具不仅可以调整Partition的AR位置，还可调整其AR数量，即改变该Topic的replication factor。\n　　\n**原理**\n　　该工具只负责将所需信息存入Zookeeper中相应节点，然后退出，不负责相关的具体操作，所有调整都由Controller完成。\n\n1. 在Zookeeper上创建`/admin/reassign_partitions`节点，并存入目标Partition列表及其对应的目标AR列表。\n2. Controller注册在`/admin/reassign_partitions`上的Watch被fire，Controller获取该列表。\n3. 对列表中的所有Partition，Controller会做如下操作：\n\n- 启动`RAR - AR`中的Replica，即新分配的Replica。（RAR = Reassigned Replicas， AR = Assigned Replicas）\n- 等待新的Replica与Leader同步\n- 如果Leader不在RAR中，从RAR中选出新的Leader\n- 停止并删除`AR - RAR`中的Replica，即不再需要的Replica\n- 删除`/admin/reassign_partitions`节点\n\n**用法**\n　　该工具有三种使用模式\n\n- generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）\n- execute模式，根据指定的reassign plan重新分配Partition\n- verify模式，验证重新分配Partition是否成功\n\n　　下面这个例子将使用该工具将Topic的所有Partition重新分配到Broker 4/5/6/7上，步骤如下：\n\n1. 使用generate模式，生成reassign plan。指定需要重新分配的Topic （{“topics”:[{“topic”:”topic1”}],”version”:1}），并存入\n\n   ```\n   /tmp/topics-to-move.json\n   ```\n\n   文件中，然后执行\n\n   ```\n   $KAFKA_HOME/bin/kafka-reassign-partitions.sh \n   \t--zookeeper localhost:2181 \n   \t--topics-to-move-json-file /tmp/topics-to-move.json  \n   \t--broker-list \"4,5,6,7\" --generate\n   ```\n\n　　结果如下图所示\n[![reassign_1](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png)\n　　\n2.　使用execute模式，执行reassign plan\n　　将上一步生成的reassignment plan存入`/tmp/reassign-plan.json`文件中，并执行\n\n```\n   $KAFKA_HOME/bin/kafka-reassign-partitions.sh \n--zookeeper localhost:2181     \n--reassignment-json-file /tmp/reassign-plan.json --execute\n```\n\n\n\n[![reassign_2](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png)\n\n　　此时，Zookeeper上`/admin/reassign_partitions`节点被创建，且其值与`/tmp/reassign-plan.json`文件的内容一致。\n[![reassign_3](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png)\n\n3.　使用verify模式，验证reassign是否完成。执行verify命令\n\n```\n$KAFKA_HOME/bin/kafka-reassign-partitions.sh \n--zookeeper localhost:2181 --verify\n--reassignment-json-file /tmp/reassign-plan.json\n```\n\n\n\n　　结果如下所示，从图中可看出topic1的所有Partititon都重新分配成功。\n[![reassign_4](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png)\n\n　　接下来用Topic Tool再次验证。\n\n```\nbin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic1\n```\n\n\n\n　　结果如下图所示，从图中可看出topic1的所有Partition都被重新分配到Broker 4/5/6/7，且每个Partition的AR与reassign plan一致。\n[![reassign_5](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png)\n\n　　需要说明的是，在使用execute之前，并不一定要使用generate模式自动生成reassign plan，使用generate模式只是为了方便。事实上，某些场景下，generate模式生成的reassign plan并不一定能满足需求，此时用户可以自己设置reassign plan。 　　\n\n## State Change Log Merge Tool\n\n**用途**\n　　该工具旨在从整个集群的Broker上收集状态改变日志，并生成一个集中的格式化的日志以帮助诊断状态改变相关的故障。每个Broker都会将其收到的状态改变相关的的指令存于名为`state-change.log`的日志文件中。某些情况下，Partition的Leader Election可能会出现问题，此时我们需要对整个集群的状态改变有个全局的了解从而诊断故障并解决问题。该工具将集群中相关的`state-change.log`日志按时间顺序合并，同时支持用户输入时间范围和目标Topic及Partition作为过滤条件，最终将格式化的结果输出。\n　　\n**用法**\n\n```\nbin/kafka-run-class.sh kafka.tools.StateChangeLogMerger\n--logs /opt/kafka_2.11-0.8.2.1/logs/state-change.log\n--topic topic1 --partitions 0,1,2,3,4,5,6,7\n```","source":"_posts/kafka高可用设计(下).md","raw":"---\ntitle: kafka高可用设计(下)\ndate: 2019-02-14 23:01:22\ntags: kafka\n---\n\n\n本文转发自[**技术世界**](http://www.jasongj.com/)，[原文链接](http://www.jasongj.com/2015/06/08/KafkaColumn3)　<http://www.jasongj.com/2015/06/08/KafkaColumn3>\n\n本文在上篇文章 基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。\n\n# 摘要\n\n　　本文在上篇文章基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。\n\n# Broker Failover过程\n\n## Controller对Broker failure的处理过程\n\n1. Controller在Zookeeper的`/brokers/ids`节点上注册Watch。一旦有Broker宕机（本文用宕机代表任何让Kafka认为其Broker die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的Znode会自动被删除，Zookeeper会fire Controller注册的Watch，Controller即可获取最新的幸存的Broker列表。\n2. Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition。\n3. 对set_p中的每一个Partition：\n   　　3.1 从`/brokers/topics/[topic]/partitions/[partition]/state`读取该Partition当前的ISR。\n   　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。\n   　　3.3 将新的Leader，ISR和新的`leader_epoch`及`controller_epoch`写入`/brokers/topics/[topic]/partitions/[partition]/state`。注意，该操作只有Controller版本在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1。\n4. 直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。\n   　　Broker failover顺序图如下所示。\n   [![broker failover sequence diagram ](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png)\n\n　　LeaderAndIsrRequest结构如下\n[![LeaderAndIsrRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png)\n\n　　LeaderAndIsrResponse结构如下\n[![LeaderAndIsrResponse](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png)\n\n<!--more-->\n\n## 创建/删除Topic\n\n1. Controller在Zookeeper的`/brokers/topics`节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。\n2. 对于删除Topic操作，Topic工具会将该Topic名字存于`/admin/delete_topics`。若`delete.topic.enable`为true，则Controller注册在`/admin/delete_topics`上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest；若为false则Controller不会在`/admin/delete_topics`上注册Watch，也就不会对该事件作出反应，此时Topic操作只被记录而不会被执行。\n3. 对于创建Topic操作，Controller从`/brokers/ids`读取当前所有可用的Broker列表，对于set_p中的每一个Partition：\n   　　3.1 从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）\n   　　3.2 将新的Leader和ISR写入`/brokers/topics/[topic]/partitions/[partition]`\n4. 直接通过RPC向相关的Broker发送LeaderAndISRRequest。\n   　　创建Topic顺序图如下所示。\n   [![create topic sequence diagram](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png)](http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png)\n\n## Broker响应请求流程\n\n　　Broker通过`kafka.network.SocketServer`及相关模块接受各种请求并作出响应。整个网络通信模块基于Java NIO开发，并采用Reactor模式，其中包含1个Acceptor负责接受客户请求，N个Processor负责读写数据，M个Handler处理业务逻辑。\n　　Acceptor的主要职责是监听并接受客户端（请求发起方，包括但不限于Producer，Consumer，Controller，Admin Tool）的连接请求，并建立和客户端的数据传输通道，然后为该客户端指定一个Processor，至此它对该客户端该次请求的任务就结束了，它可以去响应下一个客户端的连接请求了。其核心代码如下。\n[![Kafka SocketServer Acceptor_run](http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png)\n　　\n　　Processor主要负责从客户端读取数据并将响应返回给客户端，它本身并不处理具体的业务逻辑，并且其内部维护了一个队列来保存分配给它的所有SocketChannel。Processor的run方法会循环从队列中取出新的SocketChannel并将其`SelectionKey.OP_READ`注册到selector上，然后循环处理已就绪的读（请求）和写（响应）。Processor读取完数据后，将其封装成Request对象并将其交给RequestChannel。\n　　RequestChannel是Processor和KafkaRequestHandler交换数据的地方，它包含一个队列requestQueue用来存放Processor加入的Request，KafkaRequestHandler会从里面取出Request来处理；同时它还包含一个respondQueue，用来存放KafkaRequestHandler处理完Request后返还给客户端的Response。\n　　Processor会通过processNewResponses方法依次将requestChannel中responseQueue保存的Response取出，并将对应的`SelectionKey.OP_WRITE`事件注册到selector上。当selector的select方法返回时，对检测到的可写通道，调用write方法将Response返回给客户端。\n　　KafkaRequestHandler循环从RequestChannel中取Request并交给`kafka.server.KafkaApis`处理具体的业务逻辑。\n\n## LeaderAndIsrRequest响应过程\n\n　　对于收到的LeaderAndIsrRequest，Broker主要通过ReplicaManager的becomeLeaderOrFollower处理，流程如下：\n\n1. 若请求中controllerEpoch小于当前最新的controllerEpoch，则直接返回ErrorMapping.StaleControllerEpochCode。\n2. 对于请求中partitionStateInfos中的每一个元素，即（(topic, partitionId), partitionStateInfo)：\n   　　2.1 若partitionStateInfo中的leader epoch大于当前ReplicManager中存储的(topic, partitionId)对应的partition的leader epoch，则：\n   　　　　2.1.1 若当前brokerid（或者说replica id）在partitionStateInfo中，则将该partition及partitionStateInfo存入一个名为partitionState的HashMap中\n   　　　　2.1.2否则说明该Broker不在该Partition分配的Replica list中，将该信息记录于log中\n   　　2.2否则将相应的Error code（ErrorMapping.StaleLeaderEpochCode）存入Response中\n3. 筛选出partitionState中Leader与当前Broker ID相等的所有记录存入partitionsTobeLeader中，其它记录存入partitionsToBeFollower中。\n4. 若partitionsTobeLeader不为空，则对其执行makeLeaders方。\n5. 若partitionsToBeFollower不为空，则对其执行makeFollowers方法。\n6. 若highwatermak线程还未启动，则将其启动，并将hwThreadInitialized设为true。\n7. 关闭所有Idle状态的Fetcher。\n\n　　LeaderAndIsrRequest处理过程如下图所示\n[![LeaderAndIsrRequest Flow Chart](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png)\n\n## Broker启动过程\n\n　　Broker启动后首先根据其ID在Zookeeper的`/brokers/ids`zonde下创建临时子节点（[Ephemeral node](http://zookeeper.apache.org/doc/trunk/zookeeperOver.html#Nodes+and+ephemeral+nodes)），创建成功后Controller的ReplicaStateMachine注册其上的Broker Change Watch会被fire，从而通过回调KafkaController.onBrokerStartup方法完成以下步骤：\n\n1. 向所有新启动的Broker发送UpdateMetadataRequest，其定义如下。\n   [![UpdateMetadataRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png)\n2. 将新启动的Broker上的所有Replica设置为OnlineReplica状态，同时这些Broker会为这些Partition启动high watermark线程。\n3. 通过partitionStateMachine触发OnlinePartitionStateChange。\n\n## Controller Failover\n\nController也需要Failover。每个Broker都会在Controller Path (`/controller`)上注册一个Watch。当前Controller失败时，对应的Controller Path会自动消失（因为它是Ephemeral Node），此时该Watch被fire，所有“活”着的Broker都会去竞选成为新的Controller（创建新的Controller Path），但是只会有一个竞选成功（这点由Zookeeper保证）。竞选成功者即为新的Leader，竞选失败者则重新在新的Controller Path上注册Watch。因为[Zookeeper的Watch是一次性的，被fire一次之后即失效](http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkWatches)，所以需要重新注册。\n\nBroker成功竞选为新Controller后会触发KafkaController.onControllerFailover方法，并在该方法中完成如下操作：\n\n1. 读取并增加Controller Epoch。\n2. 在ReassignedPartitions Path(`/admin/reassign_partitions`)上注册Watch。\n3. 在PreferredReplicaElection Path(`/admin/preferred_replica_election`)上注册Watch。\n4. 通过partitionStateMachine在Broker Topics Patch(`/brokers/topics`)上注册Watch。\n5. 若`delete.topic.enable`设置为true（默认值是false），则partitionStateMachine在Delete Topic Patch(`/admin/delete_topics`)上注册Watch。\n6. 通过replicaStateMachine在Broker Ids Patch(`/brokers/ids`)上注册Watch。\n7. 初始化ControllerContext对象，设置当前所有Topic，“活”着的Broker列表，所有Partition的Leader及ISR等。\n8. 启动replicaStateMachine和partitionStateMachine。\n9. 将brokerState状态设置为RunningAsController。\n10. 将每个Partition的Leadership信息发送给所有“活”着的Broker。\n11. 若`auto.leader.rebalance.enable`配置为true（默认值是true），则启动partition-rebalance线程。\n12. 若`delete.topic.enable`设置为true且Delete Topic Patch(`/admin/delete_topics`)中有值，则删除相应的Topic。\n\n## Partition重新分配\n\n　　管理工具发出重新分配Partition请求后，会将相应信息写到`/admin/reassign_partitions`上，而该操作会触发ReassignedPartitionsIsrChangeListener，从而通过执行回调函数KafkaController.onPartitionReassignment来完成以下操作：\n\n1. 将Zookeeper中的AR（Current Assigned Replicas）更新为OAR（Original list of replicas for partition） + RAR（Reassigned replicas）。\n2. 强制更新Zookeeper中的leader epoch，向AR中的每个Replica发送LeaderAndIsrRequest。\n3. 将RAR - OAR中的Replica设置为NewReplica状态。\n4. 等待直到RAR中所有的Replica都与其Leader同步。\n5. 将RAR中所有的Replica都设置为OnlineReplica状态。\n6. 将Cache中的AR设置为RAR。\n7. 若Leader不在RAR中，则从RAR中重新选举出一个新的Leader并发送LeaderAndIsrRequest。若新的Leader不是从RAR中选举而出，则还要增加Zookeeper中的leader epoch。\n8. 将OAR - RAR中的所有Replica设置为OfflineReplica状态，该过程包含两部分。第一，将Zookeeper上ISR中的OAR - RAR移除并向Leader发送LeaderAndIsrRequest从而通知这些Replica已经从ISR中移除；第二，向OAR - RAR中的Replica发送StopReplicaRequest从而停止不再分配给该Partition的Replica。\n9. 将OAR - RAR中的所有Replica设置为NonExistentReplica状态从而将其从磁盘上删除。\n10. 将Zookeeper中的AR设置为RAR。\n11. 删除`/admin/reassign_partition`。\n    　　\n    **注意**：最后一步才将Zookeeper中的AR更新，因为这是唯一一个持久存储AR的地方，如果Controller在这一步之前crash，新的Controller仍然能够继续完成该过程。\n    　　以下是Partition重新分配的案例，OAR = ｛1，2，3｝，RAR = ｛4，5，6｝，Partition重新分配过程中Zookeeper中的AR和Leader/ISR路径如下\n\n| AR            | leader/isr      | Step            |\n| ------------- | --------------- | --------------- |\n| {1,2,3}       | 1/{1,2,3}       | (initial state) |\n| {1,2,3,4,5,6} | 1/{1,2,3}       | (step 2)        |\n| {1,2,3,4,5,6} | 1/{1,2,3,4,5,6} | (step 4)        |\n| {1,2,3,4,5,6} | 4/{1,2,3,4,5,6} | (step 7)        |\n| {1,2,3,4,5,6} | 4/{4,5,6}       | (step 8)        |\n| {4,5,6}       | 4/{4,5,6}       | (step 10)       |\n\n## Follower从Leader Fetch数据\n\n　　Follower通过向Leader发送FetchRequest获取消息，FetchRequest结构如下\n[![FetchRequest](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png)\n　　从FetchRequest的结构可以看出，每个Fetch请求都要指定最大等待时间和最小获取字节数，以及由TopicAndPartition和PartitionFetchInfo构成的Map。实际上，Follower从Leader数据和Consumer从Broker Fetch数据，都是通过FetchRequest请求完成，所以在FetchRequest结构中，其中一个字段是clientID，并且其默认值是ConsumerConfig.DefaultClientId。\n　　\n　　Leader收到Fetch请求后，Kafka通过KafkaApis.handleFetchRequest响应该请求，响应过程如下：\n\n1. replicaManager根据请求读出数据存入dataRead中。\n2. 如果该请求来自Follower则更新其相应的LEO（log end offset）以及相应Partition的High Watermark\n3. 根据dataRead算出可读消息长度（单位为字节）并存入bytesReadable中。\n4. 满足下面4个条件中的1个，则立即将相应的数据返回\n\n- Fetch请求不希望等待，即fetchRequest.macWait <= 0\n- Fetch请求不要求一定能取到消息，即fetchRequest.numPartitions <= 0，也即requestInfo为空\n- 有足够的数据可供返回，即bytesReadable >= fetchRequest.minBytes\n- 读取数据时发生异常\n\n1. 若不满足以上4个条件，FetchRequest将不会立即返回，并将该请求封装成DelayedFetch。检查该DeplayedFetch是否满足，若满足则返回请求，否则将该请求加入Watch列表\n\n　　Leader通过以FetchResponse的形式将消息返回给Follower，FetchResponse结构如下\n[![FetchResponse](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png)\n\n\\#Replication工具\n\n## Topic Tool\n\n　　`$KAFKA_HOME/bin/kafka-topics.sh`，该工具可用于创建、删除、修改、查看某个Topic，也可用于列出所有Topic。另外，该工具还可修改以下配置。\n\n```\nunclean.leader.election.enable\ndelete.retention.ms\nsegment.jitter.ms\nretention.ms\nflush.ms\nsegment.bytes\nflush.messages\nsegment.ms\nretention.bytes\ncleanup.policy\nsegment.index.bytes\nmin.cleanable.dirty.ratio\nmax.message.bytes\nfile.delete.delay.ms\nmin.insync.replicas\nindex.interval.bytes\n```\n\n\n\n## Replica Verification Tool\n\n　　`$KAFKA_HOME/bin/kafka-replica-verification.sh`，该工具用来验证所指定的一个或多个Topic下每个Partition对应的所有Replica是否都同步。可通过`topic-white-list`这一参数指定所需要验证的所有Topic，支持正则表达式。 　　\n\n## Preferred Replica Leader Election Tool\n\n**用途**\n　　有了Replication机制后，每个Partition可能有多个备份。某个Partition的Replica列表叫作AR（Assigned Replicas），AR中的第一个Replica即为“Preferred Replica”。创建一个新的Topic或者给已有Topic增加Partition时，Kafka保证Preferred Replica被均匀分布到集群中的所有Broker上。理想情况下，Preferred Replica会被选为Leader。以上两点保证了所有Partition的Leader被均匀分布到了集群当中，这一点非常重要，因为所有的读写操作都由Leader完成，若Leader分布过于集中，会造成集群负载不均衡。但是，随着集群的运行，该平衡可能会因为Broker的宕机而被打破，该工具就是用来帮助恢复Leader分配的平衡。\n　　事实上，每个Topic从失败中恢复过来后，它默认会被设置为Follower角色，除非某个Partition的Replica全部宕机，而当前Broker是该Partition的AR中第一个恢复回来的Replica。因此，某个Partition的Leader（Preferred Replica）宕机并恢复后，它很可能不再是该Partition的Leader，但仍然是Preferred Replica。\n　　\n**原理**\n\n1. 在Zookeeper上创建`/admin/preferred_replica_election`节点，并存入需要调整Preferred Replica的Partition信息。\n2. Controller一直Watch该节点，一旦该节点被创建，Controller会收到通知，并获取该内容。\n3. Controller读取Preferred Replica，如果发现该Replica当前并非是Leader并且它在该Partition的ISR中，Controller向该Replica发送LeaderAndIsrRequest，使该Replica成为Leader。如果该Replica当前并非是Leader，且不在ISR中，Controller为了保证没有数据丢失，并不会将其设置为Leader。 　\n\n**用法**\n　　`$KAFKA_HOME/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181`\n\n　　在包含8个Broker的Kafka集群上，创建1个名为topic1，replication-factor为3，Partition数为8的Topic，使用`$KAFKA_HOME/bin/kafka-topics.sh --describe --topic topic1 --zookeeper localhost:2181`命令查看其Partition/Replica分布。\n\n　　查询结果如下图所示，从图中可以看到，Kafka将所有Replica均匀分布到了整个集群，并且Leader也均匀分布。\n[![preferred_topic_test_1](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png)\n\n　　手动停止部分Broker，topic1的Partition/Replica分布如下图所示。从图中可以看到，由于Broker 1/2/4都被停止，Partition 0的Leader由原来的1变为3，Partition 1的Leader由原来的2变为5，Partition 2的Leader由原来的3变为6，Partition 3的Leader由原来的4变为7。\n[![preferred_topic_test_2](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png)　　\n　　\n　　再重新启动ID为1的Broker，topic1的Partition/Replica分布如下。可以看到，虽然Broker 1已经启动（Partition 0和Partition5的ISR中有1），但是1并不是任何一个Parititon的Leader，而Broker 5/6/7都是2个Partition的Leader，即Leader的分布不均衡——一个Broker最多是2个Partition的Leader，而最少是0个Partition的Leader。\n[![preferred_topic_test_3](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png)\n　　\n　　运行该工具后，topic1的Partition/Replica分布如下图所示。由图可见，除了Partition 1和Partition 3由于Broker 2和Broker 4还未启动，所以其Leader不是其Preferred Repliac外，其它所有Partition的Leader都是其Preferred Replica。同时，与运行该工具前相比，Leader的分配更均匀——一个Broker最多是2个Parittion的Leader，最少是1个Partition的Leader。\n[![preferred_topic_test_4](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png)\n　　\n　　启动Broker 2和Broker 4，Leader分布与上一步相比并未变化，如下图所示。\n[![preferred_topic_test_5](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png)\n\n　　再次运行该工具，所有Partition的Leader都由其Preferred Replica承担，Leader分布更均匀——每个Broker承担1个Partition的Leader角色。\n　　\n　　除了手动运行该工具使Leader分配均匀外，Kafka还提供了自动平衡Leader分配的功能，该功能可通过将`auto.leader.rebalance.enable`设置为true开启，它将周期性检查Leader分配是否平衡，若不平衡度超过一定阈值则自动由Controller尝试将各Partition的Leader设置为其Preferred Replica。检查周期由`leader.imbalance.check.interval.seconds`指定，不平衡度阈值由`leader.imbalance.per.broker.percentage`指定。 　　\n\n## Kafka Reassign Partitions Tool\n\n**用途**\n　　该工具的设计目标与Preferred Replica Leader Election Tool有些类似，都旨在促进Kafka集群的负载均衡。不同的是，Preferred Replica Leader Election只能在Partition的AR范围内调整其Leader，使Leader分布均匀，而该工具还可以调整Partition的AR。\n　　Follower需要从Leader Fetch数据以保持与Leader同步，所以仅仅保持Leader分布的平衡对整个集群的负载均衡来说是不够的。另外，生产环境下，随着负载的增大，可能需要给Kafka集群扩容。向Kafka集群中增加Broker非常简单方便，但是对于已有的Topic，并不会自动将其Partition迁移到新加入的Broker上，此时可用该工具达到此目的。某些场景下，实际负载可能远小于最初预期负载，此时可用该工具将分布在整个集群上的Partition重装分配到某些机器上，然后可以停止不需要的Broker从而实现节约资源的目的。\n　　需要说明的是，该工具不仅可以调整Partition的AR位置，还可调整其AR数量，即改变该Topic的replication factor。\n　　\n**原理**\n　　该工具只负责将所需信息存入Zookeeper中相应节点，然后退出，不负责相关的具体操作，所有调整都由Controller完成。\n\n1. 在Zookeeper上创建`/admin/reassign_partitions`节点，并存入目标Partition列表及其对应的目标AR列表。\n2. Controller注册在`/admin/reassign_partitions`上的Watch被fire，Controller获取该列表。\n3. 对列表中的所有Partition，Controller会做如下操作：\n\n- 启动`RAR - AR`中的Replica，即新分配的Replica。（RAR = Reassigned Replicas， AR = Assigned Replicas）\n- 等待新的Replica与Leader同步\n- 如果Leader不在RAR中，从RAR中选出新的Leader\n- 停止并删除`AR - RAR`中的Replica，即不再需要的Replica\n- 删除`/admin/reassign_partitions`节点\n\n**用法**\n　　该工具有三种使用模式\n\n- generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）\n- execute模式，根据指定的reassign plan重新分配Partition\n- verify模式，验证重新分配Partition是否成功\n\n　　下面这个例子将使用该工具将Topic的所有Partition重新分配到Broker 4/5/6/7上，步骤如下：\n\n1. 使用generate模式，生成reassign plan。指定需要重新分配的Topic （{“topics”:[{“topic”:”topic1”}],”version”:1}），并存入\n\n   ```\n   /tmp/topics-to-move.json\n   ```\n\n   文件中，然后执行\n\n   ```\n   $KAFKA_HOME/bin/kafka-reassign-partitions.sh \n   \t--zookeeper localhost:2181 \n   \t--topics-to-move-json-file /tmp/topics-to-move.json  \n   \t--broker-list \"4,5,6,7\" --generate\n   ```\n\n　　结果如下图所示\n[![reassign_1](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png)\n　　\n2.　使用execute模式，执行reassign plan\n　　将上一步生成的reassignment plan存入`/tmp/reassign-plan.json`文件中，并执行\n\n```\n   $KAFKA_HOME/bin/kafka-reassign-partitions.sh \n--zookeeper localhost:2181     \n--reassignment-json-file /tmp/reassign-plan.json --execute\n```\n\n\n\n[![reassign_2](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png)\n\n　　此时，Zookeeper上`/admin/reassign_partitions`节点被创建，且其值与`/tmp/reassign-plan.json`文件的内容一致。\n[![reassign_3](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png)\n\n3.　使用verify模式，验证reassign是否完成。执行verify命令\n\n```\n$KAFKA_HOME/bin/kafka-reassign-partitions.sh \n--zookeeper localhost:2181 --verify\n--reassignment-json-file /tmp/reassign-plan.json\n```\n\n\n\n　　结果如下所示，从图中可看出topic1的所有Partititon都重新分配成功。\n[![reassign_4](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png)\n\n　　接下来用Topic Tool再次验证。\n\n```\nbin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic1\n```\n\n\n\n　　结果如下图所示，从图中可看出topic1的所有Partition都被重新分配到Broker 4/5/6/7，且每个Partition的AR与reassign plan一致。\n[![reassign_5](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png)](http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png)\n\n　　需要说明的是，在使用execute之前，并不一定要使用generate模式自动生成reassign plan，使用generate模式只是为了方便。事实上，某些场景下，generate模式生成的reassign plan并不一定能满足需求，此时用户可以自己设置reassign plan。 　　\n\n## State Change Log Merge Tool\n\n**用途**\n　　该工具旨在从整个集群的Broker上收集状态改变日志，并生成一个集中的格式化的日志以帮助诊断状态改变相关的故障。每个Broker都会将其收到的状态改变相关的的指令存于名为`state-change.log`的日志文件中。某些情况下，Partition的Leader Election可能会出现问题，此时我们需要对整个集群的状态改变有个全局的了解从而诊断故障并解决问题。该工具将集群中相关的`state-change.log`日志按时间顺序合并，同时支持用户输入时间范围和目标Topic及Partition作为过滤条件，最终将格式化的结果输出。\n　　\n**用法**\n\n```\nbin/kafka-run-class.sh kafka.tools.StateChangeLogMerger\n--logs /opt/kafka_2.11-0.8.2.1/logs/state-change.log\n--topic topic1 --partitions 0,1,2,3,4,5,6,7\n```","slug":"kafka高可用设计(下)","published":1,"updated":"2019-02-14T15:32:36.266Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubr9001hamum6glteraa","content":"<p>本文转发自<a href=\"http://www.jasongj.com/\" target=\"_blank\" rel=\"noopener\"><strong>技术世界</strong></a>，<a href=\"http://www.jasongj.com/2015/06/08/KafkaColumn3\" target=\"_blank\" rel=\"noopener\">原文链接</a>　<a href=\"http://www.jasongj.com/2015/06/08/KafkaColumn3\" target=\"_blank\" rel=\"noopener\">http://www.jasongj.com/2015/06/08/KafkaColumn3</a></p>\n<p>本文在上篇文章 基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。</p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>　　本文在上篇文章基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。</p>\n<h1 id=\"Broker-Failover过程\"><a href=\"#Broker-Failover过程\" class=\"headerlink\" title=\"Broker Failover过程\"></a>Broker Failover过程</h1><h2 id=\"Controller对Broker-failure的处理过程\"><a href=\"#Controller对Broker-failure的处理过程\" class=\"headerlink\" title=\"Controller对Broker failure的处理过程\"></a>Controller对Broker failure的处理过程</h2><ol>\n<li>Controller在Zookeeper的<code>/brokers/ids</code>节点上注册Watch。一旦有Broker宕机（本文用宕机代表任何让Kafka认为其Broker die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的Znode会自动被删除，Zookeeper会fire Controller注册的Watch，Controller即可获取最新的幸存的Broker列表。</li>\n<li>Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition。</li>\n<li>对set_p中的每一个Partition：<br>　　3.1 从<code>/brokers/topics/[topic]/partitions/[partition]/state</code>读取该Partition当前的ISR。<br>　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。<br>　　3.3 将新的Leader，ISR和新的<code>leader_epoch</code>及<code>controller_epoch</code>写入<code>/brokers/topics/[topic]/partitions/[partition]/state</code>。注意，该操作只有Controller版本在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1。</li>\n<li>直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。<br>　　Broker failover顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" alt=\"broker failover sequence diagram \"></a></li>\n</ol>\n<p>　　LeaderAndIsrRequest结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png\" alt=\"LeaderAndIsrRequest\"></a></p>\n<p>　　LeaderAndIsrResponse结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png\" alt=\"LeaderAndIsrResponse\"></a></p>\n<a id=\"more\"></a>\n<h2 id=\"创建-删除Topic\"><a href=\"#创建-删除Topic\" class=\"headerlink\" title=\"创建/删除Topic\"></a>创建/删除Topic</h2><ol>\n<li>Controller在Zookeeper的<code>/brokers/topics</code>节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。</li>\n<li>对于删除Topic操作，Topic工具会将该Topic名字存于<code>/admin/delete_topics</code>。若<code>delete.topic.enable</code>为true，则Controller注册在<code>/admin/delete_topics</code>上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest；若为false则Controller不会在<code>/admin/delete_topics</code>上注册Watch，也就不会对该事件作出反应，此时Topic操作只被记录而不会被执行。</li>\n<li>对于创建Topic操作，Controller从<code>/brokers/ids</code>读取当前所有可用的Broker列表，对于set_p中的每一个Partition：<br>　　3.1 从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）<br>　　3.2 将新的Leader和ISR写入<code>/brokers/topics/[topic]/partitions/[partition]</code></li>\n<li>直接通过RPC向相关的Broker发送LeaderAndISRRequest。<br>　　创建Topic顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png\" alt=\"create topic sequence diagram\"></a></li>\n</ol>\n<h2 id=\"Broker响应请求流程\"><a href=\"#Broker响应请求流程\" class=\"headerlink\" title=\"Broker响应请求流程\"></a>Broker响应请求流程</h2><p>　　Broker通过<code>kafka.network.SocketServer</code>及相关模块接受各种请求并作出响应。整个网络通信模块基于Java NIO开发，并采用Reactor模式，其中包含1个Acceptor负责接受客户请求，N个Processor负责读写数据，M个Handler处理业务逻辑。<br>　　Acceptor的主要职责是监听并接受客户端（请求发起方，包括但不限于Producer，Consumer，Controller，Admin Tool）的连接请求，并建立和客户端的数据传输通道，然后为该客户端指定一个Processor，至此它对该客户端该次请求的任务就结束了，它可以去响应下一个客户端的连接请求了。其核心代码如下。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png\" alt=\"Kafka SocketServer Acceptor_run\"></a><br>　　<br>　　Processor主要负责从客户端读取数据并将响应返回给客户端，它本身并不处理具体的业务逻辑，并且其内部维护了一个队列来保存分配给它的所有SocketChannel。Processor的run方法会循环从队列中取出新的SocketChannel并将其<code>SelectionKey.OP_READ</code>注册到selector上，然后循环处理已就绪的读（请求）和写（响应）。Processor读取完数据后，将其封装成Request对象并将其交给RequestChannel。<br>　　RequestChannel是Processor和KafkaRequestHandler交换数据的地方，它包含一个队列requestQueue用来存放Processor加入的Request，KafkaRequestHandler会从里面取出Request来处理；同时它还包含一个respondQueue，用来存放KafkaRequestHandler处理完Request后返还给客户端的Response。<br>　　Processor会通过processNewResponses方法依次将requestChannel中responseQueue保存的Response取出，并将对应的<code>SelectionKey.OP_WRITE</code>事件注册到selector上。当selector的select方法返回时，对检测到的可写通道，调用write方法将Response返回给客户端。<br>　　KafkaRequestHandler循环从RequestChannel中取Request并交给<code>kafka.server.KafkaApis</code>处理具体的业务逻辑。</p>\n<h2 id=\"LeaderAndIsrRequest响应过程\"><a href=\"#LeaderAndIsrRequest响应过程\" class=\"headerlink\" title=\"LeaderAndIsrRequest响应过程\"></a>LeaderAndIsrRequest响应过程</h2><p>　　对于收到的LeaderAndIsrRequest，Broker主要通过ReplicaManager的becomeLeaderOrFollower处理，流程如下：</p>\n<ol>\n<li>若请求中controllerEpoch小于当前最新的controllerEpoch，则直接返回ErrorMapping.StaleControllerEpochCode。</li>\n<li>对于请求中partitionStateInfos中的每一个元素，即（(topic, partitionId), partitionStateInfo)：<br>　　2.1 若partitionStateInfo中的leader epoch大于当前ReplicManager中存储的(topic, partitionId)对应的partition的leader epoch，则：<br>　　　　2.1.1 若当前brokerid（或者说replica id）在partitionStateInfo中，则将该partition及partitionStateInfo存入一个名为partitionState的HashMap中<br>　　　　2.1.2否则说明该Broker不在该Partition分配的Replica list中，将该信息记录于log中<br>　　2.2否则将相应的Error code（ErrorMapping.StaleLeaderEpochCode）存入Response中</li>\n<li>筛选出partitionState中Leader与当前Broker ID相等的所有记录存入partitionsTobeLeader中，其它记录存入partitionsToBeFollower中。</li>\n<li>若partitionsTobeLeader不为空，则对其执行makeLeaders方。</li>\n<li>若partitionsToBeFollower不为空，则对其执行makeFollowers方法。</li>\n<li>若highwatermak线程还未启动，则将其启动，并将hwThreadInitialized设为true。</li>\n<li>关闭所有Idle状态的Fetcher。</li>\n</ol>\n<p>　　LeaderAndIsrRequest处理过程如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png\" alt=\"LeaderAndIsrRequest Flow Chart\"></a></p>\n<h2 id=\"Broker启动过程\"><a href=\"#Broker启动过程\" class=\"headerlink\" title=\"Broker启动过程\"></a>Broker启动过程</h2><p>　　Broker启动后首先根据其ID在Zookeeper的<code>/brokers/ids</code>zonde下创建临时子节点（<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperOver.html#Nodes+and+ephemeral+nodes\" target=\"_blank\" rel=\"noopener\">Ephemeral node</a>），创建成功后Controller的ReplicaStateMachine注册其上的Broker Change Watch会被fire，从而通过回调KafkaController.onBrokerStartup方法完成以下步骤：</p>\n<ol>\n<li>向所有新启动的Broker发送UpdateMetadataRequest，其定义如下。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png\" alt=\"UpdateMetadataRequest\"></a></li>\n<li>将新启动的Broker上的所有Replica设置为OnlineReplica状态，同时这些Broker会为这些Partition启动high watermark线程。</li>\n<li>通过partitionStateMachine触发OnlinePartitionStateChange。</li>\n</ol>\n<h2 id=\"Controller-Failover\"><a href=\"#Controller-Failover\" class=\"headerlink\" title=\"Controller Failover\"></a>Controller Failover</h2><p>Controller也需要Failover。每个Broker都会在Controller Path (<code>/controller</code>)上注册一个Watch。当前Controller失败时，对应的Controller Path会自动消失（因为它是Ephemeral Node），此时该Watch被fire，所有“活”着的Broker都会去竞选成为新的Controller（创建新的Controller Path），但是只会有一个竞选成功（这点由Zookeeper保证）。竞选成功者即为新的Leader，竞选失败者则重新在新的Controller Path上注册Watch。因为<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkWatches\" target=\"_blank\" rel=\"noopener\">Zookeeper的Watch是一次性的，被fire一次之后即失效</a>，所以需要重新注册。</p>\n<p>Broker成功竞选为新Controller后会触发KafkaController.onControllerFailover方法，并在该方法中完成如下操作：</p>\n<ol>\n<li>读取并增加Controller Epoch。</li>\n<li>在ReassignedPartitions Path(<code>/admin/reassign_partitions</code>)上注册Watch。</li>\n<li>在PreferredReplicaElection Path(<code>/admin/preferred_replica_election</code>)上注册Watch。</li>\n<li>通过partitionStateMachine在Broker Topics Patch(<code>/brokers/topics</code>)上注册Watch。</li>\n<li>若<code>delete.topic.enable</code>设置为true（默认值是false），则partitionStateMachine在Delete Topic Patch(<code>/admin/delete_topics</code>)上注册Watch。</li>\n<li>通过replicaStateMachine在Broker Ids Patch(<code>/brokers/ids</code>)上注册Watch。</li>\n<li>初始化ControllerContext对象，设置当前所有Topic，“活”着的Broker列表，所有Partition的Leader及ISR等。</li>\n<li>启动replicaStateMachine和partitionStateMachine。</li>\n<li>将brokerState状态设置为RunningAsController。</li>\n<li>将每个Partition的Leadership信息发送给所有“活”着的Broker。</li>\n<li>若<code>auto.leader.rebalance.enable</code>配置为true（默认值是true），则启动partition-rebalance线程。</li>\n<li>若<code>delete.topic.enable</code>设置为true且Delete Topic Patch(<code>/admin/delete_topics</code>)中有值，则删除相应的Topic。</li>\n</ol>\n<h2 id=\"Partition重新分配\"><a href=\"#Partition重新分配\" class=\"headerlink\" title=\"Partition重新分配\"></a>Partition重新分配</h2><p>　　管理工具发出重新分配Partition请求后，会将相应信息写到<code>/admin/reassign_partitions</code>上，而该操作会触发ReassignedPartitionsIsrChangeListener，从而通过执行回调函数KafkaController.onPartitionReassignment来完成以下操作：</p>\n<ol>\n<li>将Zookeeper中的AR（Current Assigned Replicas）更新为OAR（Original list of replicas for partition） + RAR（Reassigned replicas）。</li>\n<li>强制更新Zookeeper中的leader epoch，向AR中的每个Replica发送LeaderAndIsrRequest。</li>\n<li>将RAR - OAR中的Replica设置为NewReplica状态。</li>\n<li>等待直到RAR中所有的Replica都与其Leader同步。</li>\n<li>将RAR中所有的Replica都设置为OnlineReplica状态。</li>\n<li>将Cache中的AR设置为RAR。</li>\n<li>若Leader不在RAR中，则从RAR中重新选举出一个新的Leader并发送LeaderAndIsrRequest。若新的Leader不是从RAR中选举而出，则还要增加Zookeeper中的leader epoch。</li>\n<li>将OAR - RAR中的所有Replica设置为OfflineReplica状态，该过程包含两部分。第一，将Zookeeper上ISR中的OAR - RAR移除并向Leader发送LeaderAndIsrRequest从而通知这些Replica已经从ISR中移除；第二，向OAR - RAR中的Replica发送StopReplicaRequest从而停止不再分配给该Partition的Replica。</li>\n<li>将OAR - RAR中的所有Replica设置为NonExistentReplica状态从而将其从磁盘上删除。</li>\n<li>将Zookeeper中的AR设置为RAR。</li>\n<li>删除<code>/admin/reassign_partition</code>。<br>　　<br><strong>注意</strong>：最后一步才将Zookeeper中的AR更新，因为这是唯一一个持久存储AR的地方，如果Controller在这一步之前crash，新的Controller仍然能够继续完成该过程。<br>　　以下是Partition重新分配的案例，OAR = ｛1，2，3｝，RAR = ｛4，5，6｝，Partition重新分配过程中Zookeeper中的AR和Leader/ISR路径如下</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>AR</th>\n<th>leader/isr</th>\n<th>Step</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>{1,2,3}</td>\n<td>1/{1,2,3}</td>\n<td>(initial state)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>1/{1,2,3}</td>\n<td>(step 2)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>1/{1,2,3,4,5,6}</td>\n<td>(step 4)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>4/{1,2,3,4,5,6}</td>\n<td>(step 7)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>4/{4,5,6}</td>\n<td>(step 8)</td>\n</tr>\n<tr>\n<td>{4,5,6}</td>\n<td>4/{4,5,6}</td>\n<td>(step 10)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Follower从Leader-Fetch数据\"><a href=\"#Follower从Leader-Fetch数据\" class=\"headerlink\" title=\"Follower从Leader Fetch数据\"></a>Follower从Leader Fetch数据</h2><p>　　Follower通过向Leader发送FetchRequest获取消息，FetchRequest结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png\" alt=\"FetchRequest\"></a><br>　　从FetchRequest的结构可以看出，每个Fetch请求都要指定最大等待时间和最小获取字节数，以及由TopicAndPartition和PartitionFetchInfo构成的Map。实际上，Follower从Leader数据和Consumer从Broker Fetch数据，都是通过FetchRequest请求完成，所以在FetchRequest结构中，其中一个字段是clientID，并且其默认值是ConsumerConfig.DefaultClientId。<br>　　<br>　　Leader收到Fetch请求后，Kafka通过KafkaApis.handleFetchRequest响应该请求，响应过程如下：</p>\n<ol>\n<li>replicaManager根据请求读出数据存入dataRead中。</li>\n<li>如果该请求来自Follower则更新其相应的LEO（log end offset）以及相应Partition的High Watermark</li>\n<li>根据dataRead算出可读消息长度（单位为字节）并存入bytesReadable中。</li>\n<li>满足下面4个条件中的1个，则立即将相应的数据返回</li>\n</ol>\n<ul>\n<li>Fetch请求不希望等待，即fetchRequest.macWait &lt;= 0</li>\n<li>Fetch请求不要求一定能取到消息，即fetchRequest.numPartitions &lt;= 0，也即requestInfo为空</li>\n<li>有足够的数据可供返回，即bytesReadable &gt;= fetchRequest.minBytes</li>\n<li>读取数据时发生异常</li>\n</ul>\n<ol>\n<li>若不满足以上4个条件，FetchRequest将不会立即返回，并将该请求封装成DelayedFetch。检查该DeplayedFetch是否满足，若满足则返回请求，否则将该请求加入Watch列表</li>\n</ol>\n<p>　　Leader通过以FetchResponse的形式将消息返回给Follower，FetchResponse结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png\" alt=\"FetchResponse\"></a></p>\n<p>#Replication工具</p>\n<h2 id=\"Topic-Tool\"><a href=\"#Topic-Tool\" class=\"headerlink\" title=\"Topic Tool\"></a>Topic Tool</h2><p>　　<code>$KAFKA_HOME/bin/kafka-topics.sh</code>，该工具可用于创建、删除、修改、查看某个Topic，也可用于列出所有Topic。另外，该工具还可修改以下配置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unclean.leader.election.enable</span><br><span class=\"line\">delete.retention.ms</span><br><span class=\"line\">segment.jitter.ms</span><br><span class=\"line\">retention.ms</span><br><span class=\"line\">flush.ms</span><br><span class=\"line\">segment.bytes</span><br><span class=\"line\">flush.messages</span><br><span class=\"line\">segment.ms</span><br><span class=\"line\">retention.bytes</span><br><span class=\"line\">cleanup.policy</span><br><span class=\"line\">segment.index.bytes</span><br><span class=\"line\">min.cleanable.dirty.ratio</span><br><span class=\"line\">max.message.bytes</span><br><span class=\"line\">file.delete.delay.ms</span><br><span class=\"line\">min.insync.replicas</span><br><span class=\"line\">index.interval.bytes</span><br></pre></td></tr></table></figure>\n<h2 id=\"Replica-Verification-Tool\"><a href=\"#Replica-Verification-Tool\" class=\"headerlink\" title=\"Replica Verification Tool\"></a>Replica Verification Tool</h2><p>　　<code>$KAFKA_HOME/bin/kafka-replica-verification.sh</code>，该工具用来验证所指定的一个或多个Topic下每个Partition对应的所有Replica是否都同步。可通过<code>topic-white-list</code>这一参数指定所需要验证的所有Topic，支持正则表达式。 　　</p>\n<h2 id=\"Preferred-Replica-Leader-Election-Tool\"><a href=\"#Preferred-Replica-Leader-Election-Tool\" class=\"headerlink\" title=\"Preferred Replica Leader Election Tool\"></a>Preferred Replica Leader Election Tool</h2><p><strong>用途</strong><br>　　有了Replication机制后，每个Partition可能有多个备份。某个Partition的Replica列表叫作AR（Assigned Replicas），AR中的第一个Replica即为“Preferred Replica”。创建一个新的Topic或者给已有Topic增加Partition时，Kafka保证Preferred Replica被均匀分布到集群中的所有Broker上。理想情况下，Preferred Replica会被选为Leader。以上两点保证了所有Partition的Leader被均匀分布到了集群当中，这一点非常重要，因为所有的读写操作都由Leader完成，若Leader分布过于集中，会造成集群负载不均衡。但是，随着集群的运行，该平衡可能会因为Broker的宕机而被打破，该工具就是用来帮助恢复Leader分配的平衡。<br>　　事实上，每个Topic从失败中恢复过来后，它默认会被设置为Follower角色，除非某个Partition的Replica全部宕机，而当前Broker是该Partition的AR中第一个恢复回来的Replica。因此，某个Partition的Leader（Preferred Replica）宕机并恢复后，它很可能不再是该Partition的Leader，但仍然是Preferred Replica。<br>　　<br><strong>原理</strong></p>\n<ol>\n<li>在Zookeeper上创建<code>/admin/preferred_replica_election</code>节点，并存入需要调整Preferred Replica的Partition信息。</li>\n<li>Controller一直Watch该节点，一旦该节点被创建，Controller会收到通知，并获取该内容。</li>\n<li>Controller读取Preferred Replica，如果发现该Replica当前并非是Leader并且它在该Partition的ISR中，Controller向该Replica发送LeaderAndIsrRequest，使该Replica成为Leader。如果该Replica当前并非是Leader，且不在ISR中，Controller为了保证没有数据丢失，并不会将其设置为Leader。 　</li>\n</ol>\n<p><strong>用法</strong><br>　　<code>$KAFKA_HOME/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</code></p>\n<p>　　在包含8个Broker的Kafka集群上，创建1个名为topic1，replication-factor为3，Partition数为8的Topic，使用<code>$KAFKA_HOME/bin/kafka-topics.sh --describe --topic topic1 --zookeeper localhost:2181</code>命令查看其Partition/Replica分布。</p>\n<p>　　查询结果如下图所示，从图中可以看到，Kafka将所有Replica均匀分布到了整个集群，并且Leader也均匀分布。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png\" alt=\"preferred_topic_test_1\"></a></p>\n<p>　　手动停止部分Broker，topic1的Partition/Replica分布如下图所示。从图中可以看到，由于Broker 1/2/4都被停止，Partition 0的Leader由原来的1变为3，Partition 1的Leader由原来的2变为5，Partition 2的Leader由原来的3变为6，Partition 3的Leader由原来的4变为7。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png\" alt=\"preferred_topic_test_2\"></a>　　<br>　　<br>　　再重新启动ID为1的Broker，topic1的Partition/Replica分布如下。可以看到，虽然Broker 1已经启动（Partition 0和Partition5的ISR中有1），但是1并不是任何一个Parititon的Leader，而Broker 5/6/7都是2个Partition的Leader，即Leader的分布不均衡——一个Broker最多是2个Partition的Leader，而最少是0个Partition的Leader。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png\" alt=\"preferred_topic_test_3\"></a><br>　　<br>　　运行该工具后，topic1的Partition/Replica分布如下图所示。由图可见，除了Partition 1和Partition 3由于Broker 2和Broker 4还未启动，所以其Leader不是其Preferred Repliac外，其它所有Partition的Leader都是其Preferred Replica。同时，与运行该工具前相比，Leader的分配更均匀——一个Broker最多是2个Parittion的Leader，最少是1个Partition的Leader。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png\" alt=\"preferred_topic_test_4\"></a><br>　　<br>　　启动Broker 2和Broker 4，Leader分布与上一步相比并未变化，如下图所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png\" alt=\"preferred_topic_test_5\"></a></p>\n<p>　　再次运行该工具，所有Partition的Leader都由其Preferred Replica承担，Leader分布更均匀——每个Broker承担1个Partition的Leader角色。<br>　　<br>　　除了手动运行该工具使Leader分配均匀外，Kafka还提供了自动平衡Leader分配的功能，该功能可通过将<code>auto.leader.rebalance.enable</code>设置为true开启，它将周期性检查Leader分配是否平衡，若不平衡度超过一定阈值则自动由Controller尝试将各Partition的Leader设置为其Preferred Replica。检查周期由<code>leader.imbalance.check.interval.seconds</code>指定，不平衡度阈值由<code>leader.imbalance.per.broker.percentage</code>指定。 　　</p>\n<h2 id=\"Kafka-Reassign-Partitions-Tool\"><a href=\"#Kafka-Reassign-Partitions-Tool\" class=\"headerlink\" title=\"Kafka Reassign Partitions Tool\"></a>Kafka Reassign Partitions Tool</h2><p><strong>用途</strong><br>　　该工具的设计目标与Preferred Replica Leader Election Tool有些类似，都旨在促进Kafka集群的负载均衡。不同的是，Preferred Replica Leader Election只能在Partition的AR范围内调整其Leader，使Leader分布均匀，而该工具还可以调整Partition的AR。<br>　　Follower需要从Leader Fetch数据以保持与Leader同步，所以仅仅保持Leader分布的平衡对整个集群的负载均衡来说是不够的。另外，生产环境下，随着负载的增大，可能需要给Kafka集群扩容。向Kafka集群中增加Broker非常简单方便，但是对于已有的Topic，并不会自动将其Partition迁移到新加入的Broker上，此时可用该工具达到此目的。某些场景下，实际负载可能远小于最初预期负载，此时可用该工具将分布在整个集群上的Partition重装分配到某些机器上，然后可以停止不需要的Broker从而实现节约资源的目的。<br>　　需要说明的是，该工具不仅可以调整Partition的AR位置，还可调整其AR数量，即改变该Topic的replication factor。<br>　　<br><strong>原理</strong><br>　　该工具只负责将所需信息存入Zookeeper中相应节点，然后退出，不负责相关的具体操作，所有调整都由Controller完成。</p>\n<ol>\n<li>在Zookeeper上创建<code>/admin/reassign_partitions</code>节点，并存入目标Partition列表及其对应的目标AR列表。</li>\n<li>Controller注册在<code>/admin/reassign_partitions</code>上的Watch被fire，Controller获取该列表。</li>\n<li>对列表中的所有Partition，Controller会做如下操作：</li>\n</ol>\n<ul>\n<li>启动<code>RAR - AR</code>中的Replica，即新分配的Replica。（RAR = Reassigned Replicas， AR = Assigned Replicas）</li>\n<li>等待新的Replica与Leader同步</li>\n<li>如果Leader不在RAR中，从RAR中选出新的Leader</li>\n<li>停止并删除<code>AR - RAR</code>中的Replica，即不再需要的Replica</li>\n<li>删除<code>/admin/reassign_partitions</code>节点</li>\n</ul>\n<p><strong>用法</strong><br>　　该工具有三种使用模式</p>\n<ul>\n<li>generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）</li>\n<li>execute模式，根据指定的reassign plan重新分配Partition</li>\n<li>verify模式，验证重新分配Partition是否成功</li>\n</ul>\n<p>　　下面这个例子将使用该工具将Topic的所有Partition重新分配到Broker 4/5/6/7上，步骤如下：</p>\n<ol>\n<li><p>使用generate模式，生成reassign plan。指定需要重新分配的Topic （{“topics”:[{“topic”:”topic1”}],”version”:1}），并存入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/tmp/topics-to-move.json</span><br></pre></td></tr></table></figure>\n<p>文件中，然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">\t--zookeeper localhost:2181 </span><br><span class=\"line\">\t--topics-to-move-json-file /tmp/topics-to-move.json  </span><br><span class=\"line\">\t--broker-list &quot;4,5,6,7&quot; --generate</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>　　结果如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png\" alt=\"reassign_1\"></a><br>　　<br>2.　使用execute模式，执行reassign plan<br>　　将上一步生成的reassignment plan存入<code>/tmp/reassign-plan.json</code>文件中，并执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   $KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">--zookeeper localhost:2181     </span><br><span class=\"line\">--reassignment-json-file /tmp/reassign-plan.json --execute</span><br></pre></td></tr></table></figure>\n<p><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png\" alt=\"reassign_2\"></a></p>\n<p>　　此时，Zookeeper上<code>/admin/reassign_partitions</code>节点被创建，且其值与<code>/tmp/reassign-plan.json</code>文件的内容一致。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png\" alt=\"reassign_3\"></a></p>\n<p>3.　使用verify模式，验证reassign是否完成。执行verify命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">--zookeeper localhost:2181 --verify</span><br><span class=\"line\">--reassignment-json-file /tmp/reassign-plan.json</span><br></pre></td></tr></table></figure>\n<p>　　结果如下所示，从图中可看出topic1的所有Partititon都重新分配成功。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png\" alt=\"reassign_4\"></a></p>\n<p>　　接下来用Topic Tool再次验证。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic1</span><br></pre></td></tr></table></figure>\n<p>　　结果如下图所示，从图中可看出topic1的所有Partition都被重新分配到Broker 4/5/6/7，且每个Partition的AR与reassign plan一致。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png\" alt=\"reassign_5\"></a></p>\n<p>　　需要说明的是，在使用execute之前，并不一定要使用generate模式自动生成reassign plan，使用generate模式只是为了方便。事实上，某些场景下，generate模式生成的reassign plan并不一定能满足需求，此时用户可以自己设置reassign plan。 　　</p>\n<h2 id=\"State-Change-Log-Merge-Tool\"><a href=\"#State-Change-Log-Merge-Tool\" class=\"headerlink\" title=\"State Change Log Merge Tool\"></a>State Change Log Merge Tool</h2><p><strong>用途</strong><br>　　该工具旨在从整个集群的Broker上收集状态改变日志，并生成一个集中的格式化的日志以帮助诊断状态改变相关的故障。每个Broker都会将其收到的状态改变相关的的指令存于名为<code>state-change.log</code>的日志文件中。某些情况下，Partition的Leader Election可能会出现问题，此时我们需要对整个集群的状态改变有个全局的了解从而诊断故障并解决问题。该工具将集群中相关的<code>state-change.log</code>日志按时间顺序合并，同时支持用户输入时间范围和目标Topic及Partition作为过滤条件，最终将格式化的结果输出。<br>　　<br><strong>用法</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-run-class.sh kafka.tools.StateChangeLogMerger</span><br><span class=\"line\">--logs /opt/kafka_2.11-0.8.2.1/logs/state-change.log</span><br><span class=\"line\">--topic topic1 --partitions 0,1,2,3,4,5,6,7</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>本文转发自<a href=\"http://www.jasongj.com/\" target=\"_blank\" rel=\"noopener\"><strong>技术世界</strong></a>，<a href=\"http://www.jasongj.com/2015/06/08/KafkaColumn3\" target=\"_blank\" rel=\"noopener\">原文链接</a>　<a href=\"http://www.jasongj.com/2015/06/08/KafkaColumn3\" target=\"_blank\" rel=\"noopener\">http://www.jasongj.com/2015/06/08/KafkaColumn3</a></p>\n<p>本文在上篇文章 基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。</p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>　　本文在上篇文章基础上，更加深入讲解了Kafka的HA机制，主要阐述了HA相关各种场景，如Broker failover，Controller failover，Topic创建/删除，Broker启动，Follower从Leader fetch数据等详细处理过程。同时介绍了Kafka提供的与Replication相关的工具，如重新分配Partition等。</p>\n<h1 id=\"Broker-Failover过程\"><a href=\"#Broker-Failover过程\" class=\"headerlink\" title=\"Broker Failover过程\"></a>Broker Failover过程</h1><h2 id=\"Controller对Broker-failure的处理过程\"><a href=\"#Controller对Broker-failure的处理过程\" class=\"headerlink\" title=\"Controller对Broker failure的处理过程\"></a>Controller对Broker failure的处理过程</h2><ol>\n<li>Controller在Zookeeper的<code>/brokers/ids</code>节点上注册Watch。一旦有Broker宕机（本文用宕机代表任何让Kafka认为其Broker die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的Znode会自动被删除，Zookeeper会fire Controller注册的Watch，Controller即可获取最新的幸存的Broker列表。</li>\n<li>Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition。</li>\n<li>对set_p中的每一个Partition：<br>　　3.1 从<code>/brokers/topics/[topic]/partitions/[partition]/state</code>读取该Partition当前的ISR。<br>　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。<br>　　3.3 将新的Leader，ISR和新的<code>leader_epoch</code>及<code>controller_epoch</code>写入<code>/brokers/topics/[topic]/partitions/[partition]/state</code>。注意，该操作只有Controller版本在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1。</li>\n<li>直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。<br>　　Broker failover顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_broker_failover.png\" alt=\"broker failover sequence diagram \"></a></li>\n</ol>\n<p>　　LeaderAndIsrRequest结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest.png\" alt=\"LeaderAndIsrRequest\"></a></p>\n<p>　　LeaderAndIsrResponse结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrResponse.png\" alt=\"LeaderAndIsrResponse\"></a></p>","more":"<h2 id=\"创建-删除Topic\"><a href=\"#创建-删除Topic\" class=\"headerlink\" title=\"创建/删除Topic\"></a>创建/删除Topic</h2><ol>\n<li>Controller在Zookeeper的<code>/brokers/topics</code>节点上注册Watch，一旦某个Topic被创建或删除，则Controller会通过Watch得到新创建/删除的Topic的Partition/Replica分配。</li>\n<li>对于删除Topic操作，Topic工具会将该Topic名字存于<code>/admin/delete_topics</code>。若<code>delete.topic.enable</code>为true，则Controller注册在<code>/admin/delete_topics</code>上的Watch被fire，Controller通过回调向对应的Broker发送StopReplicaRequest；若为false则Controller不会在<code>/admin/delete_topics</code>上注册Watch，也就不会对该事件作出反应，此时Topic操作只被记录而不会被执行。</li>\n<li>对于创建Topic操作，Controller从<code>/brokers/ids</code>读取当前所有可用的Broker列表，对于set_p中的每一个Partition：<br>　　3.1 从分配给该Partition的所有Replica（称为AR）中任选一个可用的Broker作为新的Leader，并将AR设置为新的ISR（因为该Topic是新创建的，所以AR中所有的Replica都没有数据，可认为它们都是同步的，也即都在ISR中，任意一个Replica都可作为Leader）<br>　　3.2 将新的Leader和ISR写入<code>/brokers/topics/[topic]/partitions/[partition]</code></li>\n<li>直接通过RPC向相关的Broker发送LeaderAndISRRequest。<br>　　创建Topic顺序图如下所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn2/kafka_create_topic.png\" alt=\"create topic sequence diagram\"></a></li>\n</ol>\n<h2 id=\"Broker响应请求流程\"><a href=\"#Broker响应请求流程\" class=\"headerlink\" title=\"Broker响应请求流程\"></a>Broker响应请求流程</h2><p>　　Broker通过<code>kafka.network.SocketServer</code>及相关模块接受各种请求并作出响应。整个网络通信模块基于Java NIO开发，并采用Reactor模式，其中包含1个Acceptor负责接受客户请求，N个Processor负责读写数据，M个Handler处理业务逻辑。<br>　　Acceptor的主要职责是监听并接受客户端（请求发起方，包括但不限于Producer，Consumer，Controller，Admin Tool）的连接请求，并建立和客户端的数据传输通道，然后为该客户端指定一个Processor，至此它对该客户端该次请求的任务就结束了，它可以去响应下一个客户端的连接请求了。其核心代码如下。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/Acceptor_run.png\" alt=\"Kafka SocketServer Acceptor_run\"></a><br>　　<br>　　Processor主要负责从客户端读取数据并将响应返回给客户端，它本身并不处理具体的业务逻辑，并且其内部维护了一个队列来保存分配给它的所有SocketChannel。Processor的run方法会循环从队列中取出新的SocketChannel并将其<code>SelectionKey.OP_READ</code>注册到selector上，然后循环处理已就绪的读（请求）和写（响应）。Processor读取完数据后，将其封装成Request对象并将其交给RequestChannel。<br>　　RequestChannel是Processor和KafkaRequestHandler交换数据的地方，它包含一个队列requestQueue用来存放Processor加入的Request，KafkaRequestHandler会从里面取出Request来处理；同时它还包含一个respondQueue，用来存放KafkaRequestHandler处理完Request后返还给客户端的Response。<br>　　Processor会通过processNewResponses方法依次将requestChannel中responseQueue保存的Response取出，并将对应的<code>SelectionKey.OP_WRITE</code>事件注册到selector上。当selector的select方法返回时，对检测到的可写通道，调用write方法将Response返回给客户端。<br>　　KafkaRequestHandler循环从RequestChannel中取Request并交给<code>kafka.server.KafkaApis</code>处理具体的业务逻辑。</p>\n<h2 id=\"LeaderAndIsrRequest响应过程\"><a href=\"#LeaderAndIsrRequest响应过程\" class=\"headerlink\" title=\"LeaderAndIsrRequest响应过程\"></a>LeaderAndIsrRequest响应过程</h2><p>　　对于收到的LeaderAndIsrRequest，Broker主要通过ReplicaManager的becomeLeaderOrFollower处理，流程如下：</p>\n<ol>\n<li>若请求中controllerEpoch小于当前最新的controllerEpoch，则直接返回ErrorMapping.StaleControllerEpochCode。</li>\n<li>对于请求中partitionStateInfos中的每一个元素，即（(topic, partitionId), partitionStateInfo)：<br>　　2.1 若partitionStateInfo中的leader epoch大于当前ReplicManager中存储的(topic, partitionId)对应的partition的leader epoch，则：<br>　　　　2.1.1 若当前brokerid（或者说replica id）在partitionStateInfo中，则将该partition及partitionStateInfo存入一个名为partitionState的HashMap中<br>　　　　2.1.2否则说明该Broker不在该Partition分配的Replica list中，将该信息记录于log中<br>　　2.2否则将相应的Error code（ErrorMapping.StaleLeaderEpochCode）存入Response中</li>\n<li>筛选出partitionState中Leader与当前Broker ID相等的所有记录存入partitionsTobeLeader中，其它记录存入partitionsToBeFollower中。</li>\n<li>若partitionsTobeLeader不为空，则对其执行makeLeaders方。</li>\n<li>若partitionsToBeFollower不为空，则对其执行makeFollowers方法。</li>\n<li>若highwatermak线程还未启动，则将其启动，并将hwThreadInitialized设为true。</li>\n<li>关闭所有Idle状态的Fetcher。</li>\n</ol>\n<p>　　LeaderAndIsrRequest处理过程如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/LeaderAndIsrRequest_Flow_Chart.png\" alt=\"LeaderAndIsrRequest Flow Chart\"></a></p>\n<h2 id=\"Broker启动过程\"><a href=\"#Broker启动过程\" class=\"headerlink\" title=\"Broker启动过程\"></a>Broker启动过程</h2><p>　　Broker启动后首先根据其ID在Zookeeper的<code>/brokers/ids</code>zonde下创建临时子节点（<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperOver.html#Nodes+and+ephemeral+nodes\" target=\"_blank\" rel=\"noopener\">Ephemeral node</a>），创建成功后Controller的ReplicaStateMachine注册其上的Broker Change Watch会被fire，从而通过回调KafkaController.onBrokerStartup方法完成以下步骤：</p>\n<ol>\n<li>向所有新启动的Broker发送UpdateMetadataRequest，其定义如下。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/UpdateMetadataRequest.png\" alt=\"UpdateMetadataRequest\"></a></li>\n<li>将新启动的Broker上的所有Replica设置为OnlineReplica状态，同时这些Broker会为这些Partition启动high watermark线程。</li>\n<li>通过partitionStateMachine触发OnlinePartitionStateChange。</li>\n</ol>\n<h2 id=\"Controller-Failover\"><a href=\"#Controller-Failover\" class=\"headerlink\" title=\"Controller Failover\"></a>Controller Failover</h2><p>Controller也需要Failover。每个Broker都会在Controller Path (<code>/controller</code>)上注册一个Watch。当前Controller失败时，对应的Controller Path会自动消失（因为它是Ephemeral Node），此时该Watch被fire，所有“活”着的Broker都会去竞选成为新的Controller（创建新的Controller Path），但是只会有一个竞选成功（这点由Zookeeper保证）。竞选成功者即为新的Leader，竞选失败者则重新在新的Controller Path上注册Watch。因为<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkWatches\" target=\"_blank\" rel=\"noopener\">Zookeeper的Watch是一次性的，被fire一次之后即失效</a>，所以需要重新注册。</p>\n<p>Broker成功竞选为新Controller后会触发KafkaController.onControllerFailover方法，并在该方法中完成如下操作：</p>\n<ol>\n<li>读取并增加Controller Epoch。</li>\n<li>在ReassignedPartitions Path(<code>/admin/reassign_partitions</code>)上注册Watch。</li>\n<li>在PreferredReplicaElection Path(<code>/admin/preferred_replica_election</code>)上注册Watch。</li>\n<li>通过partitionStateMachine在Broker Topics Patch(<code>/brokers/topics</code>)上注册Watch。</li>\n<li>若<code>delete.topic.enable</code>设置为true（默认值是false），则partitionStateMachine在Delete Topic Patch(<code>/admin/delete_topics</code>)上注册Watch。</li>\n<li>通过replicaStateMachine在Broker Ids Patch(<code>/brokers/ids</code>)上注册Watch。</li>\n<li>初始化ControllerContext对象，设置当前所有Topic，“活”着的Broker列表，所有Partition的Leader及ISR等。</li>\n<li>启动replicaStateMachine和partitionStateMachine。</li>\n<li>将brokerState状态设置为RunningAsController。</li>\n<li>将每个Partition的Leadership信息发送给所有“活”着的Broker。</li>\n<li>若<code>auto.leader.rebalance.enable</code>配置为true（默认值是true），则启动partition-rebalance线程。</li>\n<li>若<code>delete.topic.enable</code>设置为true且Delete Topic Patch(<code>/admin/delete_topics</code>)中有值，则删除相应的Topic。</li>\n</ol>\n<h2 id=\"Partition重新分配\"><a href=\"#Partition重新分配\" class=\"headerlink\" title=\"Partition重新分配\"></a>Partition重新分配</h2><p>　　管理工具发出重新分配Partition请求后，会将相应信息写到<code>/admin/reassign_partitions</code>上，而该操作会触发ReassignedPartitionsIsrChangeListener，从而通过执行回调函数KafkaController.onPartitionReassignment来完成以下操作：</p>\n<ol>\n<li>将Zookeeper中的AR（Current Assigned Replicas）更新为OAR（Original list of replicas for partition） + RAR（Reassigned replicas）。</li>\n<li>强制更新Zookeeper中的leader epoch，向AR中的每个Replica发送LeaderAndIsrRequest。</li>\n<li>将RAR - OAR中的Replica设置为NewReplica状态。</li>\n<li>等待直到RAR中所有的Replica都与其Leader同步。</li>\n<li>将RAR中所有的Replica都设置为OnlineReplica状态。</li>\n<li>将Cache中的AR设置为RAR。</li>\n<li>若Leader不在RAR中，则从RAR中重新选举出一个新的Leader并发送LeaderAndIsrRequest。若新的Leader不是从RAR中选举而出，则还要增加Zookeeper中的leader epoch。</li>\n<li>将OAR - RAR中的所有Replica设置为OfflineReplica状态，该过程包含两部分。第一，将Zookeeper上ISR中的OAR - RAR移除并向Leader发送LeaderAndIsrRequest从而通知这些Replica已经从ISR中移除；第二，向OAR - RAR中的Replica发送StopReplicaRequest从而停止不再分配给该Partition的Replica。</li>\n<li>将OAR - RAR中的所有Replica设置为NonExistentReplica状态从而将其从磁盘上删除。</li>\n<li>将Zookeeper中的AR设置为RAR。</li>\n<li>删除<code>/admin/reassign_partition</code>。<br>　　<br><strong>注意</strong>：最后一步才将Zookeeper中的AR更新，因为这是唯一一个持久存储AR的地方，如果Controller在这一步之前crash，新的Controller仍然能够继续完成该过程。<br>　　以下是Partition重新分配的案例，OAR = ｛1，2，3｝，RAR = ｛4，5，6｝，Partition重新分配过程中Zookeeper中的AR和Leader/ISR路径如下</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>AR</th>\n<th>leader/isr</th>\n<th>Step</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>{1,2,3}</td>\n<td>1/{1,2,3}</td>\n<td>(initial state)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>1/{1,2,3}</td>\n<td>(step 2)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>1/{1,2,3,4,5,6}</td>\n<td>(step 4)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>4/{1,2,3,4,5,6}</td>\n<td>(step 7)</td>\n</tr>\n<tr>\n<td>{1,2,3,4,5,6}</td>\n<td>4/{4,5,6}</td>\n<td>(step 8)</td>\n</tr>\n<tr>\n<td>{4,5,6}</td>\n<td>4/{4,5,6}</td>\n<td>(step 10)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Follower从Leader-Fetch数据\"><a href=\"#Follower从Leader-Fetch数据\" class=\"headerlink\" title=\"Follower从Leader Fetch数据\"></a>Follower从Leader Fetch数据</h2><p>　　Follower通过向Leader发送FetchRequest获取消息，FetchRequest结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchRequest.png\" alt=\"FetchRequest\"></a><br>　　从FetchRequest的结构可以看出，每个Fetch请求都要指定最大等待时间和最小获取字节数，以及由TopicAndPartition和PartitionFetchInfo构成的Map。实际上，Follower从Leader数据和Consumer从Broker Fetch数据，都是通过FetchRequest请求完成，所以在FetchRequest结构中，其中一个字段是clientID，并且其默认值是ConsumerConfig.DefaultClientId。<br>　　<br>　　Leader收到Fetch请求后，Kafka通过KafkaApis.handleFetchRequest响应该请求，响应过程如下：</p>\n<ol>\n<li>replicaManager根据请求读出数据存入dataRead中。</li>\n<li>如果该请求来自Follower则更新其相应的LEO（log end offset）以及相应Partition的High Watermark</li>\n<li>根据dataRead算出可读消息长度（单位为字节）并存入bytesReadable中。</li>\n<li>满足下面4个条件中的1个，则立即将相应的数据返回</li>\n</ol>\n<ul>\n<li>Fetch请求不希望等待，即fetchRequest.macWait &lt;= 0</li>\n<li>Fetch请求不要求一定能取到消息，即fetchRequest.numPartitions &lt;= 0，也即requestInfo为空</li>\n<li>有足够的数据可供返回，即bytesReadable &gt;= fetchRequest.minBytes</li>\n<li>读取数据时发生异常</li>\n</ul>\n<ol>\n<li>若不满足以上4个条件，FetchRequest将不会立即返回，并将该请求封装成DelayedFetch。检查该DeplayedFetch是否满足，若满足则返回请求，否则将该请求加入Watch列表</li>\n</ol>\n<p>　　Leader通过以FetchResponse的形式将消息返回给Follower，FetchResponse结构如下<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/FetchResponse.png\" alt=\"FetchResponse\"></a></p>\n<p>#Replication工具</p>\n<h2 id=\"Topic-Tool\"><a href=\"#Topic-Tool\" class=\"headerlink\" title=\"Topic Tool\"></a>Topic Tool</h2><p>　　<code>$KAFKA_HOME/bin/kafka-topics.sh</code>，该工具可用于创建、删除、修改、查看某个Topic，也可用于列出所有Topic。另外，该工具还可修改以下配置。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unclean.leader.election.enable</span><br><span class=\"line\">delete.retention.ms</span><br><span class=\"line\">segment.jitter.ms</span><br><span class=\"line\">retention.ms</span><br><span class=\"line\">flush.ms</span><br><span class=\"line\">segment.bytes</span><br><span class=\"line\">flush.messages</span><br><span class=\"line\">segment.ms</span><br><span class=\"line\">retention.bytes</span><br><span class=\"line\">cleanup.policy</span><br><span class=\"line\">segment.index.bytes</span><br><span class=\"line\">min.cleanable.dirty.ratio</span><br><span class=\"line\">max.message.bytes</span><br><span class=\"line\">file.delete.delay.ms</span><br><span class=\"line\">min.insync.replicas</span><br><span class=\"line\">index.interval.bytes</span><br></pre></td></tr></table></figure>\n<h2 id=\"Replica-Verification-Tool\"><a href=\"#Replica-Verification-Tool\" class=\"headerlink\" title=\"Replica Verification Tool\"></a>Replica Verification Tool</h2><p>　　<code>$KAFKA_HOME/bin/kafka-replica-verification.sh</code>，该工具用来验证所指定的一个或多个Topic下每个Partition对应的所有Replica是否都同步。可通过<code>topic-white-list</code>这一参数指定所需要验证的所有Topic，支持正则表达式。 　　</p>\n<h2 id=\"Preferred-Replica-Leader-Election-Tool\"><a href=\"#Preferred-Replica-Leader-Election-Tool\" class=\"headerlink\" title=\"Preferred Replica Leader Election Tool\"></a>Preferred Replica Leader Election Tool</h2><p><strong>用途</strong><br>　　有了Replication机制后，每个Partition可能有多个备份。某个Partition的Replica列表叫作AR（Assigned Replicas），AR中的第一个Replica即为“Preferred Replica”。创建一个新的Topic或者给已有Topic增加Partition时，Kafka保证Preferred Replica被均匀分布到集群中的所有Broker上。理想情况下，Preferred Replica会被选为Leader。以上两点保证了所有Partition的Leader被均匀分布到了集群当中，这一点非常重要，因为所有的读写操作都由Leader完成，若Leader分布过于集中，会造成集群负载不均衡。但是，随着集群的运行，该平衡可能会因为Broker的宕机而被打破，该工具就是用来帮助恢复Leader分配的平衡。<br>　　事实上，每个Topic从失败中恢复过来后，它默认会被设置为Follower角色，除非某个Partition的Replica全部宕机，而当前Broker是该Partition的AR中第一个恢复回来的Replica。因此，某个Partition的Leader（Preferred Replica）宕机并恢复后，它很可能不再是该Partition的Leader，但仍然是Preferred Replica。<br>　　<br><strong>原理</strong></p>\n<ol>\n<li>在Zookeeper上创建<code>/admin/preferred_replica_election</code>节点，并存入需要调整Preferred Replica的Partition信息。</li>\n<li>Controller一直Watch该节点，一旦该节点被创建，Controller会收到通知，并获取该内容。</li>\n<li>Controller读取Preferred Replica，如果发现该Replica当前并非是Leader并且它在该Partition的ISR中，Controller向该Replica发送LeaderAndIsrRequest，使该Replica成为Leader。如果该Replica当前并非是Leader，且不在ISR中，Controller为了保证没有数据丢失，并不会将其设置为Leader。 　</li>\n</ol>\n<p><strong>用法</strong><br>　　<code>$KAFKA_HOME/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</code></p>\n<p>　　在包含8个Broker的Kafka集群上，创建1个名为topic1，replication-factor为3，Partition数为8的Topic，使用<code>$KAFKA_HOME/bin/kafka-topics.sh --describe --topic topic1 --zookeeper localhost:2181</code>命令查看其Partition/Replica分布。</p>\n<p>　　查询结果如下图所示，从图中可以看到，Kafka将所有Replica均匀分布到了整个集群，并且Leader也均匀分布。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_1.png\" alt=\"preferred_topic_test_1\"></a></p>\n<p>　　手动停止部分Broker，topic1的Partition/Replica分布如下图所示。从图中可以看到，由于Broker 1/2/4都被停止，Partition 0的Leader由原来的1变为3，Partition 1的Leader由原来的2变为5，Partition 2的Leader由原来的3变为6，Partition 3的Leader由原来的4变为7。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_2.png\" alt=\"preferred_topic_test_2\"></a>　　<br>　　<br>　　再重新启动ID为1的Broker，topic1的Partition/Replica分布如下。可以看到，虽然Broker 1已经启动（Partition 0和Partition5的ISR中有1），但是1并不是任何一个Parititon的Leader，而Broker 5/6/7都是2个Partition的Leader，即Leader的分布不均衡——一个Broker最多是2个Partition的Leader，而最少是0个Partition的Leader。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_3.png\" alt=\"preferred_topic_test_3\"></a><br>　　<br>　　运行该工具后，topic1的Partition/Replica分布如下图所示。由图可见，除了Partition 1和Partition 3由于Broker 2和Broker 4还未启动，所以其Leader不是其Preferred Repliac外，其它所有Partition的Leader都是其Preferred Replica。同时，与运行该工具前相比，Leader的分配更均匀——一个Broker最多是2个Parittion的Leader，最少是1个Partition的Leader。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_4.png\" alt=\"preferred_topic_test_4\"></a><br>　　<br>　　启动Broker 2和Broker 4，Leader分布与上一步相比并未变化，如下图所示。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/preferred_topic_test_5.png\" alt=\"preferred_topic_test_5\"></a></p>\n<p>　　再次运行该工具，所有Partition的Leader都由其Preferred Replica承担，Leader分布更均匀——每个Broker承担1个Partition的Leader角色。<br>　　<br>　　除了手动运行该工具使Leader分配均匀外，Kafka还提供了自动平衡Leader分配的功能，该功能可通过将<code>auto.leader.rebalance.enable</code>设置为true开启，它将周期性检查Leader分配是否平衡，若不平衡度超过一定阈值则自动由Controller尝试将各Partition的Leader设置为其Preferred Replica。检查周期由<code>leader.imbalance.check.interval.seconds</code>指定，不平衡度阈值由<code>leader.imbalance.per.broker.percentage</code>指定。 　　</p>\n<h2 id=\"Kafka-Reassign-Partitions-Tool\"><a href=\"#Kafka-Reassign-Partitions-Tool\" class=\"headerlink\" title=\"Kafka Reassign Partitions Tool\"></a>Kafka Reassign Partitions Tool</h2><p><strong>用途</strong><br>　　该工具的设计目标与Preferred Replica Leader Election Tool有些类似，都旨在促进Kafka集群的负载均衡。不同的是，Preferred Replica Leader Election只能在Partition的AR范围内调整其Leader，使Leader分布均匀，而该工具还可以调整Partition的AR。<br>　　Follower需要从Leader Fetch数据以保持与Leader同步，所以仅仅保持Leader分布的平衡对整个集群的负载均衡来说是不够的。另外，生产环境下，随着负载的增大，可能需要给Kafka集群扩容。向Kafka集群中增加Broker非常简单方便，但是对于已有的Topic，并不会自动将其Partition迁移到新加入的Broker上，此时可用该工具达到此目的。某些场景下，实际负载可能远小于最初预期负载，此时可用该工具将分布在整个集群上的Partition重装分配到某些机器上，然后可以停止不需要的Broker从而实现节约资源的目的。<br>　　需要说明的是，该工具不仅可以调整Partition的AR位置，还可调整其AR数量，即改变该Topic的replication factor。<br>　　<br><strong>原理</strong><br>　　该工具只负责将所需信息存入Zookeeper中相应节点，然后退出，不负责相关的具体操作，所有调整都由Controller完成。</p>\n<ol>\n<li>在Zookeeper上创建<code>/admin/reassign_partitions</code>节点，并存入目标Partition列表及其对应的目标AR列表。</li>\n<li>Controller注册在<code>/admin/reassign_partitions</code>上的Watch被fire，Controller获取该列表。</li>\n<li>对列表中的所有Partition，Controller会做如下操作：</li>\n</ol>\n<ul>\n<li>启动<code>RAR - AR</code>中的Replica，即新分配的Replica。（RAR = Reassigned Replicas， AR = Assigned Replicas）</li>\n<li>等待新的Replica与Leader同步</li>\n<li>如果Leader不在RAR中，从RAR中选出新的Leader</li>\n<li>停止并删除<code>AR - RAR</code>中的Replica，即不再需要的Replica</li>\n<li>删除<code>/admin/reassign_partitions</code>节点</li>\n</ul>\n<p><strong>用法</strong><br>　　该工具有三种使用模式</p>\n<ul>\n<li>generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）</li>\n<li>execute模式，根据指定的reassign plan重新分配Partition</li>\n<li>verify模式，验证重新分配Partition是否成功</li>\n</ul>\n<p>　　下面这个例子将使用该工具将Topic的所有Partition重新分配到Broker 4/5/6/7上，步骤如下：</p>\n<ol>\n<li><p>使用generate模式，生成reassign plan。指定需要重新分配的Topic （{“topics”:[{“topic”:”topic1”}],”version”:1}），并存入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/tmp/topics-to-move.json</span><br></pre></td></tr></table></figure>\n<p>文件中，然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">\t--zookeeper localhost:2181 </span><br><span class=\"line\">\t--topics-to-move-json-file /tmp/topics-to-move.json  </span><br><span class=\"line\">\t--broker-list &quot;4,5,6,7&quot; --generate</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>　　结果如下图所示<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_1.png\" alt=\"reassign_1\"></a><br>　　<br>2.　使用execute模式，执行reassign plan<br>　　将上一步生成的reassignment plan存入<code>/tmp/reassign-plan.json</code>文件中，并执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   $KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">--zookeeper localhost:2181     </span><br><span class=\"line\">--reassignment-json-file /tmp/reassign-plan.json --execute</span><br></pre></td></tr></table></figure>\n<p><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_2.png\" alt=\"reassign_2\"></a></p>\n<p>　　此时，Zookeeper上<code>/admin/reassign_partitions</code>节点被创建，且其值与<code>/tmp/reassign-plan.json</code>文件的内容一致。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_3.png\" alt=\"reassign_3\"></a></p>\n<p>3.　使用verify模式，验证reassign是否完成。执行verify命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$KAFKA_HOME/bin/kafka-reassign-partitions.sh </span><br><span class=\"line\">--zookeeper localhost:2181 --verify</span><br><span class=\"line\">--reassignment-json-file /tmp/reassign-plan.json</span><br></pre></td></tr></table></figure>\n<p>　　结果如下所示，从图中可看出topic1的所有Partititon都重新分配成功。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_4.png\" alt=\"reassign_4\"></a></p>\n<p>　　接下来用Topic Tool再次验证。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic1</span><br></pre></td></tr></table></figure>\n<p>　　结果如下图所示，从图中可看出topic1的所有Partition都被重新分配到Broker 4/5/6/7，且每个Partition的AR与reassign plan一致。<br><a href=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png\" target=\"_blank\" rel=\"noopener\"><img src=\"http://www.jasongj.com/img/kafka/KafkaColumn3/reassign_5.png\" alt=\"reassign_5\"></a></p>\n<p>　　需要说明的是，在使用execute之前，并不一定要使用generate模式自动生成reassign plan，使用generate模式只是为了方便。事实上，某些场景下，generate模式生成的reassign plan并不一定能满足需求，此时用户可以自己设置reassign plan。 　　</p>\n<h2 id=\"State-Change-Log-Merge-Tool\"><a href=\"#State-Change-Log-Merge-Tool\" class=\"headerlink\" title=\"State Change Log Merge Tool\"></a>State Change Log Merge Tool</h2><p><strong>用途</strong><br>　　该工具旨在从整个集群的Broker上收集状态改变日志，并生成一个集中的格式化的日志以帮助诊断状态改变相关的故障。每个Broker都会将其收到的状态改变相关的的指令存于名为<code>state-change.log</code>的日志文件中。某些情况下，Partition的Leader Election可能会出现问题，此时我们需要对整个集群的状态改变有个全局的了解从而诊断故障并解决问题。该工具将集群中相关的<code>state-change.log</code>日志按时间顺序合并，同时支持用户输入时间范围和目标Topic及Partition作为过滤条件，最终将格式化的结果输出。<br>　　<br><strong>用法</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-run-class.sh kafka.tools.StateChangeLogMerger</span><br><span class=\"line\">--logs /opt/kafka_2.11-0.8.2.1/logs/state-change.log</span><br><span class=\"line\">--topic topic1 --partitions 0,1,2,3,4,5,6,7</span><br></pre></td></tr></table></figure>"},{"title":"ClickHouse系统架构概述","date":"2019-05-27T13:01:29.000Z","_content":"\n# ClickHouse系统架构概述\n\n## ClickHouse独特功能\n\n### 真正的列式数据库管理系统\n\n在一个真正的列式数据库管理系统中，除了数据本身外不应该存在其他额外的数据。这意味着为了避免在值旁边存储它们的长度“number”，你必须支持固定长度数值类型。例如，10亿个UInt8类型的数据在未压缩的情况下大约消耗1GB左右的空间，如果不是这样的话，这将对CPU的使用产生强烈影响。即使是在未压缩的情况下，紧凑的存储数据也是非常重要的，因为解压缩的速度主要取决于未压缩数据的大小。\n\n这是非常值得注意的，因为在一些其他系统中也可以将不同的列分别进行存储，但由于对其他场景进行的优化，使其无法有效的处理分析查询。例如： HBase，BigTable，Cassandra，HyperTable。在这些系统中，你可以得到每秒数十万的吞吐能力，但是无法得到每秒几亿行的吞吐能力。\n\n需要说明的是，ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。\n\n<!-- more -->\n\n### 数据压缩\n\n在一些列式数据库管理系统中(例如：InfiniDB CE and MonetDB) 不是用数据压缩。但是, 数据压缩在实现优异的存储系统中确实起着关键的作用。\n\n\n\n### 数据的磁盘存储\n\n许多的列式数据库(如 SAP HANA, Google PowerDrill)只能在内存中工作，这种方式会造成比实际更多的设备预算。ClickHouse被设计用于工作在传统磁盘上的系统，它提供每GB更低的存储成本，但如果有可以使用SSD和内存，它也会合理的利用这些资源\n\n\n\n### 多核心并行处理\n\n大型查询可以以很自然的方式在ClickHouse中进行并行化处理，以此来使用当前服务器上可用的所有资源。\n\n\n\n### 多服务器分布式处理\n\n上面提到的列式数据库管理系统中，几乎没有一个支持分布式的查询处理。 在ClickHouse中，数据可以保存在不同的shard上，每一个shard都由一组用于容错的replica组成，查询可以并行的在所有shard上进行处理。这些对用户来说是透明的\n\n\n\n### 支持SQL\n\nClickHouse支持基于SQL的查询语言，该语言大部分情况下是与SQL标准兼容的。 支持的查询包括 GROUP BY，ORDER BY，IN，JOIN以及非相关子查询。 不支持窗口函数和相关子查询。\n\n\n\n### 向量引擎\n\n为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理。\n\n\n\n### 实时的数据更新\n\nClickHouse支持在表中定义主键。为了使查询能够快速在主键中进行范围查找，数据总是以增量的方式有序的存储在MergeTree中。因此，数据可以持续不断高效的写入到表中，并且写入的过程中不会存在任何加锁的行为。\n\n\n\n### 索引\n\n按照主键对数据进行排序，这将帮助ClickHouse以几十毫秒的低延迟对数据进行特定值查找或范围查找。\n\n\n\n### 适合在线查询\n\n在线查询意味着在没有对数据做任何预处理的情况下以极低的延迟处理查询并将结果加载到用户的页面中。\n\n\n\n### 支持近似计算\n\nClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法：\n\n1. 用于近似计算的各类聚合函数，如：distinct values, medians, quantiles\n2. 基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。\n3. 不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。\n\n\n\n### 支持数据复制和数据完整性\n\nClickHouse使用异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据。在大多数情况下ClickHouse能在故障后自动恢复，在一些复杂的情况下需要少量的手动恢复。\n\n\n\n### ClickHouse可以考虑缺点的功能\n\n1. 没有完整的事物支持。\n2. 缺少高频率，低延迟的修改或删除已存在数据的能力。仅能用于批量删除或修改数据，但这符合 [GDPR](https://gdpr-info.eu/)。\n3. 稀疏索引使得ClickHouse不适合通过其键检索单行的点查询\n\n\n\n# ClickHouse性能\n\n根据Yandex的内部测试结果，ClickHouse表现出了比同类可比较产品更优的性能。你可以在 [这里](https://clickhouse.yandex/benchmark.html) 查看具体的测试结果。\n\n许多其他的测试也证实这一点。你可以使用互联网搜索到它们，或者你也可以从 [我们收集的部分相关连接](https://clickhouse.yandex/#independent-benchmarks) 中查看。\n\n\n\n## 单个大查询的吞吐量\n\n吞吐量可以使用每秒处理的行数或每秒处理的字节数来衡量。如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以2-10GB／s（未压缩）的速度进行处理（对于简单的查询，速度可以达到30GB／s）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以400MB／s的速度读取数据，并且数据压缩率是3，则数据的处理速度为1.2GB/s。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是1-2亿行每秒。\n\n对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。\n\n\n\n## 处理短查询的延迟时间\n\n如果一个查询使用主键并且没有太多行(几十万)进行处理，并且没有查询太多的列，那么在数据被page cache缓存的情况下，它的延迟应该小于50毫秒(在最佳的情况下应该小于10毫秒)。 否则，延迟取决于数据的查找次数。如果你当前使用的是HDD，在数据没有加载的情况下，查询所需要的延迟可以通过以下公式计算得知： 查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量。\n\n\n\n## 处理大量短查询的吞吐量\n\n在相同的情况下，ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。因此我们建议每秒最多查询100次。\n\n\n\n## 数据的写入性能\n\n我们建议每次写入不少于1000行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为50到200MB/s。如果您写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。\n\n\n\n# Yandex.Metrica的使用案例\n\nClickHouse最初是为 [Yandex.Metrica](https://metrica.yandex.com/) [世界第二大Web分析平台](http://w3techs.com/technologies/overview/traffic_analysis/all) 而开发的。多年来一直作为该系统的核心组件被该系统持续使用着。目前为止，该系统在ClickHouse中有超过13万亿条记录，并且每天超过200多亿个事件被处理。它允许直接从原始数据中动态查询并生成报告。本文简要介绍了ClickHouse在其早期发展阶段的目标。\n\nYandex.Metrica基于用户定义的字段，对实时访问、连接会话，生成实时的统计报表。这种需求往往需要复杂聚合方式，比如对访问用户进行去重。构建报表的数据，是实时接收存储的新数据。\n\n截至2014年4月，Yandex.Metrica每天跟踪大约120亿个事件（用户的点击和浏览）。为了可以创建自定义的报表，我们必须存储全部这些事件。同时，这些查询可能需要在几百毫秒内扫描数百万行的数据，或在几秒内扫描数亿行的数据。\n\n\n\n## Yandex.Metrica以及其他Yandex服务的使用案例\n\n在Yandex.Metrica中，ClickHouse被用于多个场景中。 它的主要任务是使用原始数据在线的提供各种数据报告。它使用374台服务器的集群，存储了20.3万亿行的数据。在去除重复与副本数据的情况下，压缩后的数据达到了2PB。未压缩前（TSV格式）它大概有17PB。\n\nClickHouse还被使用在：\n\n- 存储来自Yandex.Metrica回话重放数据。\n- 处理中间数据\n- 与Analytics一起构建全球报表。\n- 为调试Yandex.Metrica引擎运行查询\n- 分析来自API和用户界面的日志数据\n\nClickHouse在其他Yandex服务中至少有12个安装：search verticals, Market, Direct, business analytics, mobile development, AdFox, personal services等。\n\n\n\n## 聚合与非聚合数据\n\n有一种流行的观点认为，想要有效的计算统计数据，必须要聚合数据，因为聚合将降低数据量。\n\n但是数据聚合是一个有诸多限制的解决方案，例如：\n\n- 你必须提前知道用户定义的报表的字段列表\n- 用户无法自定义报表\n- 当聚合条件过多时，可能不会减少数据，聚合是无用的。\n- 存在大量报表时，有太多的聚合变化（组合爆炸）\n- 当聚合条件有非常大的基数时（如：url），数据量没有太大减少（少于两倍）\n- 聚合的数据量可能会增长而不是收缩\n- 用户不会查看我们为他生成的所有报告，大部分计算将是无用的\n- 各种聚合可能违背了数据的逻辑完整性\n\n如果我们直接使用非聚合数据而不尽兴任何聚合时，我们的计算量可能是减少的。\n\n然而，相对于聚合中很大一部分工作被离线完成，在线计算需要尽快的完成计算，因为用户在等待结果。\n\nYandex.Metrica 有一个专门用于聚合数据的系统，称为Metrage，它可以用作大部分报表。 从2009年开始，Yandex.Metrica还为非聚合数据使用专门的OLAP数据库，称为OLAPServer，它以前用于报表构建系统。 OLAPServer可以很好的工作在非聚合数据上，但是它有诸多限制，导致无法根据需要将其用于所有报表中。如，缺少对数据类型的支持（只支持数据），无法实时增量的更新数据（只能通过每天重写数据完成）。OLAPServer不是一个数据库管理系统，它只是一个数据库。\n\n为了消除OLAPServer的这些局限性，解决所有报表使用非聚合数据的问题，我们开发了ClickHouse数据库管理系统。\n\n\n\n# ClickHouse表引擎\n\n表引擎（即表的类型）决定了：\n\n- 数据的存储方式和位置，写到哪里以及从哪里读取数据\n- 支持哪些查询以及如何支持。\n- 并发数据访问。\n- 索引的使用（如果存在）。\n- 是否可以执行多线程请求。\n- 数据复制参数。\n\n在读取时，引擎只需要输出所请求的列，但在某些情况下，引擎可以在响应请求时部分处理数据。\n\n对于大多数正式的任务，应该使用MergeTree族中的引擎。\n\n\n\n## MergeTree\n\nClickhouse 中最强大的表引擎当属 `MergeTree` （合并树）引擎及该系列（`*MergeTree`）中的其他引擎。\n\n`MergeTree` 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。\n\n主要特点:\n\n- 存储的数据按主键排序。\n\n  这让你可以创建一个用于快速检索数据的小稀疏索引。\n\n- 允许使用分区，如果指定了 [主键](https://clickhouse.yandex/docs/zh/operations/table_engines/custom_partitioning_key/) 的话。\n\n  在相同数据集和相同结果集的情况下 ClickHouse 中某些带分区的操作会比普通操作更快。查询中指定了分区键时 ClickHouse 会自动截取分区数据。这也有效增加了查询性能。\n\n- 支持数据副本。\n\n  `ReplicatedMergeTree` 系列的表便是用于此。更多信息，请参阅 [数据副本](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/) 一节。\n\n- 支持数据采样。\n\n  需要的话，你可以给表设置一个采样方法。\n\n!!! 注意 [Merge](https://clickhouse.yandex/docs/zh/operations/table_engines/merge/) 引擎并不属于 `*MergeTree` 系列。\n\n\n\n### 建表\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,\n    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2\n) ENGINE = MergeTree()\n[PARTITION BY expr]\n[ORDER BY expr]\n[PRIMARY KEY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n请求参数的描述，参考 [请求描述](https://clickhouse.yandex/docs/zh/query_language/create/) 。\n\n\n\n示例：\n\n```sql\nCREATE TABLE xcloud.cdn_nginx_log_minute_agg\n(\n\tdate Date, \n\n    timeStamp DateTime,\n\n\tchannel String, \n\n\tcustomer String, \n\n\tcountry String, \n\n\tflow AggregateFunction(sum, Int64), \n\n\tvisit AggregateFunction(sum, Int64),  \n\n\tdownload_time AggregateFunction(sum, Int64), \n\n\tresponse_time AggregateFunction(sum, Int64), \n\n\tupstream_response_time AggregateFunction(sum, Int64), \n\n\tfirst_byte_time AggregateFunction(sum, Int64), \n\n\trequest_time AggregateFunction(sum, Int64), \n\n\tdownload_flow AggregateFunction(sum, Int64), \n\n\tresponse_normal AggregateFunction(sum, Int64)\n)  \n\nENGINE = ReplicatedAggregatingMergeTree('{zkpath}', '{replica}')  \n\nPARTITION BY date  ORDER BY (timeStamp, date, channel, customer, country);\n```\n\n\n\n### 数据存储\n\n表由按主键排序的数据 *片段* 组成。\n\n当数据被插入到表中时，会分成数据片段并按主键的字典序排序。例如，主键是 `(CounterID, Date)` 时，片段中数据按 `CounterID` 排序，具有相同 `CounterID` 的部分按 `Date` 排序。\n\n不同分区的数据会被分成不同的片段，ClickHouse 在后台合并数据片段以便更高效存储。不会合并来自不同分区的数据片段。这个合并机制并不保证相同主键的所有行都会合并到同一个数据片段中。\n\nClickHouse 会为每个数据片段创建一个索引文件，索引文件包含每个索引行（『标记』）的主键值。索引行号定义为 `n * index_granularity` 。最大的 `n` 等于总行数除以 `index_granularity` 的值的整数部分。对于每列，跟主键相同的索引行处也会写入『标记』。这些『标记』让你可以直接找到数据所在的列。\n\n你可以只用一单一大表并不断地一块块往里面加入数据 – `MergeTree` 引擎的就是为了这样的场景。\n\n\n\n### 主键和索引在查询中的表现\n\n我们以 `(CounterID, Date)` 以主键。排序好的索引的图示会是下面这样：\n\n```\n全部数据  :     [-------------------------------------------------------------------------]\nCounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]\nDate:           [1111111222222233331233211111222222333211111112122222223111112223311122333]\n标记:            |      |      |      |      |      |      |      |      |      |      |\n                a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3\n标记号:          0      1      2      3      4      5      6      7      8      9      10\n```\n\n如果指定查询如下：\n\n- `CounterID in ('a', 'h')`，服务器会读取标记号在 `[0, 3)` 和 `[6, 8)` 区间中的数据。\n- `CounterID IN ('a', 'h') AND Date = 3`，服务器会读取标记号在 `[1, 3)` 和 `[7, 8)` 区间中的数据。\n- `Date = 3`，服务器会读取标记号在 `[1, 10]` 区间中的数据。\n\n上面例子可以看出使用索引通常会比全表描述要高效。\n\n稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 `index_granularity * 2` 行额外的数据。大部分情况下，当 `index_granularity = 8192` 时，ClickHouse的性能并不会降级。\n\n稀疏索引让你能操作有巨量行的表。因为这些索引是常驻内存（RAM）的。\n\nClickHouse 不要求主键惟一。所以，你可以插入多条具有相同主键的行。\n\n\n\n### 主键的选择\n\n主键中列的数量并没有明确的限制。依据数据结构，你应该让主键包含多些或少些列。这样可以：\n\n- 改善索引的性能。\n\n  如果当前主键是 `(a, b)` ，然后加入另一个 `c` 列，满足下面条件时，则可以改善性能： - 有带有 `c` 列条件的查询。 - 很长的数据范围（ `index_granularity` 的数倍）里 `(a, b)` 都是相同的值，并且这种的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。\n\n- 改善数据压缩。\n\n  ClickHouse 以主键排序片段数据，所以，数据的一致性越高，压缩越好。\n\n- [CollapsingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/collapsingmergetree/#table_engine-collapsingmergetree) 和 [SummingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/) 引擎里，数据合并时，会有额外的处理逻辑。\n\n  在这种情况下，指定一个跟主键不同的 *排序键* 也是有意义的。\n\n长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 `SELECT` 查询的性能。\n\n\n\n### 选择跟排序键不一样主键\n\n指定一个跟排序键（用于排序数据片段中行的表达式） 不一样的主键（用于计算写到索引文件的每个标记值的表达式）是可以的。 这种情况下，主键表达式元组必须是排序键表达式元组的一个前缀。\n\n当使用 [SummingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/) 和 [AggregatingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/aggregatingmergetree/) 引擎时，这个特性非常有用。 通常，使用这类引擎时，表里列分两种：*维度* 和 *度量* 。 典型的查询是在 `GROUP BY` 并过虑维度的情况下统计度量列的值。 像 SummingMergeTree 和 AggregatingMergeTree ，用相同的排序键值统计行时， 通常会加上所有的维度。结果就是，这键的表达式会是一长串的列组成， 并且这组列还会因为新加维度必须频繁更新。\n\n这种情况下，主键中仅预留少量列保证高效范围扫描， 剩下的维度列放到排序键元组里。这样是合理的。\n\n[排序键的修改](https://clickhouse.yandex/docs/zh/query_language/alter/) 是轻量级的操作，因为一个新列同时被加入到表里和排序键后时，已存在的数据片段并不需要修改。由于旧的排序键是新排序键的前缀，并且刚刚添加的列中没有数据，因此在表修改时的数据对于新旧的排序键来说都是有序的。\n\n\n\n### 索引和分区在查询中的应用\n\n对于 `SELECT` 查询，ClickHouse 分析是否可以使用索引。如果 `WHERE/PREWHERE` 子句具有下面这些表达式（作为谓词链接一子项或整个）则可以使用索引：基于主键或分区键的列或表达式的部分的等式或比较运算表达式；基于主键或分区键的列或表达式的固定前缀的 `IN` 或 `LIKE` 表达式；基于主键或分区键的列的某些函数；基于主键或分区键的表达式的逻辑表达式。\n\n因此，在索引键的一个或多个区间上快速地跑查询都是可能的。下面例子中，指定标签；指定标签和日期范围；指定标签和日期；指定多个标签和日期范围等运行查询，都会非常快。\n\n要检查 ClickHouse 执行一个查询时能否使用索引，可设置 [force_index_by_date](https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-force_index_by_date) 和 [force_primary_key](https://clickhouse.yandex/docs/zh/operations/settings/settings/) 。\n\n按月分区的分区键是只能读取包含适当范围日期的数据块。这种情况下，数据块会包含很多天（最多整月）的数据。在块中，数据按主键排序，主键第一列可能不包含日期。因此，仅使用日期而没有带主键前缀条件的查询将会导致读取超过这个日期范围。\n\n\n\n### 并发数据访问\n\n应对表的并发访问，我们使用多版本机制。换言之，当同时读和更新表时，数据从当前查询到的一组片段中读取。没有冗长的的锁。插入不会阻碍读取。\n\n对表的读操作是自动并行的。\n\n\n\n## 数据副本\n\n只有 MergeTree 系列里的表可支持副本：\n\n- ReplicatedMergeTree\n- ReplicatedSummingMergeTree\n- ReplicatedReplacingMergeTree\n- ReplicatedAggregatingMergeTree\n- ReplicatedCollapsingMergeTree\n- ReplicatedVersionedCollapsingMergeTree\n- ReplicatedGraphiteMergeTree\n\n副本是表级别的，不是整个服务器级的。所以，服务器里可以同时有复制表和非复制表。\n\n副本不依赖分片。每个分片有它自己的独立副本。\n\n对于 `INSERT` 和 `ALTER` 语句操作数据的会在压缩的情况下被复制（更多信息，看 [ALTER](https://clickhouse.yandex/docs/zh/query_language/alter/#query_language_queries_alter) ）。\n\n而 `CREATE`，`DROP`，`ATTACH`，`DETACH` 和 `RENAME` 语句只会在单个服务器上执行，不会被复制。\n\n- `The CREATE TABLE` 在运行此语句的服务器上创建一个新的可复制表。如果此表已存在其他服务器上，则给该表添加新副本。\n- `The DROP TABLE` 删除运行此查询的服务器上的副本。\n- `The RENAME` 重命名一个副本。换句话说，可复制表不同的副本可以有不同的名称。\n\n要使用副本，需在配置文件中设置 ZooKeeper 集群的地址。\n\n`SELECT` 查询并不需要借助 ZooKeeper ，复本并不影响 `SELECT` 的性能，查询复制表与非复制表速度是一样的。查询分布式表时，ClickHouse的处理方式可通过设置 [max_replica_delay_for_distributed_queries](https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries) 和 [fallback_to_stale_replicas_for_distributed_queries](https://clickhouse.yandex/docs/zh/operations/settings/settings/) 修改。\n\n对于每个 `INSERT` 语句，会通过几个事务将十来个记录添加到 ZooKeeper。（确切地说，这是针对每个插入的数据块; 每个 INSERT 语句的每 `max_insert_block_size = 1048576` 行和最后剩余的都各算作一个块。）相比非复制表，写 zk 会导致 `INSERT` 的延迟略长一些。但只要你按照建议每秒不超过一个 `INSERT` 地批量插入数据，不会有任何问题。一个 ZooKeeper 集群能给整个 ClickHouse 集群支撑协调每秒几百个 `INSERT`。数据插入的吞吐量（每秒的行数）可以跟不用复制的数据一样高。\n\n对于非常大的集群，你可以把不同的 ZooKeeper 集群用于不同的分片。然而，即使 Yandex.Metrica 集群（大约300台服务器）也证明还不需要这么做。\n\n复制是多主异步。 `INSERT` 语句（以及 `ALTER` ）可以发给任意可用的服务器。数据会先插入到执行该语句的服务器上，然后被复制到其他服务器。由于它是异步的，在其他副本上最近插入的数据会有一些延迟。如果部分副本不可用，则数据在其可用时再写入。副本可用的情况下，则延迟时长是通过网络传输压缩数据块所需的时间。\n\n默认情况下，INSERT 语句仅等待一个副本写入成功后返回。如果数据只成功写入一个副本后该副本所在的服务器不再存在，则存储的数据会丢失。要启用数据写入多个副本才确认返回，使用 `insert_quorum` 选项。\n\n单个数据块写入是原子的。 INSERT 的数据按每块最多 `max_insert_block_size = 1048576` 行进行分块，换句话说，如果 `INSERT` 插入的行少于 1048576，则该 INSERT 是原子的。\n\n数据块会去重。对于被多次写的相同数据块（大小相同且具有相同顺序的相同行的数据块），该块仅会写入一次。这样设计的原因是万一在网络故障时客户端应用程序不知道数据是否成功写入DB，此时可以简单地重复 `INSERT` 。把相同的数据发送给多个副本 INSERT 并不会有问题。因为这些 `INSERT` 是完全相同的（会被去重）。去重参数参看服务器设置 [merge_tree](https://clickhouse.yandex/docs/zh/operations/server_settings/settings/) 。（注意：Replicated*MergeTree 才会去重，不需要 zookeeper 的不带 MergeTree 不会去重）\n\n在复制期间，只有要插入的源数据通过网络传输。进一步的数据转换（合并）会在所有副本上以相同的方式进行处理执行。这样可以最大限度地减少网络使用，这意味着即使副本在不同的数据中心，数据同步也能工作良好。（能在不同数据中心中的同步数据是副本机制的主要目标。）\n\n你可以给数据做任意多的副本。Yandex.Metrica 在生产中使用双副本。某一些情况下，给每台服务器都使用 RAID-5 或 RAID-6 和 RAID-10。是一种相对可靠和方便的解决方案。\n\n系统会监视副本数据同步情况，并能在发生故障后恢复。故障转移是自动的（对于小的数据差异）或半自动的（当数据差异很大时，这可能意味是有配置错误）。\n\n\n\n### 创建复制表\n\n在表引擎名称上加上 `Replicated` 前缀。例如：`ReplicatedMergeTree`。\n\n**Replicated\\*MergeTree 参数**\n\n- `zoo_path` — ZooKeeper 中该表的路径。\n- `replica_name` — ZooKeeper 中的该表的副本名称。\n\n示例:\n\n```sql\nCREATE TABLE table_name\n(\n    EventDate DateTime,\n    CounterID UInt32,\n    UserID UInt32\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}')\nPARTITION BY toYYYYMM(EventDate)\nORDER BY (CounterID, EventDate, intHash32(UserID))\nSAMPLE BY intHash32(UserID)\n```\n\n如上例所示，这些参数可以包含宏替换的占位符，即大括号的部分。它们会被替换为配置文件里 'macros' 那部分配置的值。示例：\n\n```xml\n<macros>\n    <layer>05</layer>\n    <shard>02</shard>\n    <replica>example05-02-1.yandex.ru</replica>\n</macros>\n```\n\n“ZooKeeper 中该表的路径”对每个可复制表都要是唯一的。不同分片上的表要有不同的路径。 这种情况下，路径包含下面这些部分：\n\n`/clickhouse/tables/` 是公共前缀，我们推荐使用这个。\n\n`{layer}-{shard}` 是分片标识部分。在此示例中，由于 Yandex.Metrica 集群使用了两级分片，所以它是由两部分组成的。但对于大多数情况来说，你只需保留 {shard} 占位符即可，它会替换展开为分片标识。\n\n```\ntable_name` 是该表在 ZooKeeper 中的名称。使其与 ClickHouse 中的表名相同比较好。 这里它被明确定义，跟 ClickHouse 表名不一样，它并不会被 RENAME 语句修改。\n*HINT*: you could add a database name in front of `table_name` as well. E.g. `db_name.table_name\n```\n\n副本名称用于标识同一个表分片的不同副本。你可以使用服务器名称，如上例所示。同个分片中不同副本的副本名称要唯一。\n\n你也可以显式指定这些参数，而不是使用宏替换。对于测试和配置小型集群这可能会很方便。但是，这种情况下，则不能使用分布式 DDL 语句（`ON CLUSTER`）。\n\n使用大型集群时，我们建议使用宏替换，因为它可以降低出错的可能性。\n\n在每个副本服务器上运行 `CREATE TABLE` 查询。将创建新的复制表，或给现有表添加新副本。\n\n如果其他副本上已包含了某些数据，在表上添加新副本，则在运行语句后，数据会从其他副本复制到新副本。换句话说，新副本会与其他副本同步。\n\n要删除副本，使用 `DROP TABLE`。但它只删除那个 – 位于运行该语句的服务器上的副本。\n\n\n\n### 故障恢复\n\n如果服务器启动时 ZooKeeper 不可用，则复制表会切换为只读模式。系统会定期尝试去连接 ZooKeeper。\n\n如果在 `INSERT` 期间 ZooKeeper 不可用，或者在与 ZooKeeper 交互时发生错误，则抛出异常。\n\n连接到 ZooKeeper 后，系统会检查本地文件系统中的数据集是否与预期的数据集（ ZooKeeper 存储此信息）一致。如果存在轻微的不一致，系统会通过与副本同步数据来解决。\n\n如果系统检测到损坏的数据片段（文件大小错误）或无法识别的片段（写入文件系统但未记录在 ZooKeeper 中的部分），则会把它们移动到 'detached' 子目录（不会删除）。而副本中其他任何缺少的但正常数据片段都会被复制同步。\n\n注意，ClickHouse 不会执行任何破坏性操作，例如自动删除大量数据。\n\n当服务器启动（或与 ZooKeeper 建立新会话）时，它只检查所有文件的数量和大小。 如果文件大小一致但中间某处已有字节被修改过，不会立即被检测到，只有在尝试读取 `SELECT` 查询的数据时才会检测到。该查询会引发校验和不匹配或压缩块大小不一致的异常。这种情况下，数据片段会添加到验证队列中，并在必要时从其他副本中复制。\n\n如果本地数据集与预期数据的差异太大，则会触发安全机制。服务器在日志中记录此内容并拒绝启动。这种情况很可能是配置错误，例如，一个分片上的副本意外配置为别的分片上的副本。然而，此机制的阈值设置得相当低，在正常故障恢复期间可能会出现这种情况。在这种情况下，数据恢复则是半自动模式，通过用户主动操作触发。\n\n要触发启动恢复，可在 ZooKeeper 中创建节点 `/path_to_table/replica_name/flags/force_restore_data`，节点值可以是任何内容，或运行命令来恢复所有的可复制表：\n\n```sql\nsudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data\n```\n\n然后重启服务器。启动时，服务器会删除这些标志并开始恢复。\n\n\n\n### 在数据完全丢失后的恢复\n\n如果其中一个服务器的所有数据和元数据都消失了，请按照以下步骤进行恢复：\n\n1. 在服务器上安装 ClickHouse。在包含分片标识符和副本的配置文件中正确定义宏配置，如果有用到的话，\n2. 如果服务器上有非复制表则必须手动复制，可以从副本服务器上（在 `/var/lib/clickhouse/data/db_name/table_name/` 目录中）复制它们的数据。\n3. 从副本服务器上中复制位于 `/var/lib/clickhouse/metadata/` 中的表定义信息。如果在表定义信息中显式指定了分片或副本标识符，请更正它以使其对应于该副本。（另外，启动服务器，然后会在 `/var/lib/clickhouse/metadata/` 中的.sql文件中生成所有的 `ATTACH TABLE` 语句。） 4.要开始恢复，ZooKeeper 中创建节点 `/path_to_table/replica_name/flags/force_restore_data`，节点内容不限，或运行命令来恢复所有复制的表：`sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data`\n\n然后启动服务器（如果它已运行则重启）。数据会从副本中下载。\n\n另一种恢复方式是从 ZooKeeper（`/path_to_table/replica_name`）中删除有数据丢的副本的所有元信息，然后再按照“[创建可复制表](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/#creating-replicated-tables)”中的描述重新创建副本。\n\n恢复期间的网络带宽没有限制。特别注意这一点，尤其是要一次恢复很多副本。\n\n\n\n## 自定义分区键\n\n[MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/) 系列的表（包括 [可复制表](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/) ）可以使用分区。基于 MergeTree 表的 [物化视图](https://clickhouse.yandex/docs/zh/operations/table_engines/materializedview/) 也支持分区。\n\n一个分区是指按指定规则逻辑组合一起的表的记录集。可以按任意标准进行分区，如按月，按日或按事件类型。为了减少需要操作的数据，每个分区都是分开存储的。访问数据时，ClickHouse 尽量使用这些分区的最小子集。\n\n分区是在 [建表](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#table_engine-mergetree-creating-a-table) 的 `PARTITION BY expr` 子句中指定。分区键可以是关于列的任何表达式。例如，指定按月分区，表达式为 `toYYYYMM(date_column)`：\n\n```sql\nCREATE TABLE visits\n(\n    VisitDate Date, \n    Hour UInt8, \n    ClientID UUID\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(VisitDate)\nORDER BY Hour;\n```\n\n分区键也可以是表达式元组（类似 [主键](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#primary-keys-and-indexes-in-queries) ）。例如：\n\n```sql\nENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)\nPARTITION BY (toMonday(StartDate), EventType)\nORDER BY (CounterID, StartDate, intHash32(UserID));\n```\n\n上例中，我们设置按一周内的事件类型分区。\n\n新数据插入到表中时，这些数据会存储为按主键排序的新片段（块）。插入后 10-15 分钟，同一分区的各个片段会合并为一整个片段。\n\n**那些有相同分区表达式值的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量和需要找开的文件描述符过多，导致 `SELECT` 查询效率不佳。**\n\n\n\n## ReplacingMergeTree 引擎\n\n该引擎和[MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)的不同之处在于它会删除具有相同主键的重复项。\n\n数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，因此你无法预先作出计划。有一些数据可能仍未被处理。尽管你可以调用 `OPTIMIZE` 语句发起计划外的合并，但请不要指望使用它，因为 `OPTIMIZE` 语句会引发对大量数据的读和写。\n\n因此，`ReplacingMergeTree` 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。\n\n\n\n建表:\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = ReplacingMergeTree([ver])\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n## SummingMergeTree 引擎\n\n该引擎继承自 [MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)。区别在于，当合并 `SummingMergeTree` 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度。\n\n我们推荐将该引擎和 `MergeTree` 一起使用。例如，在准备做报告的时候，将完整的数据存储在 `MergeTree` 表中，并且使用 `SummingMergeTree` 来存储聚合数据。这种方法可以使你避免因为使用不正确的主键组合方式而丢失有价值的数据。\n\n\n\n建表：\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = SummingMergeTree([columns])\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n当数据被插入到表中时，他们将被原样保存。ClickHouse 定期合并插入的数据片段，并在这个时候对所有具有相同主键的行中的列进行汇总，将这些行替换为包含汇总数据的一行记录。\n\nClickHouse 会按片段合并数据，以至于不同的数据片段中会包含具有相同主键的行，即单个汇总片段将会是不完整的。因此，聚合函数 [sum()](https://clickhouse.yandex/docs/zh/query_language/agg_functions/reference/#agg_function-sum) 和 `GROUP BY` 子句应该在（`SELECT`）查询语句中被使用，如上文中的例子所述。\n\n\n\n列中数值类型的值会被汇总。这些列的集合在参数 `columns` 中被定义。\n\n如果用于汇总的所有列中的值均为0，则该行会被删除。\n\n如果列不在主键中且无法被汇总，则会在现有的值中任选一个。\n\n主键所在的列中的值不会被汇总。\n\n\n\n## AggregatingMergeTree 引擎\n\n该引擎继承自 [MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)，并改变了数据片段的合并逻辑。 ClickHouse 会将相同主键的所有行（在一个数据片段内）替换为单个存储一系列聚合函数状态的行。\n\n可以使用 `AggregatingMergeTree` 表来做增量数据统计聚合，包括物化视图的数据聚合。\n\n引擎需使用 [AggregateFunction](https://clickhouse.yandex/docs/zh/data_types/nested_data_structures/aggregatefunction/) 类型来处理所有列。\n\n如果要按一组规则来合并减少行数，则使用 `AggregatingMergeTree` 是合适的。\n\n\n\n建表：\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = AggregatingMergeTree()\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n插入数据，需使用带有聚合 -State- 函数的 [INSERT SELECT](https://clickhouse.yandex/docs/zh/query_language/insert_into/) 语句。 从 `AggregatingMergeTree` 表中查询数据时，需使用 `GROUP BY` 子句并且要使用与插入时相同的聚合函数，但后缀要改为 `-Merge` 。\n\n在 `SELECT` 查询的结果中，对于 ClickHouse 的所有输出格式 `AggregateFunction` 类型的值都实现了特定的二进制表示法。如果直接用 `SELECT` 导出这些数据，例如如用 `TabSeparated` 格式，那么这些导出数据也能直接用 `INSERT` 语句加载导入\n\n\n\n聚合物化视图的示例, 创建一个跟踪 `test.visits` 表的 `AggregatingMergeTree` 物化视图：\n\n```sql\nCREATE MATERIALIZED VIEW test.basic\nENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate)\nAS SELECT\n    CounterID,\n    StartDate,\n    sumState(Sign)    AS Visits,\n    uniqState(UserID) AS Users\nFROM test.visits\nGROUP BY CounterID, StartDate;\n```\n\n向 `test.visits` 表中插入数据。数据会同时插入到表和视图中，并且视图 `test.basic` 会将里面的数据聚合。\n\n要获取聚合数据，我们需要在 `test.basic` 视图上执行类似 `SELECT ... GROUP BY ...` 这样的查询 ：\n\n```sql\nSELECT\n    StartDate,\n    sumMerge(Visits) AS Visits,\n    uniqMerge(Users) AS Users\nFROM test.basic\nGROUP BY StartDate\nORDER BY StartDate;\n```\n\n\n\n# 分布式引擎\n\n**分布式引擎本身不存储数据**, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 分布式引擎参数：服务器配置文件中的集群名，远程数据库名，远程表名，数据分片键（可选）。 示例：\n\n```sql\nDistributed(logs, default, hits[, sharding_key])\n```\n\n将会从位于“logs”集群中 default.hits 表所有服务器上读取数据。 远程服务器不仅用于读取数据，还会对尽可能数据做部分处理。 例如，对于使用 GROUP BY 的查询，数据首先在远程服务器聚合，之后返回聚合函数的中间状态给查询请求的服务器。再在请求的服务器上进一步汇总数据。\n\n数据库名参数除了用数据库名之外，也可用返回字符串的常量表达式。例如：currentDatabase()。\n\n集群示例配置如下：\n\n```xml\n<remote_servers>\n    <logs>\n        <shard>\n            <!-- Optional. Shard weight when writing data. Default: 1. -->\n            <weight>1</weight>\n            <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->\n            <internal_replication>false</internal_replication>\n            <replica>\n                <host>example01-01-1</host>\n                <port>9000</port>\n            </replica>\n            <replica>\n                <host>example01-01-2</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n        <shard>\n            <weight>2</weight>\n            <internal_replication>false</internal_replication>\n            <replica>\n                <host>example01-02-1</host>\n                <port>9000</port>\n            </replica>\n            <replica>\n                <host>example01-02-2</host>\n                <secure>1</secure>\n                <port>9440</port>\n            </replica>\n        </shard>\n    </logs>\n</remote_servers>\n```\n\n这里定义了一个名为‘logs’的集群，它由两个分片组成，每个分片包含两个副本。 分片是指包含数据不同部分的服务器（要读取所有数据，必须访问所有分片）。 副本是存储复制数据的服务器（要读取所有数据，访问任一副本上的数据即可）。\n\n每个服务器需要指定 `host`，`port`，和可选的 `user`，`password`，`secure`，`compression` 的参数：\n\n- `host` – 远程服务器地址。可以域名、IPv4或IPv6。如果指定域名，则服务在启动时发起一个 DNS 请求，并且请求结果会在服务器运行期间一直被记录。如果 DNS 请求失败，则服务不会启动。如果你修改了 DNS 记录，则需要重启服务。\n- `port` – 消息传递的 TCP 端口（「tcp_port」配置通常设为 9000）。不要跟 http_port 混淆。\n- `user` – 用于连接远程服务器的用户名。默认值：default。该用户必须有权限访问该远程服务器。访问权限配置在 users.xml 文件中。更多信息，请查看“访问权限”部分。\n- `password` – 用于连接远程服务器的密码。默认值：空字符串。\n- `secure` – 是否使用ssl进行连接，设为true时，通常也应该设置 `port` = 9440。服务器也要监听 9440 并有正确的证书。\n- `compression` - 是否使用数据压缩。默认值：true。\n\n配置了副本，读取操作会从每个分片里选择一个可用的副本。可配置负载平衡算法（挑选副本的方式） - 请参阅“load_balancing”设置。 如果跟服务器的连接不可用，则在尝试短超时的重连。如果重连失败，则选择下一个副本，依此类推。如果跟所有副本的连接尝试都失败，则尝试用相同的方式再重复几次。 该机制有利于系统可用性，但不保证完全容错：如有远程服务器能够接受连接，但无法正常工作或状况不佳。\n\n你可以配置一个（这种情况下，查询操作更应该称为远程查询，而不是分布式查询）或任意多个分片。在每个分片中，可以配置一个或任意多个副本。不同分片可配置不同数量的副本。\n\n可以在配置中配置任意数量的集群。\n\n要查看集群，可使用“system.clusters”表。\n\n通过分布式引擎可以像使用本地服务器一样使用集群。但是，集群不是自动扩展的：你必须编写集群配置到服务器配置文件中（最好，给所有集群的服务器写上完整配置）。\n\n不支持用分布式表查询别的分布式表（除非该表只有一个分片）。或者说，要用分布表查查询“最终”的数据表。\n\n分布式引擎需要将集群信息写入配置文件。配置文件中的集群信息会即时更新，无需重启服务器。如果你每次是要向不确定的一组分片和副本发送查询，则不适合创建分布式表 - 而应该使用“远程”表函数。 请参阅“表函数”部分。\n\n向集群写数据的方法有两种：\n\n一，自已指定要将哪些数据写入哪些服务器，并直接在每个分片上执行写入。换句话说，在分布式表上“查询”，在数据表上 INSERT。 这是最灵活的解决方案 – 你可以使用任何分片方案，对于复杂业务特性的需求，这可能是非常重要的。 这也是最佳解决方案，因为数据可以完全独立地写入不同的分片。\n\n二，在分布式表上执行 INSERT。在这种情况下，分布式表会跨服务器分发插入数据。 为了写入分布式表，必须要配置分片键（最后一个参数）。当然，如果只有一个分片，则写操作在没有分片键的情况下也能工作，因为这种情况下分片键没有意义。\n\n每个分片都可以在配置文件中定义权重。默认情况下，权重等于1。数据依据分片权重按比例分发到分片上。例如，如果有两个分片，第一个分片的权重是9，而第二个分片的权重是10，则发送 9 / 19 的行到第一个分片， 10 / 19 的行到第二个分片。\n\n分片可在配置文件中定义 'internal_replication' 参数。\n\n此参数设置为“true”时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。\n\n若此参数设置为“false”（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样。\n\n选择将一行数据发送到哪个分片的方法是，首先计算分片表达式，然后将这个计算结果除以所有分片的权重总和得到余数。该行会发送到那个包含该余数的从'prev_weight'到'prev_weights + weight'的半闭半开区间对应的分片上，其中 'prev_weights' 是该分片前面的所有分片的权重和，'weight' 是该分片的权重。例如，如果有两个分片，第一个分片权重为9，而第二个分片权重为10，则余数在 [0,9) 中的行发给第一个分片，余数在 [9,19) 中的行发给第二个分片。\n\n分片表达式可以是由常量和表列组成的任何返回整数表达式。例如，您可以使用表达式 'rand()' 来随机分配数据，或者使用 'UserID' 来按用户 ID 的余数分布（相同用户的数据将分配到单个分片上，这可降低带有用户信息的 IN 和 JOIN 的语句运行的复杂度）。如果该列数据分布不够均匀，可以将其包装在散列函数中：intHash64(UserID)。\n\n这种简单的用余数来选择分片的方案是有局限的，并不总适用。它适用于中型和大型数据（数十台服务器）的场景，但不适用于巨量数据（数百台或更多服务器）的场景。后一种情况下，应根据业务特性需求考虑的分片方案，而不是直接用分布式表的多分片。\n\nSELECT 查询会被发送到所有分片，并且无论数据在分片中如何分布（即使数据完全随机分布）都可正常工作。添加新分片时，不必将旧数据传输到该分片。你可以给新分片分配大权重然后写新数据 - 数据可能会稍分布不均，但查询会正确高效地运行。\n\n下面的情况，你需要关注分片方案：\n\n- 使用需要特定键连接数据（ IN 或 JOIN ）的查询。如果数据是用该键进行分片，则应使用本地 IN 或 JOIN 而不是 GLOBAL IN 或 GLOBAL JOIN，这样效率更高。\n- 使用大量服务器（上百或更多），但有大量小查询（个别客户的查询 - 网站，广告商或合作伙伴）。为了使小查询不影响整个集群，让单个客户的数据处于单个分片上是有意义的。或者，正如我们在 Yandex.Metrica 中所做的那样，你可以配置两级分片：将整个集群划分为“层”，一个层可以包含多个分片。单个客户的数据位于单个层上，根据需要将分片添加到层中，层中的数据随机分布。然后给每层创建分布式表，再创建一个全局的分布式表用于全局的查询。\n\n数据是异步写入的。对于分布式表的 INSERT，数据块只写本地文件系统。之后会尽快地在后台发送到远程服务器。你可以通过查看表目录中的文件列表（等待发送的数据）来检查数据是否成功发送：/var/lib/clickhouse/data/database/table/ 。\n\n如果在 INSERT 到分布式表时服务器节点丢失或重启（如，设备故障），则插入的数据可能会丢失。如果在表目录中检测到损坏的数据分片，则会将其转移到“broken”子目录，并不再使用。\n\n启用 max_parallel_replicas 选项后，会在分表的所有副本上并行查询处理。更多信息，请参阅“设置，max_parallel_replicas”部分。","source":"_posts/ClickHouse 系统架构概览.md","raw":"---\n\ntitle: ClickHouse系统架构概述\ndate: 2019-05-27 21:01:29\ntags: NoSQL\n\n---\n\n# ClickHouse系统架构概述\n\n## ClickHouse独特功能\n\n### 真正的列式数据库管理系统\n\n在一个真正的列式数据库管理系统中，除了数据本身外不应该存在其他额外的数据。这意味着为了避免在值旁边存储它们的长度“number”，你必须支持固定长度数值类型。例如，10亿个UInt8类型的数据在未压缩的情况下大约消耗1GB左右的空间，如果不是这样的话，这将对CPU的使用产生强烈影响。即使是在未压缩的情况下，紧凑的存储数据也是非常重要的，因为解压缩的速度主要取决于未压缩数据的大小。\n\n这是非常值得注意的，因为在一些其他系统中也可以将不同的列分别进行存储，但由于对其他场景进行的优化，使其无法有效的处理分析查询。例如： HBase，BigTable，Cassandra，HyperTable。在这些系统中，你可以得到每秒数十万的吞吐能力，但是无法得到每秒几亿行的吞吐能力。\n\n需要说明的是，ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。\n\n<!-- more -->\n\n### 数据压缩\n\n在一些列式数据库管理系统中(例如：InfiniDB CE and MonetDB) 不是用数据压缩。但是, 数据压缩在实现优异的存储系统中确实起着关键的作用。\n\n\n\n### 数据的磁盘存储\n\n许多的列式数据库(如 SAP HANA, Google PowerDrill)只能在内存中工作，这种方式会造成比实际更多的设备预算。ClickHouse被设计用于工作在传统磁盘上的系统，它提供每GB更低的存储成本，但如果有可以使用SSD和内存，它也会合理的利用这些资源\n\n\n\n### 多核心并行处理\n\n大型查询可以以很自然的方式在ClickHouse中进行并行化处理，以此来使用当前服务器上可用的所有资源。\n\n\n\n### 多服务器分布式处理\n\n上面提到的列式数据库管理系统中，几乎没有一个支持分布式的查询处理。 在ClickHouse中，数据可以保存在不同的shard上，每一个shard都由一组用于容错的replica组成，查询可以并行的在所有shard上进行处理。这些对用户来说是透明的\n\n\n\n### 支持SQL\n\nClickHouse支持基于SQL的查询语言，该语言大部分情况下是与SQL标准兼容的。 支持的查询包括 GROUP BY，ORDER BY，IN，JOIN以及非相关子查询。 不支持窗口函数和相关子查询。\n\n\n\n### 向量引擎\n\n为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理。\n\n\n\n### 实时的数据更新\n\nClickHouse支持在表中定义主键。为了使查询能够快速在主键中进行范围查找，数据总是以增量的方式有序的存储在MergeTree中。因此，数据可以持续不断高效的写入到表中，并且写入的过程中不会存在任何加锁的行为。\n\n\n\n### 索引\n\n按照主键对数据进行排序，这将帮助ClickHouse以几十毫秒的低延迟对数据进行特定值查找或范围查找。\n\n\n\n### 适合在线查询\n\n在线查询意味着在没有对数据做任何预处理的情况下以极低的延迟处理查询并将结果加载到用户的页面中。\n\n\n\n### 支持近似计算\n\nClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法：\n\n1. 用于近似计算的各类聚合函数，如：distinct values, medians, quantiles\n2. 基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。\n3. 不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。\n\n\n\n### 支持数据复制和数据完整性\n\nClickHouse使用异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据。在大多数情况下ClickHouse能在故障后自动恢复，在一些复杂的情况下需要少量的手动恢复。\n\n\n\n### ClickHouse可以考虑缺点的功能\n\n1. 没有完整的事物支持。\n2. 缺少高频率，低延迟的修改或删除已存在数据的能力。仅能用于批量删除或修改数据，但这符合 [GDPR](https://gdpr-info.eu/)。\n3. 稀疏索引使得ClickHouse不适合通过其键检索单行的点查询\n\n\n\n# ClickHouse性能\n\n根据Yandex的内部测试结果，ClickHouse表现出了比同类可比较产品更优的性能。你可以在 [这里](https://clickhouse.yandex/benchmark.html) 查看具体的测试结果。\n\n许多其他的测试也证实这一点。你可以使用互联网搜索到它们，或者你也可以从 [我们收集的部分相关连接](https://clickhouse.yandex/#independent-benchmarks) 中查看。\n\n\n\n## 单个大查询的吞吐量\n\n吞吐量可以使用每秒处理的行数或每秒处理的字节数来衡量。如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以2-10GB／s（未压缩）的速度进行处理（对于简单的查询，速度可以达到30GB／s）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以400MB／s的速度读取数据，并且数据压缩率是3，则数据的处理速度为1.2GB/s。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是1-2亿行每秒。\n\n对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。\n\n\n\n## 处理短查询的延迟时间\n\n如果一个查询使用主键并且没有太多行(几十万)进行处理，并且没有查询太多的列，那么在数据被page cache缓存的情况下，它的延迟应该小于50毫秒(在最佳的情况下应该小于10毫秒)。 否则，延迟取决于数据的查找次数。如果你当前使用的是HDD，在数据没有加载的情况下，查询所需要的延迟可以通过以下公式计算得知： 查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量。\n\n\n\n## 处理大量短查询的吞吐量\n\n在相同的情况下，ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。因此我们建议每秒最多查询100次。\n\n\n\n## 数据的写入性能\n\n我们建议每次写入不少于1000行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为50到200MB/s。如果您写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。\n\n\n\n# Yandex.Metrica的使用案例\n\nClickHouse最初是为 [Yandex.Metrica](https://metrica.yandex.com/) [世界第二大Web分析平台](http://w3techs.com/technologies/overview/traffic_analysis/all) 而开发的。多年来一直作为该系统的核心组件被该系统持续使用着。目前为止，该系统在ClickHouse中有超过13万亿条记录，并且每天超过200多亿个事件被处理。它允许直接从原始数据中动态查询并生成报告。本文简要介绍了ClickHouse在其早期发展阶段的目标。\n\nYandex.Metrica基于用户定义的字段，对实时访问、连接会话，生成实时的统计报表。这种需求往往需要复杂聚合方式，比如对访问用户进行去重。构建报表的数据，是实时接收存储的新数据。\n\n截至2014年4月，Yandex.Metrica每天跟踪大约120亿个事件（用户的点击和浏览）。为了可以创建自定义的报表，我们必须存储全部这些事件。同时，这些查询可能需要在几百毫秒内扫描数百万行的数据，或在几秒内扫描数亿行的数据。\n\n\n\n## Yandex.Metrica以及其他Yandex服务的使用案例\n\n在Yandex.Metrica中，ClickHouse被用于多个场景中。 它的主要任务是使用原始数据在线的提供各种数据报告。它使用374台服务器的集群，存储了20.3万亿行的数据。在去除重复与副本数据的情况下，压缩后的数据达到了2PB。未压缩前（TSV格式）它大概有17PB。\n\nClickHouse还被使用在：\n\n- 存储来自Yandex.Metrica回话重放数据。\n- 处理中间数据\n- 与Analytics一起构建全球报表。\n- 为调试Yandex.Metrica引擎运行查询\n- 分析来自API和用户界面的日志数据\n\nClickHouse在其他Yandex服务中至少有12个安装：search verticals, Market, Direct, business analytics, mobile development, AdFox, personal services等。\n\n\n\n## 聚合与非聚合数据\n\n有一种流行的观点认为，想要有效的计算统计数据，必须要聚合数据，因为聚合将降低数据量。\n\n但是数据聚合是一个有诸多限制的解决方案，例如：\n\n- 你必须提前知道用户定义的报表的字段列表\n- 用户无法自定义报表\n- 当聚合条件过多时，可能不会减少数据，聚合是无用的。\n- 存在大量报表时，有太多的聚合变化（组合爆炸）\n- 当聚合条件有非常大的基数时（如：url），数据量没有太大减少（少于两倍）\n- 聚合的数据量可能会增长而不是收缩\n- 用户不会查看我们为他生成的所有报告，大部分计算将是无用的\n- 各种聚合可能违背了数据的逻辑完整性\n\n如果我们直接使用非聚合数据而不尽兴任何聚合时，我们的计算量可能是减少的。\n\n然而，相对于聚合中很大一部分工作被离线完成，在线计算需要尽快的完成计算，因为用户在等待结果。\n\nYandex.Metrica 有一个专门用于聚合数据的系统，称为Metrage，它可以用作大部分报表。 从2009年开始，Yandex.Metrica还为非聚合数据使用专门的OLAP数据库，称为OLAPServer，它以前用于报表构建系统。 OLAPServer可以很好的工作在非聚合数据上，但是它有诸多限制，导致无法根据需要将其用于所有报表中。如，缺少对数据类型的支持（只支持数据），无法实时增量的更新数据（只能通过每天重写数据完成）。OLAPServer不是一个数据库管理系统，它只是一个数据库。\n\n为了消除OLAPServer的这些局限性，解决所有报表使用非聚合数据的问题，我们开发了ClickHouse数据库管理系统。\n\n\n\n# ClickHouse表引擎\n\n表引擎（即表的类型）决定了：\n\n- 数据的存储方式和位置，写到哪里以及从哪里读取数据\n- 支持哪些查询以及如何支持。\n- 并发数据访问。\n- 索引的使用（如果存在）。\n- 是否可以执行多线程请求。\n- 数据复制参数。\n\n在读取时，引擎只需要输出所请求的列，但在某些情况下，引擎可以在响应请求时部分处理数据。\n\n对于大多数正式的任务，应该使用MergeTree族中的引擎。\n\n\n\n## MergeTree\n\nClickhouse 中最强大的表引擎当属 `MergeTree` （合并树）引擎及该系列（`*MergeTree`）中的其他引擎。\n\n`MergeTree` 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。\n\n主要特点:\n\n- 存储的数据按主键排序。\n\n  这让你可以创建一个用于快速检索数据的小稀疏索引。\n\n- 允许使用分区，如果指定了 [主键](https://clickhouse.yandex/docs/zh/operations/table_engines/custom_partitioning_key/) 的话。\n\n  在相同数据集和相同结果集的情况下 ClickHouse 中某些带分区的操作会比普通操作更快。查询中指定了分区键时 ClickHouse 会自动截取分区数据。这也有效增加了查询性能。\n\n- 支持数据副本。\n\n  `ReplicatedMergeTree` 系列的表便是用于此。更多信息，请参阅 [数据副本](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/) 一节。\n\n- 支持数据采样。\n\n  需要的话，你可以给表设置一个采样方法。\n\n!!! 注意 [Merge](https://clickhouse.yandex/docs/zh/operations/table_engines/merge/) 引擎并不属于 `*MergeTree` 系列。\n\n\n\n### 建表\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,\n    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2\n) ENGINE = MergeTree()\n[PARTITION BY expr]\n[ORDER BY expr]\n[PRIMARY KEY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n请求参数的描述，参考 [请求描述](https://clickhouse.yandex/docs/zh/query_language/create/) 。\n\n\n\n示例：\n\n```sql\nCREATE TABLE xcloud.cdn_nginx_log_minute_agg\n(\n\tdate Date, \n\n    timeStamp DateTime,\n\n\tchannel String, \n\n\tcustomer String, \n\n\tcountry String, \n\n\tflow AggregateFunction(sum, Int64), \n\n\tvisit AggregateFunction(sum, Int64),  \n\n\tdownload_time AggregateFunction(sum, Int64), \n\n\tresponse_time AggregateFunction(sum, Int64), \n\n\tupstream_response_time AggregateFunction(sum, Int64), \n\n\tfirst_byte_time AggregateFunction(sum, Int64), \n\n\trequest_time AggregateFunction(sum, Int64), \n\n\tdownload_flow AggregateFunction(sum, Int64), \n\n\tresponse_normal AggregateFunction(sum, Int64)\n)  \n\nENGINE = ReplicatedAggregatingMergeTree('{zkpath}', '{replica}')  \n\nPARTITION BY date  ORDER BY (timeStamp, date, channel, customer, country);\n```\n\n\n\n### 数据存储\n\n表由按主键排序的数据 *片段* 组成。\n\n当数据被插入到表中时，会分成数据片段并按主键的字典序排序。例如，主键是 `(CounterID, Date)` 时，片段中数据按 `CounterID` 排序，具有相同 `CounterID` 的部分按 `Date` 排序。\n\n不同分区的数据会被分成不同的片段，ClickHouse 在后台合并数据片段以便更高效存储。不会合并来自不同分区的数据片段。这个合并机制并不保证相同主键的所有行都会合并到同一个数据片段中。\n\nClickHouse 会为每个数据片段创建一个索引文件，索引文件包含每个索引行（『标记』）的主键值。索引行号定义为 `n * index_granularity` 。最大的 `n` 等于总行数除以 `index_granularity` 的值的整数部分。对于每列，跟主键相同的索引行处也会写入『标记』。这些『标记』让你可以直接找到数据所在的列。\n\n你可以只用一单一大表并不断地一块块往里面加入数据 – `MergeTree` 引擎的就是为了这样的场景。\n\n\n\n### 主键和索引在查询中的表现\n\n我们以 `(CounterID, Date)` 以主键。排序好的索引的图示会是下面这样：\n\n```\n全部数据  :     [-------------------------------------------------------------------------]\nCounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]\nDate:           [1111111222222233331233211111222222333211111112122222223111112223311122333]\n标记:            |      |      |      |      |      |      |      |      |      |      |\n                a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3\n标记号:          0      1      2      3      4      5      6      7      8      9      10\n```\n\n如果指定查询如下：\n\n- `CounterID in ('a', 'h')`，服务器会读取标记号在 `[0, 3)` 和 `[6, 8)` 区间中的数据。\n- `CounterID IN ('a', 'h') AND Date = 3`，服务器会读取标记号在 `[1, 3)` 和 `[7, 8)` 区间中的数据。\n- `Date = 3`，服务器会读取标记号在 `[1, 10]` 区间中的数据。\n\n上面例子可以看出使用索引通常会比全表描述要高效。\n\n稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 `index_granularity * 2` 行额外的数据。大部分情况下，当 `index_granularity = 8192` 时，ClickHouse的性能并不会降级。\n\n稀疏索引让你能操作有巨量行的表。因为这些索引是常驻内存（RAM）的。\n\nClickHouse 不要求主键惟一。所以，你可以插入多条具有相同主键的行。\n\n\n\n### 主键的选择\n\n主键中列的数量并没有明确的限制。依据数据结构，你应该让主键包含多些或少些列。这样可以：\n\n- 改善索引的性能。\n\n  如果当前主键是 `(a, b)` ，然后加入另一个 `c` 列，满足下面条件时，则可以改善性能： - 有带有 `c` 列条件的查询。 - 很长的数据范围（ `index_granularity` 的数倍）里 `(a, b)` 都是相同的值，并且这种的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。\n\n- 改善数据压缩。\n\n  ClickHouse 以主键排序片段数据，所以，数据的一致性越高，压缩越好。\n\n- [CollapsingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/collapsingmergetree/#table_engine-collapsingmergetree) 和 [SummingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/) 引擎里，数据合并时，会有额外的处理逻辑。\n\n  在这种情况下，指定一个跟主键不同的 *排序键* 也是有意义的。\n\n长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 `SELECT` 查询的性能。\n\n\n\n### 选择跟排序键不一样主键\n\n指定一个跟排序键（用于排序数据片段中行的表达式） 不一样的主键（用于计算写到索引文件的每个标记值的表达式）是可以的。 这种情况下，主键表达式元组必须是排序键表达式元组的一个前缀。\n\n当使用 [SummingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/) 和 [AggregatingMergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/aggregatingmergetree/) 引擎时，这个特性非常有用。 通常，使用这类引擎时，表里列分两种：*维度* 和 *度量* 。 典型的查询是在 `GROUP BY` 并过虑维度的情况下统计度量列的值。 像 SummingMergeTree 和 AggregatingMergeTree ，用相同的排序键值统计行时， 通常会加上所有的维度。结果就是，这键的表达式会是一长串的列组成， 并且这组列还会因为新加维度必须频繁更新。\n\n这种情况下，主键中仅预留少量列保证高效范围扫描， 剩下的维度列放到排序键元组里。这样是合理的。\n\n[排序键的修改](https://clickhouse.yandex/docs/zh/query_language/alter/) 是轻量级的操作，因为一个新列同时被加入到表里和排序键后时，已存在的数据片段并不需要修改。由于旧的排序键是新排序键的前缀，并且刚刚添加的列中没有数据，因此在表修改时的数据对于新旧的排序键来说都是有序的。\n\n\n\n### 索引和分区在查询中的应用\n\n对于 `SELECT` 查询，ClickHouse 分析是否可以使用索引。如果 `WHERE/PREWHERE` 子句具有下面这些表达式（作为谓词链接一子项或整个）则可以使用索引：基于主键或分区键的列或表达式的部分的等式或比较运算表达式；基于主键或分区键的列或表达式的固定前缀的 `IN` 或 `LIKE` 表达式；基于主键或分区键的列的某些函数；基于主键或分区键的表达式的逻辑表达式。\n\n因此，在索引键的一个或多个区间上快速地跑查询都是可能的。下面例子中，指定标签；指定标签和日期范围；指定标签和日期；指定多个标签和日期范围等运行查询，都会非常快。\n\n要检查 ClickHouse 执行一个查询时能否使用索引，可设置 [force_index_by_date](https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-force_index_by_date) 和 [force_primary_key](https://clickhouse.yandex/docs/zh/operations/settings/settings/) 。\n\n按月分区的分区键是只能读取包含适当范围日期的数据块。这种情况下，数据块会包含很多天（最多整月）的数据。在块中，数据按主键排序，主键第一列可能不包含日期。因此，仅使用日期而没有带主键前缀条件的查询将会导致读取超过这个日期范围。\n\n\n\n### 并发数据访问\n\n应对表的并发访问，我们使用多版本机制。换言之，当同时读和更新表时，数据从当前查询到的一组片段中读取。没有冗长的的锁。插入不会阻碍读取。\n\n对表的读操作是自动并行的。\n\n\n\n## 数据副本\n\n只有 MergeTree 系列里的表可支持副本：\n\n- ReplicatedMergeTree\n- ReplicatedSummingMergeTree\n- ReplicatedReplacingMergeTree\n- ReplicatedAggregatingMergeTree\n- ReplicatedCollapsingMergeTree\n- ReplicatedVersionedCollapsingMergeTree\n- ReplicatedGraphiteMergeTree\n\n副本是表级别的，不是整个服务器级的。所以，服务器里可以同时有复制表和非复制表。\n\n副本不依赖分片。每个分片有它自己的独立副本。\n\n对于 `INSERT` 和 `ALTER` 语句操作数据的会在压缩的情况下被复制（更多信息，看 [ALTER](https://clickhouse.yandex/docs/zh/query_language/alter/#query_language_queries_alter) ）。\n\n而 `CREATE`，`DROP`，`ATTACH`，`DETACH` 和 `RENAME` 语句只会在单个服务器上执行，不会被复制。\n\n- `The CREATE TABLE` 在运行此语句的服务器上创建一个新的可复制表。如果此表已存在其他服务器上，则给该表添加新副本。\n- `The DROP TABLE` 删除运行此查询的服务器上的副本。\n- `The RENAME` 重命名一个副本。换句话说，可复制表不同的副本可以有不同的名称。\n\n要使用副本，需在配置文件中设置 ZooKeeper 集群的地址。\n\n`SELECT` 查询并不需要借助 ZooKeeper ，复本并不影响 `SELECT` 的性能，查询复制表与非复制表速度是一样的。查询分布式表时，ClickHouse的处理方式可通过设置 [max_replica_delay_for_distributed_queries](https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries) 和 [fallback_to_stale_replicas_for_distributed_queries](https://clickhouse.yandex/docs/zh/operations/settings/settings/) 修改。\n\n对于每个 `INSERT` 语句，会通过几个事务将十来个记录添加到 ZooKeeper。（确切地说，这是针对每个插入的数据块; 每个 INSERT 语句的每 `max_insert_block_size = 1048576` 行和最后剩余的都各算作一个块。）相比非复制表，写 zk 会导致 `INSERT` 的延迟略长一些。但只要你按照建议每秒不超过一个 `INSERT` 地批量插入数据，不会有任何问题。一个 ZooKeeper 集群能给整个 ClickHouse 集群支撑协调每秒几百个 `INSERT`。数据插入的吞吐量（每秒的行数）可以跟不用复制的数据一样高。\n\n对于非常大的集群，你可以把不同的 ZooKeeper 集群用于不同的分片。然而，即使 Yandex.Metrica 集群（大约300台服务器）也证明还不需要这么做。\n\n复制是多主异步。 `INSERT` 语句（以及 `ALTER` ）可以发给任意可用的服务器。数据会先插入到执行该语句的服务器上，然后被复制到其他服务器。由于它是异步的，在其他副本上最近插入的数据会有一些延迟。如果部分副本不可用，则数据在其可用时再写入。副本可用的情况下，则延迟时长是通过网络传输压缩数据块所需的时间。\n\n默认情况下，INSERT 语句仅等待一个副本写入成功后返回。如果数据只成功写入一个副本后该副本所在的服务器不再存在，则存储的数据会丢失。要启用数据写入多个副本才确认返回，使用 `insert_quorum` 选项。\n\n单个数据块写入是原子的。 INSERT 的数据按每块最多 `max_insert_block_size = 1048576` 行进行分块，换句话说，如果 `INSERT` 插入的行少于 1048576，则该 INSERT 是原子的。\n\n数据块会去重。对于被多次写的相同数据块（大小相同且具有相同顺序的相同行的数据块），该块仅会写入一次。这样设计的原因是万一在网络故障时客户端应用程序不知道数据是否成功写入DB，此时可以简单地重复 `INSERT` 。把相同的数据发送给多个副本 INSERT 并不会有问题。因为这些 `INSERT` 是完全相同的（会被去重）。去重参数参看服务器设置 [merge_tree](https://clickhouse.yandex/docs/zh/operations/server_settings/settings/) 。（注意：Replicated*MergeTree 才会去重，不需要 zookeeper 的不带 MergeTree 不会去重）\n\n在复制期间，只有要插入的源数据通过网络传输。进一步的数据转换（合并）会在所有副本上以相同的方式进行处理执行。这样可以最大限度地减少网络使用，这意味着即使副本在不同的数据中心，数据同步也能工作良好。（能在不同数据中心中的同步数据是副本机制的主要目标。）\n\n你可以给数据做任意多的副本。Yandex.Metrica 在生产中使用双副本。某一些情况下，给每台服务器都使用 RAID-5 或 RAID-6 和 RAID-10。是一种相对可靠和方便的解决方案。\n\n系统会监视副本数据同步情况，并能在发生故障后恢复。故障转移是自动的（对于小的数据差异）或半自动的（当数据差异很大时，这可能意味是有配置错误）。\n\n\n\n### 创建复制表\n\n在表引擎名称上加上 `Replicated` 前缀。例如：`ReplicatedMergeTree`。\n\n**Replicated\\*MergeTree 参数**\n\n- `zoo_path` — ZooKeeper 中该表的路径。\n- `replica_name` — ZooKeeper 中的该表的副本名称。\n\n示例:\n\n```sql\nCREATE TABLE table_name\n(\n    EventDate DateTime,\n    CounterID UInt32,\n    UserID UInt32\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}')\nPARTITION BY toYYYYMM(EventDate)\nORDER BY (CounterID, EventDate, intHash32(UserID))\nSAMPLE BY intHash32(UserID)\n```\n\n如上例所示，这些参数可以包含宏替换的占位符，即大括号的部分。它们会被替换为配置文件里 'macros' 那部分配置的值。示例：\n\n```xml\n<macros>\n    <layer>05</layer>\n    <shard>02</shard>\n    <replica>example05-02-1.yandex.ru</replica>\n</macros>\n```\n\n“ZooKeeper 中该表的路径”对每个可复制表都要是唯一的。不同分片上的表要有不同的路径。 这种情况下，路径包含下面这些部分：\n\n`/clickhouse/tables/` 是公共前缀，我们推荐使用这个。\n\n`{layer}-{shard}` 是分片标识部分。在此示例中，由于 Yandex.Metrica 集群使用了两级分片，所以它是由两部分组成的。但对于大多数情况来说，你只需保留 {shard} 占位符即可，它会替换展开为分片标识。\n\n```\ntable_name` 是该表在 ZooKeeper 中的名称。使其与 ClickHouse 中的表名相同比较好。 这里它被明确定义，跟 ClickHouse 表名不一样，它并不会被 RENAME 语句修改。\n*HINT*: you could add a database name in front of `table_name` as well. E.g. `db_name.table_name\n```\n\n副本名称用于标识同一个表分片的不同副本。你可以使用服务器名称，如上例所示。同个分片中不同副本的副本名称要唯一。\n\n你也可以显式指定这些参数，而不是使用宏替换。对于测试和配置小型集群这可能会很方便。但是，这种情况下，则不能使用分布式 DDL 语句（`ON CLUSTER`）。\n\n使用大型集群时，我们建议使用宏替换，因为它可以降低出错的可能性。\n\n在每个副本服务器上运行 `CREATE TABLE` 查询。将创建新的复制表，或给现有表添加新副本。\n\n如果其他副本上已包含了某些数据，在表上添加新副本，则在运行语句后，数据会从其他副本复制到新副本。换句话说，新副本会与其他副本同步。\n\n要删除副本，使用 `DROP TABLE`。但它只删除那个 – 位于运行该语句的服务器上的副本。\n\n\n\n### 故障恢复\n\n如果服务器启动时 ZooKeeper 不可用，则复制表会切换为只读模式。系统会定期尝试去连接 ZooKeeper。\n\n如果在 `INSERT` 期间 ZooKeeper 不可用，或者在与 ZooKeeper 交互时发生错误，则抛出异常。\n\n连接到 ZooKeeper 后，系统会检查本地文件系统中的数据集是否与预期的数据集（ ZooKeeper 存储此信息）一致。如果存在轻微的不一致，系统会通过与副本同步数据来解决。\n\n如果系统检测到损坏的数据片段（文件大小错误）或无法识别的片段（写入文件系统但未记录在 ZooKeeper 中的部分），则会把它们移动到 'detached' 子目录（不会删除）。而副本中其他任何缺少的但正常数据片段都会被复制同步。\n\n注意，ClickHouse 不会执行任何破坏性操作，例如自动删除大量数据。\n\n当服务器启动（或与 ZooKeeper 建立新会话）时，它只检查所有文件的数量和大小。 如果文件大小一致但中间某处已有字节被修改过，不会立即被检测到，只有在尝试读取 `SELECT` 查询的数据时才会检测到。该查询会引发校验和不匹配或压缩块大小不一致的异常。这种情况下，数据片段会添加到验证队列中，并在必要时从其他副本中复制。\n\n如果本地数据集与预期数据的差异太大，则会触发安全机制。服务器在日志中记录此内容并拒绝启动。这种情况很可能是配置错误，例如，一个分片上的副本意外配置为别的分片上的副本。然而，此机制的阈值设置得相当低，在正常故障恢复期间可能会出现这种情况。在这种情况下，数据恢复则是半自动模式，通过用户主动操作触发。\n\n要触发启动恢复，可在 ZooKeeper 中创建节点 `/path_to_table/replica_name/flags/force_restore_data`，节点值可以是任何内容，或运行命令来恢复所有的可复制表：\n\n```sql\nsudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data\n```\n\n然后重启服务器。启动时，服务器会删除这些标志并开始恢复。\n\n\n\n### 在数据完全丢失后的恢复\n\n如果其中一个服务器的所有数据和元数据都消失了，请按照以下步骤进行恢复：\n\n1. 在服务器上安装 ClickHouse。在包含分片标识符和副本的配置文件中正确定义宏配置，如果有用到的话，\n2. 如果服务器上有非复制表则必须手动复制，可以从副本服务器上（在 `/var/lib/clickhouse/data/db_name/table_name/` 目录中）复制它们的数据。\n3. 从副本服务器上中复制位于 `/var/lib/clickhouse/metadata/` 中的表定义信息。如果在表定义信息中显式指定了分片或副本标识符，请更正它以使其对应于该副本。（另外，启动服务器，然后会在 `/var/lib/clickhouse/metadata/` 中的.sql文件中生成所有的 `ATTACH TABLE` 语句。） 4.要开始恢复，ZooKeeper 中创建节点 `/path_to_table/replica_name/flags/force_restore_data`，节点内容不限，或运行命令来恢复所有复制的表：`sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data`\n\n然后启动服务器（如果它已运行则重启）。数据会从副本中下载。\n\n另一种恢复方式是从 ZooKeeper（`/path_to_table/replica_name`）中删除有数据丢的副本的所有元信息，然后再按照“[创建可复制表](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/#creating-replicated-tables)”中的描述重新创建副本。\n\n恢复期间的网络带宽没有限制。特别注意这一点，尤其是要一次恢复很多副本。\n\n\n\n## 自定义分区键\n\n[MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/) 系列的表（包括 [可复制表](https://clickhouse.yandex/docs/zh/operations/table_engines/replication/) ）可以使用分区。基于 MergeTree 表的 [物化视图](https://clickhouse.yandex/docs/zh/operations/table_engines/materializedview/) 也支持分区。\n\n一个分区是指按指定规则逻辑组合一起的表的记录集。可以按任意标准进行分区，如按月，按日或按事件类型。为了减少需要操作的数据，每个分区都是分开存储的。访问数据时，ClickHouse 尽量使用这些分区的最小子集。\n\n分区是在 [建表](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#table_engine-mergetree-creating-a-table) 的 `PARTITION BY expr` 子句中指定。分区键可以是关于列的任何表达式。例如，指定按月分区，表达式为 `toYYYYMM(date_column)`：\n\n```sql\nCREATE TABLE visits\n(\n    VisitDate Date, \n    Hour UInt8, \n    ClientID UUID\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(VisitDate)\nORDER BY Hour;\n```\n\n分区键也可以是表达式元组（类似 [主键](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#primary-keys-and-indexes-in-queries) ）。例如：\n\n```sql\nENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)\nPARTITION BY (toMonday(StartDate), EventType)\nORDER BY (CounterID, StartDate, intHash32(UserID));\n```\n\n上例中，我们设置按一周内的事件类型分区。\n\n新数据插入到表中时，这些数据会存储为按主键排序的新片段（块）。插入后 10-15 分钟，同一分区的各个片段会合并为一整个片段。\n\n**那些有相同分区表达式值的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量和需要找开的文件描述符过多，导致 `SELECT` 查询效率不佳。**\n\n\n\n## ReplacingMergeTree 引擎\n\n该引擎和[MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)的不同之处在于它会删除具有相同主键的重复项。\n\n数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，因此你无法预先作出计划。有一些数据可能仍未被处理。尽管你可以调用 `OPTIMIZE` 语句发起计划外的合并，但请不要指望使用它，因为 `OPTIMIZE` 语句会引发对大量数据的读和写。\n\n因此，`ReplacingMergeTree` 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。\n\n\n\n建表:\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = ReplacingMergeTree([ver])\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n## SummingMergeTree 引擎\n\n该引擎继承自 [MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)。区别在于，当合并 `SummingMergeTree` 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度。\n\n我们推荐将该引擎和 `MergeTree` 一起使用。例如，在准备做报告的时候，将完整的数据存储在 `MergeTree` 表中，并且使用 `SummingMergeTree` 来存储聚合数据。这种方法可以使你避免因为使用不正确的主键组合方式而丢失有价值的数据。\n\n\n\n建表：\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = SummingMergeTree([columns])\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n当数据被插入到表中时，他们将被原样保存。ClickHouse 定期合并插入的数据片段，并在这个时候对所有具有相同主键的行中的列进行汇总，将这些行替换为包含汇总数据的一行记录。\n\nClickHouse 会按片段合并数据，以至于不同的数据片段中会包含具有相同主键的行，即单个汇总片段将会是不完整的。因此，聚合函数 [sum()](https://clickhouse.yandex/docs/zh/query_language/agg_functions/reference/#agg_function-sum) 和 `GROUP BY` 子句应该在（`SELECT`）查询语句中被使用，如上文中的例子所述。\n\n\n\n列中数值类型的值会被汇总。这些列的集合在参数 `columns` 中被定义。\n\n如果用于汇总的所有列中的值均为0，则该行会被删除。\n\n如果列不在主键中且无法被汇总，则会在现有的值中任选一个。\n\n主键所在的列中的值不会被汇总。\n\n\n\n## AggregatingMergeTree 引擎\n\n该引擎继承自 [MergeTree](https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/)，并改变了数据片段的合并逻辑。 ClickHouse 会将相同主键的所有行（在一个数据片段内）替换为单个存储一系列聚合函数状态的行。\n\n可以使用 `AggregatingMergeTree` 表来做增量数据统计聚合，包括物化视图的数据聚合。\n\n引擎需使用 [AggregateFunction](https://clickhouse.yandex/docs/zh/data_types/nested_data_structures/aggregatefunction/) 类型来处理所有列。\n\n如果要按一组规则来合并减少行数，则使用 `AggregatingMergeTree` 是合适的。\n\n\n\n建表：\n\n```sql\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\n(\n    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],\n    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],\n    ...\n) ENGINE = AggregatingMergeTree()\n[PARTITION BY expr]\n[ORDER BY expr]\n[SAMPLE BY expr]\n[SETTINGS name=value, ...]\n```\n\n\n\n插入数据，需使用带有聚合 -State- 函数的 [INSERT SELECT](https://clickhouse.yandex/docs/zh/query_language/insert_into/) 语句。 从 `AggregatingMergeTree` 表中查询数据时，需使用 `GROUP BY` 子句并且要使用与插入时相同的聚合函数，但后缀要改为 `-Merge` 。\n\n在 `SELECT` 查询的结果中，对于 ClickHouse 的所有输出格式 `AggregateFunction` 类型的值都实现了特定的二进制表示法。如果直接用 `SELECT` 导出这些数据，例如如用 `TabSeparated` 格式，那么这些导出数据也能直接用 `INSERT` 语句加载导入\n\n\n\n聚合物化视图的示例, 创建一个跟踪 `test.visits` 表的 `AggregatingMergeTree` 物化视图：\n\n```sql\nCREATE MATERIALIZED VIEW test.basic\nENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate)\nAS SELECT\n    CounterID,\n    StartDate,\n    sumState(Sign)    AS Visits,\n    uniqState(UserID) AS Users\nFROM test.visits\nGROUP BY CounterID, StartDate;\n```\n\n向 `test.visits` 表中插入数据。数据会同时插入到表和视图中，并且视图 `test.basic` 会将里面的数据聚合。\n\n要获取聚合数据，我们需要在 `test.basic` 视图上执行类似 `SELECT ... GROUP BY ...` 这样的查询 ：\n\n```sql\nSELECT\n    StartDate,\n    sumMerge(Visits) AS Visits,\n    uniqMerge(Users) AS Users\nFROM test.basic\nGROUP BY StartDate\nORDER BY StartDate;\n```\n\n\n\n# 分布式引擎\n\n**分布式引擎本身不存储数据**, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 分布式引擎参数：服务器配置文件中的集群名，远程数据库名，远程表名，数据分片键（可选）。 示例：\n\n```sql\nDistributed(logs, default, hits[, sharding_key])\n```\n\n将会从位于“logs”集群中 default.hits 表所有服务器上读取数据。 远程服务器不仅用于读取数据，还会对尽可能数据做部分处理。 例如，对于使用 GROUP BY 的查询，数据首先在远程服务器聚合，之后返回聚合函数的中间状态给查询请求的服务器。再在请求的服务器上进一步汇总数据。\n\n数据库名参数除了用数据库名之外，也可用返回字符串的常量表达式。例如：currentDatabase()。\n\n集群示例配置如下：\n\n```xml\n<remote_servers>\n    <logs>\n        <shard>\n            <!-- Optional. Shard weight when writing data. Default: 1. -->\n            <weight>1</weight>\n            <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->\n            <internal_replication>false</internal_replication>\n            <replica>\n                <host>example01-01-1</host>\n                <port>9000</port>\n            </replica>\n            <replica>\n                <host>example01-01-2</host>\n                <port>9000</port>\n            </replica>\n        </shard>\n        <shard>\n            <weight>2</weight>\n            <internal_replication>false</internal_replication>\n            <replica>\n                <host>example01-02-1</host>\n                <port>9000</port>\n            </replica>\n            <replica>\n                <host>example01-02-2</host>\n                <secure>1</secure>\n                <port>9440</port>\n            </replica>\n        </shard>\n    </logs>\n</remote_servers>\n```\n\n这里定义了一个名为‘logs’的集群，它由两个分片组成，每个分片包含两个副本。 分片是指包含数据不同部分的服务器（要读取所有数据，必须访问所有分片）。 副本是存储复制数据的服务器（要读取所有数据，访问任一副本上的数据即可）。\n\n每个服务器需要指定 `host`，`port`，和可选的 `user`，`password`，`secure`，`compression` 的参数：\n\n- `host` – 远程服务器地址。可以域名、IPv4或IPv6。如果指定域名，则服务在启动时发起一个 DNS 请求，并且请求结果会在服务器运行期间一直被记录。如果 DNS 请求失败，则服务不会启动。如果你修改了 DNS 记录，则需要重启服务。\n- `port` – 消息传递的 TCP 端口（「tcp_port」配置通常设为 9000）。不要跟 http_port 混淆。\n- `user` – 用于连接远程服务器的用户名。默认值：default。该用户必须有权限访问该远程服务器。访问权限配置在 users.xml 文件中。更多信息，请查看“访问权限”部分。\n- `password` – 用于连接远程服务器的密码。默认值：空字符串。\n- `secure` – 是否使用ssl进行连接，设为true时，通常也应该设置 `port` = 9440。服务器也要监听 9440 并有正确的证书。\n- `compression` - 是否使用数据压缩。默认值：true。\n\n配置了副本，读取操作会从每个分片里选择一个可用的副本。可配置负载平衡算法（挑选副本的方式） - 请参阅“load_balancing”设置。 如果跟服务器的连接不可用，则在尝试短超时的重连。如果重连失败，则选择下一个副本，依此类推。如果跟所有副本的连接尝试都失败，则尝试用相同的方式再重复几次。 该机制有利于系统可用性，但不保证完全容错：如有远程服务器能够接受连接，但无法正常工作或状况不佳。\n\n你可以配置一个（这种情况下，查询操作更应该称为远程查询，而不是分布式查询）或任意多个分片。在每个分片中，可以配置一个或任意多个副本。不同分片可配置不同数量的副本。\n\n可以在配置中配置任意数量的集群。\n\n要查看集群，可使用“system.clusters”表。\n\n通过分布式引擎可以像使用本地服务器一样使用集群。但是，集群不是自动扩展的：你必须编写集群配置到服务器配置文件中（最好，给所有集群的服务器写上完整配置）。\n\n不支持用分布式表查询别的分布式表（除非该表只有一个分片）。或者说，要用分布表查查询“最终”的数据表。\n\n分布式引擎需要将集群信息写入配置文件。配置文件中的集群信息会即时更新，无需重启服务器。如果你每次是要向不确定的一组分片和副本发送查询，则不适合创建分布式表 - 而应该使用“远程”表函数。 请参阅“表函数”部分。\n\n向集群写数据的方法有两种：\n\n一，自已指定要将哪些数据写入哪些服务器，并直接在每个分片上执行写入。换句话说，在分布式表上“查询”，在数据表上 INSERT。 这是最灵活的解决方案 – 你可以使用任何分片方案，对于复杂业务特性的需求，这可能是非常重要的。 这也是最佳解决方案，因为数据可以完全独立地写入不同的分片。\n\n二，在分布式表上执行 INSERT。在这种情况下，分布式表会跨服务器分发插入数据。 为了写入分布式表，必须要配置分片键（最后一个参数）。当然，如果只有一个分片，则写操作在没有分片键的情况下也能工作，因为这种情况下分片键没有意义。\n\n每个分片都可以在配置文件中定义权重。默认情况下，权重等于1。数据依据分片权重按比例分发到分片上。例如，如果有两个分片，第一个分片的权重是9，而第二个分片的权重是10，则发送 9 / 19 的行到第一个分片， 10 / 19 的行到第二个分片。\n\n分片可在配置文件中定义 'internal_replication' 参数。\n\n此参数设置为“true”时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。\n\n若此参数设置为“false”（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样。\n\n选择将一行数据发送到哪个分片的方法是，首先计算分片表达式，然后将这个计算结果除以所有分片的权重总和得到余数。该行会发送到那个包含该余数的从'prev_weight'到'prev_weights + weight'的半闭半开区间对应的分片上，其中 'prev_weights' 是该分片前面的所有分片的权重和，'weight' 是该分片的权重。例如，如果有两个分片，第一个分片权重为9，而第二个分片权重为10，则余数在 [0,9) 中的行发给第一个分片，余数在 [9,19) 中的行发给第二个分片。\n\n分片表达式可以是由常量和表列组成的任何返回整数表达式。例如，您可以使用表达式 'rand()' 来随机分配数据，或者使用 'UserID' 来按用户 ID 的余数分布（相同用户的数据将分配到单个分片上，这可降低带有用户信息的 IN 和 JOIN 的语句运行的复杂度）。如果该列数据分布不够均匀，可以将其包装在散列函数中：intHash64(UserID)。\n\n这种简单的用余数来选择分片的方案是有局限的，并不总适用。它适用于中型和大型数据（数十台服务器）的场景，但不适用于巨量数据（数百台或更多服务器）的场景。后一种情况下，应根据业务特性需求考虑的分片方案，而不是直接用分布式表的多分片。\n\nSELECT 查询会被发送到所有分片，并且无论数据在分片中如何分布（即使数据完全随机分布）都可正常工作。添加新分片时，不必将旧数据传输到该分片。你可以给新分片分配大权重然后写新数据 - 数据可能会稍分布不均，但查询会正确高效地运行。\n\n下面的情况，你需要关注分片方案：\n\n- 使用需要特定键连接数据（ IN 或 JOIN ）的查询。如果数据是用该键进行分片，则应使用本地 IN 或 JOIN 而不是 GLOBAL IN 或 GLOBAL JOIN，这样效率更高。\n- 使用大量服务器（上百或更多），但有大量小查询（个别客户的查询 - 网站，广告商或合作伙伴）。为了使小查询不影响整个集群，让单个客户的数据处于单个分片上是有意义的。或者，正如我们在 Yandex.Metrica 中所做的那样，你可以配置两级分片：将整个集群划分为“层”，一个层可以包含多个分片。单个客户的数据位于单个层上，根据需要将分片添加到层中，层中的数据随机分布。然后给每层创建分布式表，再创建一个全局的分布式表用于全局的查询。\n\n数据是异步写入的。对于分布式表的 INSERT，数据块只写本地文件系统。之后会尽快地在后台发送到远程服务器。你可以通过查看表目录中的文件列表（等待发送的数据）来检查数据是否成功发送：/var/lib/clickhouse/data/database/table/ 。\n\n如果在 INSERT 到分布式表时服务器节点丢失或重启（如，设备故障），则插入的数据可能会丢失。如果在表目录中检测到损坏的数据分片，则会将其转移到“broken”子目录，并不再使用。\n\n启用 max_parallel_replicas 选项后，会在分表的所有副本上并行查询处理。更多信息，请参阅“设置，max_parallel_replicas”部分。","slug":"ClickHouse 系统架构概览","published":1,"updated":"2019-05-27T13:02:11.448Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubsh001kamumd6n5f4lt","content":"<h1 id=\"ClickHouse系统架构概述\"><a href=\"#ClickHouse系统架构概述\" class=\"headerlink\" title=\"ClickHouse系统架构概述\"></a>ClickHouse系统架构概述</h1><h2 id=\"ClickHouse独特功能\"><a href=\"#ClickHouse独特功能\" class=\"headerlink\" title=\"ClickHouse独特功能\"></a>ClickHouse独特功能</h2><h3 id=\"真正的列式数据库管理系统\"><a href=\"#真正的列式数据库管理系统\" class=\"headerlink\" title=\"真正的列式数据库管理系统\"></a>真正的列式数据库管理系统</h3><p>在一个真正的列式数据库管理系统中，除了数据本身外不应该存在其他额外的数据。这意味着为了避免在值旁边存储它们的长度“number”，你必须支持固定长度数值类型。例如，10亿个UInt8类型的数据在未压缩的情况下大约消耗1GB左右的空间，如果不是这样的话，这将对CPU的使用产生强烈影响。即使是在未压缩的情况下，紧凑的存储数据也是非常重要的，因为解压缩的速度主要取决于未压缩数据的大小。</p>\n<p>这是非常值得注意的，因为在一些其他系统中也可以将不同的列分别进行存储，但由于对其他场景进行的优化，使其无法有效的处理分析查询。例如： HBase，BigTable，Cassandra，HyperTable。在这些系统中，你可以得到每秒数十万的吞吐能力，但是无法得到每秒几亿行的吞吐能力。</p>\n<p>需要说明的是，ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。</p>\n<a id=\"more\"></a>\n<h3 id=\"数据压缩\"><a href=\"#数据压缩\" class=\"headerlink\" title=\"数据压缩\"></a>数据压缩</h3><p>在一些列式数据库管理系统中(例如：InfiniDB CE and MonetDB) 不是用数据压缩。但是, 数据压缩在实现优异的存储系统中确实起着关键的作用。</p>\n<h3 id=\"数据的磁盘存储\"><a href=\"#数据的磁盘存储\" class=\"headerlink\" title=\"数据的磁盘存储\"></a>数据的磁盘存储</h3><p>许多的列式数据库(如 SAP HANA, Google PowerDrill)只能在内存中工作，这种方式会造成比实际更多的设备预算。ClickHouse被设计用于工作在传统磁盘上的系统，它提供每GB更低的存储成本，但如果有可以使用SSD和内存，它也会合理的利用这些资源</p>\n<h3 id=\"多核心并行处理\"><a href=\"#多核心并行处理\" class=\"headerlink\" title=\"多核心并行处理\"></a>多核心并行处理</h3><p>大型查询可以以很自然的方式在ClickHouse中进行并行化处理，以此来使用当前服务器上可用的所有资源。</p>\n<h3 id=\"多服务器分布式处理\"><a href=\"#多服务器分布式处理\" class=\"headerlink\" title=\"多服务器分布式处理\"></a>多服务器分布式处理</h3><p>上面提到的列式数据库管理系统中，几乎没有一个支持分布式的查询处理。 在ClickHouse中，数据可以保存在不同的shard上，每一个shard都由一组用于容错的replica组成，查询可以并行的在所有shard上进行处理。这些对用户来说是透明的</p>\n<h3 id=\"支持SQL\"><a href=\"#支持SQL\" class=\"headerlink\" title=\"支持SQL\"></a>支持SQL</h3><p>ClickHouse支持基于SQL的查询语言，该语言大部分情况下是与SQL标准兼容的。 支持的查询包括 GROUP BY，ORDER BY，IN，JOIN以及非相关子查询。 不支持窗口函数和相关子查询。</p>\n<h3 id=\"向量引擎\"><a href=\"#向量引擎\" class=\"headerlink\" title=\"向量引擎\"></a>向量引擎</h3><p>为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理。</p>\n<h3 id=\"实时的数据更新\"><a href=\"#实时的数据更新\" class=\"headerlink\" title=\"实时的数据更新\"></a>实时的数据更新</h3><p>ClickHouse支持在表中定义主键。为了使查询能够快速在主键中进行范围查找，数据总是以增量的方式有序的存储在MergeTree中。因此，数据可以持续不断高效的写入到表中，并且写入的过程中不会存在任何加锁的行为。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>按照主键对数据进行排序，这将帮助ClickHouse以几十毫秒的低延迟对数据进行特定值查找或范围查找。</p>\n<h3 id=\"适合在线查询\"><a href=\"#适合在线查询\" class=\"headerlink\" title=\"适合在线查询\"></a>适合在线查询</h3><p>在线查询意味着在没有对数据做任何预处理的情况下以极低的延迟处理查询并将结果加载到用户的页面中。</p>\n<h3 id=\"支持近似计算\"><a href=\"#支持近似计算\" class=\"headerlink\" title=\"支持近似计算\"></a>支持近似计算</h3><p>ClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法：</p>\n<ol>\n<li>用于近似计算的各类聚合函数，如：distinct values, medians, quantiles</li>\n<li>基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。</li>\n<li>不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。</li>\n</ol>\n<h3 id=\"支持数据复制和数据完整性\"><a href=\"#支持数据复制和数据完整性\" class=\"headerlink\" title=\"支持数据复制和数据完整性\"></a>支持数据复制和数据完整性</h3><p>ClickHouse使用异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据。在大多数情况下ClickHouse能在故障后自动恢复，在一些复杂的情况下需要少量的手动恢复。</p>\n<h3 id=\"ClickHouse可以考虑缺点的功能\"><a href=\"#ClickHouse可以考虑缺点的功能\" class=\"headerlink\" title=\"ClickHouse可以考虑缺点的功能\"></a>ClickHouse可以考虑缺点的功能</h3><ol>\n<li>没有完整的事物支持。</li>\n<li>缺少高频率，低延迟的修改或删除已存在数据的能力。仅能用于批量删除或修改数据，但这符合 <a href=\"https://gdpr-info.eu/\" target=\"_blank\" rel=\"noopener\">GDPR</a>。</li>\n<li>稀疏索引使得ClickHouse不适合通过其键检索单行的点查询</li>\n</ol>\n<h1 id=\"ClickHouse性能\"><a href=\"#ClickHouse性能\" class=\"headerlink\" title=\"ClickHouse性能\"></a>ClickHouse性能</h1><p>根据Yandex的内部测试结果，ClickHouse表现出了比同类可比较产品更优的性能。你可以在 <a href=\"https://clickhouse.yandex/benchmark.html\" target=\"_blank\" rel=\"noopener\">这里</a> 查看具体的测试结果。</p>\n<p>许多其他的测试也证实这一点。你可以使用互联网搜索到它们，或者你也可以从 <a href=\"https://clickhouse.yandex/#independent-benchmarks\" target=\"_blank\" rel=\"noopener\">我们收集的部分相关连接</a> 中查看。</p>\n<h2 id=\"单个大查询的吞吐量\"><a href=\"#单个大查询的吞吐量\" class=\"headerlink\" title=\"单个大查询的吞吐量\"></a>单个大查询的吞吐量</h2><p>吞吐量可以使用每秒处理的行数或每秒处理的字节数来衡量。如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以2-10GB／s（未压缩）的速度进行处理（对于简单的查询，速度可以达到30GB／s）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以400MB／s的速度读取数据，并且数据压缩率是3，则数据的处理速度为1.2GB/s。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是1-2亿行每秒。</p>\n<p>对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。</p>\n<h2 id=\"处理短查询的延迟时间\"><a href=\"#处理短查询的延迟时间\" class=\"headerlink\" title=\"处理短查询的延迟时间\"></a>处理短查询的延迟时间</h2><p>如果一个查询使用主键并且没有太多行(几十万)进行处理，并且没有查询太多的列，那么在数据被page cache缓存的情况下，它的延迟应该小于50毫秒(在最佳的情况下应该小于10毫秒)。 否则，延迟取决于数据的查找次数。如果你当前使用的是HDD，在数据没有加载的情况下，查询所需要的延迟可以通过以下公式计算得知： 查找时间（10 ms） <em> 查询的列的数量 </em> 查询的数据块的数量。</p>\n<h2 id=\"处理大量短查询的吞吐量\"><a href=\"#处理大量短查询的吞吐量\" class=\"headerlink\" title=\"处理大量短查询的吞吐量\"></a>处理大量短查询的吞吐量</h2><p>在相同的情况下，ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。因此我们建议每秒最多查询100次。</p>\n<h2 id=\"数据的写入性能\"><a href=\"#数据的写入性能\" class=\"headerlink\" title=\"数据的写入性能\"></a>数据的写入性能</h2><p>我们建议每次写入不少于1000行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为50到200MB/s。如果您写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。</p>\n<h1 id=\"Yandex-Metrica的使用案例\"><a href=\"#Yandex-Metrica的使用案例\" class=\"headerlink\" title=\"Yandex.Metrica的使用案例\"></a>Yandex.Metrica的使用案例</h1><p>ClickHouse最初是为 <a href=\"https://metrica.yandex.com/\" target=\"_blank\" rel=\"noopener\">Yandex.Metrica</a> <a href=\"http://w3techs.com/technologies/overview/traffic_analysis/all\" target=\"_blank\" rel=\"noopener\">世界第二大Web分析平台</a> 而开发的。多年来一直作为该系统的核心组件被该系统持续使用着。目前为止，该系统在ClickHouse中有超过13万亿条记录，并且每天超过200多亿个事件被处理。它允许直接从原始数据中动态查询并生成报告。本文简要介绍了ClickHouse在其早期发展阶段的目标。</p>\n<p>Yandex.Metrica基于用户定义的字段，对实时访问、连接会话，生成实时的统计报表。这种需求往往需要复杂聚合方式，比如对访问用户进行去重。构建报表的数据，是实时接收存储的新数据。</p>\n<p>截至2014年4月，Yandex.Metrica每天跟踪大约120亿个事件（用户的点击和浏览）。为了可以创建自定义的报表，我们必须存储全部这些事件。同时，这些查询可能需要在几百毫秒内扫描数百万行的数据，或在几秒内扫描数亿行的数据。</p>\n<h2 id=\"Yandex-Metrica以及其他Yandex服务的使用案例\"><a href=\"#Yandex-Metrica以及其他Yandex服务的使用案例\" class=\"headerlink\" title=\"Yandex.Metrica以及其他Yandex服务的使用案例\"></a>Yandex.Metrica以及其他Yandex服务的使用案例</h2><p>在Yandex.Metrica中，ClickHouse被用于多个场景中。 它的主要任务是使用原始数据在线的提供各种数据报告。它使用374台服务器的集群，存储了20.3万亿行的数据。在去除重复与副本数据的情况下，压缩后的数据达到了2PB。未压缩前（TSV格式）它大概有17PB。</p>\n<p>ClickHouse还被使用在：</p>\n<ul>\n<li>存储来自Yandex.Metrica回话重放数据。</li>\n<li>处理中间数据</li>\n<li>与Analytics一起构建全球报表。</li>\n<li>为调试Yandex.Metrica引擎运行查询</li>\n<li>分析来自API和用户界面的日志数据</li>\n</ul>\n<p>ClickHouse在其他Yandex服务中至少有12个安装：search verticals, Market, Direct, business analytics, mobile development, AdFox, personal services等。</p>\n<h2 id=\"聚合与非聚合数据\"><a href=\"#聚合与非聚合数据\" class=\"headerlink\" title=\"聚合与非聚合数据\"></a>聚合与非聚合数据</h2><p>有一种流行的观点认为，想要有效的计算统计数据，必须要聚合数据，因为聚合将降低数据量。</p>\n<p>但是数据聚合是一个有诸多限制的解决方案，例如：</p>\n<ul>\n<li>你必须提前知道用户定义的报表的字段列表</li>\n<li>用户无法自定义报表</li>\n<li>当聚合条件过多时，可能不会减少数据，聚合是无用的。</li>\n<li>存在大量报表时，有太多的聚合变化（组合爆炸）</li>\n<li>当聚合条件有非常大的基数时（如：url），数据量没有太大减少（少于两倍）</li>\n<li>聚合的数据量可能会增长而不是收缩</li>\n<li>用户不会查看我们为他生成的所有报告，大部分计算将是无用的</li>\n<li>各种聚合可能违背了数据的逻辑完整性</li>\n</ul>\n<p>如果我们直接使用非聚合数据而不尽兴任何聚合时，我们的计算量可能是减少的。</p>\n<p>然而，相对于聚合中很大一部分工作被离线完成，在线计算需要尽快的完成计算，因为用户在等待结果。</p>\n<p>Yandex.Metrica 有一个专门用于聚合数据的系统，称为Metrage，它可以用作大部分报表。 从2009年开始，Yandex.Metrica还为非聚合数据使用专门的OLAP数据库，称为OLAPServer，它以前用于报表构建系统。 OLAPServer可以很好的工作在非聚合数据上，但是它有诸多限制，导致无法根据需要将其用于所有报表中。如，缺少对数据类型的支持（只支持数据），无法实时增量的更新数据（只能通过每天重写数据完成）。OLAPServer不是一个数据库管理系统，它只是一个数据库。</p>\n<p>为了消除OLAPServer的这些局限性，解决所有报表使用非聚合数据的问题，我们开发了ClickHouse数据库管理系统。</p>\n<h1 id=\"ClickHouse表引擎\"><a href=\"#ClickHouse表引擎\" class=\"headerlink\" title=\"ClickHouse表引擎\"></a>ClickHouse表引擎</h1><p>表引擎（即表的类型）决定了：</p>\n<ul>\n<li>数据的存储方式和位置，写到哪里以及从哪里读取数据</li>\n<li>支持哪些查询以及如何支持。</li>\n<li>并发数据访问。</li>\n<li>索引的使用（如果存在）。</li>\n<li>是否可以执行多线程请求。</li>\n<li>数据复制参数。</li>\n</ul>\n<p>在读取时，引擎只需要输出所请求的列，但在某些情况下，引擎可以在响应请求时部分处理数据。</p>\n<p>对于大多数正式的任务，应该使用MergeTree族中的引擎。</p>\n<h2 id=\"MergeTree\"><a href=\"#MergeTree\" class=\"headerlink\" title=\"MergeTree\"></a>MergeTree</h2><p>Clickhouse 中最强大的表引擎当属 <code>MergeTree</code> （合并树）引擎及该系列（<code>*MergeTree</code>）中的其他引擎。</p>\n<p><code>MergeTree</code> 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。</p>\n<p>主要特点:</p>\n<ul>\n<li><p>存储的数据按主键排序。</p>\n<p>这让你可以创建一个用于快速检索数据的小稀疏索引。</p>\n</li>\n<li><p>允许使用分区，如果指定了 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/custom_partitioning_key/\" target=\"_blank\" rel=\"noopener\">主键</a> 的话。</p>\n<p>在相同数据集和相同结果集的情况下 ClickHouse 中某些带分区的操作会比普通操作更快。查询中指定了分区键时 ClickHouse 会自动截取分区数据。这也有效增加了查询性能。</p>\n</li>\n<li><p>支持数据副本。</p>\n<p><code>ReplicatedMergeTree</code> 系列的表便是用于此。更多信息，请参阅 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/\" target=\"_blank\" rel=\"noopener\">数据副本</a> 一节。</p>\n</li>\n<li><p>支持数据采样。</p>\n<p>需要的话，你可以给表设置一个采样方法。</p>\n</li>\n</ul>\n<p>!!! 注意 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/merge/\" target=\"_blank\" rel=\"noopener\">Merge</a> 引擎并不属于 <code>*MergeTree</code> 系列。</p>\n<h3 id=\"建表\"><a href=\"#建表\" class=\"headerlink\" title=\"建表\"></a>建表</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">INDEX</span> index_name1 expr1 <span class=\"keyword\">TYPE</span> type1(...) GRANULARITY value1,</span><br><span class=\"line\">    <span class=\"keyword\">INDEX</span> index_name2 expr2 <span class=\"keyword\">TYPE</span> type2(...) GRANULARITY value2</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = MergeTree()</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[PRIMARY <span class=\"keyword\">KEY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>请求参数的描述，参考 <a href=\"https://clickhouse.yandex/docs/zh/query_language/create/\" target=\"_blank\" rel=\"noopener\">请求描述</a> 。</p>\n<p>示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> xcloud.cdn_nginx_log_minute_agg</span><br><span class=\"line\">(</span><br><span class=\"line\">\t<span class=\"built_in\">date</span> <span class=\"built_in\">Date</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">timeStamp</span> DateTime,</span><br><span class=\"line\"></span><br><span class=\"line\">\tchannel <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tcustomer <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tcountry <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tflow AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tvisit AggregateFunction(<span class=\"keyword\">sum</span>, Int64),  </span><br><span class=\"line\"></span><br><span class=\"line\">\tdownload_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tresponse_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tupstream_response_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tfirst_byte_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\trequest_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tdownload_flow AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tresponse_normal AggregateFunction(<span class=\"keyword\">sum</span>, Int64)</span><br><span class=\"line\">)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = ReplicatedAggregatingMergeTree(<span class=\"string\">'&#123;zkpath&#125;'</span>, <span class=\"string\">'&#123;replica&#125;'</span>)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"built_in\">date</span>  <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (<span class=\"keyword\">timeStamp</span>, <span class=\"built_in\">date</span>, channel, customer, country);</span><br></pre></td></tr></table></figure>\n<h3 id=\"数据存储\"><a href=\"#数据存储\" class=\"headerlink\" title=\"数据存储\"></a>数据存储</h3><p>表由按主键排序的数据 <em>片段</em> 组成。</p>\n<p>当数据被插入到表中时，会分成数据片段并按主键的字典序排序。例如，主键是 <code>(CounterID, Date)</code> 时，片段中数据按 <code>CounterID</code> 排序，具有相同 <code>CounterID</code> 的部分按 <code>Date</code> 排序。</p>\n<p>不同分区的数据会被分成不同的片段，ClickHouse 在后台合并数据片段以便更高效存储。不会合并来自不同分区的数据片段。这个合并机制并不保证相同主键的所有行都会合并到同一个数据片段中。</p>\n<p>ClickHouse 会为每个数据片段创建一个索引文件，索引文件包含每个索引行（『标记』）的主键值。索引行号定义为 <code>n * index_granularity</code> 。最大的 <code>n</code> 等于总行数除以 <code>index_granularity</code> 的值的整数部分。对于每列，跟主键相同的索引行处也会写入『标记』。这些『标记』让你可以直接找到数据所在的列。</p>\n<p>你可以只用一单一大表并不断地一块块往里面加入数据 – <code>MergeTree</code> 引擎的就是为了这样的场景。</p>\n<h3 id=\"主键和索引在查询中的表现\"><a href=\"#主键和索引在查询中的表现\" class=\"headerlink\" title=\"主键和索引在查询中的表现\"></a>主键和索引在查询中的表现</h3><p>我们以 <code>(CounterID, Date)</code> 以主键。排序好的索引的图示会是下面这样：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">全部数据  :     [-------------------------------------------------------------------------]</span><br><span class=\"line\">CounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]</span><br><span class=\"line\">Date:           [1111111222222233331233211111222222333211111112122222223111112223311122333]</span><br><span class=\"line\">标记:            |      |      |      |      |      |      |      |      |      |      |</span><br><span class=\"line\">                a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3</span><br><span class=\"line\">标记号:          0      1      2      3      4      5      6      7      8      9      10</span><br></pre></td></tr></table></figure>\n<p>如果指定查询如下：</p>\n<ul>\n<li><code>CounterID in (&#39;a&#39;, &#39;h&#39;)</code>，服务器会读取标记号在 <code>[0, 3)</code> 和 <code>[6, 8)</code> 区间中的数据。</li>\n<li><code>CounterID IN (&#39;a&#39;, &#39;h&#39;) AND Date = 3</code>，服务器会读取标记号在 <code>[1, 3)</code> 和 <code>[7, 8)</code> 区间中的数据。</li>\n<li><code>Date = 3</code>，服务器会读取标记号在 <code>[1, 10]</code> 区间中的数据。</li>\n</ul>\n<p>上面例子可以看出使用索引通常会比全表描述要高效。</p>\n<p>稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 <code>index_granularity * 2</code> 行额外的数据。大部分情况下，当 <code>index_granularity = 8192</code> 时，ClickHouse的性能并不会降级。</p>\n<p>稀疏索引让你能操作有巨量行的表。因为这些索引是常驻内存（RAM）的。</p>\n<p>ClickHouse 不要求主键惟一。所以，你可以插入多条具有相同主键的行。</p>\n<h3 id=\"主键的选择\"><a href=\"#主键的选择\" class=\"headerlink\" title=\"主键的选择\"></a>主键的选择</h3><p>主键中列的数量并没有明确的限制。依据数据结构，你应该让主键包含多些或少些列。这样可以：</p>\n<ul>\n<li><p>改善索引的性能。</p>\n<p>如果当前主键是 <code>(a, b)</code> ，然后加入另一个 <code>c</code> 列，满足下面条件时，则可以改善性能： - 有带有 <code>c</code> 列条件的查询。 - 很长的数据范围（ <code>index_granularity</code> 的数倍）里 <code>(a, b)</code> 都是相同的值，并且这种的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。</p>\n</li>\n<li><p>改善数据压缩。</p>\n<p>ClickHouse 以主键排序片段数据，所以，数据的一致性越高，压缩越好。</p>\n</li>\n<li><p><a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/collapsingmergetree/#table_engine-collapsingmergetree\" target=\"_blank\" rel=\"noopener\">CollapsingMergeTree</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/\" target=\"_blank\" rel=\"noopener\">SummingMergeTree</a> 引擎里，数据合并时，会有额外的处理逻辑。</p>\n<p>在这种情况下，指定一个跟主键不同的 <em>排序键</em> 也是有意义的。</p>\n</li>\n</ul>\n<p>长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 <code>SELECT</code> 查询的性能。</p>\n<h3 id=\"选择跟排序键不一样主键\"><a href=\"#选择跟排序键不一样主键\" class=\"headerlink\" title=\"选择跟排序键不一样主键\"></a>选择跟排序键不一样主键</h3><p>指定一个跟排序键（用于排序数据片段中行的表达式） 不一样的主键（用于计算写到索引文件的每个标记值的表达式）是可以的。 这种情况下，主键表达式元组必须是排序键表达式元组的一个前缀。</p>\n<p>当使用 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/\" target=\"_blank\" rel=\"noopener\">SummingMergeTree</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/aggregatingmergetree/\" target=\"_blank\" rel=\"noopener\">AggregatingMergeTree</a> 引擎时，这个特性非常有用。 通常，使用这类引擎时，表里列分两种：<em>维度</em> 和 <em>度量</em> 。 典型的查询是在 <code>GROUP BY</code> 并过虑维度的情况下统计度量列的值。 像 SummingMergeTree 和 AggregatingMergeTree ，用相同的排序键值统计行时， 通常会加上所有的维度。结果就是，这键的表达式会是一长串的列组成， 并且这组列还会因为新加维度必须频繁更新。</p>\n<p>这种情况下，主键中仅预留少量列保证高效范围扫描， 剩下的维度列放到排序键元组里。这样是合理的。</p>\n<p><a href=\"https://clickhouse.yandex/docs/zh/query_language/alter/\" target=\"_blank\" rel=\"noopener\">排序键的修改</a> 是轻量级的操作，因为一个新列同时被加入到表里和排序键后时，已存在的数据片段并不需要修改。由于旧的排序键是新排序键的前缀，并且刚刚添加的列中没有数据，因此在表修改时的数据对于新旧的排序键来说都是有序的。</p>\n<h3 id=\"索引和分区在查询中的应用\"><a href=\"#索引和分区在查询中的应用\" class=\"headerlink\" title=\"索引和分区在查询中的应用\"></a>索引和分区在查询中的应用</h3><p>对于 <code>SELECT</code> 查询，ClickHouse 分析是否可以使用索引。如果 <code>WHERE/PREWHERE</code> 子句具有下面这些表达式（作为谓词链接一子项或整个）则可以使用索引：基于主键或分区键的列或表达式的部分的等式或比较运算表达式；基于主键或分区键的列或表达式的固定前缀的 <code>IN</code> 或 <code>LIKE</code> 表达式；基于主键或分区键的列的某些函数；基于主键或分区键的表达式的逻辑表达式。</p>\n<p>因此，在索引键的一个或多个区间上快速地跑查询都是可能的。下面例子中，指定标签；指定标签和日期范围；指定标签和日期；指定多个标签和日期范围等运行查询，都会非常快。</p>\n<p>要检查 ClickHouse 执行一个查询时能否使用索引，可设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-force_index_by_date\" target=\"_blank\" rel=\"noopener\">force_index_by_date</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/\" target=\"_blank\" rel=\"noopener\">force_primary_key</a> 。</p>\n<p>按月分区的分区键是只能读取包含适当范围日期的数据块。这种情况下，数据块会包含很多天（最多整月）的数据。在块中，数据按主键排序，主键第一列可能不包含日期。因此，仅使用日期而没有带主键前缀条件的查询将会导致读取超过这个日期范围。</p>\n<h3 id=\"并发数据访问\"><a href=\"#并发数据访问\" class=\"headerlink\" title=\"并发数据访问\"></a>并发数据访问</h3><p>应对表的并发访问，我们使用多版本机制。换言之，当同时读和更新表时，数据从当前查询到的一组片段中读取。没有冗长的的锁。插入不会阻碍读取。</p>\n<p>对表的读操作是自动并行的。</p>\n<h2 id=\"数据副本\"><a href=\"#数据副本\" class=\"headerlink\" title=\"数据副本\"></a>数据副本</h2><p>只有 MergeTree 系列里的表可支持副本：</p>\n<ul>\n<li>ReplicatedMergeTree</li>\n<li>ReplicatedSummingMergeTree</li>\n<li>ReplicatedReplacingMergeTree</li>\n<li>ReplicatedAggregatingMergeTree</li>\n<li>ReplicatedCollapsingMergeTree</li>\n<li>ReplicatedVersionedCollapsingMergeTree</li>\n<li>ReplicatedGraphiteMergeTree</li>\n</ul>\n<p>副本是表级别的，不是整个服务器级的。所以，服务器里可以同时有复制表和非复制表。</p>\n<p>副本不依赖分片。每个分片有它自己的独立副本。</p>\n<p>对于 <code>INSERT</code> 和 <code>ALTER</code> 语句操作数据的会在压缩的情况下被复制（更多信息，看 <a href=\"https://clickhouse.yandex/docs/zh/query_language/alter/#query_language_queries_alter\" target=\"_blank\" rel=\"noopener\">ALTER</a> ）。</p>\n<p>而 <code>CREATE</code>，<code>DROP</code>，<code>ATTACH</code>，<code>DETACH</code> 和 <code>RENAME</code> 语句只会在单个服务器上执行，不会被复制。</p>\n<ul>\n<li><code>The CREATE TABLE</code> 在运行此语句的服务器上创建一个新的可复制表。如果此表已存在其他服务器上，则给该表添加新副本。</li>\n<li><code>The DROP TABLE</code> 删除运行此查询的服务器上的副本。</li>\n<li><code>The RENAME</code> 重命名一个副本。换句话说，可复制表不同的副本可以有不同的名称。</li>\n</ul>\n<p>要使用副本，需在配置文件中设置 ZooKeeper 集群的地址。</p>\n<p><code>SELECT</code> 查询并不需要借助 ZooKeeper ，复本并不影响 <code>SELECT</code> 的性能，查询复制表与非复制表速度是一样的。查询分布式表时，ClickHouse的处理方式可通过设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries\" target=\"_blank\" rel=\"noopener\">max_replica_delay_for_distributed_queries</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/\" target=\"_blank\" rel=\"noopener\">fallback_to_stale_replicas_for_distributed_queries</a> 修改。</p>\n<p>对于每个 <code>INSERT</code> 语句，会通过几个事务将十来个记录添加到 ZooKeeper。（确切地说，这是针对每个插入的数据块; 每个 INSERT 语句的每 <code>max_insert_block_size = 1048576</code> 行和最后剩余的都各算作一个块。）相比非复制表，写 zk 会导致 <code>INSERT</code> 的延迟略长一些。但只要你按照建议每秒不超过一个 <code>INSERT</code> 地批量插入数据，不会有任何问题。一个 ZooKeeper 集群能给整个 ClickHouse 集群支撑协调每秒几百个 <code>INSERT</code>。数据插入的吞吐量（每秒的行数）可以跟不用复制的数据一样高。</p>\n<p>对于非常大的集群，你可以把不同的 ZooKeeper 集群用于不同的分片。然而，即使 Yandex.Metrica 集群（大约300台服务器）也证明还不需要这么做。</p>\n<p>复制是多主异步。 <code>INSERT</code> 语句（以及 <code>ALTER</code> ）可以发给任意可用的服务器。数据会先插入到执行该语句的服务器上，然后被复制到其他服务器。由于它是异步的，在其他副本上最近插入的数据会有一些延迟。如果部分副本不可用，则数据在其可用时再写入。副本可用的情况下，则延迟时长是通过网络传输压缩数据块所需的时间。</p>\n<p>默认情况下，INSERT 语句仅等待一个副本写入成功后返回。如果数据只成功写入一个副本后该副本所在的服务器不再存在，则存储的数据会丢失。要启用数据写入多个副本才确认返回，使用 <code>insert_quorum</code> 选项。</p>\n<p>单个数据块写入是原子的。 INSERT 的数据按每块最多 <code>max_insert_block_size = 1048576</code> 行进行分块，换句话说，如果 <code>INSERT</code> 插入的行少于 1048576，则该 INSERT 是原子的。</p>\n<p>数据块会去重。对于被多次写的相同数据块（大小相同且具有相同顺序的相同行的数据块），该块仅会写入一次。这样设计的原因是万一在网络故障时客户端应用程序不知道数据是否成功写入DB，此时可以简单地重复 <code>INSERT</code> 。把相同的数据发送给多个副本 INSERT 并不会有问题。因为这些 <code>INSERT</code> 是完全相同的（会被去重）。去重参数参看服务器设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/server_settings/settings/\" target=\"_blank\" rel=\"noopener\">merge_tree</a> 。（注意：Replicated*MergeTree 才会去重，不需要 zookeeper 的不带 MergeTree 不会去重）</p>\n<p>在复制期间，只有要插入的源数据通过网络传输。进一步的数据转换（合并）会在所有副本上以相同的方式进行处理执行。这样可以最大限度地减少网络使用，这意味着即使副本在不同的数据中心，数据同步也能工作良好。（能在不同数据中心中的同步数据是副本机制的主要目标。）</p>\n<p>你可以给数据做任意多的副本。Yandex.Metrica 在生产中使用双副本。某一些情况下，给每台服务器都使用 RAID-5 或 RAID-6 和 RAID-10。是一种相对可靠和方便的解决方案。</p>\n<p>系统会监视副本数据同步情况，并能在发生故障后恢复。故障转移是自动的（对于小的数据差异）或半自动的（当数据差异很大时，这可能意味是有配置错误）。</p>\n<h3 id=\"创建复制表\"><a href=\"#创建复制表\" class=\"headerlink\" title=\"创建复制表\"></a>创建复制表</h3><p>在表引擎名称上加上 <code>Replicated</code> 前缀。例如：<code>ReplicatedMergeTree</code>。</p>\n<p><strong>Replicated*MergeTree 参数</strong></p>\n<ul>\n<li><code>zoo_path</code> — ZooKeeper 中该表的路径。</li>\n<li><code>replica_name</code> — ZooKeeper 中的该表的副本名称。</li>\n</ul>\n<p>示例:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> table_name</span><br><span class=\"line\">(</span><br><span class=\"line\">    EventDate DateTime,</span><br><span class=\"line\">    CounterID UInt32,</span><br><span class=\"line\">    UserID UInt32</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = ReplicatedMergeTree(<span class=\"string\">'/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_name'</span>, <span class=\"string\">'&#123;replica&#125;'</span>)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(EventDate)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (CounterID, EventDate, intHash32(UserID))</span><br><span class=\"line\"><span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> intHash32(UserID)</span><br></pre></td></tr></table></figure>\n<p>如上例所示，这些参数可以包含宏替换的占位符，即大括号的部分。它们会被替换为配置文件里 ‘macros’ 那部分配置的值。示例：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">macros</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">layer</span>&gt;</span>05<span class=\"tag\">&lt;/<span class=\"name\">layer</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span>02<span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span>example05-02-1.yandex.ru<span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">macros</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>“ZooKeeper 中该表的路径”对每个可复制表都要是唯一的。不同分片上的表要有不同的路径。 这种情况下，路径包含下面这些部分：</p>\n<p><code>/clickhouse/tables/</code> 是公共前缀，我们推荐使用这个。</p>\n<p><code>{layer}-{shard}</code> 是分片标识部分。在此示例中，由于 Yandex.Metrica 集群使用了两级分片，所以它是由两部分组成的。但对于大多数情况来说，你只需保留 {shard} 占位符即可，它会替换展开为分片标识。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">table_name` 是该表在 ZooKeeper 中的名称。使其与 ClickHouse 中的表名相同比较好。 这里它被明确定义，跟 ClickHouse 表名不一样，它并不会被 RENAME 语句修改。</span><br><span class=\"line\">*HINT*: you could add a database name in front of `table_name` as well. E.g. `db_name.table_name</span><br></pre></td></tr></table></figure>\n<p>副本名称用于标识同一个表分片的不同副本。你可以使用服务器名称，如上例所示。同个分片中不同副本的副本名称要唯一。</p>\n<p>你也可以显式指定这些参数，而不是使用宏替换。对于测试和配置小型集群这可能会很方便。但是，这种情况下，则不能使用分布式 DDL 语句（<code>ON CLUSTER</code>）。</p>\n<p>使用大型集群时，我们建议使用宏替换，因为它可以降低出错的可能性。</p>\n<p>在每个副本服务器上运行 <code>CREATE TABLE</code> 查询。将创建新的复制表，或给现有表添加新副本。</p>\n<p>如果其他副本上已包含了某些数据，在表上添加新副本，则在运行语句后，数据会从其他副本复制到新副本。换句话说，新副本会与其他副本同步。</p>\n<p>要删除副本，使用 <code>DROP TABLE</code>。但它只删除那个 – 位于运行该语句的服务器上的副本。</p>\n<h3 id=\"故障恢复\"><a href=\"#故障恢复\" class=\"headerlink\" title=\"故障恢复\"></a>故障恢复</h3><p>如果服务器启动时 ZooKeeper 不可用，则复制表会切换为只读模式。系统会定期尝试去连接 ZooKeeper。</p>\n<p>如果在 <code>INSERT</code> 期间 ZooKeeper 不可用，或者在与 ZooKeeper 交互时发生错误，则抛出异常。</p>\n<p>连接到 ZooKeeper 后，系统会检查本地文件系统中的数据集是否与预期的数据集（ ZooKeeper 存储此信息）一致。如果存在轻微的不一致，系统会通过与副本同步数据来解决。</p>\n<p>如果系统检测到损坏的数据片段（文件大小错误）或无法识别的片段（写入文件系统但未记录在 ZooKeeper 中的部分），则会把它们移动到 ‘detached’ 子目录（不会删除）。而副本中其他任何缺少的但正常数据片段都会被复制同步。</p>\n<p>注意，ClickHouse 不会执行任何破坏性操作，例如自动删除大量数据。</p>\n<p>当服务器启动（或与 ZooKeeper 建立新会话）时，它只检查所有文件的数量和大小。 如果文件大小一致但中间某处已有字节被修改过，不会立即被检测到，只有在尝试读取 <code>SELECT</code> 查询的数据时才会检测到。该查询会引发校验和不匹配或压缩块大小不一致的异常。这种情况下，数据片段会添加到验证队列中，并在必要时从其他副本中复制。</p>\n<p>如果本地数据集与预期数据的差异太大，则会触发安全机制。服务器在日志中记录此内容并拒绝启动。这种情况很可能是配置错误，例如，一个分片上的副本意外配置为别的分片上的副本。然而，此机制的阈值设置得相当低，在正常故障恢复期间可能会出现这种情况。在这种情况下，数据恢复则是半自动模式，通过用户主动操作触发。</p>\n<p>要触发启动恢复，可在 ZooKeeper 中创建节点 <code>/path_to_table/replica_name/flags/force_restore_data</code>，节点值可以是任何内容，或运行命令来恢复所有的可复制表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data</span><br></pre></td></tr></table></figure>\n<p>然后重启服务器。启动时，服务器会删除这些标志并开始恢复。</p>\n<h3 id=\"在数据完全丢失后的恢复\"><a href=\"#在数据完全丢失后的恢复\" class=\"headerlink\" title=\"在数据完全丢失后的恢复\"></a>在数据完全丢失后的恢复</h3><p>如果其中一个服务器的所有数据和元数据都消失了，请按照以下步骤进行恢复：</p>\n<ol>\n<li>在服务器上安装 ClickHouse。在包含分片标识符和副本的配置文件中正确定义宏配置，如果有用到的话，</li>\n<li>如果服务器上有非复制表则必须手动复制，可以从副本服务器上（在 <code>/var/lib/clickhouse/data/db_name/table_name/</code> 目录中）复制它们的数据。</li>\n<li>从副本服务器上中复制位于 <code>/var/lib/clickhouse/metadata/</code> 中的表定义信息。如果在表定义信息中显式指定了分片或副本标识符，请更正它以使其对应于该副本。（另外，启动服务器，然后会在 <code>/var/lib/clickhouse/metadata/</code> 中的.sql文件中生成所有的 <code>ATTACH TABLE</code> 语句。） 4.要开始恢复，ZooKeeper 中创建节点 <code>/path_to_table/replica_name/flags/force_restore_data</code>，节点内容不限，或运行命令来恢复所有复制的表：<code>sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data</code></li>\n</ol>\n<p>然后启动服务器（如果它已运行则重启）。数据会从副本中下载。</p>\n<p>另一种恢复方式是从 ZooKeeper（<code>/path_to_table/replica_name</code>）中删除有数据丢的副本的所有元信息，然后再按照“<a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/#creating-replicated-tables\" target=\"_blank\" rel=\"noopener\">创建可复制表</a>”中的描述重新创建副本。</p>\n<p>恢复期间的网络带宽没有限制。特别注意这一点，尤其是要一次恢复很多副本。</p>\n<h2 id=\"自定义分区键\"><a href=\"#自定义分区键\" class=\"headerlink\" title=\"自定义分区键\"></a>自定义分区键</h2><p><a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a> 系列的表（包括 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/\" target=\"_blank\" rel=\"noopener\">可复制表</a> ）可以使用分区。基于 MergeTree 表的 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/materializedview/\" target=\"_blank\" rel=\"noopener\">物化视图</a> 也支持分区。</p>\n<p>一个分区是指按指定规则逻辑组合一起的表的记录集。可以按任意标准进行分区，如按月，按日或按事件类型。为了减少需要操作的数据，每个分区都是分开存储的。访问数据时，ClickHouse 尽量使用这些分区的最小子集。</p>\n<p>分区是在 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#table_engine-mergetree-creating-a-table\" target=\"_blank\" rel=\"noopener\">建表</a> 的 <code>PARTITION BY expr</code> 子句中指定。分区键可以是关于列的任何表达式。例如，指定按月分区，表达式为 <code>toYYYYMM(date_column)</code>：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> visits</span><br><span class=\"line\">(</span><br><span class=\"line\">    VisitDate <span class=\"built_in\">Date</span>, </span><br><span class=\"line\">    <span class=\"keyword\">Hour</span> UInt8, </span><br><span class=\"line\">    ClientID <span class=\"keyword\">UUID</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = MergeTree()</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(VisitDate)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">Hour</span>;</span><br></pre></td></tr></table></figure>\n<p>分区键也可以是表达式元组（类似 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#primary-keys-and-indexes-in-queries\" target=\"_blank\" rel=\"noopener\">主键</a> ）。例如：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)</span><br><span class=\"line\">PARTITION BY (toMonday(StartDate), EventType)</span><br><span class=\"line\">ORDER BY (CounterID, StartDate, intHash32(UserID));</span><br></pre></td></tr></table></figure>\n<p>上例中，我们设置按一周内的事件类型分区。</p>\n<p>新数据插入到表中时，这些数据会存储为按主键排序的新片段（块）。插入后 10-15 分钟，同一分区的各个片段会合并为一整个片段。</p>\n<p><strong>那些有相同分区表达式值的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量和需要找开的文件描述符过多，导致 <code>SELECT</code> 查询效率不佳。</strong></p>\n<h2 id=\"ReplacingMergeTree-引擎\"><a href=\"#ReplacingMergeTree-引擎\" class=\"headerlink\" title=\"ReplacingMergeTree 引擎\"></a>ReplacingMergeTree 引擎</h2><p>该引擎和<a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>的不同之处在于它会删除具有相同主键的重复项。</p>\n<p>数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，因此你无法预先作出计划。有一些数据可能仍未被处理。尽管你可以调用 <code>OPTIMIZE</code> 语句发起计划外的合并，但请不要指望使用它，因为 <code>OPTIMIZE</code> 语句会引发对大量数据的读和写。</p>\n<p>因此，<code>ReplacingMergeTree</code> 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。</p>\n<p>建表:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = ReplacingMergeTree([ver])</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<h2 id=\"SummingMergeTree-引擎\"><a href=\"#SummingMergeTree-引擎\" class=\"headerlink\" title=\"SummingMergeTree 引擎\"></a>SummingMergeTree 引擎</h2><p>该引擎继承自 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>。区别在于，当合并 <code>SummingMergeTree</code> 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度。</p>\n<p>我们推荐将该引擎和 <code>MergeTree</code> 一起使用。例如，在准备做报告的时候，将完整的数据存储在 <code>MergeTree</code> 表中，并且使用 <code>SummingMergeTree</code> 来存储聚合数据。这种方法可以使你避免因为使用不正确的主键组合方式而丢失有价值的数据。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = SummingMergeTree([<span class=\"keyword\">columns</span>])</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>当数据被插入到表中时，他们将被原样保存。ClickHouse 定期合并插入的数据片段，并在这个时候对所有具有相同主键的行中的列进行汇总，将这些行替换为包含汇总数据的一行记录。</p>\n<p>ClickHouse 会按片段合并数据，以至于不同的数据片段中会包含具有相同主键的行，即单个汇总片段将会是不完整的。因此，聚合函数 <a href=\"https://clickhouse.yandex/docs/zh/query_language/agg_functions/reference/#agg_function-sum\" target=\"_blank\" rel=\"noopener\">sum()</a> 和 <code>GROUP BY</code> 子句应该在（<code>SELECT</code>）查询语句中被使用，如上文中的例子所述。</p>\n<p>列中数值类型的值会被汇总。这些列的集合在参数 <code>columns</code> 中被定义。</p>\n<p>如果用于汇总的所有列中的值均为0，则该行会被删除。</p>\n<p>如果列不在主键中且无法被汇总，则会在现有的值中任选一个。</p>\n<p>主键所在的列中的值不会被汇总。</p>\n<h2 id=\"AggregatingMergeTree-引擎\"><a href=\"#AggregatingMergeTree-引擎\" class=\"headerlink\" title=\"AggregatingMergeTree 引擎\"></a>AggregatingMergeTree 引擎</h2><p>该引擎继承自 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>，并改变了数据片段的合并逻辑。 ClickHouse 会将相同主键的所有行（在一个数据片段内）替换为单个存储一系列聚合函数状态的行。</p>\n<p>可以使用 <code>AggregatingMergeTree</code> 表来做增量数据统计聚合，包括物化视图的数据聚合。</p>\n<p>引擎需使用 <a href=\"https://clickhouse.yandex/docs/zh/data_types/nested_data_structures/aggregatefunction/\" target=\"_blank\" rel=\"noopener\">AggregateFunction</a> 类型来处理所有列。</p>\n<p>如果要按一组规则来合并减少行数，则使用 <code>AggregatingMergeTree</code> 是合适的。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = AggregatingMergeTree()</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>插入数据，需使用带有聚合 -State- 函数的 <a href=\"https://clickhouse.yandex/docs/zh/query_language/insert_into/\" target=\"_blank\" rel=\"noopener\">INSERT SELECT</a> 语句。 从 <code>AggregatingMergeTree</code> 表中查询数据时，需使用 <code>GROUP BY</code> 子句并且要使用与插入时相同的聚合函数，但后缀要改为 <code>-Merge</code> 。</p>\n<p>在 <code>SELECT</code> 查询的结果中，对于 ClickHouse 的所有输出格式 <code>AggregateFunction</code> 类型的值都实现了特定的二进制表示法。如果直接用 <code>SELECT</code> 导出这些数据，例如如用 <code>TabSeparated</code> 格式，那么这些导出数据也能直接用 <code>INSERT</code> 语句加载导入</p>\n<p>聚合物化视图的示例, 创建一个跟踪 <code>test.visits</code> 表的 <code>AggregatingMergeTree</code> 物化视图：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">MATERIALIZED</span> <span class=\"keyword\">VIEW</span> test.basic</span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = AggregatingMergeTree() <span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(StartDate) <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (CounterID, StartDate)</span><br><span class=\"line\"><span class=\"keyword\">AS</span> <span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    CounterID,</span><br><span class=\"line\">    StartDate,</span><br><span class=\"line\">    sumState(<span class=\"keyword\">Sign</span>)    <span class=\"keyword\">AS</span> Visits,</span><br><span class=\"line\">    uniqState(UserID) <span class=\"keyword\">AS</span> <span class=\"keyword\">Users</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> test.visits</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> CounterID, StartDate;</span><br></pre></td></tr></table></figure>\n<p>向 <code>test.visits</code> 表中插入数据。数据会同时插入到表和视图中，并且视图 <code>test.basic</code> 会将里面的数据聚合。</p>\n<p>要获取聚合数据，我们需要在 <code>test.basic</code> 视图上执行类似 <code>SELECT ... GROUP BY ...</code> 这样的查询 ：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    StartDate,</span><br><span class=\"line\">    sumMerge(Visits) <span class=\"keyword\">AS</span> Visits,</span><br><span class=\"line\">    uniqMerge(<span class=\"keyword\">Users</span>) <span class=\"keyword\">AS</span> <span class=\"keyword\">Users</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> test.basic</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> StartDate</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> StartDate;</span><br></pre></td></tr></table></figure>\n<h1 id=\"分布式引擎\"><a href=\"#分布式引擎\" class=\"headerlink\" title=\"分布式引擎\"></a>分布式引擎</h1><p><strong>分布式引擎本身不存储数据</strong>, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 分布式引擎参数：服务器配置文件中的集群名，远程数据库名，远程表名，数据分片键（可选）。 示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Distributed(logs, default, hits[, sharding_key])</span><br></pre></td></tr></table></figure>\n<p>将会从位于“logs”集群中 default.hits 表所有服务器上读取数据。 远程服务器不仅用于读取数据，还会对尽可能数据做部分处理。 例如，对于使用 GROUP BY 的查询，数据首先在远程服务器聚合，之后返回聚合函数的中间状态给查询请求的服务器。再在请求的服务器上进一步汇总数据。</p>\n<p>数据库名参数除了用数据库名之外，也可用返回字符串的常量表达式。例如：currentDatabase()。</p>\n<p>集群示例配置如下：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">remote_servers</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">logs</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">            <span class=\"comment\">&lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">            <span class=\"comment\">&lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-01-1<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-01-2<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-02-1<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-02-2<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">secure</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">secure</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9440<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">logs</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">remote_servers</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>这里定义了一个名为‘logs’的集群，它由两个分片组成，每个分片包含两个副本。 分片是指包含数据不同部分的服务器（要读取所有数据，必须访问所有分片）。 副本是存储复制数据的服务器（要读取所有数据，访问任一副本上的数据即可）。</p>\n<p>每个服务器需要指定 <code>host</code>，<code>port</code>，和可选的 <code>user</code>，<code>password</code>，<code>secure</code>，<code>compression</code> 的参数：</p>\n<ul>\n<li><code>host</code> – 远程服务器地址。可以域名、IPv4或IPv6。如果指定域名，则服务在启动时发起一个 DNS 请求，并且请求结果会在服务器运行期间一直被记录。如果 DNS 请求失败，则服务不会启动。如果你修改了 DNS 记录，则需要重启服务。</li>\n<li><code>port</code> – 消息传递的 TCP 端口（「tcp_port」配置通常设为 9000）。不要跟 http_port 混淆。</li>\n<li><code>user</code> – 用于连接远程服务器的用户名。默认值：default。该用户必须有权限访问该远程服务器。访问权限配置在 users.xml 文件中。更多信息，请查看“访问权限”部分。</li>\n<li><code>password</code> – 用于连接远程服务器的密码。默认值：空字符串。</li>\n<li><code>secure</code> – 是否使用ssl进行连接，设为true时，通常也应该设置 <code>port</code> = 9440。服务器也要监听 9440 并有正确的证书。</li>\n<li><code>compression</code> - 是否使用数据压缩。默认值：true。</li>\n</ul>\n<p>配置了副本，读取操作会从每个分片里选择一个可用的副本。可配置负载平衡算法（挑选副本的方式） - 请参阅“load_balancing”设置。 如果跟服务器的连接不可用，则在尝试短超时的重连。如果重连失败，则选择下一个副本，依此类推。如果跟所有副本的连接尝试都失败，则尝试用相同的方式再重复几次。 该机制有利于系统可用性，但不保证完全容错：如有远程服务器能够接受连接，但无法正常工作或状况不佳。</p>\n<p>你可以配置一个（这种情况下，查询操作更应该称为远程查询，而不是分布式查询）或任意多个分片。在每个分片中，可以配置一个或任意多个副本。不同分片可配置不同数量的副本。</p>\n<p>可以在配置中配置任意数量的集群。</p>\n<p>要查看集群，可使用“system.clusters”表。</p>\n<p>通过分布式引擎可以像使用本地服务器一样使用集群。但是，集群不是自动扩展的：你必须编写集群配置到服务器配置文件中（最好，给所有集群的服务器写上完整配置）。</p>\n<p>不支持用分布式表查询别的分布式表（除非该表只有一个分片）。或者说，要用分布表查查询“最终”的数据表。</p>\n<p>分布式引擎需要将集群信息写入配置文件。配置文件中的集群信息会即时更新，无需重启服务器。如果你每次是要向不确定的一组分片和副本发送查询，则不适合创建分布式表 - 而应该使用“远程”表函数。 请参阅“表函数”部分。</p>\n<p>向集群写数据的方法有两种：</p>\n<p>一，自已指定要将哪些数据写入哪些服务器，并直接在每个分片上执行写入。换句话说，在分布式表上“查询”，在数据表上 INSERT。 这是最灵活的解决方案 – 你可以使用任何分片方案，对于复杂业务特性的需求，这可能是非常重要的。 这也是最佳解决方案，因为数据可以完全独立地写入不同的分片。</p>\n<p>二，在分布式表上执行 INSERT。在这种情况下，分布式表会跨服务器分发插入数据。 为了写入分布式表，必须要配置分片键（最后一个参数）。当然，如果只有一个分片，则写操作在没有分片键的情况下也能工作，因为这种情况下分片键没有意义。</p>\n<p>每个分片都可以在配置文件中定义权重。默认情况下，权重等于1。数据依据分片权重按比例分发到分片上。例如，如果有两个分片，第一个分片的权重是9，而第二个分片的权重是10，则发送 9 / 19 的行到第一个分片， 10 / 19 的行到第二个分片。</p>\n<p>分片可在配置文件中定义 ‘internal_replication’ 参数。</p>\n<p>此参数设置为“true”时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。</p>\n<p>若此参数设置为“false”（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样。</p>\n<p>选择将一行数据发送到哪个分片的方法是，首先计算分片表达式，然后将这个计算结果除以所有分片的权重总和得到余数。该行会发送到那个包含该余数的从’prev_weight’到’prev_weights + weight’的半闭半开区间对应的分片上，其中 ‘prev_weights’ 是该分片前面的所有分片的权重和，’weight’ 是该分片的权重。例如，如果有两个分片，第一个分片权重为9，而第二个分片权重为10，则余数在 [0,9) 中的行发给第一个分片，余数在 [9,19) 中的行发给第二个分片。</p>\n<p>分片表达式可以是由常量和表列组成的任何返回整数表达式。例如，您可以使用表达式 ‘rand()’ 来随机分配数据，或者使用 ‘UserID’ 来按用户 ID 的余数分布（相同用户的数据将分配到单个分片上，这可降低带有用户信息的 IN 和 JOIN 的语句运行的复杂度）。如果该列数据分布不够均匀，可以将其包装在散列函数中：intHash64(UserID)。</p>\n<p>这种简单的用余数来选择分片的方案是有局限的，并不总适用。它适用于中型和大型数据（数十台服务器）的场景，但不适用于巨量数据（数百台或更多服务器）的场景。后一种情况下，应根据业务特性需求考虑的分片方案，而不是直接用分布式表的多分片。</p>\n<p>SELECT 查询会被发送到所有分片，并且无论数据在分片中如何分布（即使数据完全随机分布）都可正常工作。添加新分片时，不必将旧数据传输到该分片。你可以给新分片分配大权重然后写新数据 - 数据可能会稍分布不均，但查询会正确高效地运行。</p>\n<p>下面的情况，你需要关注分片方案：</p>\n<ul>\n<li>使用需要特定键连接数据（ IN 或 JOIN ）的查询。如果数据是用该键进行分片，则应使用本地 IN 或 JOIN 而不是 GLOBAL IN 或 GLOBAL JOIN，这样效率更高。</li>\n<li>使用大量服务器（上百或更多），但有大量小查询（个别客户的查询 - 网站，广告商或合作伙伴）。为了使小查询不影响整个集群，让单个客户的数据处于单个分片上是有意义的。或者，正如我们在 Yandex.Metrica 中所做的那样，你可以配置两级分片：将整个集群划分为“层”，一个层可以包含多个分片。单个客户的数据位于单个层上，根据需要将分片添加到层中，层中的数据随机分布。然后给每层创建分布式表，再创建一个全局的分布式表用于全局的查询。</li>\n</ul>\n<p>数据是异步写入的。对于分布式表的 INSERT，数据块只写本地文件系统。之后会尽快地在后台发送到远程服务器。你可以通过查看表目录中的文件列表（等待发送的数据）来检查数据是否成功发送：/var/lib/clickhouse/data/database/table/ 。</p>\n<p>如果在 INSERT 到分布式表时服务器节点丢失或重启（如，设备故障），则插入的数据可能会丢失。如果在表目录中检测到损坏的数据分片，则会将其转移到“broken”子目录，并不再使用。</p>\n<p>启用 max_parallel_replicas 选项后，会在分表的所有副本上并行查询处理。更多信息，请参阅“设置，max_parallel_replicas”部分。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"ClickHouse系统架构概述\"><a href=\"#ClickHouse系统架构概述\" class=\"headerlink\" title=\"ClickHouse系统架构概述\"></a>ClickHouse系统架构概述</h1><h2 id=\"ClickHouse独特功能\"><a href=\"#ClickHouse独特功能\" class=\"headerlink\" title=\"ClickHouse独特功能\"></a>ClickHouse独特功能</h2><h3 id=\"真正的列式数据库管理系统\"><a href=\"#真正的列式数据库管理系统\" class=\"headerlink\" title=\"真正的列式数据库管理系统\"></a>真正的列式数据库管理系统</h3><p>在一个真正的列式数据库管理系统中，除了数据本身外不应该存在其他额外的数据。这意味着为了避免在值旁边存储它们的长度“number”，你必须支持固定长度数值类型。例如，10亿个UInt8类型的数据在未压缩的情况下大约消耗1GB左右的空间，如果不是这样的话，这将对CPU的使用产生强烈影响。即使是在未压缩的情况下，紧凑的存储数据也是非常重要的，因为解压缩的速度主要取决于未压缩数据的大小。</p>\n<p>这是非常值得注意的，因为在一些其他系统中也可以将不同的列分别进行存储，但由于对其他场景进行的优化，使其无法有效的处理分析查询。例如： HBase，BigTable，Cassandra，HyperTable。在这些系统中，你可以得到每秒数十万的吞吐能力，但是无法得到每秒几亿行的吞吐能力。</p>\n<p>需要说明的是，ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。</p>","more":"<h3 id=\"数据压缩\"><a href=\"#数据压缩\" class=\"headerlink\" title=\"数据压缩\"></a>数据压缩</h3><p>在一些列式数据库管理系统中(例如：InfiniDB CE and MonetDB) 不是用数据压缩。但是, 数据压缩在实现优异的存储系统中确实起着关键的作用。</p>\n<h3 id=\"数据的磁盘存储\"><a href=\"#数据的磁盘存储\" class=\"headerlink\" title=\"数据的磁盘存储\"></a>数据的磁盘存储</h3><p>许多的列式数据库(如 SAP HANA, Google PowerDrill)只能在内存中工作，这种方式会造成比实际更多的设备预算。ClickHouse被设计用于工作在传统磁盘上的系统，它提供每GB更低的存储成本，但如果有可以使用SSD和内存，它也会合理的利用这些资源</p>\n<h3 id=\"多核心并行处理\"><a href=\"#多核心并行处理\" class=\"headerlink\" title=\"多核心并行处理\"></a>多核心并行处理</h3><p>大型查询可以以很自然的方式在ClickHouse中进行并行化处理，以此来使用当前服务器上可用的所有资源。</p>\n<h3 id=\"多服务器分布式处理\"><a href=\"#多服务器分布式处理\" class=\"headerlink\" title=\"多服务器分布式处理\"></a>多服务器分布式处理</h3><p>上面提到的列式数据库管理系统中，几乎没有一个支持分布式的查询处理。 在ClickHouse中，数据可以保存在不同的shard上，每一个shard都由一组用于容错的replica组成，查询可以并行的在所有shard上进行处理。这些对用户来说是透明的</p>\n<h3 id=\"支持SQL\"><a href=\"#支持SQL\" class=\"headerlink\" title=\"支持SQL\"></a>支持SQL</h3><p>ClickHouse支持基于SQL的查询语言，该语言大部分情况下是与SQL标准兼容的。 支持的查询包括 GROUP BY，ORDER BY，IN，JOIN以及非相关子查询。 不支持窗口函数和相关子查询。</p>\n<h3 id=\"向量引擎\"><a href=\"#向量引擎\" class=\"headerlink\" title=\"向量引擎\"></a>向量引擎</h3><p>为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理。</p>\n<h3 id=\"实时的数据更新\"><a href=\"#实时的数据更新\" class=\"headerlink\" title=\"实时的数据更新\"></a>实时的数据更新</h3><p>ClickHouse支持在表中定义主键。为了使查询能够快速在主键中进行范围查找，数据总是以增量的方式有序的存储在MergeTree中。因此，数据可以持续不断高效的写入到表中，并且写入的过程中不会存在任何加锁的行为。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>按照主键对数据进行排序，这将帮助ClickHouse以几十毫秒的低延迟对数据进行特定值查找或范围查找。</p>\n<h3 id=\"适合在线查询\"><a href=\"#适合在线查询\" class=\"headerlink\" title=\"适合在线查询\"></a>适合在线查询</h3><p>在线查询意味着在没有对数据做任何预处理的情况下以极低的延迟处理查询并将结果加载到用户的页面中。</p>\n<h3 id=\"支持近似计算\"><a href=\"#支持近似计算\" class=\"headerlink\" title=\"支持近似计算\"></a>支持近似计算</h3><p>ClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法：</p>\n<ol>\n<li>用于近似计算的各类聚合函数，如：distinct values, medians, quantiles</li>\n<li>基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。</li>\n<li>不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。</li>\n</ol>\n<h3 id=\"支持数据复制和数据完整性\"><a href=\"#支持数据复制和数据完整性\" class=\"headerlink\" title=\"支持数据复制和数据完整性\"></a>支持数据复制和数据完整性</h3><p>ClickHouse使用异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据。在大多数情况下ClickHouse能在故障后自动恢复，在一些复杂的情况下需要少量的手动恢复。</p>\n<h3 id=\"ClickHouse可以考虑缺点的功能\"><a href=\"#ClickHouse可以考虑缺点的功能\" class=\"headerlink\" title=\"ClickHouse可以考虑缺点的功能\"></a>ClickHouse可以考虑缺点的功能</h3><ol>\n<li>没有完整的事物支持。</li>\n<li>缺少高频率，低延迟的修改或删除已存在数据的能力。仅能用于批量删除或修改数据，但这符合 <a href=\"https://gdpr-info.eu/\" target=\"_blank\" rel=\"noopener\">GDPR</a>。</li>\n<li>稀疏索引使得ClickHouse不适合通过其键检索单行的点查询</li>\n</ol>\n<h1 id=\"ClickHouse性能\"><a href=\"#ClickHouse性能\" class=\"headerlink\" title=\"ClickHouse性能\"></a>ClickHouse性能</h1><p>根据Yandex的内部测试结果，ClickHouse表现出了比同类可比较产品更优的性能。你可以在 <a href=\"https://clickhouse.yandex/benchmark.html\" target=\"_blank\" rel=\"noopener\">这里</a> 查看具体的测试结果。</p>\n<p>许多其他的测试也证实这一点。你可以使用互联网搜索到它们，或者你也可以从 <a href=\"https://clickhouse.yandex/#independent-benchmarks\" target=\"_blank\" rel=\"noopener\">我们收集的部分相关连接</a> 中查看。</p>\n<h2 id=\"单个大查询的吞吐量\"><a href=\"#单个大查询的吞吐量\" class=\"headerlink\" title=\"单个大查询的吞吐量\"></a>单个大查询的吞吐量</h2><p>吞吐量可以使用每秒处理的行数或每秒处理的字节数来衡量。如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以2-10GB／s（未压缩）的速度进行处理（对于简单的查询，速度可以达到30GB／s）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以400MB／s的速度读取数据，并且数据压缩率是3，则数据的处理速度为1.2GB/s。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是1-2亿行每秒。</p>\n<p>对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。</p>\n<h2 id=\"处理短查询的延迟时间\"><a href=\"#处理短查询的延迟时间\" class=\"headerlink\" title=\"处理短查询的延迟时间\"></a>处理短查询的延迟时间</h2><p>如果一个查询使用主键并且没有太多行(几十万)进行处理，并且没有查询太多的列，那么在数据被page cache缓存的情况下，它的延迟应该小于50毫秒(在最佳的情况下应该小于10毫秒)。 否则，延迟取决于数据的查找次数。如果你当前使用的是HDD，在数据没有加载的情况下，查询所需要的延迟可以通过以下公式计算得知： 查找时间（10 ms） <em> 查询的列的数量 </em> 查询的数据块的数量。</p>\n<h2 id=\"处理大量短查询的吞吐量\"><a href=\"#处理大量短查询的吞吐量\" class=\"headerlink\" title=\"处理大量短查询的吞吐量\"></a>处理大量短查询的吞吐量</h2><p>在相同的情况下，ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。因此我们建议每秒最多查询100次。</p>\n<h2 id=\"数据的写入性能\"><a href=\"#数据的写入性能\" class=\"headerlink\" title=\"数据的写入性能\"></a>数据的写入性能</h2><p>我们建议每次写入不少于1000行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为50到200MB/s。如果您写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。</p>\n<h1 id=\"Yandex-Metrica的使用案例\"><a href=\"#Yandex-Metrica的使用案例\" class=\"headerlink\" title=\"Yandex.Metrica的使用案例\"></a>Yandex.Metrica的使用案例</h1><p>ClickHouse最初是为 <a href=\"https://metrica.yandex.com/\" target=\"_blank\" rel=\"noopener\">Yandex.Metrica</a> <a href=\"http://w3techs.com/technologies/overview/traffic_analysis/all\" target=\"_blank\" rel=\"noopener\">世界第二大Web分析平台</a> 而开发的。多年来一直作为该系统的核心组件被该系统持续使用着。目前为止，该系统在ClickHouse中有超过13万亿条记录，并且每天超过200多亿个事件被处理。它允许直接从原始数据中动态查询并生成报告。本文简要介绍了ClickHouse在其早期发展阶段的目标。</p>\n<p>Yandex.Metrica基于用户定义的字段，对实时访问、连接会话，生成实时的统计报表。这种需求往往需要复杂聚合方式，比如对访问用户进行去重。构建报表的数据，是实时接收存储的新数据。</p>\n<p>截至2014年4月，Yandex.Metrica每天跟踪大约120亿个事件（用户的点击和浏览）。为了可以创建自定义的报表，我们必须存储全部这些事件。同时，这些查询可能需要在几百毫秒内扫描数百万行的数据，或在几秒内扫描数亿行的数据。</p>\n<h2 id=\"Yandex-Metrica以及其他Yandex服务的使用案例\"><a href=\"#Yandex-Metrica以及其他Yandex服务的使用案例\" class=\"headerlink\" title=\"Yandex.Metrica以及其他Yandex服务的使用案例\"></a>Yandex.Metrica以及其他Yandex服务的使用案例</h2><p>在Yandex.Metrica中，ClickHouse被用于多个场景中。 它的主要任务是使用原始数据在线的提供各种数据报告。它使用374台服务器的集群，存储了20.3万亿行的数据。在去除重复与副本数据的情况下，压缩后的数据达到了2PB。未压缩前（TSV格式）它大概有17PB。</p>\n<p>ClickHouse还被使用在：</p>\n<ul>\n<li>存储来自Yandex.Metrica回话重放数据。</li>\n<li>处理中间数据</li>\n<li>与Analytics一起构建全球报表。</li>\n<li>为调试Yandex.Metrica引擎运行查询</li>\n<li>分析来自API和用户界面的日志数据</li>\n</ul>\n<p>ClickHouse在其他Yandex服务中至少有12个安装：search verticals, Market, Direct, business analytics, mobile development, AdFox, personal services等。</p>\n<h2 id=\"聚合与非聚合数据\"><a href=\"#聚合与非聚合数据\" class=\"headerlink\" title=\"聚合与非聚合数据\"></a>聚合与非聚合数据</h2><p>有一种流行的观点认为，想要有效的计算统计数据，必须要聚合数据，因为聚合将降低数据量。</p>\n<p>但是数据聚合是一个有诸多限制的解决方案，例如：</p>\n<ul>\n<li>你必须提前知道用户定义的报表的字段列表</li>\n<li>用户无法自定义报表</li>\n<li>当聚合条件过多时，可能不会减少数据，聚合是无用的。</li>\n<li>存在大量报表时，有太多的聚合变化（组合爆炸）</li>\n<li>当聚合条件有非常大的基数时（如：url），数据量没有太大减少（少于两倍）</li>\n<li>聚合的数据量可能会增长而不是收缩</li>\n<li>用户不会查看我们为他生成的所有报告，大部分计算将是无用的</li>\n<li>各种聚合可能违背了数据的逻辑完整性</li>\n</ul>\n<p>如果我们直接使用非聚合数据而不尽兴任何聚合时，我们的计算量可能是减少的。</p>\n<p>然而，相对于聚合中很大一部分工作被离线完成，在线计算需要尽快的完成计算，因为用户在等待结果。</p>\n<p>Yandex.Metrica 有一个专门用于聚合数据的系统，称为Metrage，它可以用作大部分报表。 从2009年开始，Yandex.Metrica还为非聚合数据使用专门的OLAP数据库，称为OLAPServer，它以前用于报表构建系统。 OLAPServer可以很好的工作在非聚合数据上，但是它有诸多限制，导致无法根据需要将其用于所有报表中。如，缺少对数据类型的支持（只支持数据），无法实时增量的更新数据（只能通过每天重写数据完成）。OLAPServer不是一个数据库管理系统，它只是一个数据库。</p>\n<p>为了消除OLAPServer的这些局限性，解决所有报表使用非聚合数据的问题，我们开发了ClickHouse数据库管理系统。</p>\n<h1 id=\"ClickHouse表引擎\"><a href=\"#ClickHouse表引擎\" class=\"headerlink\" title=\"ClickHouse表引擎\"></a>ClickHouse表引擎</h1><p>表引擎（即表的类型）决定了：</p>\n<ul>\n<li>数据的存储方式和位置，写到哪里以及从哪里读取数据</li>\n<li>支持哪些查询以及如何支持。</li>\n<li>并发数据访问。</li>\n<li>索引的使用（如果存在）。</li>\n<li>是否可以执行多线程请求。</li>\n<li>数据复制参数。</li>\n</ul>\n<p>在读取时，引擎只需要输出所请求的列，但在某些情况下，引擎可以在响应请求时部分处理数据。</p>\n<p>对于大多数正式的任务，应该使用MergeTree族中的引擎。</p>\n<h2 id=\"MergeTree\"><a href=\"#MergeTree\" class=\"headerlink\" title=\"MergeTree\"></a>MergeTree</h2><p>Clickhouse 中最强大的表引擎当属 <code>MergeTree</code> （合并树）引擎及该系列（<code>*MergeTree</code>）中的其他引擎。</p>\n<p><code>MergeTree</code> 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。</p>\n<p>主要特点:</p>\n<ul>\n<li><p>存储的数据按主键排序。</p>\n<p>这让你可以创建一个用于快速检索数据的小稀疏索引。</p>\n</li>\n<li><p>允许使用分区，如果指定了 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/custom_partitioning_key/\" target=\"_blank\" rel=\"noopener\">主键</a> 的话。</p>\n<p>在相同数据集和相同结果集的情况下 ClickHouse 中某些带分区的操作会比普通操作更快。查询中指定了分区键时 ClickHouse 会自动截取分区数据。这也有效增加了查询性能。</p>\n</li>\n<li><p>支持数据副本。</p>\n<p><code>ReplicatedMergeTree</code> 系列的表便是用于此。更多信息，请参阅 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/\" target=\"_blank\" rel=\"noopener\">数据副本</a> 一节。</p>\n</li>\n<li><p>支持数据采样。</p>\n<p>需要的话，你可以给表设置一个采样方法。</p>\n</li>\n</ul>\n<p>!!! 注意 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/merge/\" target=\"_blank\" rel=\"noopener\">Merge</a> 引擎并不属于 <code>*MergeTree</code> 系列。</p>\n<h3 id=\"建表\"><a href=\"#建表\" class=\"headerlink\" title=\"建表\"></a>建表</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"keyword\">INDEX</span> index_name1 expr1 <span class=\"keyword\">TYPE</span> type1(...) GRANULARITY value1,</span><br><span class=\"line\">    <span class=\"keyword\">INDEX</span> index_name2 expr2 <span class=\"keyword\">TYPE</span> type2(...) GRANULARITY value2</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = MergeTree()</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[PRIMARY <span class=\"keyword\">KEY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>请求参数的描述，参考 <a href=\"https://clickhouse.yandex/docs/zh/query_language/create/\" target=\"_blank\" rel=\"noopener\">请求描述</a> 。</p>\n<p>示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> xcloud.cdn_nginx_log_minute_agg</span><br><span class=\"line\">(</span><br><span class=\"line\">\t<span class=\"built_in\">date</span> <span class=\"built_in\">Date</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">timeStamp</span> DateTime,</span><br><span class=\"line\"></span><br><span class=\"line\">\tchannel <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tcustomer <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tcountry <span class=\"keyword\">String</span>, </span><br><span class=\"line\"></span><br><span class=\"line\">\tflow AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tvisit AggregateFunction(<span class=\"keyword\">sum</span>, Int64),  </span><br><span class=\"line\"></span><br><span class=\"line\">\tdownload_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tresponse_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tupstream_response_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tfirst_byte_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\trequest_time AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tdownload_flow AggregateFunction(<span class=\"keyword\">sum</span>, Int64), </span><br><span class=\"line\"></span><br><span class=\"line\">\tresponse_normal AggregateFunction(<span class=\"keyword\">sum</span>, Int64)</span><br><span class=\"line\">)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = ReplicatedAggregatingMergeTree(<span class=\"string\">'&#123;zkpath&#125;'</span>, <span class=\"string\">'&#123;replica&#125;'</span>)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> <span class=\"built_in\">date</span>  <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (<span class=\"keyword\">timeStamp</span>, <span class=\"built_in\">date</span>, channel, customer, country);</span><br></pre></td></tr></table></figure>\n<h3 id=\"数据存储\"><a href=\"#数据存储\" class=\"headerlink\" title=\"数据存储\"></a>数据存储</h3><p>表由按主键排序的数据 <em>片段</em> 组成。</p>\n<p>当数据被插入到表中时，会分成数据片段并按主键的字典序排序。例如，主键是 <code>(CounterID, Date)</code> 时，片段中数据按 <code>CounterID</code> 排序，具有相同 <code>CounterID</code> 的部分按 <code>Date</code> 排序。</p>\n<p>不同分区的数据会被分成不同的片段，ClickHouse 在后台合并数据片段以便更高效存储。不会合并来自不同分区的数据片段。这个合并机制并不保证相同主键的所有行都会合并到同一个数据片段中。</p>\n<p>ClickHouse 会为每个数据片段创建一个索引文件，索引文件包含每个索引行（『标记』）的主键值。索引行号定义为 <code>n * index_granularity</code> 。最大的 <code>n</code> 等于总行数除以 <code>index_granularity</code> 的值的整数部分。对于每列，跟主键相同的索引行处也会写入『标记』。这些『标记』让你可以直接找到数据所在的列。</p>\n<p>你可以只用一单一大表并不断地一块块往里面加入数据 – <code>MergeTree</code> 引擎的就是为了这样的场景。</p>\n<h3 id=\"主键和索引在查询中的表现\"><a href=\"#主键和索引在查询中的表现\" class=\"headerlink\" title=\"主键和索引在查询中的表现\"></a>主键和索引在查询中的表现</h3><p>我们以 <code>(CounterID, Date)</code> 以主键。排序好的索引的图示会是下面这样：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">全部数据  :     [-------------------------------------------------------------------------]</span><br><span class=\"line\">CounterID:      [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]</span><br><span class=\"line\">Date:           [1111111222222233331233211111222222333211111112122222223111112223311122333]</span><br><span class=\"line\">标记:            |      |      |      |      |      |      |      |      |      |      |</span><br><span class=\"line\">                a,1    a,2    a,3    b,3    e,2    e,3    g,1    h,2    i,1    i,3    l,3</span><br><span class=\"line\">标记号:          0      1      2      3      4      5      6      7      8      9      10</span><br></pre></td></tr></table></figure>\n<p>如果指定查询如下：</p>\n<ul>\n<li><code>CounterID in (&#39;a&#39;, &#39;h&#39;)</code>，服务器会读取标记号在 <code>[0, 3)</code> 和 <code>[6, 8)</code> 区间中的数据。</li>\n<li><code>CounterID IN (&#39;a&#39;, &#39;h&#39;) AND Date = 3</code>，服务器会读取标记号在 <code>[1, 3)</code> 和 <code>[7, 8)</code> 区间中的数据。</li>\n<li><code>Date = 3</code>，服务器会读取标记号在 <code>[1, 10]</code> 区间中的数据。</li>\n</ul>\n<p>上面例子可以看出使用索引通常会比全表描述要高效。</p>\n<p>稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 <code>index_granularity * 2</code> 行额外的数据。大部分情况下，当 <code>index_granularity = 8192</code> 时，ClickHouse的性能并不会降级。</p>\n<p>稀疏索引让你能操作有巨量行的表。因为这些索引是常驻内存（RAM）的。</p>\n<p>ClickHouse 不要求主键惟一。所以，你可以插入多条具有相同主键的行。</p>\n<h3 id=\"主键的选择\"><a href=\"#主键的选择\" class=\"headerlink\" title=\"主键的选择\"></a>主键的选择</h3><p>主键中列的数量并没有明确的限制。依据数据结构，你应该让主键包含多些或少些列。这样可以：</p>\n<ul>\n<li><p>改善索引的性能。</p>\n<p>如果当前主键是 <code>(a, b)</code> ，然后加入另一个 <code>c</code> 列，满足下面条件时，则可以改善性能： - 有带有 <code>c</code> 列条件的查询。 - 很长的数据范围（ <code>index_granularity</code> 的数倍）里 <code>(a, b)</code> 都是相同的值，并且这种的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。</p>\n</li>\n<li><p>改善数据压缩。</p>\n<p>ClickHouse 以主键排序片段数据，所以，数据的一致性越高，压缩越好。</p>\n</li>\n<li><p><a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/collapsingmergetree/#table_engine-collapsingmergetree\" target=\"_blank\" rel=\"noopener\">CollapsingMergeTree</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/\" target=\"_blank\" rel=\"noopener\">SummingMergeTree</a> 引擎里，数据合并时，会有额外的处理逻辑。</p>\n<p>在这种情况下，指定一个跟主键不同的 <em>排序键</em> 也是有意义的。</p>\n</li>\n</ul>\n<p>长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 <code>SELECT</code> 查询的性能。</p>\n<h3 id=\"选择跟排序键不一样主键\"><a href=\"#选择跟排序键不一样主键\" class=\"headerlink\" title=\"选择跟排序键不一样主键\"></a>选择跟排序键不一样主键</h3><p>指定一个跟排序键（用于排序数据片段中行的表达式） 不一样的主键（用于计算写到索引文件的每个标记值的表达式）是可以的。 这种情况下，主键表达式元组必须是排序键表达式元组的一个前缀。</p>\n<p>当使用 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/summingmergetree/\" target=\"_blank\" rel=\"noopener\">SummingMergeTree</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/aggregatingmergetree/\" target=\"_blank\" rel=\"noopener\">AggregatingMergeTree</a> 引擎时，这个特性非常有用。 通常，使用这类引擎时，表里列分两种：<em>维度</em> 和 <em>度量</em> 。 典型的查询是在 <code>GROUP BY</code> 并过虑维度的情况下统计度量列的值。 像 SummingMergeTree 和 AggregatingMergeTree ，用相同的排序键值统计行时， 通常会加上所有的维度。结果就是，这键的表达式会是一长串的列组成， 并且这组列还会因为新加维度必须频繁更新。</p>\n<p>这种情况下，主键中仅预留少量列保证高效范围扫描， 剩下的维度列放到排序键元组里。这样是合理的。</p>\n<p><a href=\"https://clickhouse.yandex/docs/zh/query_language/alter/\" target=\"_blank\" rel=\"noopener\">排序键的修改</a> 是轻量级的操作，因为一个新列同时被加入到表里和排序键后时，已存在的数据片段并不需要修改。由于旧的排序键是新排序键的前缀，并且刚刚添加的列中没有数据，因此在表修改时的数据对于新旧的排序键来说都是有序的。</p>\n<h3 id=\"索引和分区在查询中的应用\"><a href=\"#索引和分区在查询中的应用\" class=\"headerlink\" title=\"索引和分区在查询中的应用\"></a>索引和分区在查询中的应用</h3><p>对于 <code>SELECT</code> 查询，ClickHouse 分析是否可以使用索引。如果 <code>WHERE/PREWHERE</code> 子句具有下面这些表达式（作为谓词链接一子项或整个）则可以使用索引：基于主键或分区键的列或表达式的部分的等式或比较运算表达式；基于主键或分区键的列或表达式的固定前缀的 <code>IN</code> 或 <code>LIKE</code> 表达式；基于主键或分区键的列的某些函数；基于主键或分区键的表达式的逻辑表达式。</p>\n<p>因此，在索引键的一个或多个区间上快速地跑查询都是可能的。下面例子中，指定标签；指定标签和日期范围；指定标签和日期；指定多个标签和日期范围等运行查询，都会非常快。</p>\n<p>要检查 ClickHouse 执行一个查询时能否使用索引，可设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-force_index_by_date\" target=\"_blank\" rel=\"noopener\">force_index_by_date</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/\" target=\"_blank\" rel=\"noopener\">force_primary_key</a> 。</p>\n<p>按月分区的分区键是只能读取包含适当范围日期的数据块。这种情况下，数据块会包含很多天（最多整月）的数据。在块中，数据按主键排序，主键第一列可能不包含日期。因此，仅使用日期而没有带主键前缀条件的查询将会导致读取超过这个日期范围。</p>\n<h3 id=\"并发数据访问\"><a href=\"#并发数据访问\" class=\"headerlink\" title=\"并发数据访问\"></a>并发数据访问</h3><p>应对表的并发访问，我们使用多版本机制。换言之，当同时读和更新表时，数据从当前查询到的一组片段中读取。没有冗长的的锁。插入不会阻碍读取。</p>\n<p>对表的读操作是自动并行的。</p>\n<h2 id=\"数据副本\"><a href=\"#数据副本\" class=\"headerlink\" title=\"数据副本\"></a>数据副本</h2><p>只有 MergeTree 系列里的表可支持副本：</p>\n<ul>\n<li>ReplicatedMergeTree</li>\n<li>ReplicatedSummingMergeTree</li>\n<li>ReplicatedReplacingMergeTree</li>\n<li>ReplicatedAggregatingMergeTree</li>\n<li>ReplicatedCollapsingMergeTree</li>\n<li>ReplicatedVersionedCollapsingMergeTree</li>\n<li>ReplicatedGraphiteMergeTree</li>\n</ul>\n<p>副本是表级别的，不是整个服务器级的。所以，服务器里可以同时有复制表和非复制表。</p>\n<p>副本不依赖分片。每个分片有它自己的独立副本。</p>\n<p>对于 <code>INSERT</code> 和 <code>ALTER</code> 语句操作数据的会在压缩的情况下被复制（更多信息，看 <a href=\"https://clickhouse.yandex/docs/zh/query_language/alter/#query_language_queries_alter\" target=\"_blank\" rel=\"noopener\">ALTER</a> ）。</p>\n<p>而 <code>CREATE</code>，<code>DROP</code>，<code>ATTACH</code>，<code>DETACH</code> 和 <code>RENAME</code> 语句只会在单个服务器上执行，不会被复制。</p>\n<ul>\n<li><code>The CREATE TABLE</code> 在运行此语句的服务器上创建一个新的可复制表。如果此表已存在其他服务器上，则给该表添加新副本。</li>\n<li><code>The DROP TABLE</code> 删除运行此查询的服务器上的副本。</li>\n<li><code>The RENAME</code> 重命名一个副本。换句话说，可复制表不同的副本可以有不同的名称。</li>\n</ul>\n<p>要使用副本，需在配置文件中设置 ZooKeeper 集群的地址。</p>\n<p><code>SELECT</code> 查询并不需要借助 ZooKeeper ，复本并不影响 <code>SELECT</code> 的性能，查询复制表与非复制表速度是一样的。查询分布式表时，ClickHouse的处理方式可通过设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries\" target=\"_blank\" rel=\"noopener\">max_replica_delay_for_distributed_queries</a> 和 <a href=\"https://clickhouse.yandex/docs/zh/operations/settings/settings/\" target=\"_blank\" rel=\"noopener\">fallback_to_stale_replicas_for_distributed_queries</a> 修改。</p>\n<p>对于每个 <code>INSERT</code> 语句，会通过几个事务将十来个记录添加到 ZooKeeper。（确切地说，这是针对每个插入的数据块; 每个 INSERT 语句的每 <code>max_insert_block_size = 1048576</code> 行和最后剩余的都各算作一个块。）相比非复制表，写 zk 会导致 <code>INSERT</code> 的延迟略长一些。但只要你按照建议每秒不超过一个 <code>INSERT</code> 地批量插入数据，不会有任何问题。一个 ZooKeeper 集群能给整个 ClickHouse 集群支撑协调每秒几百个 <code>INSERT</code>。数据插入的吞吐量（每秒的行数）可以跟不用复制的数据一样高。</p>\n<p>对于非常大的集群，你可以把不同的 ZooKeeper 集群用于不同的分片。然而，即使 Yandex.Metrica 集群（大约300台服务器）也证明还不需要这么做。</p>\n<p>复制是多主异步。 <code>INSERT</code> 语句（以及 <code>ALTER</code> ）可以发给任意可用的服务器。数据会先插入到执行该语句的服务器上，然后被复制到其他服务器。由于它是异步的，在其他副本上最近插入的数据会有一些延迟。如果部分副本不可用，则数据在其可用时再写入。副本可用的情况下，则延迟时长是通过网络传输压缩数据块所需的时间。</p>\n<p>默认情况下，INSERT 语句仅等待一个副本写入成功后返回。如果数据只成功写入一个副本后该副本所在的服务器不再存在，则存储的数据会丢失。要启用数据写入多个副本才确认返回，使用 <code>insert_quorum</code> 选项。</p>\n<p>单个数据块写入是原子的。 INSERT 的数据按每块最多 <code>max_insert_block_size = 1048576</code> 行进行分块，换句话说，如果 <code>INSERT</code> 插入的行少于 1048576，则该 INSERT 是原子的。</p>\n<p>数据块会去重。对于被多次写的相同数据块（大小相同且具有相同顺序的相同行的数据块），该块仅会写入一次。这样设计的原因是万一在网络故障时客户端应用程序不知道数据是否成功写入DB，此时可以简单地重复 <code>INSERT</code> 。把相同的数据发送给多个副本 INSERT 并不会有问题。因为这些 <code>INSERT</code> 是完全相同的（会被去重）。去重参数参看服务器设置 <a href=\"https://clickhouse.yandex/docs/zh/operations/server_settings/settings/\" target=\"_blank\" rel=\"noopener\">merge_tree</a> 。（注意：Replicated*MergeTree 才会去重，不需要 zookeeper 的不带 MergeTree 不会去重）</p>\n<p>在复制期间，只有要插入的源数据通过网络传输。进一步的数据转换（合并）会在所有副本上以相同的方式进行处理执行。这样可以最大限度地减少网络使用，这意味着即使副本在不同的数据中心，数据同步也能工作良好。（能在不同数据中心中的同步数据是副本机制的主要目标。）</p>\n<p>你可以给数据做任意多的副本。Yandex.Metrica 在生产中使用双副本。某一些情况下，给每台服务器都使用 RAID-5 或 RAID-6 和 RAID-10。是一种相对可靠和方便的解决方案。</p>\n<p>系统会监视副本数据同步情况，并能在发生故障后恢复。故障转移是自动的（对于小的数据差异）或半自动的（当数据差异很大时，这可能意味是有配置错误）。</p>\n<h3 id=\"创建复制表\"><a href=\"#创建复制表\" class=\"headerlink\" title=\"创建复制表\"></a>创建复制表</h3><p>在表引擎名称上加上 <code>Replicated</code> 前缀。例如：<code>ReplicatedMergeTree</code>。</p>\n<p><strong>Replicated*MergeTree 参数</strong></p>\n<ul>\n<li><code>zoo_path</code> — ZooKeeper 中该表的路径。</li>\n<li><code>replica_name</code> — ZooKeeper 中的该表的副本名称。</li>\n</ul>\n<p>示例:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> table_name</span><br><span class=\"line\">(</span><br><span class=\"line\">    EventDate DateTime,</span><br><span class=\"line\">    CounterID UInt32,</span><br><span class=\"line\">    UserID UInt32</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = ReplicatedMergeTree(<span class=\"string\">'/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_name'</span>, <span class=\"string\">'&#123;replica&#125;'</span>)</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(EventDate)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (CounterID, EventDate, intHash32(UserID))</span><br><span class=\"line\"><span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> intHash32(UserID)</span><br></pre></td></tr></table></figure>\n<p>如上例所示，这些参数可以包含宏替换的占位符，即大括号的部分。它们会被替换为配置文件里 ‘macros’ 那部分配置的值。示例：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">macros</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">layer</span>&gt;</span>05<span class=\"tag\">&lt;/<span class=\"name\">layer</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span>02<span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span>example05-02-1.yandex.ru<span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">macros</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>“ZooKeeper 中该表的路径”对每个可复制表都要是唯一的。不同分片上的表要有不同的路径。 这种情况下，路径包含下面这些部分：</p>\n<p><code>/clickhouse/tables/</code> 是公共前缀，我们推荐使用这个。</p>\n<p><code>{layer}-{shard}</code> 是分片标识部分。在此示例中，由于 Yandex.Metrica 集群使用了两级分片，所以它是由两部分组成的。但对于大多数情况来说，你只需保留 {shard} 占位符即可，它会替换展开为分片标识。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">table_name` 是该表在 ZooKeeper 中的名称。使其与 ClickHouse 中的表名相同比较好。 这里它被明确定义，跟 ClickHouse 表名不一样，它并不会被 RENAME 语句修改。</span><br><span class=\"line\">*HINT*: you could add a database name in front of `table_name` as well. E.g. `db_name.table_name</span><br></pre></td></tr></table></figure>\n<p>副本名称用于标识同一个表分片的不同副本。你可以使用服务器名称，如上例所示。同个分片中不同副本的副本名称要唯一。</p>\n<p>你也可以显式指定这些参数，而不是使用宏替换。对于测试和配置小型集群这可能会很方便。但是，这种情况下，则不能使用分布式 DDL 语句（<code>ON CLUSTER</code>）。</p>\n<p>使用大型集群时，我们建议使用宏替换，因为它可以降低出错的可能性。</p>\n<p>在每个副本服务器上运行 <code>CREATE TABLE</code> 查询。将创建新的复制表，或给现有表添加新副本。</p>\n<p>如果其他副本上已包含了某些数据，在表上添加新副本，则在运行语句后，数据会从其他副本复制到新副本。换句话说，新副本会与其他副本同步。</p>\n<p>要删除副本，使用 <code>DROP TABLE</code>。但它只删除那个 – 位于运行该语句的服务器上的副本。</p>\n<h3 id=\"故障恢复\"><a href=\"#故障恢复\" class=\"headerlink\" title=\"故障恢复\"></a>故障恢复</h3><p>如果服务器启动时 ZooKeeper 不可用，则复制表会切换为只读模式。系统会定期尝试去连接 ZooKeeper。</p>\n<p>如果在 <code>INSERT</code> 期间 ZooKeeper 不可用，或者在与 ZooKeeper 交互时发生错误，则抛出异常。</p>\n<p>连接到 ZooKeeper 后，系统会检查本地文件系统中的数据集是否与预期的数据集（ ZooKeeper 存储此信息）一致。如果存在轻微的不一致，系统会通过与副本同步数据来解决。</p>\n<p>如果系统检测到损坏的数据片段（文件大小错误）或无法识别的片段（写入文件系统但未记录在 ZooKeeper 中的部分），则会把它们移动到 ‘detached’ 子目录（不会删除）。而副本中其他任何缺少的但正常数据片段都会被复制同步。</p>\n<p>注意，ClickHouse 不会执行任何破坏性操作，例如自动删除大量数据。</p>\n<p>当服务器启动（或与 ZooKeeper 建立新会话）时，它只检查所有文件的数量和大小。 如果文件大小一致但中间某处已有字节被修改过，不会立即被检测到，只有在尝试读取 <code>SELECT</code> 查询的数据时才会检测到。该查询会引发校验和不匹配或压缩块大小不一致的异常。这种情况下，数据片段会添加到验证队列中，并在必要时从其他副本中复制。</p>\n<p>如果本地数据集与预期数据的差异太大，则会触发安全机制。服务器在日志中记录此内容并拒绝启动。这种情况很可能是配置错误，例如，一个分片上的副本意外配置为别的分片上的副本。然而，此机制的阈值设置得相当低，在正常故障恢复期间可能会出现这种情况。在这种情况下，数据恢复则是半自动模式，通过用户主动操作触发。</p>\n<p>要触发启动恢复，可在 ZooKeeper 中创建节点 <code>/path_to_table/replica_name/flags/force_restore_data</code>，节点值可以是任何内容，或运行命令来恢复所有的可复制表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data</span><br></pre></td></tr></table></figure>\n<p>然后重启服务器。启动时，服务器会删除这些标志并开始恢复。</p>\n<h3 id=\"在数据完全丢失后的恢复\"><a href=\"#在数据完全丢失后的恢复\" class=\"headerlink\" title=\"在数据完全丢失后的恢复\"></a>在数据完全丢失后的恢复</h3><p>如果其中一个服务器的所有数据和元数据都消失了，请按照以下步骤进行恢复：</p>\n<ol>\n<li>在服务器上安装 ClickHouse。在包含分片标识符和副本的配置文件中正确定义宏配置，如果有用到的话，</li>\n<li>如果服务器上有非复制表则必须手动复制，可以从副本服务器上（在 <code>/var/lib/clickhouse/data/db_name/table_name/</code> 目录中）复制它们的数据。</li>\n<li>从副本服务器上中复制位于 <code>/var/lib/clickhouse/metadata/</code> 中的表定义信息。如果在表定义信息中显式指定了分片或副本标识符，请更正它以使其对应于该副本。（另外，启动服务器，然后会在 <code>/var/lib/clickhouse/metadata/</code> 中的.sql文件中生成所有的 <code>ATTACH TABLE</code> 语句。） 4.要开始恢复，ZooKeeper 中创建节点 <code>/path_to_table/replica_name/flags/force_restore_data</code>，节点内容不限，或运行命令来恢复所有复制的表：<code>sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data</code></li>\n</ol>\n<p>然后启动服务器（如果它已运行则重启）。数据会从副本中下载。</p>\n<p>另一种恢复方式是从 ZooKeeper（<code>/path_to_table/replica_name</code>）中删除有数据丢的副本的所有元信息，然后再按照“<a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/#creating-replicated-tables\" target=\"_blank\" rel=\"noopener\">创建可复制表</a>”中的描述重新创建副本。</p>\n<p>恢复期间的网络带宽没有限制。特别注意这一点，尤其是要一次恢复很多副本。</p>\n<h2 id=\"自定义分区键\"><a href=\"#自定义分区键\" class=\"headerlink\" title=\"自定义分区键\"></a>自定义分区键</h2><p><a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a> 系列的表（包括 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/replication/\" target=\"_blank\" rel=\"noopener\">可复制表</a> ）可以使用分区。基于 MergeTree 表的 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/materializedview/\" target=\"_blank\" rel=\"noopener\">物化视图</a> 也支持分区。</p>\n<p>一个分区是指按指定规则逻辑组合一起的表的记录集。可以按任意标准进行分区，如按月，按日或按事件类型。为了减少需要操作的数据，每个分区都是分开存储的。访问数据时，ClickHouse 尽量使用这些分区的最小子集。</p>\n<p>分区是在 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#table_engine-mergetree-creating-a-table\" target=\"_blank\" rel=\"noopener\">建表</a> 的 <code>PARTITION BY expr</code> 子句中指定。分区键可以是关于列的任何表达式。例如，指定按月分区，表达式为 <code>toYYYYMM(date_column)</code>：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> visits</span><br><span class=\"line\">(</span><br><span class=\"line\">    VisitDate <span class=\"built_in\">Date</span>, </span><br><span class=\"line\">    <span class=\"keyword\">Hour</span> UInt8, </span><br><span class=\"line\">    ClientID <span class=\"keyword\">UUID</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = MergeTree()</span><br><span class=\"line\"><span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(VisitDate)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">Hour</span>;</span><br></pre></td></tr></table></figure>\n<p>分区键也可以是表达式元组（类似 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/#primary-keys-and-indexes-in-queries\" target=\"_blank\" rel=\"noopener\">主键</a> ）。例如：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign)</span><br><span class=\"line\">PARTITION BY (toMonday(StartDate), EventType)</span><br><span class=\"line\">ORDER BY (CounterID, StartDate, intHash32(UserID));</span><br></pre></td></tr></table></figure>\n<p>上例中，我们设置按一周内的事件类型分区。</p>\n<p>新数据插入到表中时，这些数据会存储为按主键排序的新片段（块）。插入后 10-15 分钟，同一分区的各个片段会合并为一整个片段。</p>\n<p><strong>那些有相同分区表达式值的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量和需要找开的文件描述符过多，导致 <code>SELECT</code> 查询效率不佳。</strong></p>\n<h2 id=\"ReplacingMergeTree-引擎\"><a href=\"#ReplacingMergeTree-引擎\" class=\"headerlink\" title=\"ReplacingMergeTree 引擎\"></a>ReplacingMergeTree 引擎</h2><p>该引擎和<a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>的不同之处在于它会删除具有相同主键的重复项。</p>\n<p>数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，因此你无法预先作出计划。有一些数据可能仍未被处理。尽管你可以调用 <code>OPTIMIZE</code> 语句发起计划外的合并，但请不要指望使用它，因为 <code>OPTIMIZE</code> 语句会引发对大量数据的读和写。</p>\n<p>因此，<code>ReplacingMergeTree</code> 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。</p>\n<p>建表:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = ReplacingMergeTree([ver])</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<h2 id=\"SummingMergeTree-引擎\"><a href=\"#SummingMergeTree-引擎\" class=\"headerlink\" title=\"SummingMergeTree 引擎\"></a>SummingMergeTree 引擎</h2><p>该引擎继承自 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>。区别在于，当合并 <code>SummingMergeTree</code> 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度。</p>\n<p>我们推荐将该引擎和 <code>MergeTree</code> 一起使用。例如，在准备做报告的时候，将完整的数据存储在 <code>MergeTree</code> 表中，并且使用 <code>SummingMergeTree</code> 来存储聚合数据。这种方法可以使你避免因为使用不正确的主键组合方式而丢失有价值的数据。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = SummingMergeTree([<span class=\"keyword\">columns</span>])</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>当数据被插入到表中时，他们将被原样保存。ClickHouse 定期合并插入的数据片段，并在这个时候对所有具有相同主键的行中的列进行汇总，将这些行替换为包含汇总数据的一行记录。</p>\n<p>ClickHouse 会按片段合并数据，以至于不同的数据片段中会包含具有相同主键的行，即单个汇总片段将会是不完整的。因此，聚合函数 <a href=\"https://clickhouse.yandex/docs/zh/query_language/agg_functions/reference/#agg_function-sum\" target=\"_blank\" rel=\"noopener\">sum()</a> 和 <code>GROUP BY</code> 子句应该在（<code>SELECT</code>）查询语句中被使用，如上文中的例子所述。</p>\n<p>列中数值类型的值会被汇总。这些列的集合在参数 <code>columns</code> 中被定义。</p>\n<p>如果用于汇总的所有列中的值均为0，则该行会被删除。</p>\n<p>如果列不在主键中且无法被汇总，则会在现有的值中任选一个。</p>\n<p>主键所在的列中的值不会被汇总。</p>\n<h2 id=\"AggregatingMergeTree-引擎\"><a href=\"#AggregatingMergeTree-引擎\" class=\"headerlink\" title=\"AggregatingMergeTree 引擎\"></a>AggregatingMergeTree 引擎</h2><p>该引擎继承自 <a href=\"https://clickhouse.yandex/docs/zh/operations/table_engines/mergetree/\" target=\"_blank\" rel=\"noopener\">MergeTree</a>，并改变了数据片段的合并逻辑。 ClickHouse 会将相同主键的所有行（在一个数据片段内）替换为单个存储一系列聚合函数状态的行。</p>\n<p>可以使用 <code>AggregatingMergeTree</code> 表来做增量数据统计聚合，包括物化视图的数据聚合。</p>\n<p>引擎需使用 <a href=\"https://clickhouse.yandex/docs/zh/data_types/nested_data_structures/aggregatefunction/\" target=\"_blank\" rel=\"noopener\">AggregateFunction</a> 类型来处理所有列。</p>\n<p>如果要按一组规则来合并减少行数，则使用 <code>AggregatingMergeTree</code> 是合适的。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> [<span class=\"keyword\">IF</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">EXISTS</span>] [db.]table_name [<span class=\"keyword\">ON</span> CLUSTER cluster]</span><br><span class=\"line\">(</span><br><span class=\"line\">    name1 [type1] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr1],</span><br><span class=\"line\">    name2 [type2] [<span class=\"keyword\">DEFAULT</span>|<span class=\"keyword\">MATERIALIZED</span>|<span class=\"keyword\">ALIAS</span> expr2],</span><br><span class=\"line\">    ...</span><br><span class=\"line\">) <span class=\"keyword\">ENGINE</span> = AggregatingMergeTree()</span><br><span class=\"line\">[<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SAMPLE</span> <span class=\"keyword\">BY</span> expr]</span><br><span class=\"line\">[<span class=\"keyword\">SETTINGS</span> <span class=\"keyword\">name</span>=<span class=\"keyword\">value</span>, ...]</span><br></pre></td></tr></table></figure>\n<p>插入数据，需使用带有聚合 -State- 函数的 <a href=\"https://clickhouse.yandex/docs/zh/query_language/insert_into/\" target=\"_blank\" rel=\"noopener\">INSERT SELECT</a> 语句。 从 <code>AggregatingMergeTree</code> 表中查询数据时，需使用 <code>GROUP BY</code> 子句并且要使用与插入时相同的聚合函数，但后缀要改为 <code>-Merge</code> 。</p>\n<p>在 <code>SELECT</code> 查询的结果中，对于 ClickHouse 的所有输出格式 <code>AggregateFunction</code> 类型的值都实现了特定的二进制表示法。如果直接用 <code>SELECT</code> 导出这些数据，例如如用 <code>TabSeparated</code> 格式，那么这些导出数据也能直接用 <code>INSERT</code> 语句加载导入</p>\n<p>聚合物化视图的示例, 创建一个跟踪 <code>test.visits</code> 表的 <code>AggregatingMergeTree</code> 物化视图：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">MATERIALIZED</span> <span class=\"keyword\">VIEW</span> test.basic</span><br><span class=\"line\"><span class=\"keyword\">ENGINE</span> = AggregatingMergeTree() <span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> toYYYYMM(StartDate) <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> (CounterID, StartDate)</span><br><span class=\"line\"><span class=\"keyword\">AS</span> <span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    CounterID,</span><br><span class=\"line\">    StartDate,</span><br><span class=\"line\">    sumState(<span class=\"keyword\">Sign</span>)    <span class=\"keyword\">AS</span> Visits,</span><br><span class=\"line\">    uniqState(UserID) <span class=\"keyword\">AS</span> <span class=\"keyword\">Users</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> test.visits</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> CounterID, StartDate;</span><br></pre></td></tr></table></figure>\n<p>向 <code>test.visits</code> 表中插入数据。数据会同时插入到表和视图中，并且视图 <code>test.basic</code> 会将里面的数据聚合。</p>\n<p>要获取聚合数据，我们需要在 <code>test.basic</code> 视图上执行类似 <code>SELECT ... GROUP BY ...</code> 这样的查询 ：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    StartDate,</span><br><span class=\"line\">    sumMerge(Visits) <span class=\"keyword\">AS</span> Visits,</span><br><span class=\"line\">    uniqMerge(<span class=\"keyword\">Users</span>) <span class=\"keyword\">AS</span> <span class=\"keyword\">Users</span></span><br><span class=\"line\"><span class=\"keyword\">FROM</span> test.basic</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> StartDate</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> StartDate;</span><br></pre></td></tr></table></figure>\n<h1 id=\"分布式引擎\"><a href=\"#分布式引擎\" class=\"headerlink\" title=\"分布式引擎\"></a>分布式引擎</h1><p><strong>分布式引擎本身不存储数据</strong>, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 分布式引擎参数：服务器配置文件中的集群名，远程数据库名，远程表名，数据分片键（可选）。 示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Distributed(logs, default, hits[, sharding_key])</span><br></pre></td></tr></table></figure>\n<p>将会从位于“logs”集群中 default.hits 表所有服务器上读取数据。 远程服务器不仅用于读取数据，还会对尽可能数据做部分处理。 例如，对于使用 GROUP BY 的查询，数据首先在远程服务器聚合，之后返回聚合函数的中间状态给查询请求的服务器。再在请求的服务器上进一步汇总数据。</p>\n<p>数据库名参数除了用数据库名之外，也可用返回字符串的常量表达式。例如：currentDatabase()。</p>\n<p>集群示例配置如下：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">remote_servers</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">logs</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">            <span class=\"comment\">&lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">            <span class=\"comment\">&lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-01-1<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-01-2<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">weight</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">weight</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">internal_replication</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">internal_replication</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-02-1<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9000<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">host</span>&gt;</span>example01-02-2<span class=\"tag\">&lt;/<span class=\"name\">host</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">secure</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">secure</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">port</span>&gt;</span>9440<span class=\"tag\">&lt;/<span class=\"name\">port</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">replica</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">shard</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">logs</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">remote_servers</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>这里定义了一个名为‘logs’的集群，它由两个分片组成，每个分片包含两个副本。 分片是指包含数据不同部分的服务器（要读取所有数据，必须访问所有分片）。 副本是存储复制数据的服务器（要读取所有数据，访问任一副本上的数据即可）。</p>\n<p>每个服务器需要指定 <code>host</code>，<code>port</code>，和可选的 <code>user</code>，<code>password</code>，<code>secure</code>，<code>compression</code> 的参数：</p>\n<ul>\n<li><code>host</code> – 远程服务器地址。可以域名、IPv4或IPv6。如果指定域名，则服务在启动时发起一个 DNS 请求，并且请求结果会在服务器运行期间一直被记录。如果 DNS 请求失败，则服务不会启动。如果你修改了 DNS 记录，则需要重启服务。</li>\n<li><code>port</code> – 消息传递的 TCP 端口（「tcp_port」配置通常设为 9000）。不要跟 http_port 混淆。</li>\n<li><code>user</code> – 用于连接远程服务器的用户名。默认值：default。该用户必须有权限访问该远程服务器。访问权限配置在 users.xml 文件中。更多信息，请查看“访问权限”部分。</li>\n<li><code>password</code> – 用于连接远程服务器的密码。默认值：空字符串。</li>\n<li><code>secure</code> – 是否使用ssl进行连接，设为true时，通常也应该设置 <code>port</code> = 9440。服务器也要监听 9440 并有正确的证书。</li>\n<li><code>compression</code> - 是否使用数据压缩。默认值：true。</li>\n</ul>\n<p>配置了副本，读取操作会从每个分片里选择一个可用的副本。可配置负载平衡算法（挑选副本的方式） - 请参阅“load_balancing”设置。 如果跟服务器的连接不可用，则在尝试短超时的重连。如果重连失败，则选择下一个副本，依此类推。如果跟所有副本的连接尝试都失败，则尝试用相同的方式再重复几次。 该机制有利于系统可用性，但不保证完全容错：如有远程服务器能够接受连接，但无法正常工作或状况不佳。</p>\n<p>你可以配置一个（这种情况下，查询操作更应该称为远程查询，而不是分布式查询）或任意多个分片。在每个分片中，可以配置一个或任意多个副本。不同分片可配置不同数量的副本。</p>\n<p>可以在配置中配置任意数量的集群。</p>\n<p>要查看集群，可使用“system.clusters”表。</p>\n<p>通过分布式引擎可以像使用本地服务器一样使用集群。但是，集群不是自动扩展的：你必须编写集群配置到服务器配置文件中（最好，给所有集群的服务器写上完整配置）。</p>\n<p>不支持用分布式表查询别的分布式表（除非该表只有一个分片）。或者说，要用分布表查查询“最终”的数据表。</p>\n<p>分布式引擎需要将集群信息写入配置文件。配置文件中的集群信息会即时更新，无需重启服务器。如果你每次是要向不确定的一组分片和副本发送查询，则不适合创建分布式表 - 而应该使用“远程”表函数。 请参阅“表函数”部分。</p>\n<p>向集群写数据的方法有两种：</p>\n<p>一，自已指定要将哪些数据写入哪些服务器，并直接在每个分片上执行写入。换句话说，在分布式表上“查询”，在数据表上 INSERT。 这是最灵活的解决方案 – 你可以使用任何分片方案，对于复杂业务特性的需求，这可能是非常重要的。 这也是最佳解决方案，因为数据可以完全独立地写入不同的分片。</p>\n<p>二，在分布式表上执行 INSERT。在这种情况下，分布式表会跨服务器分发插入数据。 为了写入分布式表，必须要配置分片键（最后一个参数）。当然，如果只有一个分片，则写操作在没有分片键的情况下也能工作，因为这种情况下分片键没有意义。</p>\n<p>每个分片都可以在配置文件中定义权重。默认情况下，权重等于1。数据依据分片权重按比例分发到分片上。例如，如果有两个分片，第一个分片的权重是9，而第二个分片的权重是10，则发送 9 / 19 的行到第一个分片， 10 / 19 的行到第二个分片。</p>\n<p>分片可在配置文件中定义 ‘internal_replication’ 参数。</p>\n<p>此参数设置为“true”时，写操作只选一个正常的副本写入数据。如果分布式表的子表是复制表(*ReplicaMergeTree)，请使用此方案。换句话说，这其实是把数据的复制工作交给实际需要写入数据的表本身而不是分布式表。</p>\n<p>若此参数设置为“false”（默认值），写操作会将数据写入所有副本。实质上，这意味着要分布式表本身来复制数据。这种方式不如使用复制表的好，因为不会检查副本的一致性，并且随着时间的推移，副本数据可能会有些不一样。</p>\n<p>选择将一行数据发送到哪个分片的方法是，首先计算分片表达式，然后将这个计算结果除以所有分片的权重总和得到余数。该行会发送到那个包含该余数的从’prev_weight’到’prev_weights + weight’的半闭半开区间对应的分片上，其中 ‘prev_weights’ 是该分片前面的所有分片的权重和，’weight’ 是该分片的权重。例如，如果有两个分片，第一个分片权重为9，而第二个分片权重为10，则余数在 [0,9) 中的行发给第一个分片，余数在 [9,19) 中的行发给第二个分片。</p>\n<p>分片表达式可以是由常量和表列组成的任何返回整数表达式。例如，您可以使用表达式 ‘rand()’ 来随机分配数据，或者使用 ‘UserID’ 来按用户 ID 的余数分布（相同用户的数据将分配到单个分片上，这可降低带有用户信息的 IN 和 JOIN 的语句运行的复杂度）。如果该列数据分布不够均匀，可以将其包装在散列函数中：intHash64(UserID)。</p>\n<p>这种简单的用余数来选择分片的方案是有局限的，并不总适用。它适用于中型和大型数据（数十台服务器）的场景，但不适用于巨量数据（数百台或更多服务器）的场景。后一种情况下，应根据业务特性需求考虑的分片方案，而不是直接用分布式表的多分片。</p>\n<p>SELECT 查询会被发送到所有分片，并且无论数据在分片中如何分布（即使数据完全随机分布）都可正常工作。添加新分片时，不必将旧数据传输到该分片。你可以给新分片分配大权重然后写新数据 - 数据可能会稍分布不均，但查询会正确高效地运行。</p>\n<p>下面的情况，你需要关注分片方案：</p>\n<ul>\n<li>使用需要特定键连接数据（ IN 或 JOIN ）的查询。如果数据是用该键进行分片，则应使用本地 IN 或 JOIN 而不是 GLOBAL IN 或 GLOBAL JOIN，这样效率更高。</li>\n<li>使用大量服务器（上百或更多），但有大量小查询（个别客户的查询 - 网站，广告商或合作伙伴）。为了使小查询不影响整个集群，让单个客户的数据处于单个分片上是有意义的。或者，正如我们在 Yandex.Metrica 中所做的那样，你可以配置两级分片：将整个集群划分为“层”，一个层可以包含多个分片。单个客户的数据位于单个层上，根据需要将分片添加到层中，层中的数据随机分布。然后给每层创建分布式表，再创建一个全局的分布式表用于全局的查询。</li>\n</ul>\n<p>数据是异步写入的。对于分布式表的 INSERT，数据块只写本地文件系统。之后会尽快地在后台发送到远程服务器。你可以通过查看表目录中的文件列表（等待发送的数据）来检查数据是否成功发送：/var/lib/clickhouse/data/database/table/ 。</p>\n<p>如果在 INSERT 到分布式表时服务器节点丢失或重启（如，设备故障），则插入的数据可能会丢失。如果在表目录中检测到损坏的数据分片，则会将其转移到“broken”子目录，并不再使用。</p>\n<p>启用 max_parallel_replicas 选项后，会在分表的所有副本上并行查询处理。更多信息，请参阅“设置，max_parallel_replicas”部分。</p>"},{"title":"Raft 一致性算法论文译文","date":"2019-04-05T12:04:50.000Z","_content":"\n# Raft 一致性算法论文译文\n\nRaft论文翻译转载自[Raft 一致性算法论文译文](<http://blog.luoyuanhang.com/2017/02/02/raft-paper-in-zh-CN/>)\n\n# 摘要\n\nRaft 是一种用来管理日志复制的一致性算法。它和 Paxos 的性能和功能是一样的，但是它和 Paxos 的结构不一样；这使得 Raft 更容易理解并且更易于建立实际的系统。为了提高理解性，Raft 将一致性算法分为了几个部分，例如领导选取（leader selection），日志复制（log replication）和安全性（safety），同时它使用了更强的一致性来减少了必须需要考虑的状态。从用户学习的结果来看，Raft 比 Paxos 更容易学会。Raft 还包括了一种新的机制来使得动态改变集群成员，它使用重叠大多数（overlapping majorities）来保证安全。\n\n# 1 引言\n\n一致性算法允许一组机器像一个整体一样工作，即使其中的一些机器出了错误也能正常工作。正因为此，他们扮演着建立大规模可靠的软件系统的关键角色。在过去的十年中 Paxos 一直都主导着有关一致性算法的讨论：大多数一致性算法的实现都基于它或者受它影响，并且 Paxos 也成为了教学生关于一致性知识的主要工具。\n\n不幸的是，尽管在降低它的复杂性方面做了许多努力，Paxos 依旧很难理解。并且，Paxos 需要经过复杂的修改才能应用于实际中。这些导致了系统构构建者和学生都十分头疼。\n\n在被 Paxos 折磨之后，我们开始寻找一种在系统构建和教学上更好的新的一致性算法。我们的首要目标是让它易于理解：我们能不能定义一种面向实际系统的一致性算法并且比 Paxos 更容易学习呢？并且，我们希望这种算法能凭直觉就能明白，这对于一个系统构建者来说是十分必要的。对于一个算法，不仅仅是让它工作起来很重要，知道它是如何工作的更重要。\n\n我们工作的结果是一种新的一致性算法，叫做 Raft。在设计 Raft 的过程中我们应用了许多专门的技巧来提升理解性，包括算法分解（分为领导选取（leader selection），日志复制（log replication）和安全性（safety））和减少状态（state space reduction）（相对于 Paxos，Raft 减少了非确定性的程度和服务器互相不一致的方式）。在两所学校的43个学生的研究中发现，Raft 比 Paxos 要更容易理解：在学习了两种算法之后，其中的33个学生回答 Raft 的问题要比回答 Paxos 的问题要好。\n\nRaft 算法和现在一些已经有的算法在一些地方很相似（主要是 [Oki 和 Liskov 的 Viewstamped Replication](http://www.pmg.csail.mit.edu/papers/vr.pdf)。但是 Raft 有几个新的特性：\n\n- 强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。\n- 领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。\n- 成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。\n\n我们认为，Raft 在教学方面和实际实现方面比 Paxos 和其他算法更出众。它比其他算法更简单、更容易理解；它能满足一个实际系统的需求；它拥有许多开源的实现并且被许多公司所使用；它的安全特性已经被证明；并且它的效率和其他算法相比也具有竞争力。\n\n这篇论文剩下的部分会讲如下内容：复制状态机（replicated state machine）问题（第2节），讨论 Paxos 的优缺点（第3节），讨论我们用的为了达到提升理解性的方法（第4节），陈述 Raft 一致性算法（第5~8节），评价 Raft 算法（第9节），对相关工作的讨论（第10节）。\n\n<!--more-->\n\n# 2 复制状态机（Replicated State Machine）\n\n一致性算法是在[复制状态机](https://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf)的背景下提出来的。在这个方法中，在一组服务器的状态机产生同样的状态的副本因此即使有一些服务器崩溃了这组服务器也还能继续执行。复制状态机在分布式系统中被用于解决许多有关容错的问题。例如，GFS，HDFS还有 RAMCloud 这些大规模的系统都是用一个单独的集群领导者，使用一个单独的复制状态机来进行领导选取和存储配置信息来应对领导者的崩溃。使用复制状态机的例子有 Chubby 和 ZooKeeper。\n\n\n\n\n\n[![img](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg)](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg)\n\n图-1：复制状态机的架构。一致性算法管理来自客户端状态命令的复制日志。状态机处理的日志中的命令的顺序都是一致的，因此会得到相同的执行结果。\n\n\n\n\n\n如图-1所示，复制状态机是通过复制日志来实现的。每一台服务器保存着一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的，所以每个状态机的状态都是相同的，执行的命令是相同的，最后的执行结果也就是一样的了。\n\n如何保证复制日志一致就是一致性算法的工作了。在一台服务器上，一致性模块接受客户端的命令并且把命令加入到它的日志中。它和其他服务器上的一致性模块进行通信来确保每一个日志最终包含相同序列的请求，即使有一些服务器宕机了。一旦这些命令被正确的复制了，每一个服务器的状态机都会按同样的顺序去执行它们，然后将结果返回给客户端。最终，这些服务器看起来就像一台可靠的状态机。\n\n应用于实际系统的一致性算法一般有以下特性：\n\n- 确保安全性（从来不会返回一个错误的结果），即使在所有的非拜占庭（Non-Byzantine）情况下，包括网络延迟、分区、丢包、冗余和乱序的情况下。\n- 高可用性，只要集群中的大部分机器都能运行，可以互相通信并且可以和客户端通信，这个集群就可用。因此，一般来说，一个拥有 5 台机器的集群可以容忍其中的 2 台的失败（fail）。服务器停止工作了我们就认为它失败（fail）了，没准一会当它们拥有稳定的存储时就能从中恢复过来，重新加入到集群中。\n- 不依赖时序保证一致性，时钟错误和极端情况下的消息延迟在最坏的情况下才会引起可用性问题。\n- 通常情况下，一条命令能够尽可能快的在大多数节点对一轮远程调用作出相应时完成，一少部分慢的机器不会影响系统的整体性能。\n\n# 3 Paxos 算法的不足\n\n在过去的10年中，Leslie Lamport 的 Paxos 算法几乎已经成为了一致性算法的代名词：它是授课中最常见的算法，同时也是许多一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，例如一个单一复制日志条目（single replicated log entry）。我们把这个子集叫做单一决策 Paxos（single-decree Paxos）。之后 Paxos通过组合多个这种协议来完成一系列的决策，例如一个日志（multi-Paxos）。Paxos 确保安全性和活跃性（liveness），并且它支持集群成员的变更。它的正确性已经被证明，通常情况下也很高效。\n\n不幸的是，Paxos 有两个致命的缺点。第一个是 Paxos 太难以理解。它的完整的解释晦涩难懂；很少有人能完全理解，只有少数人成功的读懂了它。并且大家做了许多努力来用一些简单的术语来描述它。尽管这些解释都关注于单一决策子集问题，但仍具有挑战性。在 NSDI 2012 会议上的一次非正式调查显示，我们发现大家对 Paxos 都感到不满意，其中甚至包括一些有经验的研究员。我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。\n\n我们假定 Paxos 的晦涩来源于它将单决策子集作为它的基础。单决策（Single-decree）Paxos 是晦涩且微妙的：它被划分为两个没有简单直观解释的阶段，并且难以独立理解。正因为如此，它不能很直观的让我们知道为什么单一决策协议能够工作。为多决策 Paxos 设计的规则又添加了额外的复杂性和精巧性。我们相信多决策问题能够分解为其它更直观的方式。\n\nPaxos 的第二个缺点是它难以在实际环境中实现。其中一个原因是，对于多决策 Paxos （multi-Paxos） ，大家还没有一个一致同意的算法。Lamport 的描述大部分都是有关于单决策 Paxos （single-decree Paxos）；他仅仅描述了实现多决策的可能的方法，缺少许多细节。有许多实现 Paxos 和优化 Paxos 的尝试，但是他们都和 Lamport 的描述有些出入。例如，Chubby 实现的是一个类似 Paxos 的算法，但是在许多情况下的细节没有公开。\n\n另外，Paxos 的结构也是不容易在一个实际系统中进行实现的，这是单决策问题分解带来的又一个问题。例如，从许多日志条目中选出条目然后把它们融合到一个序列化的日志中并没有带来什么好处，它仅仅增加了复杂性。围绕着日志来设计一个系统是更简单、更高效的：新日志按照严格的顺序添加到日志中去。另一个问题是，Paxos 使用对等的点对点的实现作为它的核心（尽管它最终提出了一种弱领导者的形式来优化性能）。这种方法在只有一个决策被制定的情况下才显得有效，但是很少有现实中的系统使用它。如果要做许多的决策，选择一个领导人，由领带人来协调是更简单有效的方法。\n\n因此，在实际的系统应用中和 Paxos 算法都相差很大。所有开始于 Paxos 的实现都会遇到很多问题，然后由此衍生出了许多与 Paxos 有很大不同的架构。这是既费时又容易出错的，并且理解 Paxos 的难度又非常大。Paxos 算法在它正确性的理论证明上是很好的，但是在实现上的价值就远远不足了。来自 Chubby 的实现的一条评论就能够说明：\n\n> Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。\n\n正因为存在这些问题，我们认为 Paxos 不仅对于系统的构建者来说不友好，同时也不利于教学。鉴于一致性算法对于大规模软件系统的重要性，我们决定试着来设计一种另外的比 Paxos 更好的一致性算法。Raft 就是这样的一个算法。\n\n# 4 易于理解的设计\n\n设计 Raft 的目标有如下几个：\n\n- 它必须提供一个完整的、实际的基础来进行系统构建，为的是减少开发者的工作；\n- 它必须在所有情况下都能保证安全可用；\n- 它对于常规操作必须高效；\n- 最重要的目标是：**易于理解**，它必须使得大多数人能够很容易的理解；\n- 另外，它必须能让开发者有一个直观的认识，这样才能使系统构建者们去对它进行扩展。\n\n在设计 Raft 的过程中，我们不得不在许多种方法中做出选择。当面临这种情况时，我们通常会权衡可理解性：每种方法的可理解性是如何的？（例如，它的状态空间有多复杂？它是不是有很细微的含义？）它的可读性如何？读者能不能轻易地理解这个方法和它的含义？\n\n我们意识到对这种可理解性的分析具有高度的主观性；尽管如此，我们使用了两种适用的方式。第一种是众所周知的问题分解：我们尽可能将问题分解成为若干个可解决的、可被理解的小问题。例如，在 Raft 中，我们把问题分解成为了**领导选取（leader election）**、**日志复制（log replication）**、**安全（safety）**和**成员变化（membership changes）**。\n\n我们采用的第二个方法是通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致并且尽可能消除不确定性。特别地，日志之间不允许出现空洞，并且 Raft 限制了限制了日志不一致的可能性。尽管在大多数情况下，我们都都在试图消除不确定性，但是有时候有些情况下，不确定性使得算法更易理解。尤其是，随机化方法使得不确定性增加，但是它减少了状态空间。我们使用随机化来简化了 Raft 中的领导选取算法。\n\n# 5 Raft 一致性算法\n\nRaft 是一种用来管理第 2 章中提到的复制日志的算法。表-2 为了方便参考是一个算法的总结版本，表-3 列举了算法中的关键性质；表格中的这些元素将会在这一章剩下的部分中分别进行讨论。\n\n**状态：**\n\n在所有服务器上持久存在的：（在响应远程过程调用 RPC 之前稳定存储的）\n\n| 名称        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| currentTerm | 服务器最后知道的任期号（从0开始递增）                        |\n| votedFor    | 在当前任期内收到选票的候选人 id（如果没有就为 null）         |\n| log[]       | 日志条目；每个条目包含状态机的要执行命令和从领导人处收到时的任期号 |\n\n在所有服务器上不稳定存在的：\n\n| 名称        | 描述                                              |\n| ----------- | ------------------------------------------------- |\n| commitIndex | 已知的被提交的最大日志条目的索引值（从0开始递增） |\n| lastApplied | 被状态机执行的最大日志条目的索引值（从0开始递增） |\n\n在领导人服务器上不稳定存在的：（在选举之后初始化的）\n\n| 名称         | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| nextIndex[]  | 对于每一个服务器，记录需要发给它的下一个日志条目的索引（初始化为领导人上一条日志的索引值+1） |\n| matchIndex[] | 对于每一个服务器，记录已经复制到该服务器的日志的最高索引值（从0开始递增） |\n\n表-2-i\n\n**附加日志远程过程调用 （AppendEntries RPC）**\n\n由领导人来调用复制日志（5.3节）；也会用作heartbeat\n\n| 参数         | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| term         | 领导人的任期号                                               |\n| leaderId     | 领导人的 id，为了其他服务器能重定向到客户端                  |\n| prevLogIndex | 最新日志之前的日志的索引值                                   |\n| prevLogTerm  | 最新日志之前的日志的领导人任期号                             |\n| entries[]    | 将要存储的日志条目（表示 heartbeat 时为空，有时会为了效率发送超过一条） |\n| leaderCommit | 领导人提交的日志条目索引值                                   |\n\n| 返回值  | 描述                                                         |\n| ------- | ------------------------------------------------------------ |\n| term    | 当前的任期号，用于领导人更新自己的任期号                     |\n| success | 如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 |\n\n**接受者需要实现：**\n\n1. 如果 `term < currentTerm`返回 false（5.1节）\n2. 如果在`prevLogIndex`处的日志的任期号与`prevLogTerm`不匹配时，返回 false（5.3节）\n3. 如果一条已经存在的日志与新的冲突（index 相同但是任期号 term 不同），则删除已经存在的日志和它之后所有的日志（5.3节）\n4. 添加任何在已有的日志中不存在的条目\n5. 如果`leaderCommit > commitIndex`，将`commitIndex`设置为`leaderCommit`和最新日志条目索引号中较小的一个\n\n表-2-ii\n\n**投票请求 RPC（RequestVote RPC）**\n\n由候选人发起收集选票（5.2节）\n\n| 参数         | 描述                           |\n| ------------ | ------------------------------ |\n| term         | 候选人的任期号                 |\n| candidateId  | 请求投票的候选人 id            |\n| lastLogIndex | 候选人最新日志条目的索引值     |\n| lastLogTerm  | 候选人最新日志条目对应的任期号 |\n\n| 返回值      | 描述                             |\n| ----------- | -------------------------------- |\n| term        | 目前的任期号，用于候选人更新自己 |\n| voteGranted | 如果候选人收到选票为 true        |\n\n**接受者需要实现：**\n\n1. 如果`term < currentTerm`返回 false（5.1节）\n2. 如果`votedFor`为空或者与`candidateId`相同，并且候选人的日志和自己的日志一样新，则给该候选人投票（5.2节 和 5.4节）\n\n表-2-iii\n\n**服务器需要遵守的规则：**\n\n所有服务器：\n\n- 如果`commitIndex > lastApplied`，`lastApplied`自增，将`log[lastApplied]`应用到状态机（5.3节）\n- 如果 RPC 的请求或者响应中包含一个 term T 大于 `currentTerm`，则`currentTerm`赋值为 T，并切换状态为追随者（Follower）（5.1节）\n\n追随者（followers）: 5.2节\n\n- 响应来自候选人和领导人的 RPC\n- 如果在超过选取领导人时间之前没有收到来自当前领导人的`AppendEntries RPC`或者没有收到候选人的投票请求，则自己转换状态为候选人\n\n候选人：5.2节\n\n- 转变为选举人之后开始选举：\n  - `currentTerm`自增\n  - 给自己投票\n  - 重置选举计时器\n  - 向其他服务器发送`RequestVote RPC`\n- 如果收到了来自大多数服务器的投票：成为领导人\n- 如果收到了来自新领导人的`AppendEntries RPC（heartbeat）`：转换状态为追随者\n- 如果选举超时：开始新一轮的选举\n\n领导人：\n\n- 一旦成为领导人：向其他所有服务器发送空的`AppendEntries RPC（heartbeat）`;在空闲时间重复发送以防止选举超时（5.2节）\n\n- 如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端（5.3节）\n\n- 对于一个追随者来说，如果上一次收到的日志索引大于将要收到的日志索引（nextIndex）：通过\n\n  ```\n  AppendEntries RPC\n  ```\n\n  将 nextIndex 之后的所有日志条目发送出去\n\n  - 如果发送成功：将该追随者的 `nextIndex`和`matchIndex`更新\n  - 如果由于日志不一致导致`AppendEntries RPC`失败：`nextIndex`递减并且重新发送（5.3节）\n\n- 如果存在一个满足`N > commitIndex`和`matchIndex[i] >= N`并且`log[N].term == currentTerm`的 N，则将`commitIndex`赋值为 N\n\n表-2-iv\n\n表-2：Raft 一致性算法的总结（不包括成员变化 membership changes 和日志压缩 log compaction）\n\n\n\n\n\n| 性质                                   | 描述                                                         |\n| -------------------------------------- | ------------------------------------------------------------ |\n| 选举安全原则（Election Safety）        | 一个任期（term）内最多允许有一个领导人被选上（5.2节）        |\n| 领导人只增加原则（Leader Append-Only） | 领导人永远不会覆盖或者删除自己的日志，它只会增加条目         |\n| 日志匹配原则（Log Matching）           | 如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同（5.3 节） |\n| 领导人完全原则（Leader Completeness)   | 如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中 |\n| 状态机安全原则（State Machine Safety） | 如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目（5.4.3节） |\n\n表-3：Raft 算法保证这些特性任何时刻都成立\n\n\n\n\n\nRaft 通过首先选出一个领导人来实现一致性，然后给予领导人完全管理复制日志（replicated log）的责任。领导人接收来自客户端的日志条目，并把它们复制到其他的服务器上，领带人还要告诉服务器们什么时候将日志条目应用到它们的状态机是安全的。通过选出领导人能够简化复制日志的管理工作。例如，领导人能够决定将新的日志条目放到哪，而并不需要和其他的服务器商议，数据流被简化成从领导人流向其他服务器。如果领导人宕机或者和其他服务器失去连接，就可以选取下一个领导人。\n\n通过选出领导人，Raft 将一致性问题分解成为三个相对独立的子问题：\n\n- **领导人选取（Leader election）：** 在一个领导人宕机之后必须要选取一个新的领导人（5.2节）\n- **日志复制（Log replication）：** 领导人必须从客户端接收日志然后复制到集群中的其他服务器，并且强制要求其他服务器的日志保持和自己相同\n- **安全性（Safety）：** Raft 的关键的安全特性是 表-3 中提到的状态机安全原则（State Machine Safety）:如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。5.4节阐述了 Raft 是如何保证这条原则的，解决方案涉及到一个对于选举机制另外的限制，这一部分会在 5.2节 中说明。\n\n在说明了一致性算法之后，本章会讨论有关可用性（availability）的问题和系统中时序（timing）的问题。\n\n## 5.1 Raft 基础\n\n一个 Raft 集群包括若干服务器；对于一个典型的 5 服务器集群，该集群能够容忍 2 台机器不能正常工作，而整个系统保持正常。在任意的时间，每一个服务器一定会处于以下三种状态中的一个：*领导人*、*候选人*、*追随者*。在正常情况下，只有一个服务器是领导人，剩下的服务器是追随者。追随者们是被动的：他们不会发送任何请求，只是响应来自领导人和候选人的请求。领导人来处理所有来自客户端的请求（如果一个客户端与追随者进行通信，追随者会将信息发送给领导人）。候选人是用来选取一个新的领导人的，这一部分会在 5.2节 进行阐释。图-4 阐述了这些状态，和它们之间的转换；它们的转换会在下边进行讨论。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg)\n\n图-4：服务器的状态。追随者只响应其他服务器的请求。如果追随者没有收到任何消息，它会成为一个候选人并且开始一次选举。收到大多数服务器投票的候选人会成为新的领导人。领导人在它们宕机之前会一直保持领导人的状态。\n\n\n\n\n\n[![img](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg)](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg)\n\n图-5：时间被分为一个个的任期（term），每一个任期的开始都是领导人选举。在成功选举之后，一个领导人会在任期内管理整个集群。如果选举失败，该任期就会因为没有领带人而结束。这个转变会在不同的时间的不同服务器上观察到。\n\n\n\n\n\n如 图-5 所示，Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），就像 5.2节 所描述的那样，一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最少要有一个领导人。\n\n不同的服务器可能会在任期内观察到多次不同的状态转换，在某些情况下，一台服务器可能看不到一次选举或者一个完整的任期。任期在 Raft 中充当逻辑时钟的角色，并且它们允许服务器检测过期的信息，比如过时的领导人。每一台服务器都存储着一个当前任期的数字，这个数字会单调的增加。当服务器之间进行通信时，会互相交换当前任期号；如果一台服务器的当前任期号比其它服务器的小，则更新为较大的任期号。如果一个候选人或者领导人意识到它的任期号过时了，它会立刻转换为追随者状态。如果一台服务器收到的请求的任期号是过时的，那么它会拒绝此次请求。\n\nRaft 中的服务器通过远程过程调用（RPC）来通信，基本的 Raft 一致性算法仅需要 2 种 RPC。RequestVote RPC 是候选人在选举过程中触发的（5.2节），AppendEntries RPC 是领导人触发的，为的是复制日志条目和提供一种心跳（heartbeat）机制（5.3节）。第7章加入了第三种 RPC 来在各个服务器之间传输快照（snapshot）。如果服务器没有及时收到 RPC 的响应，它们会重试，并且它们能够并行的发出 RPC 来获得最好的性能。\n\n## 5.2 领导人选取\n\nRaft 使用一种心跳机制（heartbeat）来触发领导人的选取。当服务器启动时，它们会初始化为追随者。一太服务器会一直保持追随者的状态只要它们能够收到来自领导人或者候选人的有效 RPC。领导人会向所有追随者周期性发送心跳（heartbeat，不带有任何日志条目的 AppendEntries RPC）来保证它们的领导人地位。如果一个追随者在一个周期内没有收到心跳信息，就叫做选举超时（election timeout）,然后它就会假定没有可用的领导人并且开始一次选举来选出一个新的领导人。\n\n为了开始选举，一个追随者会自增它的当前任期并且转换状态为候选人。然后，它会给自己投票并且给集群中的其他服务器发送 RequestVote RPC。一个候选人会一直处于该状态，直到下列三种情形之一发生：\n\n- 它赢得了选举；\n- 另一台服务器赢得了选举；\n- 一段时间后没有任何一台服务器赢得了选举\n\n这些情形会在下面的章节中分别讨论。\n\n一个候选人如果在一个任期内收到了来自集群中大多数服务器的投票就会赢得选举。在一个任期内，一台服务器最多能给一个候选人投票，按照先到先服务原则（first-come-first-served）（注意：在 5.4节 针对投票添加了一个额外的限制）。大多数原则使得在一个任期内最多有一个候选人能赢得选举（表-3 中提到的选举安全原则）。一旦有一个候选人赢得了选举，它就会成为领导人。然后它会像其他服务器发送心跳信息来建立自己的领导地位并且组织新的选举。\n\n当一个候选人等待别人的选票时，它有可能会收到来自其他服务器发来的声明其为领导人的 AppendEntries RPC。如果这个领导人的任期（包含在它的 RPC 中）比当前候选人的当前任期要大，则候选人认为该领导人合法，并且转换自己的状态为追随者。如果在这个 RPC 中的任期小于候选人的当前任期，则候选人会拒绝此次 RPC， 继续保持候选人状态。\n\n第三种情形是一个候选人既没有赢得选举也没有输掉选举：如果许多追随者在同一时刻都成为了候选人，选票会被分散，可能没有候选人能获得大多数的选票。当这种情形发生时，每一个候选人都会超时，并且通过自增任期号和发起另一轮 RequestVote RPC 来开始新的选举。然而，如果没有其它手段来分配选票的话，这种情形可能会无限的重复下去。\n\nRaft 使用随机的选举超时时间来确保第三种情形很少发生，并且能够快速解决。为了防止在一开始是选票就被瓜分，选举超时时间是在一个固定的间隔内随机选出来的（例如，150~300ms）。这种机制使得在大多数情况下只有一个服务器会率先超时，它会在其它服务器超时之前赢得选举并且向其它服务器发送心跳信息。同样的机制被用于选票一开始被瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，在超时进行下一次选举之前一直等待。这能够减小在新的选举中一开始选票就被瓜分的可能性。9.3节 展示了这种方法能够快速的选出一个领导人。\n\n选举是一个理解性引导我们设计替代算法的一个例子。最开始时，我们计划使用一种排名系统：给每一个候选人分配一个唯一的排名，用于在竞争的候选人之中选择领导人。如果一个候选人发现了另一个比它排名高的候选人，那么它会回到追随者的状态，这样排名高的候选人会很容易地赢得选举。但是我们发现这种方法在可用性方面有一点问题（一个低排名的服务器在高排名的服务器宕机后，需要等待超时才能再次成为候选人，但是如果它这么做的太快，它能重置选举领带人的过程）。我们对这个算法做了多次调整，但是每次调整后都会出现一些新的问题。最终我们认为随机重试的方法是更明确并且更易于理解的。\n\n## 5.3 日志复制\n\n一旦选出了领导人，它就开始接收客户端的请求。每一个客户端请求都包含一条需要被复制状态机（replicated state machine）执行的命令。领导人把这条命令作为新的日志条目加入到它的日志中去，然后并行的向其他服务器发起 AppendEntries RPC ，要求其它服务器复制这个条目。当这个条目被安全的复制之后（下面的部分会详细阐述），领导人会将这个条目应用到它的状态机中并且会向客户端返回执行结果。如果追随者崩溃了或者运行缓慢或者是网络丢包了，领导人会无限的重试 AppendEntries RPC（甚至在它向客户端响应之后）知道所有的追随者最终存储了所有的日志条目。\n\n\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg)\n\n图-6：日志由有序编号的日志条目组成。每个日志条目包含它被创建时的任期号（每个方块中的数字），并且包含用于状态机执行的命令。如果一个条目能够被状态机安全执行，就被认为可以提交了。\n\n\n\n\n\n日志就像 图-6 所示那样组织的。每个日志条目存储着一条被状态机执行的命令和当这条日志条目被领导人接收时的任期号。日志条目中的任期号用来检测在不同服务器上日志的不一致性，并且能确保 图-3 中的一些特性。每个日志条目也包含一个整数索引来表示它在日志中的位置。\n\n领导人决定什么时候将日志条目应用到状态机是安全的；这种条目被称为可被提交（commited）。Raft 保证可被提交（commited）的日志条目是持久化的并且最终会被所有可用的状态机执行。一旦被领导人创建的条目已经复制到了大多数的服务器上，这个条目就称为可被提交的（例如，图-6中的7号条目）。领导人日志中之前的条目都是可被提交的（commited），包括由之前的领导人创建的条目。5.4节将会讨论当领导人更替之后这条规则的应用问题的细节，并且也讨论了这种提交方式是安全的。领导人跟踪记录它所知道的被提交条目的最大索引值，并且这个索引值会包含在之后的 AppendEntries RPC 中（包括心跳 heartbeat 中），为的是让其他服务器都知道这条条目已经提交。一旦一个追随者知道了一个日志条目已经被提交，它会将该条目应用至本地的状态机（按照日志顺序）。\n\n我们设计了 Raft 日志机制来保证不同服务器上日志的一致性。这样做不仅简化了系统的行为使得它更可预测，并且也是保证安全性不可或缺的一部分。Raft 保证以下特性，并且也保证了 表-3 中的日志匹配原则（Log Matching Property）:\n\n- 如果在不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。\n- 如果在不同日志中的两个条目有着相同的索引和任期号，则它们之间的所有条目都是完全一样的。\n\n第一条特性源于领导人在一个任期里在给定的一个日志索引位置最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，领导人会把新日志条目紧接着之前的条目的索引位置和任期号都包含在里面。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg)\n\n图-7：当最上边的领导人掌权之后，追随者日志可能有以下情况（a~f）。一个格子表示一个日志条目；格子中的数字是它的任期。一个追随者可能会丢失一些条目（a, b）；可能多出来一些未提交的条目（c, d）；或者两种情况都有（e, f）。例如，场景 f 在如下情况下就会发生：如果一台服务器在任期2时是领导人并且往它的日志中添加了一些条目，然后在将它们提交之前就宕机了，之后它很快重启了，成为了任期3的领导人，又往它的日志中添加了一些条目，然后在任期2和任期3中的条目提交之前它又宕机了并且几个任期内都一直处于宕机状态。\n\n\n\n\n\n在一般情况下，领导人和追随者们的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，领导人的崩溃会导致日志不一致（旧的领导人可能没有完全复制完日志中的所有条目）。这些不一致会导致一系列领导人和追随者崩溃。图-7 阐述了一些追随者可能和新的领导人日志不同的情况。一个追随者可能会丢失掉领导人上的一些条目，也有可能包含一些领导人没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。\n\n在 Raft 算法中，领导人通过强制追随者们复制它的日志来处理日志的不一致。这就意味着，在追随者上的冲突日志会被领导者的日志覆盖。5.4节会说明当添加了一个额外的限制之后这是安全的。\n\n为了使得追随者的日志同自己的一致，领导人需要找到追随者同它的日志一致的地方，然后删除追随者在该位置之后的条目，然后将自己在该位置之后的条目发送给追随者。这些操作都在 AppendEntries RPC 进行一致性检查时完成。领导人给每一个追随者维护了一个`nextIndex`，它表示领导人将要发送给该追随者的下一条日志条目的索引。当一个领导人开始掌权时，它会将`nextIndex`初始化为它的最新的日志条目索引数+1（图-7 中的 11）。如果一个追随者的日志和领导者的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，领导人会将`nextIndex`递减然后重试 AppendEntries RPC。最终`nextIndex`会达到一个领导人和追随者日志一致的地方。这时，AppendEntries 会返回成功，追随者中冲突的日志条目都被移除了，并且添加所缺少的上了领导人的日志条目。一旦 AppendEntries 返回成功，追随者和领导人的日志就一致了，这样的状态会保持到该任期结束。\n\n如果需要的话，算法还可以进行优化来减少 AppendEntries RPC 失败的次数。例如，当拒绝了一个 AppendEntries 请求，追随者可以记录下冲突日志条目的任期号和自己存储那个任期的最早的索引。通过这些信息，领导人能够直接递减`nextIndex`跨过那个任期内所有的冲突条目；这样的话，一个冲突的任期需要一次 AppendEntries RPC，而不是每一个冲突条目需要一次 AppendEntries RPC。在实践中，我们怀疑这种优化是否是必要的，因为AppendEntries 一致性检查很少失败并且也不太可能出现大量的日志条目不一致的情况。\n\n通过这种机制，一个领导人在掌权时不需要采取另外特殊的方式来恢复日志的一致性。它只需要使用一些常规的操作，通过响应 AppendEntries 一致性检查的失败能使得日志自动的趋于一致。一个领导人从来不会覆盖或者删除自己的日志（表-3 中的领导人只增加原则）。\n\n这个日志复制机制展示了在第2章中阐述的所希望的一致性特性：Raft 能够接受，复制并且应用新的日志条目只要大部分的服务器是正常的。在通常情况下，一条新的日志条目可以在一轮 RPC 内完成在集群的大多数服务器上的复制；并且一个速度很慢的追随者并不会影响整体的性能。\n\n\\##5.4 安全性\n\n之前的章节中讨论了 Raft 算法是如何进行领导选取和复制日志的。然而，到目前为止这个机制还不能保证每一个状态机能按照相同的顺序执行同样的指令。例如，当领导人提交了若干日志条目的同时一个追随者可能宕机了，之后它又被选为了领导人然后用新的日志条目覆盖掉了旧的那些，最后，不同的状态机可能执行不同的命令序列。\n\n这一节通过在领带人选取部分加入了一个限制来完善了 Raft 算法。这个限制能够保证对于固定的任期，任何的领导人都拥有之前任期提交的全部日志条目（表-3 中的领导人完全原则）。有了这一限制，日志提交的规则就更清晰了。最后，我们提出了对于领导人完全原则的简单证明并且展示了它是如何修正复制状态机的行为的。\n\n\\###5.4.1 选举限制\n\n在所有的以领导人为基础的一致性算法中，领导人最终必须要存储全部已经提交的日志条目。在一些一致性算法中，例如：[Viewstamped Replication](http://people.csail.mit.edu/cowling/vr/vr-revisited.pdf)，即使一开始没有包含全部已提交的条目也可以被选为领导人。这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，这个过程要么在选举过程中完成，要么在选举之后立即开始。不幸的是，这种方式大大增加了复杂性。Raft 使用了一种更简单的方式来保证在新的领导人开始选举的时候在之前任期的所有已提交的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。\n\nRaft 使用投票的方式来阻止没有包含全部日志条目的服务器赢得选举。一个候选人为了赢得选举必须要和集群中的大多数进行通信，这就意味着每一条已经提交的日志条目最少在其中一台服务器上出现。如果候选人的日志至少和大多数服务器上的日志一样新（up-to-date，这个概念会在下边有详细介绍），那么它一定包含有全部的已经提交的日志条目。RequestVote RPC 实现了这个限制：这个 RPC（远程过程调用）包括候选人的日志信息，如果它自己的日志比候选人的日志要新，那么它会拒绝候选人的投票请求。\n\nRaft 通过比较日志中最后一个条目的索引和任期号来决定两个日志哪一个更新。如果两个日志的任期号不同，任期号大的更新；如果任期号相同，更长的日志更新。\n\n\\###5.4.2 提交之前任期的日志条目\n\n[![img](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg)](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg)\n\n图-8：如图的时间序列说明了为什么领导人不能通过之前任期的日志条目判断它的提交状态。（a）中的 S1 是领导人并且部分复制了索引2上的日志条目。（b）中 S1 崩溃了；S5 通过 S3，S4 和自己的选票赢得了选举，并且在索引2上接收了另一条日志条目。（c）中 S5 崩溃了，S1 重启了，通过 S2，S3 和自己的选票赢得了选举，并且继续索引2处的复制，这时任期2的日志条目已经在大部分服务器上完成了复制，但是还并没有提交。如果在（d）时刻 S1 崩溃了，S5 会通过 S2，S3，S4 的选票成为领导人，然后用它自己在任期3的日志条目覆盖掉其他服务器的日志条目。然而，如果在崩溃之前，S1 在它的当前任期在大多数服务器上复制了一条日志条目，就像在（e）中那样，那么这条条目就会被提交（S5就不会赢得选举）。在这时，之前的日志条目就会正常被提交。\n\n\n\n\n\n正如 5.3节 中描述的那样，只要一个日志条目被存在了在多数的服务器上，领导人就知道当前任期就可以提交该条目了。如果领导人在提交之前就崩溃了，之后的领导人会试着继续完成对日志的复制。然而，领导人并不能断定存储在大多数服务器上的日志条目一定在之前的任期中被提交了。图-8 说明了一种情况，一条存储在了大多数服务器上的日志条目仍然被新上任的领导人覆盖了。\n\n为了消除 图-8 中描述的问题，Raft 从来不会通过计算复制的数目来提交之前人气的日志条目。只有领导人当前任期的日志条目才能通过计算数目来进行提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配原则（Log Matching Property），之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，通过观察该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。\n\n因为当领导人从之前任期复制日志条目时日志条目保留了它们最开始的任期号，所以这使得 Raft 在提交规则中增加了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要从之前的任期中复制日志条目，它必须要使用当前的新任期号。Raft 的方法使得判断日志更加容易，因为它们全程都保持着同样的任期号。另外，和其它的一致性算法相比，Raft 算法中的新领导人会发送更少的之前任期的日志条目（其他算法必须要发送冗余的日志条目并且在它们被提交之前来重新排序）。\n\n### 5.4.3 安全性论证\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg)\n\n图-9：如果 S1（任期 T 的领导人）在它的任期提交了一条日志条目，并且 S5 在之后的任期 U 成为了领导人，那么最少会有一台服务器（S3）接收了这条日志条目并且会给 S5 投票。\n\n\n\n\n\n给出了完整的 Raft 算法，现在我们能够更精确的论证领导人完全原则（Leader Completeness)（这基于 9.2节 提出的安全性证明）。我们假定领导人完全原则是不成立的，然后推导出矛盾。假定任期 T 的领导人 leaderT在它的任期提交了一个日志条目，但是这条日志条目并没有存储在之后的任期中的领导人上。我们设大于 T 的最小的任期 U 的领导人（leaderU） 没有存储这条日志条目。\n\n1. 在 leaderU 选举时一定没有那条被提交的日志条目（领导人从来不会删除或者覆盖日志条目）。\n2. leaderT 复制了这个条目到集群的大多数的服务器上。因此，只是有一台服务器（投票者）即接收了来自 leaderT 的日志条目并且给 leaderU 投票，就像 图-9 中所示那样。这个投票者是产生矛盾的关键。\n3. 投票者必须在给 leaderU 投票之前接收来自 leaderT 的日志条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（它的当前任期会比 T 要大）。\n4. 投票者会在它给 leaderU 投票时存储那个条目，因为任何中间的领导人都保有该条目（基于假设），领导人从来不会移除这个条目，并且追随者也只会在和领导人冲突时才会移除日志条目。\n5. 投票者给 leaderU 投票了，所以 leaderU 的日志必须和投票者的一样新。这就导致了一个矛盾。\n6. 首先，如果投票者和 leaderU 最后一条日志条目的任期号相同，那么 leaderU 的日志一定和投票者的一样长，因此它的日志包含全部投票者的日志条目。这是矛盾的，因为在假设中投票者和 leaderU 包含的已提交条目是不同的。\n7. 除此之外， leaderU 的最后一条日志的任期号一定比投票者的大。另外，它也比 T 要大，因为投票者的最后一条日志条目的任期号最小也要是 T（它包含了所有任期 T 提交的日志条目）。创建 leaderU 最后一条日志条目的上一任领导人必须包含已经提交的日志条目（基于假设）。那么，根据日志匹配原则（Log Matching），leaderU 也一定包含那条提交的日志条目，这也是矛盾的。\n8. 这时就完成了矛盾推导。因此，所有比任期 T 大的领导人一定包含所有在任期 T 提交的日志条目。\n9. 日志匹配原则（Log Matching）保证了未来的领导人也会包含被间接提交的日志条目，就像 图-8 中（d）时刻索引为2的条目。\n\n通过给出了 领导人完全原则（Leader Completeness)，我们能够证明 表-3 中的状态机安全原则（State Machine Safety），状态机安全原则（State Machine Safety）讲的是如果一台服务器将给定索引上的日志条目应用到了它自己的状态机上，其它服务器的同一索引位置不可能应用的是其它条目。在一个服务器应用一条日志条目到它自己的状态机中时，它的日志必须和领导人的日志在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性（Log Completeness Property）保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。\n\n最后，Raft 算法需要服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。\n\n\\##5.5 追随者和候选人崩溃\n\n截止到目前，我们只讨论了领导人崩溃的问题。追随者和候选人崩溃的问题解决起来要比领导人崩溃要简单得多，这两者崩溃的处理方式是一样的。如果一个追随者或者候选人崩溃了，那么之后的发送给它的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限的重试来处理这些失败；如果崩溃的服务器重启了，RPC 就会成功完成。如果一个服务器在收到了 RPC 之后但是在响应之前崩溃了，那么它会在重启之后再次收到同一个 RPC。因为 Raft 中的 RPC 都是幂等的，因此不会有什么问题。例如，如果一个追随者收到了一个已经包含在它的日志中的 AppendEntries 请求，它会忽视这个新的请求。\n\n\\##5.6 时序和可用性\n\n我们对于 Raft 的要求之一就是安全性不依赖于时序（timing）：系统不能仅仅因为一些事件发生的比预想的快一些或慢一些就产生错误。然而，可用性（系统可以及时响应客户端的特性）不可避免的要依赖时序。例如，如果消息交换在服务器崩溃时花费更多的时间，候选人不会等待太长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。\n\n领导人选取是 Raft 中对时序要求最关键的地方。Raft 会选出并且保持一个稳定的领导人只有系统满足下列时序要求（timing requirement）：\n\nbroadcastTime << electionTimeout << MTBF\n\n在这个不等式中，`broadcastTime`指的是一台服务器并行的向集群中的其他服务器发送 RPC 并且收到它们的响应的平均时间；`electionTimeout`指的就是在 5.2节 描述的选举超时时间；`MTBF`指的是单个服务器发生故障的间隔时间的平均数。`broadcastTime`应该比`electionTimeout`小一个数量级，为的是使领导人能够持续发送心跳信息（heartbeat）来阻止追随者们开始选举；根据已经给出的随机化选举超时时间方法，这个不等式也使得瓜分选票的情况变成不可能。`electionTimeout`也要比`MTBF`小几个数量级，为的是使得系统稳定运行。当领导人崩溃时，整个大约会在`electionTimeout`的时间内不可用；我们希望这种情况仅占全部时间的很小的一部分。\n\n`broadcastTime`和`MTBF`是由系统决定的性质，但是`electionTimeout`是我们必须做出选择的。Raft 的 RPC 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，这取决于存储的技术。因此，`electionTimeout`一般在 10ms 到 500ms 之间。大多数的服务器的`MTBF`都在几个月甚至更长，很容易满足这个时序需求。\n\n\\#6 集群成员变化\n\n截止到目前，我们都假定集群的配置（加入到一致性算法的服务器集合）是固定的。在实际中，我们会经常更改配置，例如，替换掉那些崩溃的机器或者更改复制级别。虽然通过关闭整个集群，升级配置文件，然后重启整个集群也可以解决这个问题，但是这回导致在更改配置的过程中，整个集群不可用。另外，如果存在需要手工操作，那么就会有操作失误的风险。为了避免这些问题，我们决定采用自动改变配置并且把这部分加入到了 Raft 一致性算法中。\n\n为了让配置修改机制能够安全，那么在转换的过程中在任何时间点两个领导人不能再同一个任期被同时选为领导人。不幸的是，服务器集群从旧的配置直接升级到新的配置的任何方法都是不安全的，一次性自动的转换所有服务器是不可能的，所以集群可以在转换的过程中划分成两个单独的组（如 图-10 所示）。\n\n\n\n[![img](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg)](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg)\n\n图-10：从一个配置切换到另一个配置是不安全的因为不同的服务器会在不同的时间点进行切换。在这个例子中，集群数量从三台转换成五台。不幸的是，在一个时间点有两个服务器能被选举成为领导人，一个是在使用旧的配置的机器中（Cold）选出的领导人，另一个领导人是通过新的配置（Cnew）选出来的。\n\n\n\n\n\n为了保证安全性，集群配置的调整必须使用两阶段（two-phase）方法。有许多种实现两阶段方法的实现。例如，一些系统在第一个阶段先把旧的配置设为无效使得它无法处理客户端请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡配置，我们称其为共同一致（joint consensus）；一旦共同一致被提交了，然后系统再切换到新的配置。共同一致是旧的配置和新的配置的组合：\n\n- 日志条目被复制给集群中新、老配置的所有服务器。\n- 新、老配置的服务器都能成为领导人。\n- 需要分别在两种配置上获得大多数的支持才能达成一致（针对选举和提交）\n\n共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然能够响应服务器请求。\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg)\n\n图-11：集群配置变更的时间线。虚线表示的是已经被创建但是还没提交的配置条目，实线表示的是最新提交的配置条目。领导人首先在它的日志中创建 Cold,new配置条目并且将它提交到Cold,new（使用旧配置的大部分服务器和使用新配置的大部分服务器）。然后创建它创建Cnew配置条目并且将它提交到使用新配置的大部分机器上。这样就不存在Cold和Cnew能够分别同时做出决定的时刻。\n\n\n\n\n\n集群配置在复制日志中用特殊的日志条目来存储和通信；图-11 展示了配置变更的过程。当一个领导人接收到一个改变配置 Cold 为 Cnew 的请求，它会为了共同一致以前面描述的日志条目和副本的形式将配置存储起来（图中的 Cold,new）。一旦一个服务器将新的配置日志条目增加到它的日志中，它就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论它是否已经被提交）。这意味着领导人要使用 Cold,new 的规则来决定日志条目 Cold,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 Cold 配置也可能是 Cold,new 配置，这取决于赢得选举的候选人是否已经接收到了 Cold,new 配置。在任何情况下， Cnew 配置在这一时期都不会单方面的做出决定。\n\n一旦 Cold,new 被提交，那么无论是 Cold 还是 Cnew，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 Cnew 配置的日志条目并复制给集群就是安全的了。另外，每个服务器在收到新的配置的时候就会立即生效。当新的配置在 Cnew 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如 图-11，Cold 和 Cnew 没有任何机会同时做出单方面的决定；这就保证了安全性。\n\n针对重新配置提出了三个问题。第一个问题是一开始的时候新的服务器可能没有任何日志条目。如果它们在这个状态下加入到集群中，那么它们需要一段时间来更新追赶，在这个阶段它们还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群中来（领导人复制日志给他们，但是不把它们考虑到大多数中）。一旦新的服务器追赶上了集群中的其它机器，重新配置可以像上面描述的一样处理。\n\n第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 Cnew 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括自己；它复制日志但是不把它自己看作是大多数之一。当 Cnew 被提交时，会发生领导人过渡，因为这时是新的配置可以独立工作的最早的时间点（总是能够在 Cnew 配置下选出新的领导人）。在此之前，可能只能从 Cold 中选出领导人。\n\n第三个问题是，移除不在 Cnew 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳（heartbeat），所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新的任期号的 RequestVote RPC，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。\n\n为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略 RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个 RequestVote RPC，它不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么它就不会被更大的任期号废除。\n\n\\#7 日志压缩\n\nRaft 产生的日志在持续的正常操作中不断增长，但是在实际的系统中，它不会无限的增长下去。随着日志的不断增长，它会占据越来越多的空间并且花费更多的时间重置。如果没有一个机制使得它能够废弃在日志中不断累积的过时的信息就会引起可用性问题。\n\n快照（snapshot）是最简单的压缩方式。在快照中，全部的当前系统状态都被写入到快照中，存储到持久化的存储中，然后在那个时刻之前的全部日志都可以被丢弃。在 Chubby 和 ZooKeeper 中都使用了快照技术，这一章的剩下的部分会介绍 Raft 中使用的快照技术。\n\n增量压缩（incremental approaches）的方法，例如日志清理（log cleaning）或者日志结构合并树（log-structured merge trees），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以使用和快照相同的接口来实现 LSM tree ，但是日志清除方法就需要修改 Raft 了。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg)\n\n图-12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。\n\n\n\n\n\n图-12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也将一些少量的元数据包含到快照中：最后被包含的索引（last included index）指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期（last included term）指的是该条目的任期号。保留这些数据是为了支持快照前的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 章），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。\n\n尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 章）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给它们。\n\n**安装快照 RPC（InstallSnapshot RPC）**\n\n在领导人发送快照给跟随者时使用调用。领导人总是按顺序发送。\n\n| 参数              | 描述                             |\n| ----------------- | -------------------------------- |\n| term              | 领导人的任期                     |\n| leaderId          | 为了追随者能重定向到客户端       |\n| lastIncludedIndex | 快照中包含的最后日志条目的索引值 |\n| lastIncludedTerm  | 快照中包含的最后日志条目的任期号 |\n| offset            | 分块在快照中的偏移量             |\n| data[]            | 快照块的原始数据                 |\n| done              | 如果是最后一块数据则为真         |\n\n| 返回值 | 描述                            |\n| ------ | ------------------------------- |\n| term   | currentTerm，用于领导人更新自己 |\n\n接受者需要实现：\n\n1. 如果`term < currentTerm`立刻回复\n2. 如果是第一个分块（offset 为 0）则创建新的快照\n3. 在指定的偏移量写入数据\n4. 如果 `done`为 false，则回复并继续等待之后的数据\n5. 保存快照文件，丢弃所有存在的或者部分有着更小索引号的快照\n6. 如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保留并且回复\n7. 丢弃全部日志\n8. 能够使用快照来恢复状态机（并且装载快照中的集群配置）\n\n表-13：InstallSnapshot RPC 的总结。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生存的信号，所以跟随者可以重置选举超时计时器。\n\n\n\n\n\n在这种情况下领导人使用一种叫做安装快照（InstallSnapshot）的新的 RPC 来发送快照给太落后的跟随者；见 表-13。当跟随者通过这种 RPC 接收到快照时，它必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃它所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须是正确的和并且被保留下来。\n\n这种快照的方式背离了 Raft 的强领导人原则（strong leader principle），因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织它们的数据了。\n\n我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。\n\n还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，它就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。\n\n第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制（copy-on-write）的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。\n\n\\#8 客户端交互\n\n这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端是如何发现领导人的和 Raft 是如何支持线性化语义（linearizable semantics）的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。\n\nRaft 中的客户端将所有请求发送给领导人。当客户端启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供它最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。\n\n我们 Raft 的目标是要实现线性化语义（linearizable semantics）（每一次操作立即执行，在它调用和收到回复之间只执行一次）。但是，如上述所说，Raft 是可以多次执行同一条命令的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。\n\n只读（read-only）的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回过期数据(stale data)的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是它还不知道。线性化的读操作必须不能返回过期数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全原则（Leader Completeness Property）保证了领导人一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来进行实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废除了（如果一个更新的领导人被选举出来，它自己的信息就已经过期了）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳（heartbeat）信息来处理这个问题。另外，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时序来保证安全性（它假设时间误差是有界的）。\n\n\\#9 实现和评价\n\n我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。\n\n这一章会从三个方面来评估 Raft 算法：可理解性、正确性和性能。\n\n\\##9.1 可理解性\n\n为了比较 Paxos 和 Raft 算法的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文除了日志压缩之外的所有内容；Paxos 课程包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者独立的区别从第一个算法处学来的经验。我们计算参加人员的每一个小测验的得分来看参与者是否对 Raft 的理解更好。\n\n| 因素           | 消除偏见的手段                                               | 复习材料 |\n| -------------- | ------------------------------------------------------------ | -------- |\n| 相同的讲课质量 | 使用相同的讲师。Paxos 的讲义是基于之前在几所大学中使用的材料的并且做了改进。Paxos 的讲义要长 14% | 视频     |\n| 相同的测试难度 | 用难度给问题分组，在测试中成对出现                           | 测验     |\n| 公平的打分     | 使用红字标题。随机顺序打分，两个测验交替进行。               | 红字标题 |\n\n表-1：考虑到的可能造成偏见的因素，以及解决方案和对应的复习材料\n\n我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表-1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。\n\n\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg)\n\n图-14：表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩的散点图。在对角线之上的点表示在 Raft 获得了更高分数的学生。\n\n\n\n\n\n参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图-14 展示了每个参与者的得分。一对 t -测试表明，拥有 95% 的可信度，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。\n\n我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型显示，对小测验的选择会产生 12.5 分的差别在对 Raft 的好感度上。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft 的小测验得分会比 Paxos 低 6.3 分；我们不知道为什么，但这在统计学上是这样的。\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg)\n\n图-15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。\n\n\n\n\n\n我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图-15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。\n\n关于 Raft 用户学习有一个更加详细的讨论，详见[http://ramcloud.stanford.edu/ ̃ongaro/thesis.pdf](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)\n\n\\##9.2 正确性\n\n在第5章，我们已经进行了一个[正式的说明](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)，和对一致性机制的安全性证明。这个正式说明通过 [TLA+](https://www.amazon.com/Specifying-Systems-Language-Hardware-Engineers/dp/032114306X) 让 表-2 中的信息非常清晰。它大约有 400 行并且充当了证明的主题。同时对于任何想实现的人也是十分有用的。我们非常机械的通过 TLA 证明系统证明了日志完全特性（Log Completeness Property）。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明这个说明中的类型安全 type safety）。而且，我们已经写了一个[非正式的证明](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)关于状态机安全性质是完备的，并且是相当清晰的（大约 3500 个词）。\n\n\\##9.3 性能\n\nRaft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。\n\n我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答以下两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg)\n\n\n\n图-16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。\n\n\n\n为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图-16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。\n\n图-16 上面的图表表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程由于太多的选票瓜分的情况往往都需要花费超过 10 秒钟。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。\n\n图-16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。\n\n\\#10 相关工作\n\n已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：\n\n- Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰的论文。\n- 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。\n- 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 有着很大的差别。\n- Paxos 可以应用的性能优化。\n- Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。\n\nRaft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。\n\n像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。\n\n和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。\n\nRaft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。\n\n一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致（joint consensus）的方法因为它对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Raft 没有采用 Lamport 的基于 α 的方法是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较而言，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。和 VR、SMART 比较而言，Raft 的方法同时需要更少的额外机制来实现。\n\n\\#11 总结\n\n算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。\n\n在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。\n\n\\#12 鸣谢\n\n这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。\n\n\\#引用\n\n1. BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154.\n2. BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350.\n3. CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317.\n4. CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407.\n5. CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218.\n6. CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264.\n7. COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154.\n8. GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43.\n9. GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210.\n10. HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492.\n11. HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158.\n12. JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems & Networks (2011), IEEE Computer Society, pp. 245–256.\n13. KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.\n14. LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565.\n15. LAMPORT, L. The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169.\n16. LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25.\n17. LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002.\n18. LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.\n19. LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103.\n20. LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.\n21. LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13.\n22. LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.\n23. LogCabin source code. logcabin/logcabin.\n24. LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115.\n25. MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384.\n26. MAZIE` RES, D. Paxos made practical.<http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf> , Jan. 2007.\n27. MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM.\n28. Raft user study. <http://ramcloud.stanford.edu/~ongaro/userstudy/>.\n29. OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17.\n30. O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385.\n31. ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress).<http://ramcloud.stanford.edu/~ongaro/thesis.pdf>\n32. ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX.\n33. OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130.\n34. Raft consensus algorithm website. [http://raftconsensus.github.io](http://raftconsensus.github.io/).\n35. REED, B. Personal communications, May 17, 2013.\n36. ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52.\n37. SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319.\n38. SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10.\n39. VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.https://github.com/iissnan/hexo-theme-next)","source":"_posts/Raft 一致性算法论文译文.md","raw":"---\ntitle: Raft 一致性算法论文译文\ndate: 2019-04-05 20:04:50\ntags: raft\n---\n\n# Raft 一致性算法论文译文\n\nRaft论文翻译转载自[Raft 一致性算法论文译文](<http://blog.luoyuanhang.com/2017/02/02/raft-paper-in-zh-CN/>)\n\n# 摘要\n\nRaft 是一种用来管理日志复制的一致性算法。它和 Paxos 的性能和功能是一样的，但是它和 Paxos 的结构不一样；这使得 Raft 更容易理解并且更易于建立实际的系统。为了提高理解性，Raft 将一致性算法分为了几个部分，例如领导选取（leader selection），日志复制（log replication）和安全性（safety），同时它使用了更强的一致性来减少了必须需要考虑的状态。从用户学习的结果来看，Raft 比 Paxos 更容易学会。Raft 还包括了一种新的机制来使得动态改变集群成员，它使用重叠大多数（overlapping majorities）来保证安全。\n\n# 1 引言\n\n一致性算法允许一组机器像一个整体一样工作，即使其中的一些机器出了错误也能正常工作。正因为此，他们扮演着建立大规模可靠的软件系统的关键角色。在过去的十年中 Paxos 一直都主导着有关一致性算法的讨论：大多数一致性算法的实现都基于它或者受它影响，并且 Paxos 也成为了教学生关于一致性知识的主要工具。\n\n不幸的是，尽管在降低它的复杂性方面做了许多努力，Paxos 依旧很难理解。并且，Paxos 需要经过复杂的修改才能应用于实际中。这些导致了系统构构建者和学生都十分头疼。\n\n在被 Paxos 折磨之后，我们开始寻找一种在系统构建和教学上更好的新的一致性算法。我们的首要目标是让它易于理解：我们能不能定义一种面向实际系统的一致性算法并且比 Paxos 更容易学习呢？并且，我们希望这种算法能凭直觉就能明白，这对于一个系统构建者来说是十分必要的。对于一个算法，不仅仅是让它工作起来很重要，知道它是如何工作的更重要。\n\n我们工作的结果是一种新的一致性算法，叫做 Raft。在设计 Raft 的过程中我们应用了许多专门的技巧来提升理解性，包括算法分解（分为领导选取（leader selection），日志复制（log replication）和安全性（safety））和减少状态（state space reduction）（相对于 Paxos，Raft 减少了非确定性的程度和服务器互相不一致的方式）。在两所学校的43个学生的研究中发现，Raft 比 Paxos 要更容易理解：在学习了两种算法之后，其中的33个学生回答 Raft 的问题要比回答 Paxos 的问题要好。\n\nRaft 算法和现在一些已经有的算法在一些地方很相似（主要是 [Oki 和 Liskov 的 Viewstamped Replication](http://www.pmg.csail.mit.edu/papers/vr.pdf)。但是 Raft 有几个新的特性：\n\n- 强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。\n- 领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。\n- 成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。\n\n我们认为，Raft 在教学方面和实际实现方面比 Paxos 和其他算法更出众。它比其他算法更简单、更容易理解；它能满足一个实际系统的需求；它拥有许多开源的实现并且被许多公司所使用；它的安全特性已经被证明；并且它的效率和其他算法相比也具有竞争力。\n\n这篇论文剩下的部分会讲如下内容：复制状态机（replicated state machine）问题（第2节），讨论 Paxos 的优缺点（第3节），讨论我们用的为了达到提升理解性的方法（第4节），陈述 Raft 一致性算法（第5~8节），评价 Raft 算法（第9节），对相关工作的讨论（第10节）。\n\n<!--more-->\n\n# 2 复制状态机（Replicated State Machine）\n\n一致性算法是在[复制状态机](https://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf)的背景下提出来的。在这个方法中，在一组服务器的状态机产生同样的状态的副本因此即使有一些服务器崩溃了这组服务器也还能继续执行。复制状态机在分布式系统中被用于解决许多有关容错的问题。例如，GFS，HDFS还有 RAMCloud 这些大规模的系统都是用一个单独的集群领导者，使用一个单独的复制状态机来进行领导选取和存储配置信息来应对领导者的崩溃。使用复制状态机的例子有 Chubby 和 ZooKeeper。\n\n\n\n\n\n[![img](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg)](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg)\n\n图-1：复制状态机的架构。一致性算法管理来自客户端状态命令的复制日志。状态机处理的日志中的命令的顺序都是一致的，因此会得到相同的执行结果。\n\n\n\n\n\n如图-1所示，复制状态机是通过复制日志来实现的。每一台服务器保存着一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的，所以每个状态机的状态都是相同的，执行的命令是相同的，最后的执行结果也就是一样的了。\n\n如何保证复制日志一致就是一致性算法的工作了。在一台服务器上，一致性模块接受客户端的命令并且把命令加入到它的日志中。它和其他服务器上的一致性模块进行通信来确保每一个日志最终包含相同序列的请求，即使有一些服务器宕机了。一旦这些命令被正确的复制了，每一个服务器的状态机都会按同样的顺序去执行它们，然后将结果返回给客户端。最终，这些服务器看起来就像一台可靠的状态机。\n\n应用于实际系统的一致性算法一般有以下特性：\n\n- 确保安全性（从来不会返回一个错误的结果），即使在所有的非拜占庭（Non-Byzantine）情况下，包括网络延迟、分区、丢包、冗余和乱序的情况下。\n- 高可用性，只要集群中的大部分机器都能运行，可以互相通信并且可以和客户端通信，这个集群就可用。因此，一般来说，一个拥有 5 台机器的集群可以容忍其中的 2 台的失败（fail）。服务器停止工作了我们就认为它失败（fail）了，没准一会当它们拥有稳定的存储时就能从中恢复过来，重新加入到集群中。\n- 不依赖时序保证一致性，时钟错误和极端情况下的消息延迟在最坏的情况下才会引起可用性问题。\n- 通常情况下，一条命令能够尽可能快的在大多数节点对一轮远程调用作出相应时完成，一少部分慢的机器不会影响系统的整体性能。\n\n# 3 Paxos 算法的不足\n\n在过去的10年中，Leslie Lamport 的 Paxos 算法几乎已经成为了一致性算法的代名词：它是授课中最常见的算法，同时也是许多一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，例如一个单一复制日志条目（single replicated log entry）。我们把这个子集叫做单一决策 Paxos（single-decree Paxos）。之后 Paxos通过组合多个这种协议来完成一系列的决策，例如一个日志（multi-Paxos）。Paxos 确保安全性和活跃性（liveness），并且它支持集群成员的变更。它的正确性已经被证明，通常情况下也很高效。\n\n不幸的是，Paxos 有两个致命的缺点。第一个是 Paxos 太难以理解。它的完整的解释晦涩难懂；很少有人能完全理解，只有少数人成功的读懂了它。并且大家做了许多努力来用一些简单的术语来描述它。尽管这些解释都关注于单一决策子集问题，但仍具有挑战性。在 NSDI 2012 会议上的一次非正式调查显示，我们发现大家对 Paxos 都感到不满意，其中甚至包括一些有经验的研究员。我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。\n\n我们假定 Paxos 的晦涩来源于它将单决策子集作为它的基础。单决策（Single-decree）Paxos 是晦涩且微妙的：它被划分为两个没有简单直观解释的阶段，并且难以独立理解。正因为如此，它不能很直观的让我们知道为什么单一决策协议能够工作。为多决策 Paxos 设计的规则又添加了额外的复杂性和精巧性。我们相信多决策问题能够分解为其它更直观的方式。\n\nPaxos 的第二个缺点是它难以在实际环境中实现。其中一个原因是，对于多决策 Paxos （multi-Paxos） ，大家还没有一个一致同意的算法。Lamport 的描述大部分都是有关于单决策 Paxos （single-decree Paxos）；他仅仅描述了实现多决策的可能的方法，缺少许多细节。有许多实现 Paxos 和优化 Paxos 的尝试，但是他们都和 Lamport 的描述有些出入。例如，Chubby 实现的是一个类似 Paxos 的算法，但是在许多情况下的细节没有公开。\n\n另外，Paxos 的结构也是不容易在一个实际系统中进行实现的，这是单决策问题分解带来的又一个问题。例如，从许多日志条目中选出条目然后把它们融合到一个序列化的日志中并没有带来什么好处，它仅仅增加了复杂性。围绕着日志来设计一个系统是更简单、更高效的：新日志按照严格的顺序添加到日志中去。另一个问题是，Paxos 使用对等的点对点的实现作为它的核心（尽管它最终提出了一种弱领导者的形式来优化性能）。这种方法在只有一个决策被制定的情况下才显得有效，但是很少有现实中的系统使用它。如果要做许多的决策，选择一个领导人，由领带人来协调是更简单有效的方法。\n\n因此，在实际的系统应用中和 Paxos 算法都相差很大。所有开始于 Paxos 的实现都会遇到很多问题，然后由此衍生出了许多与 Paxos 有很大不同的架构。这是既费时又容易出错的，并且理解 Paxos 的难度又非常大。Paxos 算法在它正确性的理论证明上是很好的，但是在实现上的价值就远远不足了。来自 Chubby 的实现的一条评论就能够说明：\n\n> Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。\n\n正因为存在这些问题，我们认为 Paxos 不仅对于系统的构建者来说不友好，同时也不利于教学。鉴于一致性算法对于大规模软件系统的重要性，我们决定试着来设计一种另外的比 Paxos 更好的一致性算法。Raft 就是这样的一个算法。\n\n# 4 易于理解的设计\n\n设计 Raft 的目标有如下几个：\n\n- 它必须提供一个完整的、实际的基础来进行系统构建，为的是减少开发者的工作；\n- 它必须在所有情况下都能保证安全可用；\n- 它对于常规操作必须高效；\n- 最重要的目标是：**易于理解**，它必须使得大多数人能够很容易的理解；\n- 另外，它必须能让开发者有一个直观的认识，这样才能使系统构建者们去对它进行扩展。\n\n在设计 Raft 的过程中，我们不得不在许多种方法中做出选择。当面临这种情况时，我们通常会权衡可理解性：每种方法的可理解性是如何的？（例如，它的状态空间有多复杂？它是不是有很细微的含义？）它的可读性如何？读者能不能轻易地理解这个方法和它的含义？\n\n我们意识到对这种可理解性的分析具有高度的主观性；尽管如此，我们使用了两种适用的方式。第一种是众所周知的问题分解：我们尽可能将问题分解成为若干个可解决的、可被理解的小问题。例如，在 Raft 中，我们把问题分解成为了**领导选取（leader election）**、**日志复制（log replication）**、**安全（safety）**和**成员变化（membership changes）**。\n\n我们采用的第二个方法是通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致并且尽可能消除不确定性。特别地，日志之间不允许出现空洞，并且 Raft 限制了限制了日志不一致的可能性。尽管在大多数情况下，我们都都在试图消除不确定性，但是有时候有些情况下，不确定性使得算法更易理解。尤其是，随机化方法使得不确定性增加，但是它减少了状态空间。我们使用随机化来简化了 Raft 中的领导选取算法。\n\n# 5 Raft 一致性算法\n\nRaft 是一种用来管理第 2 章中提到的复制日志的算法。表-2 为了方便参考是一个算法的总结版本，表-3 列举了算法中的关键性质；表格中的这些元素将会在这一章剩下的部分中分别进行讨论。\n\n**状态：**\n\n在所有服务器上持久存在的：（在响应远程过程调用 RPC 之前稳定存储的）\n\n| 名称        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| currentTerm | 服务器最后知道的任期号（从0开始递增）                        |\n| votedFor    | 在当前任期内收到选票的候选人 id（如果没有就为 null）         |\n| log[]       | 日志条目；每个条目包含状态机的要执行命令和从领导人处收到时的任期号 |\n\n在所有服务器上不稳定存在的：\n\n| 名称        | 描述                                              |\n| ----------- | ------------------------------------------------- |\n| commitIndex | 已知的被提交的最大日志条目的索引值（从0开始递增） |\n| lastApplied | 被状态机执行的最大日志条目的索引值（从0开始递增） |\n\n在领导人服务器上不稳定存在的：（在选举之后初始化的）\n\n| 名称         | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| nextIndex[]  | 对于每一个服务器，记录需要发给它的下一个日志条目的索引（初始化为领导人上一条日志的索引值+1） |\n| matchIndex[] | 对于每一个服务器，记录已经复制到该服务器的日志的最高索引值（从0开始递增） |\n\n表-2-i\n\n**附加日志远程过程调用 （AppendEntries RPC）**\n\n由领导人来调用复制日志（5.3节）；也会用作heartbeat\n\n| 参数         | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| term         | 领导人的任期号                                               |\n| leaderId     | 领导人的 id，为了其他服务器能重定向到客户端                  |\n| prevLogIndex | 最新日志之前的日志的索引值                                   |\n| prevLogTerm  | 最新日志之前的日志的领导人任期号                             |\n| entries[]    | 将要存储的日志条目（表示 heartbeat 时为空，有时会为了效率发送超过一条） |\n| leaderCommit | 领导人提交的日志条目索引值                                   |\n\n| 返回值  | 描述                                                         |\n| ------- | ------------------------------------------------------------ |\n| term    | 当前的任期号，用于领导人更新自己的任期号                     |\n| success | 如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 |\n\n**接受者需要实现：**\n\n1. 如果 `term < currentTerm`返回 false（5.1节）\n2. 如果在`prevLogIndex`处的日志的任期号与`prevLogTerm`不匹配时，返回 false（5.3节）\n3. 如果一条已经存在的日志与新的冲突（index 相同但是任期号 term 不同），则删除已经存在的日志和它之后所有的日志（5.3节）\n4. 添加任何在已有的日志中不存在的条目\n5. 如果`leaderCommit > commitIndex`，将`commitIndex`设置为`leaderCommit`和最新日志条目索引号中较小的一个\n\n表-2-ii\n\n**投票请求 RPC（RequestVote RPC）**\n\n由候选人发起收集选票（5.2节）\n\n| 参数         | 描述                           |\n| ------------ | ------------------------------ |\n| term         | 候选人的任期号                 |\n| candidateId  | 请求投票的候选人 id            |\n| lastLogIndex | 候选人最新日志条目的索引值     |\n| lastLogTerm  | 候选人最新日志条目对应的任期号 |\n\n| 返回值      | 描述                             |\n| ----------- | -------------------------------- |\n| term        | 目前的任期号，用于候选人更新自己 |\n| voteGranted | 如果候选人收到选票为 true        |\n\n**接受者需要实现：**\n\n1. 如果`term < currentTerm`返回 false（5.1节）\n2. 如果`votedFor`为空或者与`candidateId`相同，并且候选人的日志和自己的日志一样新，则给该候选人投票（5.2节 和 5.4节）\n\n表-2-iii\n\n**服务器需要遵守的规则：**\n\n所有服务器：\n\n- 如果`commitIndex > lastApplied`，`lastApplied`自增，将`log[lastApplied]`应用到状态机（5.3节）\n- 如果 RPC 的请求或者响应中包含一个 term T 大于 `currentTerm`，则`currentTerm`赋值为 T，并切换状态为追随者（Follower）（5.1节）\n\n追随者（followers）: 5.2节\n\n- 响应来自候选人和领导人的 RPC\n- 如果在超过选取领导人时间之前没有收到来自当前领导人的`AppendEntries RPC`或者没有收到候选人的投票请求，则自己转换状态为候选人\n\n候选人：5.2节\n\n- 转变为选举人之后开始选举：\n  - `currentTerm`自增\n  - 给自己投票\n  - 重置选举计时器\n  - 向其他服务器发送`RequestVote RPC`\n- 如果收到了来自大多数服务器的投票：成为领导人\n- 如果收到了来自新领导人的`AppendEntries RPC（heartbeat）`：转换状态为追随者\n- 如果选举超时：开始新一轮的选举\n\n领导人：\n\n- 一旦成为领导人：向其他所有服务器发送空的`AppendEntries RPC（heartbeat）`;在空闲时间重复发送以防止选举超时（5.2节）\n\n- 如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端（5.3节）\n\n- 对于一个追随者来说，如果上一次收到的日志索引大于将要收到的日志索引（nextIndex）：通过\n\n  ```\n  AppendEntries RPC\n  ```\n\n  将 nextIndex 之后的所有日志条目发送出去\n\n  - 如果发送成功：将该追随者的 `nextIndex`和`matchIndex`更新\n  - 如果由于日志不一致导致`AppendEntries RPC`失败：`nextIndex`递减并且重新发送（5.3节）\n\n- 如果存在一个满足`N > commitIndex`和`matchIndex[i] >= N`并且`log[N].term == currentTerm`的 N，则将`commitIndex`赋值为 N\n\n表-2-iv\n\n表-2：Raft 一致性算法的总结（不包括成员变化 membership changes 和日志压缩 log compaction）\n\n\n\n\n\n| 性质                                   | 描述                                                         |\n| -------------------------------------- | ------------------------------------------------------------ |\n| 选举安全原则（Election Safety）        | 一个任期（term）内最多允许有一个领导人被选上（5.2节）        |\n| 领导人只增加原则（Leader Append-Only） | 领导人永远不会覆盖或者删除自己的日志，它只会增加条目         |\n| 日志匹配原则（Log Matching）           | 如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同（5.3 节） |\n| 领导人完全原则（Leader Completeness)   | 如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中 |\n| 状态机安全原则（State Machine Safety） | 如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目（5.4.3节） |\n\n表-3：Raft 算法保证这些特性任何时刻都成立\n\n\n\n\n\nRaft 通过首先选出一个领导人来实现一致性，然后给予领导人完全管理复制日志（replicated log）的责任。领导人接收来自客户端的日志条目，并把它们复制到其他的服务器上，领带人还要告诉服务器们什么时候将日志条目应用到它们的状态机是安全的。通过选出领导人能够简化复制日志的管理工作。例如，领导人能够决定将新的日志条目放到哪，而并不需要和其他的服务器商议，数据流被简化成从领导人流向其他服务器。如果领导人宕机或者和其他服务器失去连接，就可以选取下一个领导人。\n\n通过选出领导人，Raft 将一致性问题分解成为三个相对独立的子问题：\n\n- **领导人选取（Leader election）：** 在一个领导人宕机之后必须要选取一个新的领导人（5.2节）\n- **日志复制（Log replication）：** 领导人必须从客户端接收日志然后复制到集群中的其他服务器，并且强制要求其他服务器的日志保持和自己相同\n- **安全性（Safety）：** Raft 的关键的安全特性是 表-3 中提到的状态机安全原则（State Machine Safety）:如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。5.4节阐述了 Raft 是如何保证这条原则的，解决方案涉及到一个对于选举机制另外的限制，这一部分会在 5.2节 中说明。\n\n在说明了一致性算法之后，本章会讨论有关可用性（availability）的问题和系统中时序（timing）的问题。\n\n## 5.1 Raft 基础\n\n一个 Raft 集群包括若干服务器；对于一个典型的 5 服务器集群，该集群能够容忍 2 台机器不能正常工作，而整个系统保持正常。在任意的时间，每一个服务器一定会处于以下三种状态中的一个：*领导人*、*候选人*、*追随者*。在正常情况下，只有一个服务器是领导人，剩下的服务器是追随者。追随者们是被动的：他们不会发送任何请求，只是响应来自领导人和候选人的请求。领导人来处理所有来自客户端的请求（如果一个客户端与追随者进行通信，追随者会将信息发送给领导人）。候选人是用来选取一个新的领导人的，这一部分会在 5.2节 进行阐释。图-4 阐述了这些状态，和它们之间的转换；它们的转换会在下边进行讨论。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg)\n\n图-4：服务器的状态。追随者只响应其他服务器的请求。如果追随者没有收到任何消息，它会成为一个候选人并且开始一次选举。收到大多数服务器投票的候选人会成为新的领导人。领导人在它们宕机之前会一直保持领导人的状态。\n\n\n\n\n\n[![img](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg)](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg)\n\n图-5：时间被分为一个个的任期（term），每一个任期的开始都是领导人选举。在成功选举之后，一个领导人会在任期内管理整个集群。如果选举失败，该任期就会因为没有领带人而结束。这个转变会在不同的时间的不同服务器上观察到。\n\n\n\n\n\n如 图-5 所示，Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），就像 5.2节 所描述的那样，一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最少要有一个领导人。\n\n不同的服务器可能会在任期内观察到多次不同的状态转换，在某些情况下，一台服务器可能看不到一次选举或者一个完整的任期。任期在 Raft 中充当逻辑时钟的角色，并且它们允许服务器检测过期的信息，比如过时的领导人。每一台服务器都存储着一个当前任期的数字，这个数字会单调的增加。当服务器之间进行通信时，会互相交换当前任期号；如果一台服务器的当前任期号比其它服务器的小，则更新为较大的任期号。如果一个候选人或者领导人意识到它的任期号过时了，它会立刻转换为追随者状态。如果一台服务器收到的请求的任期号是过时的，那么它会拒绝此次请求。\n\nRaft 中的服务器通过远程过程调用（RPC）来通信，基本的 Raft 一致性算法仅需要 2 种 RPC。RequestVote RPC 是候选人在选举过程中触发的（5.2节），AppendEntries RPC 是领导人触发的，为的是复制日志条目和提供一种心跳（heartbeat）机制（5.3节）。第7章加入了第三种 RPC 来在各个服务器之间传输快照（snapshot）。如果服务器没有及时收到 RPC 的响应，它们会重试，并且它们能够并行的发出 RPC 来获得最好的性能。\n\n## 5.2 领导人选取\n\nRaft 使用一种心跳机制（heartbeat）来触发领导人的选取。当服务器启动时，它们会初始化为追随者。一太服务器会一直保持追随者的状态只要它们能够收到来自领导人或者候选人的有效 RPC。领导人会向所有追随者周期性发送心跳（heartbeat，不带有任何日志条目的 AppendEntries RPC）来保证它们的领导人地位。如果一个追随者在一个周期内没有收到心跳信息，就叫做选举超时（election timeout）,然后它就会假定没有可用的领导人并且开始一次选举来选出一个新的领导人。\n\n为了开始选举，一个追随者会自增它的当前任期并且转换状态为候选人。然后，它会给自己投票并且给集群中的其他服务器发送 RequestVote RPC。一个候选人会一直处于该状态，直到下列三种情形之一发生：\n\n- 它赢得了选举；\n- 另一台服务器赢得了选举；\n- 一段时间后没有任何一台服务器赢得了选举\n\n这些情形会在下面的章节中分别讨论。\n\n一个候选人如果在一个任期内收到了来自集群中大多数服务器的投票就会赢得选举。在一个任期内，一台服务器最多能给一个候选人投票，按照先到先服务原则（first-come-first-served）（注意：在 5.4节 针对投票添加了一个额外的限制）。大多数原则使得在一个任期内最多有一个候选人能赢得选举（表-3 中提到的选举安全原则）。一旦有一个候选人赢得了选举，它就会成为领导人。然后它会像其他服务器发送心跳信息来建立自己的领导地位并且组织新的选举。\n\n当一个候选人等待别人的选票时，它有可能会收到来自其他服务器发来的声明其为领导人的 AppendEntries RPC。如果这个领导人的任期（包含在它的 RPC 中）比当前候选人的当前任期要大，则候选人认为该领导人合法，并且转换自己的状态为追随者。如果在这个 RPC 中的任期小于候选人的当前任期，则候选人会拒绝此次 RPC， 继续保持候选人状态。\n\n第三种情形是一个候选人既没有赢得选举也没有输掉选举：如果许多追随者在同一时刻都成为了候选人，选票会被分散，可能没有候选人能获得大多数的选票。当这种情形发生时，每一个候选人都会超时，并且通过自增任期号和发起另一轮 RequestVote RPC 来开始新的选举。然而，如果没有其它手段来分配选票的话，这种情形可能会无限的重复下去。\n\nRaft 使用随机的选举超时时间来确保第三种情形很少发生，并且能够快速解决。为了防止在一开始是选票就被瓜分，选举超时时间是在一个固定的间隔内随机选出来的（例如，150~300ms）。这种机制使得在大多数情况下只有一个服务器会率先超时，它会在其它服务器超时之前赢得选举并且向其它服务器发送心跳信息。同样的机制被用于选票一开始被瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，在超时进行下一次选举之前一直等待。这能够减小在新的选举中一开始选票就被瓜分的可能性。9.3节 展示了这种方法能够快速的选出一个领导人。\n\n选举是一个理解性引导我们设计替代算法的一个例子。最开始时，我们计划使用一种排名系统：给每一个候选人分配一个唯一的排名，用于在竞争的候选人之中选择领导人。如果一个候选人发现了另一个比它排名高的候选人，那么它会回到追随者的状态，这样排名高的候选人会很容易地赢得选举。但是我们发现这种方法在可用性方面有一点问题（一个低排名的服务器在高排名的服务器宕机后，需要等待超时才能再次成为候选人，但是如果它这么做的太快，它能重置选举领带人的过程）。我们对这个算法做了多次调整，但是每次调整后都会出现一些新的问题。最终我们认为随机重试的方法是更明确并且更易于理解的。\n\n## 5.3 日志复制\n\n一旦选出了领导人，它就开始接收客户端的请求。每一个客户端请求都包含一条需要被复制状态机（replicated state machine）执行的命令。领导人把这条命令作为新的日志条目加入到它的日志中去，然后并行的向其他服务器发起 AppendEntries RPC ，要求其它服务器复制这个条目。当这个条目被安全的复制之后（下面的部分会详细阐述），领导人会将这个条目应用到它的状态机中并且会向客户端返回执行结果。如果追随者崩溃了或者运行缓慢或者是网络丢包了，领导人会无限的重试 AppendEntries RPC（甚至在它向客户端响应之后）知道所有的追随者最终存储了所有的日志条目。\n\n\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg)\n\n图-6：日志由有序编号的日志条目组成。每个日志条目包含它被创建时的任期号（每个方块中的数字），并且包含用于状态机执行的命令。如果一个条目能够被状态机安全执行，就被认为可以提交了。\n\n\n\n\n\n日志就像 图-6 所示那样组织的。每个日志条目存储着一条被状态机执行的命令和当这条日志条目被领导人接收时的任期号。日志条目中的任期号用来检测在不同服务器上日志的不一致性，并且能确保 图-3 中的一些特性。每个日志条目也包含一个整数索引来表示它在日志中的位置。\n\n领导人决定什么时候将日志条目应用到状态机是安全的；这种条目被称为可被提交（commited）。Raft 保证可被提交（commited）的日志条目是持久化的并且最终会被所有可用的状态机执行。一旦被领导人创建的条目已经复制到了大多数的服务器上，这个条目就称为可被提交的（例如，图-6中的7号条目）。领导人日志中之前的条目都是可被提交的（commited），包括由之前的领导人创建的条目。5.4节将会讨论当领导人更替之后这条规则的应用问题的细节，并且也讨论了这种提交方式是安全的。领导人跟踪记录它所知道的被提交条目的最大索引值，并且这个索引值会包含在之后的 AppendEntries RPC 中（包括心跳 heartbeat 中），为的是让其他服务器都知道这条条目已经提交。一旦一个追随者知道了一个日志条目已经被提交，它会将该条目应用至本地的状态机（按照日志顺序）。\n\n我们设计了 Raft 日志机制来保证不同服务器上日志的一致性。这样做不仅简化了系统的行为使得它更可预测，并且也是保证安全性不可或缺的一部分。Raft 保证以下特性，并且也保证了 表-3 中的日志匹配原则（Log Matching Property）:\n\n- 如果在不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。\n- 如果在不同日志中的两个条目有着相同的索引和任期号，则它们之间的所有条目都是完全一样的。\n\n第一条特性源于领导人在一个任期里在给定的一个日志索引位置最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，领导人会把新日志条目紧接着之前的条目的索引位置和任期号都包含在里面。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg)\n\n图-7：当最上边的领导人掌权之后，追随者日志可能有以下情况（a~f）。一个格子表示一个日志条目；格子中的数字是它的任期。一个追随者可能会丢失一些条目（a, b）；可能多出来一些未提交的条目（c, d）；或者两种情况都有（e, f）。例如，场景 f 在如下情况下就会发生：如果一台服务器在任期2时是领导人并且往它的日志中添加了一些条目，然后在将它们提交之前就宕机了，之后它很快重启了，成为了任期3的领导人，又往它的日志中添加了一些条目，然后在任期2和任期3中的条目提交之前它又宕机了并且几个任期内都一直处于宕机状态。\n\n\n\n\n\n在一般情况下，领导人和追随者们的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，领导人的崩溃会导致日志不一致（旧的领导人可能没有完全复制完日志中的所有条目）。这些不一致会导致一系列领导人和追随者崩溃。图-7 阐述了一些追随者可能和新的领导人日志不同的情况。一个追随者可能会丢失掉领导人上的一些条目，也有可能包含一些领导人没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。\n\n在 Raft 算法中，领导人通过强制追随者们复制它的日志来处理日志的不一致。这就意味着，在追随者上的冲突日志会被领导者的日志覆盖。5.4节会说明当添加了一个额外的限制之后这是安全的。\n\n为了使得追随者的日志同自己的一致，领导人需要找到追随者同它的日志一致的地方，然后删除追随者在该位置之后的条目，然后将自己在该位置之后的条目发送给追随者。这些操作都在 AppendEntries RPC 进行一致性检查时完成。领导人给每一个追随者维护了一个`nextIndex`，它表示领导人将要发送给该追随者的下一条日志条目的索引。当一个领导人开始掌权时，它会将`nextIndex`初始化为它的最新的日志条目索引数+1（图-7 中的 11）。如果一个追随者的日志和领导者的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，领导人会将`nextIndex`递减然后重试 AppendEntries RPC。最终`nextIndex`会达到一个领导人和追随者日志一致的地方。这时，AppendEntries 会返回成功，追随者中冲突的日志条目都被移除了，并且添加所缺少的上了领导人的日志条目。一旦 AppendEntries 返回成功，追随者和领导人的日志就一致了，这样的状态会保持到该任期结束。\n\n如果需要的话，算法还可以进行优化来减少 AppendEntries RPC 失败的次数。例如，当拒绝了一个 AppendEntries 请求，追随者可以记录下冲突日志条目的任期号和自己存储那个任期的最早的索引。通过这些信息，领导人能够直接递减`nextIndex`跨过那个任期内所有的冲突条目；这样的话，一个冲突的任期需要一次 AppendEntries RPC，而不是每一个冲突条目需要一次 AppendEntries RPC。在实践中，我们怀疑这种优化是否是必要的，因为AppendEntries 一致性检查很少失败并且也不太可能出现大量的日志条目不一致的情况。\n\n通过这种机制，一个领导人在掌权时不需要采取另外特殊的方式来恢复日志的一致性。它只需要使用一些常规的操作，通过响应 AppendEntries 一致性检查的失败能使得日志自动的趋于一致。一个领导人从来不会覆盖或者删除自己的日志（表-3 中的领导人只增加原则）。\n\n这个日志复制机制展示了在第2章中阐述的所希望的一致性特性：Raft 能够接受，复制并且应用新的日志条目只要大部分的服务器是正常的。在通常情况下，一条新的日志条目可以在一轮 RPC 内完成在集群的大多数服务器上的复制；并且一个速度很慢的追随者并不会影响整体的性能。\n\n\\##5.4 安全性\n\n之前的章节中讨论了 Raft 算法是如何进行领导选取和复制日志的。然而，到目前为止这个机制还不能保证每一个状态机能按照相同的顺序执行同样的指令。例如，当领导人提交了若干日志条目的同时一个追随者可能宕机了，之后它又被选为了领导人然后用新的日志条目覆盖掉了旧的那些，最后，不同的状态机可能执行不同的命令序列。\n\n这一节通过在领带人选取部分加入了一个限制来完善了 Raft 算法。这个限制能够保证对于固定的任期，任何的领导人都拥有之前任期提交的全部日志条目（表-3 中的领导人完全原则）。有了这一限制，日志提交的规则就更清晰了。最后，我们提出了对于领导人完全原则的简单证明并且展示了它是如何修正复制状态机的行为的。\n\n\\###5.4.1 选举限制\n\n在所有的以领导人为基础的一致性算法中，领导人最终必须要存储全部已经提交的日志条目。在一些一致性算法中，例如：[Viewstamped Replication](http://people.csail.mit.edu/cowling/vr/vr-revisited.pdf)，即使一开始没有包含全部已提交的条目也可以被选为领导人。这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，这个过程要么在选举过程中完成，要么在选举之后立即开始。不幸的是，这种方式大大增加了复杂性。Raft 使用了一种更简单的方式来保证在新的领导人开始选举的时候在之前任期的所有已提交的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。\n\nRaft 使用投票的方式来阻止没有包含全部日志条目的服务器赢得选举。一个候选人为了赢得选举必须要和集群中的大多数进行通信，这就意味着每一条已经提交的日志条目最少在其中一台服务器上出现。如果候选人的日志至少和大多数服务器上的日志一样新（up-to-date，这个概念会在下边有详细介绍），那么它一定包含有全部的已经提交的日志条目。RequestVote RPC 实现了这个限制：这个 RPC（远程过程调用）包括候选人的日志信息，如果它自己的日志比候选人的日志要新，那么它会拒绝候选人的投票请求。\n\nRaft 通过比较日志中最后一个条目的索引和任期号来决定两个日志哪一个更新。如果两个日志的任期号不同，任期号大的更新；如果任期号相同，更长的日志更新。\n\n\\###5.4.2 提交之前任期的日志条目\n\n[![img](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg)](http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg)\n\n图-8：如图的时间序列说明了为什么领导人不能通过之前任期的日志条目判断它的提交状态。（a）中的 S1 是领导人并且部分复制了索引2上的日志条目。（b）中 S1 崩溃了；S5 通过 S3，S4 和自己的选票赢得了选举，并且在索引2上接收了另一条日志条目。（c）中 S5 崩溃了，S1 重启了，通过 S2，S3 和自己的选票赢得了选举，并且继续索引2处的复制，这时任期2的日志条目已经在大部分服务器上完成了复制，但是还并没有提交。如果在（d）时刻 S1 崩溃了，S5 会通过 S2，S3，S4 的选票成为领导人，然后用它自己在任期3的日志条目覆盖掉其他服务器的日志条目。然而，如果在崩溃之前，S1 在它的当前任期在大多数服务器上复制了一条日志条目，就像在（e）中那样，那么这条条目就会被提交（S5就不会赢得选举）。在这时，之前的日志条目就会正常被提交。\n\n\n\n\n\n正如 5.3节 中描述的那样，只要一个日志条目被存在了在多数的服务器上，领导人就知道当前任期就可以提交该条目了。如果领导人在提交之前就崩溃了，之后的领导人会试着继续完成对日志的复制。然而，领导人并不能断定存储在大多数服务器上的日志条目一定在之前的任期中被提交了。图-8 说明了一种情况，一条存储在了大多数服务器上的日志条目仍然被新上任的领导人覆盖了。\n\n为了消除 图-8 中描述的问题，Raft 从来不会通过计算复制的数目来提交之前人气的日志条目。只有领导人当前任期的日志条目才能通过计算数目来进行提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配原则（Log Matching Property），之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，通过观察该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。\n\n因为当领导人从之前任期复制日志条目时日志条目保留了它们最开始的任期号，所以这使得 Raft 在提交规则中增加了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要从之前的任期中复制日志条目，它必须要使用当前的新任期号。Raft 的方法使得判断日志更加容易，因为它们全程都保持着同样的任期号。另外，和其它的一致性算法相比，Raft 算法中的新领导人会发送更少的之前任期的日志条目（其他算法必须要发送冗余的日志条目并且在它们被提交之前来重新排序）。\n\n### 5.4.3 安全性论证\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg)\n\n图-9：如果 S1（任期 T 的领导人）在它的任期提交了一条日志条目，并且 S5 在之后的任期 U 成为了领导人，那么最少会有一台服务器（S3）接收了这条日志条目并且会给 S5 投票。\n\n\n\n\n\n给出了完整的 Raft 算法，现在我们能够更精确的论证领导人完全原则（Leader Completeness)（这基于 9.2节 提出的安全性证明）。我们假定领导人完全原则是不成立的，然后推导出矛盾。假定任期 T 的领导人 leaderT在它的任期提交了一个日志条目，但是这条日志条目并没有存储在之后的任期中的领导人上。我们设大于 T 的最小的任期 U 的领导人（leaderU） 没有存储这条日志条目。\n\n1. 在 leaderU 选举时一定没有那条被提交的日志条目（领导人从来不会删除或者覆盖日志条目）。\n2. leaderT 复制了这个条目到集群的大多数的服务器上。因此，只是有一台服务器（投票者）即接收了来自 leaderT 的日志条目并且给 leaderU 投票，就像 图-9 中所示那样。这个投票者是产生矛盾的关键。\n3. 投票者必须在给 leaderU 投票之前接收来自 leaderT 的日志条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（它的当前任期会比 T 要大）。\n4. 投票者会在它给 leaderU 投票时存储那个条目，因为任何中间的领导人都保有该条目（基于假设），领导人从来不会移除这个条目，并且追随者也只会在和领导人冲突时才会移除日志条目。\n5. 投票者给 leaderU 投票了，所以 leaderU 的日志必须和投票者的一样新。这就导致了一个矛盾。\n6. 首先，如果投票者和 leaderU 最后一条日志条目的任期号相同，那么 leaderU 的日志一定和投票者的一样长，因此它的日志包含全部投票者的日志条目。这是矛盾的，因为在假设中投票者和 leaderU 包含的已提交条目是不同的。\n7. 除此之外， leaderU 的最后一条日志的任期号一定比投票者的大。另外，它也比 T 要大，因为投票者的最后一条日志条目的任期号最小也要是 T（它包含了所有任期 T 提交的日志条目）。创建 leaderU 最后一条日志条目的上一任领导人必须包含已经提交的日志条目（基于假设）。那么，根据日志匹配原则（Log Matching），leaderU 也一定包含那条提交的日志条目，这也是矛盾的。\n8. 这时就完成了矛盾推导。因此，所有比任期 T 大的领导人一定包含所有在任期 T 提交的日志条目。\n9. 日志匹配原则（Log Matching）保证了未来的领导人也会包含被间接提交的日志条目，就像 图-8 中（d）时刻索引为2的条目。\n\n通过给出了 领导人完全原则（Leader Completeness)，我们能够证明 表-3 中的状态机安全原则（State Machine Safety），状态机安全原则（State Machine Safety）讲的是如果一台服务器将给定索引上的日志条目应用到了它自己的状态机上，其它服务器的同一索引位置不可能应用的是其它条目。在一个服务器应用一条日志条目到它自己的状态机中时，它的日志必须和领导人的日志在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性（Log Completeness Property）保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。\n\n最后，Raft 算法需要服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。\n\n\\##5.5 追随者和候选人崩溃\n\n截止到目前，我们只讨论了领导人崩溃的问题。追随者和候选人崩溃的问题解决起来要比领导人崩溃要简单得多，这两者崩溃的处理方式是一样的。如果一个追随者或者候选人崩溃了，那么之后的发送给它的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限的重试来处理这些失败；如果崩溃的服务器重启了，RPC 就会成功完成。如果一个服务器在收到了 RPC 之后但是在响应之前崩溃了，那么它会在重启之后再次收到同一个 RPC。因为 Raft 中的 RPC 都是幂等的，因此不会有什么问题。例如，如果一个追随者收到了一个已经包含在它的日志中的 AppendEntries 请求，它会忽视这个新的请求。\n\n\\##5.6 时序和可用性\n\n我们对于 Raft 的要求之一就是安全性不依赖于时序（timing）：系统不能仅仅因为一些事件发生的比预想的快一些或慢一些就产生错误。然而，可用性（系统可以及时响应客户端的特性）不可避免的要依赖时序。例如，如果消息交换在服务器崩溃时花费更多的时间，候选人不会等待太长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。\n\n领导人选取是 Raft 中对时序要求最关键的地方。Raft 会选出并且保持一个稳定的领导人只有系统满足下列时序要求（timing requirement）：\n\nbroadcastTime << electionTimeout << MTBF\n\n在这个不等式中，`broadcastTime`指的是一台服务器并行的向集群中的其他服务器发送 RPC 并且收到它们的响应的平均时间；`electionTimeout`指的就是在 5.2节 描述的选举超时时间；`MTBF`指的是单个服务器发生故障的间隔时间的平均数。`broadcastTime`应该比`electionTimeout`小一个数量级，为的是使领导人能够持续发送心跳信息（heartbeat）来阻止追随者们开始选举；根据已经给出的随机化选举超时时间方法，这个不等式也使得瓜分选票的情况变成不可能。`electionTimeout`也要比`MTBF`小几个数量级，为的是使得系统稳定运行。当领导人崩溃时，整个大约会在`electionTimeout`的时间内不可用；我们希望这种情况仅占全部时间的很小的一部分。\n\n`broadcastTime`和`MTBF`是由系统决定的性质，但是`electionTimeout`是我们必须做出选择的。Raft 的 RPC 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，这取决于存储的技术。因此，`electionTimeout`一般在 10ms 到 500ms 之间。大多数的服务器的`MTBF`都在几个月甚至更长，很容易满足这个时序需求。\n\n\\#6 集群成员变化\n\n截止到目前，我们都假定集群的配置（加入到一致性算法的服务器集合）是固定的。在实际中，我们会经常更改配置，例如，替换掉那些崩溃的机器或者更改复制级别。虽然通过关闭整个集群，升级配置文件，然后重启整个集群也可以解决这个问题，但是这回导致在更改配置的过程中，整个集群不可用。另外，如果存在需要手工操作，那么就会有操作失误的风险。为了避免这些问题，我们决定采用自动改变配置并且把这部分加入到了 Raft 一致性算法中。\n\n为了让配置修改机制能够安全，那么在转换的过程中在任何时间点两个领导人不能再同一个任期被同时选为领导人。不幸的是，服务器集群从旧的配置直接升级到新的配置的任何方法都是不安全的，一次性自动的转换所有服务器是不可能的，所以集群可以在转换的过程中划分成两个单独的组（如 图-10 所示）。\n\n\n\n[![img](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg)](http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg)\n\n图-10：从一个配置切换到另一个配置是不安全的因为不同的服务器会在不同的时间点进行切换。在这个例子中，集群数量从三台转换成五台。不幸的是，在一个时间点有两个服务器能被选举成为领导人，一个是在使用旧的配置的机器中（Cold）选出的领导人，另一个领导人是通过新的配置（Cnew）选出来的。\n\n\n\n\n\n为了保证安全性，集群配置的调整必须使用两阶段（two-phase）方法。有许多种实现两阶段方法的实现。例如，一些系统在第一个阶段先把旧的配置设为无效使得它无法处理客户端请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡配置，我们称其为共同一致（joint consensus）；一旦共同一致被提交了，然后系统再切换到新的配置。共同一致是旧的配置和新的配置的组合：\n\n- 日志条目被复制给集群中新、老配置的所有服务器。\n- 新、老配置的服务器都能成为领导人。\n- 需要分别在两种配置上获得大多数的支持才能达成一致（针对选举和提交）\n\n共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然能够响应服务器请求。\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg)\n\n图-11：集群配置变更的时间线。虚线表示的是已经被创建但是还没提交的配置条目，实线表示的是最新提交的配置条目。领导人首先在它的日志中创建 Cold,new配置条目并且将它提交到Cold,new（使用旧配置的大部分服务器和使用新配置的大部分服务器）。然后创建它创建Cnew配置条目并且将它提交到使用新配置的大部分机器上。这样就不存在Cold和Cnew能够分别同时做出决定的时刻。\n\n\n\n\n\n集群配置在复制日志中用特殊的日志条目来存储和通信；图-11 展示了配置变更的过程。当一个领导人接收到一个改变配置 Cold 为 Cnew 的请求，它会为了共同一致以前面描述的日志条目和副本的形式将配置存储起来（图中的 Cold,new）。一旦一个服务器将新的配置日志条目增加到它的日志中，它就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论它是否已经被提交）。这意味着领导人要使用 Cold,new 的规则来决定日志条目 Cold,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 Cold 配置也可能是 Cold,new 配置，这取决于赢得选举的候选人是否已经接收到了 Cold,new 配置。在任何情况下， Cnew 配置在这一时期都不会单方面的做出决定。\n\n一旦 Cold,new 被提交，那么无论是 Cold 还是 Cnew，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 Cnew 配置的日志条目并复制给集群就是安全的了。另外，每个服务器在收到新的配置的时候就会立即生效。当新的配置在 Cnew 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如 图-11，Cold 和 Cnew 没有任何机会同时做出单方面的决定；这就保证了安全性。\n\n针对重新配置提出了三个问题。第一个问题是一开始的时候新的服务器可能没有任何日志条目。如果它们在这个状态下加入到集群中，那么它们需要一段时间来更新追赶，在这个阶段它们还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群中来（领导人复制日志给他们，但是不把它们考虑到大多数中）。一旦新的服务器追赶上了集群中的其它机器，重新配置可以像上面描述的一样处理。\n\n第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 Cnew 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括自己；它复制日志但是不把它自己看作是大多数之一。当 Cnew 被提交时，会发生领导人过渡，因为这时是新的配置可以独立工作的最早的时间点（总是能够在 Cnew 配置下选出新的领导人）。在此之前，可能只能从 Cold 中选出领导人。\n\n第三个问题是，移除不在 Cnew 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳（heartbeat），所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新的任期号的 RequestVote RPC，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。\n\n为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略 RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个 RequestVote RPC，它不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么它就不会被更大的任期号废除。\n\n\\#7 日志压缩\n\nRaft 产生的日志在持续的正常操作中不断增长，但是在实际的系统中，它不会无限的增长下去。随着日志的不断增长，它会占据越来越多的空间并且花费更多的时间重置。如果没有一个机制使得它能够废弃在日志中不断累积的过时的信息就会引起可用性问题。\n\n快照（snapshot）是最简单的压缩方式。在快照中，全部的当前系统状态都被写入到快照中，存储到持久化的存储中，然后在那个时刻之前的全部日志都可以被丢弃。在 Chubby 和 ZooKeeper 中都使用了快照技术，这一章的剩下的部分会介绍 Raft 中使用的快照技术。\n\n增量压缩（incremental approaches）的方法，例如日志清理（log cleaning）或者日志结构合并树（log-structured merge trees），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以使用和快照相同的接口来实现 LSM tree ，但是日志清除方法就需要修改 Raft 了。\n\n\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg)\n\n图-12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。\n\n\n\n\n\n图-12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也将一些少量的元数据包含到快照中：最后被包含的索引（last included index）指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期（last included term）指的是该条目的任期号。保留这些数据是为了支持快照前的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 章），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。\n\n尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 章）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给它们。\n\n**安装快照 RPC（InstallSnapshot RPC）**\n\n在领导人发送快照给跟随者时使用调用。领导人总是按顺序发送。\n\n| 参数              | 描述                             |\n| ----------------- | -------------------------------- |\n| term              | 领导人的任期                     |\n| leaderId          | 为了追随者能重定向到客户端       |\n| lastIncludedIndex | 快照中包含的最后日志条目的索引值 |\n| lastIncludedTerm  | 快照中包含的最后日志条目的任期号 |\n| offset            | 分块在快照中的偏移量             |\n| data[]            | 快照块的原始数据                 |\n| done              | 如果是最后一块数据则为真         |\n\n| 返回值 | 描述                            |\n| ------ | ------------------------------- |\n| term   | currentTerm，用于领导人更新自己 |\n\n接受者需要实现：\n\n1. 如果`term < currentTerm`立刻回复\n2. 如果是第一个分块（offset 为 0）则创建新的快照\n3. 在指定的偏移量写入数据\n4. 如果 `done`为 false，则回复并继续等待之后的数据\n5. 保存快照文件，丢弃所有存在的或者部分有着更小索引号的快照\n6. 如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保留并且回复\n7. 丢弃全部日志\n8. 能够使用快照来恢复状态机（并且装载快照中的集群配置）\n\n表-13：InstallSnapshot RPC 的总结。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生存的信号，所以跟随者可以重置选举超时计时器。\n\n\n\n\n\n在这种情况下领导人使用一种叫做安装快照（InstallSnapshot）的新的 RPC 来发送快照给太落后的跟随者；见 表-13。当跟随者通过这种 RPC 接收到快照时，它必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃它所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须是正确的和并且被保留下来。\n\n这种快照的方式背离了 Raft 的强领导人原则（strong leader principle），因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织它们的数据了。\n\n我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。\n\n还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，它就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。\n\n第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制（copy-on-write）的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。\n\n\\#8 客户端交互\n\n这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端是如何发现领导人的和 Raft 是如何支持线性化语义（linearizable semantics）的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。\n\nRaft 中的客户端将所有请求发送给领导人。当客户端启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供它最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。\n\n我们 Raft 的目标是要实现线性化语义（linearizable semantics）（每一次操作立即执行，在它调用和收到回复之间只执行一次）。但是，如上述所说，Raft 是可以多次执行同一条命令的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。\n\n只读（read-only）的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回过期数据(stale data)的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是它还不知道。线性化的读操作必须不能返回过期数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全原则（Leader Completeness Property）保证了领导人一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来进行实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废除了（如果一个更新的领导人被选举出来，它自己的信息就已经过期了）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳（heartbeat）信息来处理这个问题。另外，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时序来保证安全性（它假设时间误差是有界的）。\n\n\\#9 实现和评价\n\n我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。\n\n这一章会从三个方面来评估 Raft 算法：可理解性、正确性和性能。\n\n\\##9.1 可理解性\n\n为了比较 Paxos 和 Raft 算法的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文除了日志压缩之外的所有内容；Paxos 课程包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者独立的区别从第一个算法处学来的经验。我们计算参加人员的每一个小测验的得分来看参与者是否对 Raft 的理解更好。\n\n| 因素           | 消除偏见的手段                                               | 复习材料 |\n| -------------- | ------------------------------------------------------------ | -------- |\n| 相同的讲课质量 | 使用相同的讲师。Paxos 的讲义是基于之前在几所大学中使用的材料的并且做了改进。Paxos 的讲义要长 14% | 视频     |\n| 相同的测试难度 | 用难度给问题分组，在测试中成对出现                           | 测验     |\n| 公平的打分     | 使用红字标题。随机顺序打分，两个测验交替进行。               | 红字标题 |\n\n表-1：考虑到的可能造成偏见的因素，以及解决方案和对应的复习材料\n\n我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表-1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。\n\n\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg)\n\n图-14：表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩的散点图。在对角线之上的点表示在 Raft 获得了更高分数的学生。\n\n\n\n\n\n参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图-14 展示了每个参与者的得分。一对 t -测试表明，拥有 95% 的可信度，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。\n\n我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型显示，对小测验的选择会产生 12.5 分的差别在对 Raft 的好感度上。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft 的小测验得分会比 Paxos 低 6.3 分；我们不知道为什么，但这在统计学上是这样的。\n\n\n\n[![img](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg)](http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg)\n\n图-15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。\n\n\n\n\n\n我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图-15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。\n\n关于 Raft 用户学习有一个更加详细的讨论，详见[http://ramcloud.stanford.edu/ ̃ongaro/thesis.pdf](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)\n\n\\##9.2 正确性\n\n在第5章，我们已经进行了一个[正式的说明](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)，和对一致性机制的安全性证明。这个正式说明通过 [TLA+](https://www.amazon.com/Specifying-Systems-Language-Hardware-Engineers/dp/032114306X) 让 表-2 中的信息非常清晰。它大约有 400 行并且充当了证明的主题。同时对于任何想实现的人也是十分有用的。我们非常机械的通过 TLA 证明系统证明了日志完全特性（Log Completeness Property）。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明这个说明中的类型安全 type safety）。而且，我们已经写了一个[非正式的证明](http://ramcloud.stanford.edu/~ongaro/thesis.pdf)关于状态机安全性质是完备的，并且是相当清晰的（大约 3500 个词）。\n\n\\##9.3 性能\n\nRaft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。\n\n我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答以下两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？\n\n\n\n[![img](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg)](http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg)\n\n\n\n图-16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。\n\n\n\n为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图-16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。\n\n图-16 上面的图表表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程由于太多的选票瓜分的情况往往都需要花费超过 10 秒钟。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。\n\n图-16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。\n\n\\#10 相关工作\n\n已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：\n\n- Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰的论文。\n- 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。\n- 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 有着很大的差别。\n- Paxos 可以应用的性能优化。\n- Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。\n\nRaft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。\n\n像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。\n\n和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。\n\nRaft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。\n\n一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致（joint consensus）的方法因为它对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Raft 没有采用 Lamport 的基于 α 的方法是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较而言，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。和 VR、SMART 比较而言，Raft 的方法同时需要更少的额外机制来实现。\n\n\\#11 总结\n\n算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。\n\n在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。\n\n\\#12 鸣谢\n\n这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。\n\n\\#引用\n\n1. BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154.\n2. BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350.\n3. CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317.\n4. CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407.\n5. CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218.\n6. CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264.\n7. COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154.\n8. GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43.\n9. GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210.\n10. HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492.\n11. HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158.\n12. JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems & Networks (2011), IEEE Computer Society, pp. 245–256.\n13. KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.\n14. LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565.\n15. LAMPORT, L. The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169.\n16. LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25.\n17. LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002.\n18. LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.\n19. LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103.\n20. LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.\n21. LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13.\n22. LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.\n23. LogCabin source code. logcabin/logcabin.\n24. LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115.\n25. MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384.\n26. MAZIE` RES, D. Paxos made practical.<http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf> , Jan. 2007.\n27. MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM.\n28. Raft user study. <http://ramcloud.stanford.edu/~ongaro/userstudy/>.\n29. OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17.\n30. O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385.\n31. ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress).<http://ramcloud.stanford.edu/~ongaro/thesis.pdf>\n32. ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX.\n33. OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130.\n34. Raft consensus algorithm website. [http://raftconsensus.github.io](http://raftconsensus.github.io/).\n35. REED, B. Personal communications, May 17, 2013.\n36. ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52.\n37. SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319.\n38. SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10.\n39. VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.https://github.com/iissnan/hexo-theme-next)","slug":"Raft 一致性算法论文译文","published":1,"updated":"2019-05-21T14:36:57.656Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjw6dubsm001namumxh9ow6lr","content":"<h1 id=\"Raft-一致性算法论文译文\"><a href=\"#Raft-一致性算法论文译文\" class=\"headerlink\" title=\"Raft 一致性算法论文译文\"></a>Raft 一致性算法论文译文</h1><p>Raft论文翻译转载自<a href=\"http://blog.luoyuanhang.com/2017/02/02/raft-paper-in-zh-CN/\" target=\"_blank\" rel=\"noopener\">Raft 一致性算法论文译文</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>Raft 是一种用来管理日志复制的一致性算法。它和 Paxos 的性能和功能是一样的，但是它和 Paxos 的结构不一样；这使得 Raft 更容易理解并且更易于建立实际的系统。为了提高理解性，Raft 将一致性算法分为了几个部分，例如领导选取（leader selection），日志复制（log replication）和安全性（safety），同时它使用了更强的一致性来减少了必须需要考虑的状态。从用户学习的结果来看，Raft 比 Paxos 更容易学会。Raft 还包括了一种新的机制来使得动态改变集群成员，它使用重叠大多数（overlapping majorities）来保证安全。</p>\n<h1 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h1><p>一致性算法允许一组机器像一个整体一样工作，即使其中的一些机器出了错误也能正常工作。正因为此，他们扮演着建立大规模可靠的软件系统的关键角色。在过去的十年中 Paxos 一直都主导着有关一致性算法的讨论：大多数一致性算法的实现都基于它或者受它影响，并且 Paxos 也成为了教学生关于一致性知识的主要工具。</p>\n<p>不幸的是，尽管在降低它的复杂性方面做了许多努力，Paxos 依旧很难理解。并且，Paxos 需要经过复杂的修改才能应用于实际中。这些导致了系统构构建者和学生都十分头疼。</p>\n<p>在被 Paxos 折磨之后，我们开始寻找一种在系统构建和教学上更好的新的一致性算法。我们的首要目标是让它易于理解：我们能不能定义一种面向实际系统的一致性算法并且比 Paxos 更容易学习呢？并且，我们希望这种算法能凭直觉就能明白，这对于一个系统构建者来说是十分必要的。对于一个算法，不仅仅是让它工作起来很重要，知道它是如何工作的更重要。</p>\n<p>我们工作的结果是一种新的一致性算法，叫做 Raft。在设计 Raft 的过程中我们应用了许多专门的技巧来提升理解性，包括算法分解（分为领导选取（leader selection），日志复制（log replication）和安全性（safety））和减少状态（state space reduction）（相对于 Paxos，Raft 减少了非确定性的程度和服务器互相不一致的方式）。在两所学校的43个学生的研究中发现，Raft 比 Paxos 要更容易理解：在学习了两种算法之后，其中的33个学生回答 Raft 的问题要比回答 Paxos 的问题要好。</p>\n<p>Raft 算法和现在一些已经有的算法在一些地方很相似（主要是 <a href=\"http://www.pmg.csail.mit.edu/papers/vr.pdf\" target=\"_blank\" rel=\"noopener\">Oki 和 Liskov 的 Viewstamped Replication</a>。但是 Raft 有几个新的特性：</p>\n<ul>\n<li>强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。</li>\n<li>领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。</li>\n<li>成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。</li>\n</ul>\n<p>我们认为，Raft 在教学方面和实际实现方面比 Paxos 和其他算法更出众。它比其他算法更简单、更容易理解；它能满足一个实际系统的需求；它拥有许多开源的实现并且被许多公司所使用；它的安全特性已经被证明；并且它的效率和其他算法相比也具有竞争力。</p>\n<p>这篇论文剩下的部分会讲如下内容：复制状态机（replicated state machine）问题（第2节），讨论 Paxos 的优缺点（第3节），讨论我们用的为了达到提升理解性的方法（第4节），陈述 Raft 一致性算法（第5~8节），评价 Raft 算法（第9节），对相关工作的讨论（第10节）。</p>\n<a id=\"more\"></a>\n<h1 id=\"2-复制状态机（Replicated-State-Machine）\"><a href=\"#2-复制状态机（Replicated-State-Machine）\" class=\"headerlink\" title=\"2 复制状态机（Replicated State Machine）\"></a>2 复制状态机（Replicated State Machine）</h1><p>一致性算法是在<a href=\"https://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf\" target=\"_blank\" rel=\"noopener\">复制状态机</a>的背景下提出来的。在这个方法中，在一组服务器的状态机产生同样的状态的副本因此即使有一些服务器崩溃了这组服务器也还能继续执行。复制状态机在分布式系统中被用于解决许多有关容错的问题。例如，GFS，HDFS还有 RAMCloud 这些大规模的系统都是用一个单独的集群领导者，使用一个单独的复制状态机来进行领导选取和存储配置信息来应对领导者的崩溃。使用复制状态机的例子有 Chubby 和 ZooKeeper。</p>\n<p><a href=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg\" alt=\"img\"></a></p>\n<p>图-1：复制状态机的架构。一致性算法管理来自客户端状态命令的复制日志。状态机处理的日志中的命令的顺序都是一致的，因此会得到相同的执行结果。</p>\n<p>如图-1所示，复制状态机是通过复制日志来实现的。每一台服务器保存着一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的，所以每个状态机的状态都是相同的，执行的命令是相同的，最后的执行结果也就是一样的了。</p>\n<p>如何保证复制日志一致就是一致性算法的工作了。在一台服务器上，一致性模块接受客户端的命令并且把命令加入到它的日志中。它和其他服务器上的一致性模块进行通信来确保每一个日志最终包含相同序列的请求，即使有一些服务器宕机了。一旦这些命令被正确的复制了，每一个服务器的状态机都会按同样的顺序去执行它们，然后将结果返回给客户端。最终，这些服务器看起来就像一台可靠的状态机。</p>\n<p>应用于实际系统的一致性算法一般有以下特性：</p>\n<ul>\n<li>确保安全性（从来不会返回一个错误的结果），即使在所有的非拜占庭（Non-Byzantine）情况下，包括网络延迟、分区、丢包、冗余和乱序的情况下。</li>\n<li>高可用性，只要集群中的大部分机器都能运行，可以互相通信并且可以和客户端通信，这个集群就可用。因此，一般来说，一个拥有 5 台机器的集群可以容忍其中的 2 台的失败（fail）。服务器停止工作了我们就认为它失败（fail）了，没准一会当它们拥有稳定的存储时就能从中恢复过来，重新加入到集群中。</li>\n<li>不依赖时序保证一致性，时钟错误和极端情况下的消息延迟在最坏的情况下才会引起可用性问题。</li>\n<li>通常情况下，一条命令能够尽可能快的在大多数节点对一轮远程调用作出相应时完成，一少部分慢的机器不会影响系统的整体性能。</li>\n</ul>\n<h1 id=\"3-Paxos-算法的不足\"><a href=\"#3-Paxos-算法的不足\" class=\"headerlink\" title=\"3 Paxos 算法的不足\"></a>3 Paxos 算法的不足</h1><p>在过去的10年中，Leslie Lamport 的 Paxos 算法几乎已经成为了一致性算法的代名词：它是授课中最常见的算法，同时也是许多一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，例如一个单一复制日志条目（single replicated log entry）。我们把这个子集叫做单一决策 Paxos（single-decree Paxos）。之后 Paxos通过组合多个这种协议来完成一系列的决策，例如一个日志（multi-Paxos）。Paxos 确保安全性和活跃性（liveness），并且它支持集群成员的变更。它的正确性已经被证明，通常情况下也很高效。</p>\n<p>不幸的是，Paxos 有两个致命的缺点。第一个是 Paxos 太难以理解。它的完整的解释晦涩难懂；很少有人能完全理解，只有少数人成功的读懂了它。并且大家做了许多努力来用一些简单的术语来描述它。尽管这些解释都关注于单一决策子集问题，但仍具有挑战性。在 NSDI 2012 会议上的一次非正式调查显示，我们发现大家对 Paxos 都感到不满意，其中甚至包括一些有经验的研究员。我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。</p>\n<p>我们假定 Paxos 的晦涩来源于它将单决策子集作为它的基础。单决策（Single-decree）Paxos 是晦涩且微妙的：它被划分为两个没有简单直观解释的阶段，并且难以独立理解。正因为如此，它不能很直观的让我们知道为什么单一决策协议能够工作。为多决策 Paxos 设计的规则又添加了额外的复杂性和精巧性。我们相信多决策问题能够分解为其它更直观的方式。</p>\n<p>Paxos 的第二个缺点是它难以在实际环境中实现。其中一个原因是，对于多决策 Paxos （multi-Paxos） ，大家还没有一个一致同意的算法。Lamport 的描述大部分都是有关于单决策 Paxos （single-decree Paxos）；他仅仅描述了实现多决策的可能的方法，缺少许多细节。有许多实现 Paxos 和优化 Paxos 的尝试，但是他们都和 Lamport 的描述有些出入。例如，Chubby 实现的是一个类似 Paxos 的算法，但是在许多情况下的细节没有公开。</p>\n<p>另外，Paxos 的结构也是不容易在一个实际系统中进行实现的，这是单决策问题分解带来的又一个问题。例如，从许多日志条目中选出条目然后把它们融合到一个序列化的日志中并没有带来什么好处，它仅仅增加了复杂性。围绕着日志来设计一个系统是更简单、更高效的：新日志按照严格的顺序添加到日志中去。另一个问题是，Paxos 使用对等的点对点的实现作为它的核心（尽管它最终提出了一种弱领导者的形式来优化性能）。这种方法在只有一个决策被制定的情况下才显得有效，但是很少有现实中的系统使用它。如果要做许多的决策，选择一个领导人，由领带人来协调是更简单有效的方法。</p>\n<p>因此，在实际的系统应用中和 Paxos 算法都相差很大。所有开始于 Paxos 的实现都会遇到很多问题，然后由此衍生出了许多与 Paxos 有很大不同的架构。这是既费时又容易出错的，并且理解 Paxos 的难度又非常大。Paxos 算法在它正确性的理论证明上是很好的，但是在实现上的价值就远远不足了。来自 Chubby 的实现的一条评论就能够说明：</p>\n<blockquote>\n<p>Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。</p>\n</blockquote>\n<p>正因为存在这些问题，我们认为 Paxos 不仅对于系统的构建者来说不友好，同时也不利于教学。鉴于一致性算法对于大规模软件系统的重要性，我们决定试着来设计一种另外的比 Paxos 更好的一致性算法。Raft 就是这样的一个算法。</p>\n<h1 id=\"4-易于理解的设计\"><a href=\"#4-易于理解的设计\" class=\"headerlink\" title=\"4 易于理解的设计\"></a>4 易于理解的设计</h1><p>设计 Raft 的目标有如下几个：</p>\n<ul>\n<li>它必须提供一个完整的、实际的基础来进行系统构建，为的是减少开发者的工作；</li>\n<li>它必须在所有情况下都能保证安全可用；</li>\n<li>它对于常规操作必须高效；</li>\n<li>最重要的目标是：<strong>易于理解</strong>，它必须使得大多数人能够很容易的理解；</li>\n<li>另外，它必须能让开发者有一个直观的认识，这样才能使系统构建者们去对它进行扩展。</li>\n</ul>\n<p>在设计 Raft 的过程中，我们不得不在许多种方法中做出选择。当面临这种情况时，我们通常会权衡可理解性：每种方法的可理解性是如何的？（例如，它的状态空间有多复杂？它是不是有很细微的含义？）它的可读性如何？读者能不能轻易地理解这个方法和它的含义？</p>\n<p>我们意识到对这种可理解性的分析具有高度的主观性；尽管如此，我们使用了两种适用的方式。第一种是众所周知的问题分解：我们尽可能将问题分解成为若干个可解决的、可被理解的小问题。例如，在 Raft 中，我们把问题分解成为了<strong>领导选取（leader election）</strong>、<strong>日志复制（log replication）</strong>、<strong>安全（safety）</strong>和<strong>成员变化（membership changes）</strong>。</p>\n<p>我们采用的第二个方法是通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致并且尽可能消除不确定性。特别地，日志之间不允许出现空洞，并且 Raft 限制了限制了日志不一致的可能性。尽管在大多数情况下，我们都都在试图消除不确定性，但是有时候有些情况下，不确定性使得算法更易理解。尤其是，随机化方法使得不确定性增加，但是它减少了状态空间。我们使用随机化来简化了 Raft 中的领导选取算法。</p>\n<h1 id=\"5-Raft-一致性算法\"><a href=\"#5-Raft-一致性算法\" class=\"headerlink\" title=\"5 Raft 一致性算法\"></a>5 Raft 一致性算法</h1><p>Raft 是一种用来管理第 2 章中提到的复制日志的算法。表-2 为了方便参考是一个算法的总结版本，表-3 列举了算法中的关键性质；表格中的这些元素将会在这一章剩下的部分中分别进行讨论。</p>\n<p><strong>状态：</strong></p>\n<p>在所有服务器上持久存在的：（在响应远程过程调用 RPC 之前稳定存储的）</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>currentTerm</td>\n<td>服务器最后知道的任期号（从0开始递增）</td>\n</tr>\n<tr>\n<td>votedFor</td>\n<td>在当前任期内收到选票的候选人 id（如果没有就为 null）</td>\n</tr>\n<tr>\n<td>log[]</td>\n<td>日志条目；每个条目包含状态机的要执行命令和从领导人处收到时的任期号</td>\n</tr>\n</tbody>\n</table>\n<p>在所有服务器上不稳定存在的：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>commitIndex</td>\n<td>已知的被提交的最大日志条目的索引值（从0开始递增）</td>\n</tr>\n<tr>\n<td>lastApplied</td>\n<td>被状态机执行的最大日志条目的索引值（从0开始递增）</td>\n</tr>\n</tbody>\n</table>\n<p>在领导人服务器上不稳定存在的：（在选举之后初始化的）</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>nextIndex[]</td>\n<td>对于每一个服务器，记录需要发给它的下一个日志条目的索引（初始化为领导人上一条日志的索引值+1）</td>\n</tr>\n<tr>\n<td>matchIndex[]</td>\n<td>对于每一个服务器，记录已经复制到该服务器的日志的最高索引值（从0开始递增）</td>\n</tr>\n</tbody>\n</table>\n<p>表-2-i</p>\n<p><strong>附加日志远程过程调用 （AppendEntries RPC）</strong></p>\n<p>由领导人来调用复制日志（5.3节）；也会用作heartbeat</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>领导人的任期号</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>领导人的 id，为了其他服务器能重定向到客户端</td>\n</tr>\n<tr>\n<td>prevLogIndex</td>\n<td>最新日志之前的日志的索引值</td>\n</tr>\n<tr>\n<td>prevLogTerm</td>\n<td>最新日志之前的日志的领导人任期号</td>\n</tr>\n<tr>\n<td>entries[]</td>\n<td>将要存储的日志条目（表示 heartbeat 时为空，有时会为了效率发送超过一条）</td>\n</tr>\n<tr>\n<td>leaderCommit</td>\n<td>领导人提交的日志条目索引值</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>当前的任期号，用于领导人更新自己的任期号</td>\n</tr>\n<tr>\n<td>success</td>\n<td>如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真</td>\n</tr>\n</tbody>\n</table>\n<p><strong>接受者需要实现：</strong></p>\n<ol>\n<li>如果 <code>term &lt; currentTerm</code>返回 false（5.1节）</li>\n<li>如果在<code>prevLogIndex</code>处的日志的任期号与<code>prevLogTerm</code>不匹配时，返回 false（5.3节）</li>\n<li>如果一条已经存在的日志与新的冲突（index 相同但是任期号 term 不同），则删除已经存在的日志和它之后所有的日志（5.3节）</li>\n<li>添加任何在已有的日志中不存在的条目</li>\n<li>如果<code>leaderCommit &gt; commitIndex</code>，将<code>commitIndex</code>设置为<code>leaderCommit</code>和最新日志条目索引号中较小的一个</li>\n</ol>\n<p>表-2-ii</p>\n<p><strong>投票请求 RPC（RequestVote RPC）</strong></p>\n<p>由候选人发起收集选票（5.2节）</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>候选人的任期号</td>\n</tr>\n<tr>\n<td>candidateId</td>\n<td>请求投票的候选人 id</td>\n</tr>\n<tr>\n<td>lastLogIndex</td>\n<td>候选人最新日志条目的索引值</td>\n</tr>\n<tr>\n<td>lastLogTerm</td>\n<td>候选人最新日志条目对应的任期号</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>目前的任期号，用于候选人更新自己</td>\n</tr>\n<tr>\n<td>voteGranted</td>\n<td>如果候选人收到选票为 true</td>\n</tr>\n</tbody>\n</table>\n<p><strong>接受者需要实现：</strong></p>\n<ol>\n<li>如果<code>term &lt; currentTerm</code>返回 false（5.1节）</li>\n<li>如果<code>votedFor</code>为空或者与<code>candidateId</code>相同，并且候选人的日志和自己的日志一样新，则给该候选人投票（5.2节 和 5.4节）</li>\n</ol>\n<p>表-2-iii</p>\n<p><strong>服务器需要遵守的规则：</strong></p>\n<p>所有服务器：</p>\n<ul>\n<li>如果<code>commitIndex &gt; lastApplied</code>，<code>lastApplied</code>自增，将<code>log[lastApplied]</code>应用到状态机（5.3节）</li>\n<li>如果 RPC 的请求或者响应中包含一个 term T 大于 <code>currentTerm</code>，则<code>currentTerm</code>赋值为 T，并切换状态为追随者（Follower）（5.1节）</li>\n</ul>\n<p>追随者（followers）: 5.2节</p>\n<ul>\n<li>响应来自候选人和领导人的 RPC</li>\n<li>如果在超过选取领导人时间之前没有收到来自当前领导人的<code>AppendEntries RPC</code>或者没有收到候选人的投票请求，则自己转换状态为候选人</li>\n</ul>\n<p>候选人：5.2节</p>\n<ul>\n<li>转变为选举人之后开始选举：<ul>\n<li><code>currentTerm</code>自增</li>\n<li>给自己投票</li>\n<li>重置选举计时器</li>\n<li>向其他服务器发送<code>RequestVote RPC</code></li>\n</ul>\n</li>\n<li>如果收到了来自大多数服务器的投票：成为领导人</li>\n<li>如果收到了来自新领导人的<code>AppendEntries RPC（heartbeat）</code>：转换状态为追随者</li>\n<li>如果选举超时：开始新一轮的选举</li>\n</ul>\n<p>领导人：</p>\n<ul>\n<li><p>一旦成为领导人：向其他所有服务器发送空的<code>AppendEntries RPC（heartbeat）</code>;在空闲时间重复发送以防止选举超时（5.2节）</p>\n</li>\n<li><p>如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端（5.3节）</p>\n</li>\n<li><p>对于一个追随者来说，如果上一次收到的日志索引大于将要收到的日志索引（nextIndex）：通过</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AppendEntries RPC</span><br></pre></td></tr></table></figure>\n<p>将 nextIndex 之后的所有日志条目发送出去</p>\n<ul>\n<li>如果发送成功：将该追随者的 <code>nextIndex</code>和<code>matchIndex</code>更新</li>\n<li>如果由于日志不一致导致<code>AppendEntries RPC</code>失败：<code>nextIndex</code>递减并且重新发送（5.3节）</li>\n</ul>\n</li>\n<li><p>如果存在一个满足<code>N &gt; commitIndex</code>和<code>matchIndex[i] &gt;= N</code>并且<code>log[N].term == currentTerm</code>的 N，则将<code>commitIndex</code>赋值为 N</p>\n</li>\n</ul>\n<p>表-2-iv</p>\n<p>表-2：Raft 一致性算法的总结（不包括成员变化 membership changes 和日志压缩 log compaction）</p>\n<table>\n<thead>\n<tr>\n<th>性质</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>选举安全原则（Election Safety）</td>\n<td>一个任期（term）内最多允许有一个领导人被选上（5.2节）</td>\n</tr>\n<tr>\n<td>领导人只增加原则（Leader Append-Only）</td>\n<td>领导人永远不会覆盖或者删除自己的日志，它只会增加条目</td>\n</tr>\n<tr>\n<td>日志匹配原则（Log Matching）</td>\n<td>如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同（5.3 节）</td>\n</tr>\n<tr>\n<td>领导人完全原则（Leader Completeness)</td>\n<td>如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中</td>\n</tr>\n<tr>\n<td>状态机安全原则（State Machine Safety）</td>\n<td>如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目（5.4.3节）</td>\n</tr>\n</tbody>\n</table>\n<p>表-3：Raft 算法保证这些特性任何时刻都成立</p>\n<p>Raft 通过首先选出一个领导人来实现一致性，然后给予领导人完全管理复制日志（replicated log）的责任。领导人接收来自客户端的日志条目，并把它们复制到其他的服务器上，领带人还要告诉服务器们什么时候将日志条目应用到它们的状态机是安全的。通过选出领导人能够简化复制日志的管理工作。例如，领导人能够决定将新的日志条目放到哪，而并不需要和其他的服务器商议，数据流被简化成从领导人流向其他服务器。如果领导人宕机或者和其他服务器失去连接，就可以选取下一个领导人。</p>\n<p>通过选出领导人，Raft 将一致性问题分解成为三个相对独立的子问题：</p>\n<ul>\n<li><strong>领导人选取（Leader election）：</strong> 在一个领导人宕机之后必须要选取一个新的领导人（5.2节）</li>\n<li><strong>日志复制（Log replication）：</strong> 领导人必须从客户端接收日志然后复制到集群中的其他服务器，并且强制要求其他服务器的日志保持和自己相同</li>\n<li><strong>安全性（Safety）：</strong> Raft 的关键的安全特性是 表-3 中提到的状态机安全原则（State Machine Safety）:如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。5.4节阐述了 Raft 是如何保证这条原则的，解决方案涉及到一个对于选举机制另外的限制，这一部分会在 5.2节 中说明。</li>\n</ul>\n<p>在说明了一致性算法之后，本章会讨论有关可用性（availability）的问题和系统中时序（timing）的问题。</p>\n<h2 id=\"5-1-Raft-基础\"><a href=\"#5-1-Raft-基础\" class=\"headerlink\" title=\"5.1 Raft 基础\"></a>5.1 Raft 基础</h2><p>一个 Raft 集群包括若干服务器；对于一个典型的 5 服务器集群，该集群能够容忍 2 台机器不能正常工作，而整个系统保持正常。在任意的时间，每一个服务器一定会处于以下三种状态中的一个：<em>领导人</em>、<em>候选人</em>、<em>追随者</em>。在正常情况下，只有一个服务器是领导人，剩下的服务器是追随者。追随者们是被动的：他们不会发送任何请求，只是响应来自领导人和候选人的请求。领导人来处理所有来自客户端的请求（如果一个客户端与追随者进行通信，追随者会将信息发送给领导人）。候选人是用来选取一个新的领导人的，这一部分会在 5.2节 进行阐释。图-4 阐述了这些状态，和它们之间的转换；它们的转换会在下边进行讨论。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg\" alt=\"img\"></a></p>\n<p>图-4：服务器的状态。追随者只响应其他服务器的请求。如果追随者没有收到任何消息，它会成为一个候选人并且开始一次选举。收到大多数服务器投票的候选人会成为新的领导人。领导人在它们宕机之前会一直保持领导人的状态。</p>\n<p><a href=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg\" alt=\"img\"></a></p>\n<p>图-5：时间被分为一个个的任期（term），每一个任期的开始都是领导人选举。在成功选举之后，一个领导人会在任期内管理整个集群。如果选举失败，该任期就会因为没有领带人而结束。这个转变会在不同的时间的不同服务器上观察到。</p>\n<p>如 图-5 所示，Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），就像 5.2节 所描述的那样，一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最少要有一个领导人。</p>\n<p>不同的服务器可能会在任期内观察到多次不同的状态转换，在某些情况下，一台服务器可能看不到一次选举或者一个完整的任期。任期在 Raft 中充当逻辑时钟的角色，并且它们允许服务器检测过期的信息，比如过时的领导人。每一台服务器都存储着一个当前任期的数字，这个数字会单调的增加。当服务器之间进行通信时，会互相交换当前任期号；如果一台服务器的当前任期号比其它服务器的小，则更新为较大的任期号。如果一个候选人或者领导人意识到它的任期号过时了，它会立刻转换为追随者状态。如果一台服务器收到的请求的任期号是过时的，那么它会拒绝此次请求。</p>\n<p>Raft 中的服务器通过远程过程调用（RPC）来通信，基本的 Raft 一致性算法仅需要 2 种 RPC。RequestVote RPC 是候选人在选举过程中触发的（5.2节），AppendEntries RPC 是领导人触发的，为的是复制日志条目和提供一种心跳（heartbeat）机制（5.3节）。第7章加入了第三种 RPC 来在各个服务器之间传输快照（snapshot）。如果服务器没有及时收到 RPC 的响应，它们会重试，并且它们能够并行的发出 RPC 来获得最好的性能。</p>\n<h2 id=\"5-2-领导人选取\"><a href=\"#5-2-领导人选取\" class=\"headerlink\" title=\"5.2 领导人选取\"></a>5.2 领导人选取</h2><p>Raft 使用一种心跳机制（heartbeat）来触发领导人的选取。当服务器启动时，它们会初始化为追随者。一太服务器会一直保持追随者的状态只要它们能够收到来自领导人或者候选人的有效 RPC。领导人会向所有追随者周期性发送心跳（heartbeat，不带有任何日志条目的 AppendEntries RPC）来保证它们的领导人地位。如果一个追随者在一个周期内没有收到心跳信息，就叫做选举超时（election timeout）,然后它就会假定没有可用的领导人并且开始一次选举来选出一个新的领导人。</p>\n<p>为了开始选举，一个追随者会自增它的当前任期并且转换状态为候选人。然后，它会给自己投票并且给集群中的其他服务器发送 RequestVote RPC。一个候选人会一直处于该状态，直到下列三种情形之一发生：</p>\n<ul>\n<li>它赢得了选举；</li>\n<li>另一台服务器赢得了选举；</li>\n<li>一段时间后没有任何一台服务器赢得了选举</li>\n</ul>\n<p>这些情形会在下面的章节中分别讨论。</p>\n<p>一个候选人如果在一个任期内收到了来自集群中大多数服务器的投票就会赢得选举。在一个任期内，一台服务器最多能给一个候选人投票，按照先到先服务原则（first-come-first-served）（注意：在 5.4节 针对投票添加了一个额外的限制）。大多数原则使得在一个任期内最多有一个候选人能赢得选举（表-3 中提到的选举安全原则）。一旦有一个候选人赢得了选举，它就会成为领导人。然后它会像其他服务器发送心跳信息来建立自己的领导地位并且组织新的选举。</p>\n<p>当一个候选人等待别人的选票时，它有可能会收到来自其他服务器发来的声明其为领导人的 AppendEntries RPC。如果这个领导人的任期（包含在它的 RPC 中）比当前候选人的当前任期要大，则候选人认为该领导人合法，并且转换自己的状态为追随者。如果在这个 RPC 中的任期小于候选人的当前任期，则候选人会拒绝此次 RPC， 继续保持候选人状态。</p>\n<p>第三种情形是一个候选人既没有赢得选举也没有输掉选举：如果许多追随者在同一时刻都成为了候选人，选票会被分散，可能没有候选人能获得大多数的选票。当这种情形发生时，每一个候选人都会超时，并且通过自增任期号和发起另一轮 RequestVote RPC 来开始新的选举。然而，如果没有其它手段来分配选票的话，这种情形可能会无限的重复下去。</p>\n<p>Raft 使用随机的选举超时时间来确保第三种情形很少发生，并且能够快速解决。为了防止在一开始是选票就被瓜分，选举超时时间是在一个固定的间隔内随机选出来的（例如，150~300ms）。这种机制使得在大多数情况下只有一个服务器会率先超时，它会在其它服务器超时之前赢得选举并且向其它服务器发送心跳信息。同样的机制被用于选票一开始被瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，在超时进行下一次选举之前一直等待。这能够减小在新的选举中一开始选票就被瓜分的可能性。9.3节 展示了这种方法能够快速的选出一个领导人。</p>\n<p>选举是一个理解性引导我们设计替代算法的一个例子。最开始时，我们计划使用一种排名系统：给每一个候选人分配一个唯一的排名，用于在竞争的候选人之中选择领导人。如果一个候选人发现了另一个比它排名高的候选人，那么它会回到追随者的状态，这样排名高的候选人会很容易地赢得选举。但是我们发现这种方法在可用性方面有一点问题（一个低排名的服务器在高排名的服务器宕机后，需要等待超时才能再次成为候选人，但是如果它这么做的太快，它能重置选举领带人的过程）。我们对这个算法做了多次调整，但是每次调整后都会出现一些新的问题。最终我们认为随机重试的方法是更明确并且更易于理解的。</p>\n<h2 id=\"5-3-日志复制\"><a href=\"#5-3-日志复制\" class=\"headerlink\" title=\"5.3 日志复制\"></a>5.3 日志复制</h2><p>一旦选出了领导人，它就开始接收客户端的请求。每一个客户端请求都包含一条需要被复制状态机（replicated state machine）执行的命令。领导人把这条命令作为新的日志条目加入到它的日志中去，然后并行的向其他服务器发起 AppendEntries RPC ，要求其它服务器复制这个条目。当这个条目被安全的复制之后（下面的部分会详细阐述），领导人会将这个条目应用到它的状态机中并且会向客户端返回执行结果。如果追随者崩溃了或者运行缓慢或者是网络丢包了，领导人会无限的重试 AppendEntries RPC（甚至在它向客户端响应之后）知道所有的追随者最终存储了所有的日志条目。</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg\" alt=\"img\"></a></p>\n<p>图-6：日志由有序编号的日志条目组成。每个日志条目包含它被创建时的任期号（每个方块中的数字），并且包含用于状态机执行的命令。如果一个条目能够被状态机安全执行，就被认为可以提交了。</p>\n<p>日志就像 图-6 所示那样组织的。每个日志条目存储着一条被状态机执行的命令和当这条日志条目被领导人接收时的任期号。日志条目中的任期号用来检测在不同服务器上日志的不一致性，并且能确保 图-3 中的一些特性。每个日志条目也包含一个整数索引来表示它在日志中的位置。</p>\n<p>领导人决定什么时候将日志条目应用到状态机是安全的；这种条目被称为可被提交（commited）。Raft 保证可被提交（commited）的日志条目是持久化的并且最终会被所有可用的状态机执行。一旦被领导人创建的条目已经复制到了大多数的服务器上，这个条目就称为可被提交的（例如，图-6中的7号条目）。领导人日志中之前的条目都是可被提交的（commited），包括由之前的领导人创建的条目。5.4节将会讨论当领导人更替之后这条规则的应用问题的细节，并且也讨论了这种提交方式是安全的。领导人跟踪记录它所知道的被提交条目的最大索引值，并且这个索引值会包含在之后的 AppendEntries RPC 中（包括心跳 heartbeat 中），为的是让其他服务器都知道这条条目已经提交。一旦一个追随者知道了一个日志条目已经被提交，它会将该条目应用至本地的状态机（按照日志顺序）。</p>\n<p>我们设计了 Raft 日志机制来保证不同服务器上日志的一致性。这样做不仅简化了系统的行为使得它更可预测，并且也是保证安全性不可或缺的一部分。Raft 保证以下特性，并且也保证了 表-3 中的日志匹配原则（Log Matching Property）:</p>\n<ul>\n<li>如果在不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。</li>\n<li>如果在不同日志中的两个条目有着相同的索引和任期号，则它们之间的所有条目都是完全一样的。</li>\n</ul>\n<p>第一条特性源于领导人在一个任期里在给定的一个日志索引位置最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，领导人会把新日志条目紧接着之前的条目的索引位置和任期号都包含在里面。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg\" alt=\"img\"></a></p>\n<p>图-7：当最上边的领导人掌权之后，追随者日志可能有以下情况（a~f）。一个格子表示一个日志条目；格子中的数字是它的任期。一个追随者可能会丢失一些条目（a, b）；可能多出来一些未提交的条目（c, d）；或者两种情况都有（e, f）。例如，场景 f 在如下情况下就会发生：如果一台服务器在任期2时是领导人并且往它的日志中添加了一些条目，然后在将它们提交之前就宕机了，之后它很快重启了，成为了任期3的领导人，又往它的日志中添加了一些条目，然后在任期2和任期3中的条目提交之前它又宕机了并且几个任期内都一直处于宕机状态。</p>\n<p>在一般情况下，领导人和追随者们的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，领导人的崩溃会导致日志不一致（旧的领导人可能没有完全复制完日志中的所有条目）。这些不一致会导致一系列领导人和追随者崩溃。图-7 阐述了一些追随者可能和新的领导人日志不同的情况。一个追随者可能会丢失掉领导人上的一些条目，也有可能包含一些领导人没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。</p>\n<p>在 Raft 算法中，领导人通过强制追随者们复制它的日志来处理日志的不一致。这就意味着，在追随者上的冲突日志会被领导者的日志覆盖。5.4节会说明当添加了一个额外的限制之后这是安全的。</p>\n<p>为了使得追随者的日志同自己的一致，领导人需要找到追随者同它的日志一致的地方，然后删除追随者在该位置之后的条目，然后将自己在该位置之后的条目发送给追随者。这些操作都在 AppendEntries RPC 进行一致性检查时完成。领导人给每一个追随者维护了一个<code>nextIndex</code>，它表示领导人将要发送给该追随者的下一条日志条目的索引。当一个领导人开始掌权时，它会将<code>nextIndex</code>初始化为它的最新的日志条目索引数+1（图-7 中的 11）。如果一个追随者的日志和领导者的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，领导人会将<code>nextIndex</code>递减然后重试 AppendEntries RPC。最终<code>nextIndex</code>会达到一个领导人和追随者日志一致的地方。这时，AppendEntries 会返回成功，追随者中冲突的日志条目都被移除了，并且添加所缺少的上了领导人的日志条目。一旦 AppendEntries 返回成功，追随者和领导人的日志就一致了，这样的状态会保持到该任期结束。</p>\n<p>如果需要的话，算法还可以进行优化来减少 AppendEntries RPC 失败的次数。例如，当拒绝了一个 AppendEntries 请求，追随者可以记录下冲突日志条目的任期号和自己存储那个任期的最早的索引。通过这些信息，领导人能够直接递减<code>nextIndex</code>跨过那个任期内所有的冲突条目；这样的话，一个冲突的任期需要一次 AppendEntries RPC，而不是每一个冲突条目需要一次 AppendEntries RPC。在实践中，我们怀疑这种优化是否是必要的，因为AppendEntries 一致性检查很少失败并且也不太可能出现大量的日志条目不一致的情况。</p>\n<p>通过这种机制，一个领导人在掌权时不需要采取另外特殊的方式来恢复日志的一致性。它只需要使用一些常规的操作，通过响应 AppendEntries 一致性检查的失败能使得日志自动的趋于一致。一个领导人从来不会覆盖或者删除自己的日志（表-3 中的领导人只增加原则）。</p>\n<p>这个日志复制机制展示了在第2章中阐述的所希望的一致性特性：Raft 能够接受，复制并且应用新的日志条目只要大部分的服务器是正常的。在通常情况下，一条新的日志条目可以在一轮 RPC 内完成在集群的大多数服务器上的复制；并且一个速度很慢的追随者并不会影响整体的性能。</p>\n<p>##5.4 安全性</p>\n<p>之前的章节中讨论了 Raft 算法是如何进行领导选取和复制日志的。然而，到目前为止这个机制还不能保证每一个状态机能按照相同的顺序执行同样的指令。例如，当领导人提交了若干日志条目的同时一个追随者可能宕机了，之后它又被选为了领导人然后用新的日志条目覆盖掉了旧的那些，最后，不同的状态机可能执行不同的命令序列。</p>\n<p>这一节通过在领带人选取部分加入了一个限制来完善了 Raft 算法。这个限制能够保证对于固定的任期，任何的领导人都拥有之前任期提交的全部日志条目（表-3 中的领导人完全原则）。有了这一限制，日志提交的规则就更清晰了。最后，我们提出了对于领导人完全原则的简单证明并且展示了它是如何修正复制状态机的行为的。</p>\n<p>###5.4.1 选举限制</p>\n<p>在所有的以领导人为基础的一致性算法中，领导人最终必须要存储全部已经提交的日志条目。在一些一致性算法中，例如：<a href=\"http://people.csail.mit.edu/cowling/vr/vr-revisited.pdf\" target=\"_blank\" rel=\"noopener\">Viewstamped Replication</a>，即使一开始没有包含全部已提交的条目也可以被选为领导人。这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，这个过程要么在选举过程中完成，要么在选举之后立即开始。不幸的是，这种方式大大增加了复杂性。Raft 使用了一种更简单的方式来保证在新的领导人开始选举的时候在之前任期的所有已提交的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。</p>\n<p>Raft 使用投票的方式来阻止没有包含全部日志条目的服务器赢得选举。一个候选人为了赢得选举必须要和集群中的大多数进行通信，这就意味着每一条已经提交的日志条目最少在其中一台服务器上出现。如果候选人的日志至少和大多数服务器上的日志一样新（up-to-date，这个概念会在下边有详细介绍），那么它一定包含有全部的已经提交的日志条目。RequestVote RPC 实现了这个限制：这个 RPC（远程过程调用）包括候选人的日志信息，如果它自己的日志比候选人的日志要新，那么它会拒绝候选人的投票请求。</p>\n<p>Raft 通过比较日志中最后一个条目的索引和任期号来决定两个日志哪一个更新。如果两个日志的任期号不同，任期号大的更新；如果任期号相同，更长的日志更新。</p>\n<p>###5.4.2 提交之前任期的日志条目</p>\n<p><a href=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg\" alt=\"img\"></a></p>\n<p>图-8：如图的时间序列说明了为什么领导人不能通过之前任期的日志条目判断它的提交状态。（a）中的 S1 是领导人并且部分复制了索引2上的日志条目。（b）中 S1 崩溃了；S5 通过 S3，S4 和自己的选票赢得了选举，并且在索引2上接收了另一条日志条目。（c）中 S5 崩溃了，S1 重启了，通过 S2，S3 和自己的选票赢得了选举，并且继续索引2处的复制，这时任期2的日志条目已经在大部分服务器上完成了复制，但是还并没有提交。如果在（d）时刻 S1 崩溃了，S5 会通过 S2，S3，S4 的选票成为领导人，然后用它自己在任期3的日志条目覆盖掉其他服务器的日志条目。然而，如果在崩溃之前，S1 在它的当前任期在大多数服务器上复制了一条日志条目，就像在（e）中那样，那么这条条目就会被提交（S5就不会赢得选举）。在这时，之前的日志条目就会正常被提交。</p>\n<p>正如 5.3节 中描述的那样，只要一个日志条目被存在了在多数的服务器上，领导人就知道当前任期就可以提交该条目了。如果领导人在提交之前就崩溃了，之后的领导人会试着继续完成对日志的复制。然而，领导人并不能断定存储在大多数服务器上的日志条目一定在之前的任期中被提交了。图-8 说明了一种情况，一条存储在了大多数服务器上的日志条目仍然被新上任的领导人覆盖了。</p>\n<p>为了消除 图-8 中描述的问题，Raft 从来不会通过计算复制的数目来提交之前人气的日志条目。只有领导人当前任期的日志条目才能通过计算数目来进行提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配原则（Log Matching Property），之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，通过观察该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。</p>\n<p>因为当领导人从之前任期复制日志条目时日志条目保留了它们最开始的任期号，所以这使得 Raft 在提交规则中增加了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要从之前的任期中复制日志条目，它必须要使用当前的新任期号。Raft 的方法使得判断日志更加容易，因为它们全程都保持着同样的任期号。另外，和其它的一致性算法相比，Raft 算法中的新领导人会发送更少的之前任期的日志条目（其他算法必须要发送冗余的日志条目并且在它们被提交之前来重新排序）。</p>\n<h3 id=\"5-4-3-安全性论证\"><a href=\"#5-4-3-安全性论证\" class=\"headerlink\" title=\"5.4.3 安全性论证\"></a>5.4.3 安全性论证</h3><p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg\" alt=\"img\"></a></p>\n<p>图-9：如果 S1（任期 T 的领导人）在它的任期提交了一条日志条目，并且 S5 在之后的任期 U 成为了领导人，那么最少会有一台服务器（S3）接收了这条日志条目并且会给 S5 投票。</p>\n<p>给出了完整的 Raft 算法，现在我们能够更精确的论证领导人完全原则（Leader Completeness)（这基于 9.2节 提出的安全性证明）。我们假定领导人完全原则是不成立的，然后推导出矛盾。假定任期 T 的领导人 leaderT在它的任期提交了一个日志条目，但是这条日志条目并没有存储在之后的任期中的领导人上。我们设大于 T 的最小的任期 U 的领导人（leaderU） 没有存储这条日志条目。</p>\n<ol>\n<li>在 leaderU 选举时一定没有那条被提交的日志条目（领导人从来不会删除或者覆盖日志条目）。</li>\n<li>leaderT 复制了这个条目到集群的大多数的服务器上。因此，只是有一台服务器（投票者）即接收了来自 leaderT 的日志条目并且给 leaderU 投票，就像 图-9 中所示那样。这个投票者是产生矛盾的关键。</li>\n<li>投票者必须在给 leaderU 投票之前接收来自 leaderT 的日志条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（它的当前任期会比 T 要大）。</li>\n<li>投票者会在它给 leaderU 投票时存储那个条目，因为任何中间的领导人都保有该条目（基于假设），领导人从来不会移除这个条目，并且追随者也只会在和领导人冲突时才会移除日志条目。</li>\n<li>投票者给 leaderU 投票了，所以 leaderU 的日志必须和投票者的一样新。这就导致了一个矛盾。</li>\n<li>首先，如果投票者和 leaderU 最后一条日志条目的任期号相同，那么 leaderU 的日志一定和投票者的一样长，因此它的日志包含全部投票者的日志条目。这是矛盾的，因为在假设中投票者和 leaderU 包含的已提交条目是不同的。</li>\n<li>除此之外， leaderU 的最后一条日志的任期号一定比投票者的大。另外，它也比 T 要大，因为投票者的最后一条日志条目的任期号最小也要是 T（它包含了所有任期 T 提交的日志条目）。创建 leaderU 最后一条日志条目的上一任领导人必须包含已经提交的日志条目（基于假设）。那么，根据日志匹配原则（Log Matching），leaderU 也一定包含那条提交的日志条目，这也是矛盾的。</li>\n<li>这时就完成了矛盾推导。因此，所有比任期 T 大的领导人一定包含所有在任期 T 提交的日志条目。</li>\n<li>日志匹配原则（Log Matching）保证了未来的领导人也会包含被间接提交的日志条目，就像 图-8 中（d）时刻索引为2的条目。</li>\n</ol>\n<p>通过给出了 领导人完全原则（Leader Completeness)，我们能够证明 表-3 中的状态机安全原则（State Machine Safety），状态机安全原则（State Machine Safety）讲的是如果一台服务器将给定索引上的日志条目应用到了它自己的状态机上，其它服务器的同一索引位置不可能应用的是其它条目。在一个服务器应用一条日志条目到它自己的状态机中时，它的日志必须和领导人的日志在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性（Log Completeness Property）保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。</p>\n<p>最后，Raft 算法需要服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。</p>\n<p>##5.5 追随者和候选人崩溃</p>\n<p>截止到目前，我们只讨论了领导人崩溃的问题。追随者和候选人崩溃的问题解决起来要比领导人崩溃要简单得多，这两者崩溃的处理方式是一样的。如果一个追随者或者候选人崩溃了，那么之后的发送给它的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限的重试来处理这些失败；如果崩溃的服务器重启了，RPC 就会成功完成。如果一个服务器在收到了 RPC 之后但是在响应之前崩溃了，那么它会在重启之后再次收到同一个 RPC。因为 Raft 中的 RPC 都是幂等的，因此不会有什么问题。例如，如果一个追随者收到了一个已经包含在它的日志中的 AppendEntries 请求，它会忽视这个新的请求。</p>\n<p>##5.6 时序和可用性</p>\n<p>我们对于 Raft 的要求之一就是安全性不依赖于时序（timing）：系统不能仅仅因为一些事件发生的比预想的快一些或慢一些就产生错误。然而，可用性（系统可以及时响应客户端的特性）不可避免的要依赖时序。例如，如果消息交换在服务器崩溃时花费更多的时间，候选人不会等待太长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。</p>\n<p>领导人选取是 Raft 中对时序要求最关键的地方。Raft 会选出并且保持一个稳定的领导人只有系统满足下列时序要求（timing requirement）：</p>\n<p>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</p>\n<p>在这个不等式中，<code>broadcastTime</code>指的是一台服务器并行的向集群中的其他服务器发送 RPC 并且收到它们的响应的平均时间；<code>electionTimeout</code>指的就是在 5.2节 描述的选举超时时间；<code>MTBF</code>指的是单个服务器发生故障的间隔时间的平均数。<code>broadcastTime</code>应该比<code>electionTimeout</code>小一个数量级，为的是使领导人能够持续发送心跳信息（heartbeat）来阻止追随者们开始选举；根据已经给出的随机化选举超时时间方法，这个不等式也使得瓜分选票的情况变成不可能。<code>electionTimeout</code>也要比<code>MTBF</code>小几个数量级，为的是使得系统稳定运行。当领导人崩溃时，整个大约会在<code>electionTimeout</code>的时间内不可用；我们希望这种情况仅占全部时间的很小的一部分。</p>\n<p><code>broadcastTime</code>和<code>MTBF</code>是由系统决定的性质，但是<code>electionTimeout</code>是我们必须做出选择的。Raft 的 RPC 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，这取决于存储的技术。因此，<code>electionTimeout</code>一般在 10ms 到 500ms 之间。大多数的服务器的<code>MTBF</code>都在几个月甚至更长，很容易满足这个时序需求。</p>\n<p>#6 集群成员变化</p>\n<p>截止到目前，我们都假定集群的配置（加入到一致性算法的服务器集合）是固定的。在实际中，我们会经常更改配置，例如，替换掉那些崩溃的机器或者更改复制级别。虽然通过关闭整个集群，升级配置文件，然后重启整个集群也可以解决这个问题，但是这回导致在更改配置的过程中，整个集群不可用。另外，如果存在需要手工操作，那么就会有操作失误的风险。为了避免这些问题，我们决定采用自动改变配置并且把这部分加入到了 Raft 一致性算法中。</p>\n<p>为了让配置修改机制能够安全，那么在转换的过程中在任何时间点两个领导人不能再同一个任期被同时选为领导人。不幸的是，服务器集群从旧的配置直接升级到新的配置的任何方法都是不安全的，一次性自动的转换所有服务器是不可能的，所以集群可以在转换的过程中划分成两个单独的组（如 图-10 所示）。</p>\n<p><a href=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg\" alt=\"img\"></a></p>\n<p>图-10：从一个配置切换到另一个配置是不安全的因为不同的服务器会在不同的时间点进行切换。在这个例子中，集群数量从三台转换成五台。不幸的是，在一个时间点有两个服务器能被选举成为领导人，一个是在使用旧的配置的机器中（Cold）选出的领导人，另一个领导人是通过新的配置（Cnew）选出来的。</p>\n<p>为了保证安全性，集群配置的调整必须使用两阶段（two-phase）方法。有许多种实现两阶段方法的实现。例如，一些系统在第一个阶段先把旧的配置设为无效使得它无法处理客户端请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡配置，我们称其为共同一致（joint consensus）；一旦共同一致被提交了，然后系统再切换到新的配置。共同一致是旧的配置和新的配置的组合：</p>\n<ul>\n<li>日志条目被复制给集群中新、老配置的所有服务器。</li>\n<li>新、老配置的服务器都能成为领导人。</li>\n<li>需要分别在两种配置上获得大多数的支持才能达成一致（针对选举和提交）</li>\n</ul>\n<p>共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然能够响应服务器请求。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg\" alt=\"img\"></a></p>\n<p>图-11：集群配置变更的时间线。虚线表示的是已经被创建但是还没提交的配置条目，实线表示的是最新提交的配置条目。领导人首先在它的日志中创建 Cold,new配置条目并且将它提交到Cold,new（使用旧配置的大部分服务器和使用新配置的大部分服务器）。然后创建它创建Cnew配置条目并且将它提交到使用新配置的大部分机器上。这样就不存在Cold和Cnew能够分别同时做出决定的时刻。</p>\n<p>集群配置在复制日志中用特殊的日志条目来存储和通信；图-11 展示了配置变更的过程。当一个领导人接收到一个改变配置 Cold 为 Cnew 的请求，它会为了共同一致以前面描述的日志条目和副本的形式将配置存储起来（图中的 Cold,new）。一旦一个服务器将新的配置日志条目增加到它的日志中，它就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论它是否已经被提交）。这意味着领导人要使用 Cold,new 的规则来决定日志条目 Cold,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 Cold 配置也可能是 Cold,new 配置，这取决于赢得选举的候选人是否已经接收到了 Cold,new 配置。在任何情况下， Cnew 配置在这一时期都不会单方面的做出决定。</p>\n<p>一旦 Cold,new 被提交，那么无论是 Cold 还是 Cnew，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 Cnew 配置的日志条目并复制给集群就是安全的了。另外，每个服务器在收到新的配置的时候就会立即生效。当新的配置在 Cnew 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如 图-11，Cold 和 Cnew 没有任何机会同时做出单方面的决定；这就保证了安全性。</p>\n<p>针对重新配置提出了三个问题。第一个问题是一开始的时候新的服务器可能没有任何日志条目。如果它们在这个状态下加入到集群中，那么它们需要一段时间来更新追赶，在这个阶段它们还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群中来（领导人复制日志给他们，但是不把它们考虑到大多数中）。一旦新的服务器追赶上了集群中的其它机器，重新配置可以像上面描述的一样处理。</p>\n<p>第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 Cnew 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括自己；它复制日志但是不把它自己看作是大多数之一。当 Cnew 被提交时，会发生领导人过渡，因为这时是新的配置可以独立工作的最早的时间点（总是能够在 Cnew 配置下选出新的领导人）。在此之前，可能只能从 Cold 中选出领导人。</p>\n<p>第三个问题是，移除不在 Cnew 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳（heartbeat），所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新的任期号的 RequestVote RPC，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。</p>\n<p>为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略 RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个 RequestVote RPC，它不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么它就不会被更大的任期号废除。</p>\n<p>#7 日志压缩</p>\n<p>Raft 产生的日志在持续的正常操作中不断增长，但是在实际的系统中，它不会无限的增长下去。随着日志的不断增长，它会占据越来越多的空间并且花费更多的时间重置。如果没有一个机制使得它能够废弃在日志中不断累积的过时的信息就会引起可用性问题。</p>\n<p>快照（snapshot）是最简单的压缩方式。在快照中，全部的当前系统状态都被写入到快照中，存储到持久化的存储中，然后在那个时刻之前的全部日志都可以被丢弃。在 Chubby 和 ZooKeeper 中都使用了快照技术，这一章的剩下的部分会介绍 Raft 中使用的快照技术。</p>\n<p>增量压缩（incremental approaches）的方法，例如日志清理（log cleaning）或者日志结构合并树（log-structured merge trees），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以使用和快照相同的接口来实现 LSM tree ，但是日志清除方法就需要修改 Raft 了。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg\" alt=\"img\"></a></p>\n<p>图-12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。</p>\n<p>图-12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也将一些少量的元数据包含到快照中：最后被包含的索引（last included index）指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期（last included term）指的是该条目的任期号。保留这些数据是为了支持快照前的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 章），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。</p>\n<p>尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 章）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给它们。</p>\n<p><strong>安装快照 RPC（InstallSnapshot RPC）</strong></p>\n<p>在领导人发送快照给跟随者时使用调用。领导人总是按顺序发送。</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>领导人的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>为了追随者能重定向到客户端</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照中包含的最后日志条目的索引值</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>快照中包含的最后日志条目的任期号</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>分块在快照中的偏移量</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>快照块的原始数据</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果是最后一块数据则为真</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>currentTerm，用于领导人更新自己</td>\n</tr>\n</tbody>\n</table>\n<p>接受者需要实现：</p>\n<ol>\n<li>如果<code>term &lt; currentTerm</code>立刻回复</li>\n<li>如果是第一个分块（offset 为 0）则创建新的快照</li>\n<li>在指定的偏移量写入数据</li>\n<li>如果 <code>done</code>为 false，则回复并继续等待之后的数据</li>\n<li>保存快照文件，丢弃所有存在的或者部分有着更小索引号的快照</li>\n<li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保留并且回复</li>\n<li>丢弃全部日志</li>\n<li>能够使用快照来恢复状态机（并且装载快照中的集群配置）</li>\n</ol>\n<p>表-13：InstallSnapshot RPC 的总结。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生存的信号，所以跟随者可以重置选举超时计时器。</p>\n<p>在这种情况下领导人使用一种叫做安装快照（InstallSnapshot）的新的 RPC 来发送快照给太落后的跟随者；见 表-13。当跟随者通过这种 RPC 接收到快照时，它必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃它所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须是正确的和并且被保留下来。</p>\n<p>这种快照的方式背离了 Raft 的强领导人原则（strong leader principle），因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织它们的数据了。</p>\n<p>我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。</p>\n<p>还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，它就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。</p>\n<p>第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制（copy-on-write）的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。</p>\n<p>#8 客户端交互</p>\n<p>这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端是如何发现领导人的和 Raft 是如何支持线性化语义（linearizable semantics）的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。</p>\n<p>Raft 中的客户端将所有请求发送给领导人。当客户端启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供它最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。</p>\n<p>我们 Raft 的目标是要实现线性化语义（linearizable semantics）（每一次操作立即执行，在它调用和收到回复之间只执行一次）。但是，如上述所说，Raft 是可以多次执行同一条命令的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。</p>\n<p>只读（read-only）的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回过期数据(stale data)的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是它还不知道。线性化的读操作必须不能返回过期数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全原则（Leader Completeness Property）保证了领导人一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来进行实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废除了（如果一个更新的领导人被选举出来，它自己的信息就已经过期了）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳（heartbeat）信息来处理这个问题。另外，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时序来保证安全性（它假设时间误差是有界的）。</p>\n<p>#9 实现和评价</p>\n<p>我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。</p>\n<p>这一章会从三个方面来评估 Raft 算法：可理解性、正确性和性能。</p>\n<p>##9.1 可理解性</p>\n<p>为了比较 Paxos 和 Raft 算法的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文除了日志压缩之外的所有内容；Paxos 课程包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者独立的区别从第一个算法处学来的经验。我们计算参加人员的每一个小测验的得分来看参与者是否对 Raft 的理解更好。</p>\n<table>\n<thead>\n<tr>\n<th>因素</th>\n<th>消除偏见的手段</th>\n<th>复习材料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>相同的讲课质量</td>\n<td>使用相同的讲师。Paxos 的讲义是基于之前在几所大学中使用的材料的并且做了改进。Paxos 的讲义要长 14%</td>\n<td>视频</td>\n</tr>\n<tr>\n<td>相同的测试难度</td>\n<td>用难度给问题分组，在测试中成对出现</td>\n<td>测验</td>\n</tr>\n<tr>\n<td>公平的打分</td>\n<td>使用红字标题。随机顺序打分，两个测验交替进行。</td>\n<td>红字标题</td>\n</tr>\n</tbody>\n</table>\n<p>表-1：考虑到的可能造成偏见的因素，以及解决方案和对应的复习材料</p>\n<p>我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表-1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg\" alt=\"img\"></a></p>\n<p>图-14：表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩的散点图。在对角线之上的点表示在 Raft 获得了更高分数的学生。</p>\n<p>参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图-14 展示了每个参与者的得分。一对 t -测试表明，拥有 95% 的可信度，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。</p>\n<p>我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型显示，对小测验的选择会产生 12.5 分的差别在对 Raft 的好感度上。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft 的小测验得分会比 Paxos 低 6.3 分；我们不知道为什么，但这在统计学上是这样的。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg\" alt=\"img\"></a></p>\n<p>图-15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。</p>\n<p>我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图-15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。</p>\n<p>关于 Raft 用户学习有一个更加详细的讨论，详见<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/ ̃ongaro/thesis.pdf</a></p>\n<p>##9.2 正确性</p>\n<p>在第5章，我们已经进行了一个<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">正式的说明</a>，和对一致性机制的安全性证明。这个正式说明通过 <a href=\"https://www.amazon.com/Specifying-Systems-Language-Hardware-Engineers/dp/032114306X\" target=\"_blank\" rel=\"noopener\">TLA+</a> 让 表-2 中的信息非常清晰。它大约有 400 行并且充当了证明的主题。同时对于任何想实现的人也是十分有用的。我们非常机械的通过 TLA 证明系统证明了日志完全特性（Log Completeness Property）。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明这个说明中的类型安全 type safety）。而且，我们已经写了一个<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">非正式的证明</a>关于状态机安全性质是完备的，并且是相当清晰的（大约 3500 个词）。</p>\n<p>##9.3 性能</p>\n<p>Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。</p>\n<p>我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答以下两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg\" alt=\"img\"></a></p>\n<p>图-16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。</p>\n<p>为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图-16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。</p>\n<p>图-16 上面的图表表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程由于太多的选票瓜分的情况往往都需要花费超过 10 秒钟。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。</p>\n<p>图-16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。</p>\n<p>#10 相关工作</p>\n<p>已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：</p>\n<ul>\n<li>Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰的论文。</li>\n<li>关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。</li>\n<li>实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 有着很大的差别。</li>\n<li>Paxos 可以应用的性能优化。</li>\n<li>Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。</li>\n</ul>\n<p>Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。</p>\n<p>像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。</p>\n<p>和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。</p>\n<p>Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。</p>\n<p>一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致（joint consensus）的方法因为它对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Raft 没有采用 Lamport 的基于 α 的方法是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较而言，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。和 VR、SMART 比较而言，Raft 的方法同时需要更少的额外机制来实现。</p>\n<p>#11 总结</p>\n<p>算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。</p>\n<p>在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。</p>\n<p>#12 鸣谢</p>\n<p>这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。</p>\n<p>#引用</p>\n<ol>\n<li>BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154.</li>\n<li>BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350.</li>\n<li>CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317.</li>\n<li>CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407.</li>\n<li>CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218.</li>\n<li>CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264.</li>\n<li>COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154.</li>\n<li>GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43.</li>\n<li>GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210.</li>\n<li>HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492.</li>\n<li>HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158.</li>\n<li>JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems &amp; Networks (2011), IEEE Computer Society, pp. 245–256.</li>\n<li>KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.</li>\n<li>LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565.</li>\n<li>LAMPORT, L. The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169.</li>\n<li>LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25.</li>\n<li>LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002.</li>\n<li>LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.</li>\n<li>LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103.</li>\n<li>LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.</li>\n<li>LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13.</li>\n<li>LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.</li>\n<li>LogCabin source code. logcabin/logcabin.</li>\n<li>LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115.</li>\n<li>MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384.</li>\n<li>MAZIE` RES, D. Paxos made practical.<a href=\"http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf\" target=\"_blank\" rel=\"noopener\">http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf</a> , Jan. 2007.</li>\n<li>MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM.</li>\n<li>Raft user study. <a href=\"http://ramcloud.stanford.edu/~ongaro/userstudy/\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/~ongaro/userstudy/</a>.</li>\n<li>OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17.</li>\n<li>O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385.</li>\n<li>ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress).<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/~ongaro/thesis.pdf</a></li>\n<li>ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX.</li>\n<li>OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130.</li>\n<li>Raft consensus algorithm website. <a href=\"http://raftconsensus.github.io/\" target=\"_blank\" rel=\"noopener\">http://raftconsensus.github.io</a>.</li>\n<li>REED, B. Personal communications, May 17, 2013.</li>\n<li>ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52.</li>\n<li>SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319.</li>\n<li>SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10.</li>\n<li>VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.<a href=\"https://github.com/iissnan/hexo-theme-next\" target=\"_blank\" rel=\"noopener\">https://github.com/iissnan/hexo-theme-next</a>)</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"Raft-一致性算法论文译文\"><a href=\"#Raft-一致性算法论文译文\" class=\"headerlink\" title=\"Raft 一致性算法论文译文\"></a>Raft 一致性算法论文译文</h1><p>Raft论文翻译转载自<a href=\"http://blog.luoyuanhang.com/2017/02/02/raft-paper-in-zh-CN/\" target=\"_blank\" rel=\"noopener\">Raft 一致性算法论文译文</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>Raft 是一种用来管理日志复制的一致性算法。它和 Paxos 的性能和功能是一样的，但是它和 Paxos 的结构不一样；这使得 Raft 更容易理解并且更易于建立实际的系统。为了提高理解性，Raft 将一致性算法分为了几个部分，例如领导选取（leader selection），日志复制（log replication）和安全性（safety），同时它使用了更强的一致性来减少了必须需要考虑的状态。从用户学习的结果来看，Raft 比 Paxos 更容易学会。Raft 还包括了一种新的机制来使得动态改变集群成员，它使用重叠大多数（overlapping majorities）来保证安全。</p>\n<h1 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h1><p>一致性算法允许一组机器像一个整体一样工作，即使其中的一些机器出了错误也能正常工作。正因为此，他们扮演着建立大规模可靠的软件系统的关键角色。在过去的十年中 Paxos 一直都主导着有关一致性算法的讨论：大多数一致性算法的实现都基于它或者受它影响，并且 Paxos 也成为了教学生关于一致性知识的主要工具。</p>\n<p>不幸的是，尽管在降低它的复杂性方面做了许多努力，Paxos 依旧很难理解。并且，Paxos 需要经过复杂的修改才能应用于实际中。这些导致了系统构构建者和学生都十分头疼。</p>\n<p>在被 Paxos 折磨之后，我们开始寻找一种在系统构建和教学上更好的新的一致性算法。我们的首要目标是让它易于理解：我们能不能定义一种面向实际系统的一致性算法并且比 Paxos 更容易学习呢？并且，我们希望这种算法能凭直觉就能明白，这对于一个系统构建者来说是十分必要的。对于一个算法，不仅仅是让它工作起来很重要，知道它是如何工作的更重要。</p>\n<p>我们工作的结果是一种新的一致性算法，叫做 Raft。在设计 Raft 的过程中我们应用了许多专门的技巧来提升理解性，包括算法分解（分为领导选取（leader selection），日志复制（log replication）和安全性（safety））和减少状态（state space reduction）（相对于 Paxos，Raft 减少了非确定性的程度和服务器互相不一致的方式）。在两所学校的43个学生的研究中发现，Raft 比 Paxos 要更容易理解：在学习了两种算法之后，其中的33个学生回答 Raft 的问题要比回答 Paxos 的问题要好。</p>\n<p>Raft 算法和现在一些已经有的算法在一些地方很相似（主要是 <a href=\"http://www.pmg.csail.mit.edu/papers/vr.pdf\" target=\"_blank\" rel=\"noopener\">Oki 和 Liskov 的 Viewstamped Replication</a>。但是 Raft 有几个新的特性：</p>\n<ul>\n<li>强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。</li>\n<li>领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。</li>\n<li>成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。</li>\n</ul>\n<p>我们认为，Raft 在教学方面和实际实现方面比 Paxos 和其他算法更出众。它比其他算法更简单、更容易理解；它能满足一个实际系统的需求；它拥有许多开源的实现并且被许多公司所使用；它的安全特性已经被证明；并且它的效率和其他算法相比也具有竞争力。</p>\n<p>这篇论文剩下的部分会讲如下内容：复制状态机（replicated state machine）问题（第2节），讨论 Paxos 的优缺点（第3节），讨论我们用的为了达到提升理解性的方法（第4节），陈述 Raft 一致性算法（第5~8节），评价 Raft 算法（第9节），对相关工作的讨论（第10节）。</p>","more":"<h1 id=\"2-复制状态机（Replicated-State-Machine）\"><a href=\"#2-复制状态机（Replicated-State-Machine）\" class=\"headerlink\" title=\"2 复制状态机（Replicated State Machine）\"></a>2 复制状态机（Replicated State Machine）</h1><p>一致性算法是在<a href=\"https://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf\" target=\"_blank\" rel=\"noopener\">复制状态机</a>的背景下提出来的。在这个方法中，在一组服务器的状态机产生同样的状态的副本因此即使有一些服务器崩溃了这组服务器也还能继续执行。复制状态机在分布式系统中被用于解决许多有关容错的问题。例如，GFS，HDFS还有 RAMCloud 这些大规模的系统都是用一个单独的集群领导者，使用一个单独的复制状态机来进行领导选取和存储配置信息来应对领导者的崩溃。使用复制状态机的例子有 Chubby 和 ZooKeeper。</p>\n<p><a href=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg\" alt=\"img\"></a></p>\n<p>图-1：复制状态机的架构。一致性算法管理来自客户端状态命令的复制日志。状态机处理的日志中的命令的顺序都是一致的，因此会得到相同的执行结果。</p>\n<p>如图-1所示，复制状态机是通过复制日志来实现的。每一台服务器保存着一份日志，日志中包含一系列的命令，状态机会按顺序执行这些命令。因为每一台计算机的状态机都是确定的，所以每个状态机的状态都是相同的，执行的命令是相同的，最后的执行结果也就是一样的了。</p>\n<p>如何保证复制日志一致就是一致性算法的工作了。在一台服务器上，一致性模块接受客户端的命令并且把命令加入到它的日志中。它和其他服务器上的一致性模块进行通信来确保每一个日志最终包含相同序列的请求，即使有一些服务器宕机了。一旦这些命令被正确的复制了，每一个服务器的状态机都会按同样的顺序去执行它们，然后将结果返回给客户端。最终，这些服务器看起来就像一台可靠的状态机。</p>\n<p>应用于实际系统的一致性算法一般有以下特性：</p>\n<ul>\n<li>确保安全性（从来不会返回一个错误的结果），即使在所有的非拜占庭（Non-Byzantine）情况下，包括网络延迟、分区、丢包、冗余和乱序的情况下。</li>\n<li>高可用性，只要集群中的大部分机器都能运行，可以互相通信并且可以和客户端通信，这个集群就可用。因此，一般来说，一个拥有 5 台机器的集群可以容忍其中的 2 台的失败（fail）。服务器停止工作了我们就认为它失败（fail）了，没准一会当它们拥有稳定的存储时就能从中恢复过来，重新加入到集群中。</li>\n<li>不依赖时序保证一致性，时钟错误和极端情况下的消息延迟在最坏的情况下才会引起可用性问题。</li>\n<li>通常情况下，一条命令能够尽可能快的在大多数节点对一轮远程调用作出相应时完成，一少部分慢的机器不会影响系统的整体性能。</li>\n</ul>\n<h1 id=\"3-Paxos-算法的不足\"><a href=\"#3-Paxos-算法的不足\" class=\"headerlink\" title=\"3 Paxos 算法的不足\"></a>3 Paxos 算法的不足</h1><p>在过去的10年中，Leslie Lamport 的 Paxos 算法几乎已经成为了一致性算法的代名词：它是授课中最常见的算法，同时也是许多一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，例如一个单一复制日志条目（single replicated log entry）。我们把这个子集叫做单一决策 Paxos（single-decree Paxos）。之后 Paxos通过组合多个这种协议来完成一系列的决策，例如一个日志（multi-Paxos）。Paxos 确保安全性和活跃性（liveness），并且它支持集群成员的变更。它的正确性已经被证明，通常情况下也很高效。</p>\n<p>不幸的是，Paxos 有两个致命的缺点。第一个是 Paxos 太难以理解。它的完整的解释晦涩难懂；很少有人能完全理解，只有少数人成功的读懂了它。并且大家做了许多努力来用一些简单的术语来描述它。尽管这些解释都关注于单一决策子集问题，但仍具有挑战性。在 NSDI 2012 会议上的一次非正式调查显示，我们发现大家对 Paxos 都感到不满意，其中甚至包括一些有经验的研究员。我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。</p>\n<p>我们假定 Paxos 的晦涩来源于它将单决策子集作为它的基础。单决策（Single-decree）Paxos 是晦涩且微妙的：它被划分为两个没有简单直观解释的阶段，并且难以独立理解。正因为如此，它不能很直观的让我们知道为什么单一决策协议能够工作。为多决策 Paxos 设计的规则又添加了额外的复杂性和精巧性。我们相信多决策问题能够分解为其它更直观的方式。</p>\n<p>Paxos 的第二个缺点是它难以在实际环境中实现。其中一个原因是，对于多决策 Paxos （multi-Paxos） ，大家还没有一个一致同意的算法。Lamport 的描述大部分都是有关于单决策 Paxos （single-decree Paxos）；他仅仅描述了实现多决策的可能的方法，缺少许多细节。有许多实现 Paxos 和优化 Paxos 的尝试，但是他们都和 Lamport 的描述有些出入。例如，Chubby 实现的是一个类似 Paxos 的算法，但是在许多情况下的细节没有公开。</p>\n<p>另外，Paxos 的结构也是不容易在一个实际系统中进行实现的，这是单决策问题分解带来的又一个问题。例如，从许多日志条目中选出条目然后把它们融合到一个序列化的日志中并没有带来什么好处，它仅仅增加了复杂性。围绕着日志来设计一个系统是更简单、更高效的：新日志按照严格的顺序添加到日志中去。另一个问题是，Paxos 使用对等的点对点的实现作为它的核心（尽管它最终提出了一种弱领导者的形式来优化性能）。这种方法在只有一个决策被制定的情况下才显得有效，但是很少有现实中的系统使用它。如果要做许多的决策，选择一个领导人，由领带人来协调是更简单有效的方法。</p>\n<p>因此，在实际的系统应用中和 Paxos 算法都相差很大。所有开始于 Paxos 的实现都会遇到很多问题，然后由此衍生出了许多与 Paxos 有很大不同的架构。这是既费时又容易出错的，并且理解 Paxos 的难度又非常大。Paxos 算法在它正确性的理论证明上是很好的，但是在实现上的价值就远远不足了。来自 Chubby 的实现的一条评论就能够说明：</p>\n<blockquote>\n<p>Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。</p>\n</blockquote>\n<p>正因为存在这些问题，我们认为 Paxos 不仅对于系统的构建者来说不友好，同时也不利于教学。鉴于一致性算法对于大规模软件系统的重要性，我们决定试着来设计一种另外的比 Paxos 更好的一致性算法。Raft 就是这样的一个算法。</p>\n<h1 id=\"4-易于理解的设计\"><a href=\"#4-易于理解的设计\" class=\"headerlink\" title=\"4 易于理解的设计\"></a>4 易于理解的设计</h1><p>设计 Raft 的目标有如下几个：</p>\n<ul>\n<li>它必须提供一个完整的、实际的基础来进行系统构建，为的是减少开发者的工作；</li>\n<li>它必须在所有情况下都能保证安全可用；</li>\n<li>它对于常规操作必须高效；</li>\n<li>最重要的目标是：<strong>易于理解</strong>，它必须使得大多数人能够很容易的理解；</li>\n<li>另外，它必须能让开发者有一个直观的认识，这样才能使系统构建者们去对它进行扩展。</li>\n</ul>\n<p>在设计 Raft 的过程中，我们不得不在许多种方法中做出选择。当面临这种情况时，我们通常会权衡可理解性：每种方法的可理解性是如何的？（例如，它的状态空间有多复杂？它是不是有很细微的含义？）它的可读性如何？读者能不能轻易地理解这个方法和它的含义？</p>\n<p>我们意识到对这种可理解性的分析具有高度的主观性；尽管如此，我们使用了两种适用的方式。第一种是众所周知的问题分解：我们尽可能将问题分解成为若干个可解决的、可被理解的小问题。例如，在 Raft 中，我们把问题分解成为了<strong>领导选取（leader election）</strong>、<strong>日志复制（log replication）</strong>、<strong>安全（safety）</strong>和<strong>成员变化（membership changes）</strong>。</p>\n<p>我们采用的第二个方法是通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致并且尽可能消除不确定性。特别地，日志之间不允许出现空洞，并且 Raft 限制了限制了日志不一致的可能性。尽管在大多数情况下，我们都都在试图消除不确定性，但是有时候有些情况下，不确定性使得算法更易理解。尤其是，随机化方法使得不确定性增加，但是它减少了状态空间。我们使用随机化来简化了 Raft 中的领导选取算法。</p>\n<h1 id=\"5-Raft-一致性算法\"><a href=\"#5-Raft-一致性算法\" class=\"headerlink\" title=\"5 Raft 一致性算法\"></a>5 Raft 一致性算法</h1><p>Raft 是一种用来管理第 2 章中提到的复制日志的算法。表-2 为了方便参考是一个算法的总结版本，表-3 列举了算法中的关键性质；表格中的这些元素将会在这一章剩下的部分中分别进行讨论。</p>\n<p><strong>状态：</strong></p>\n<p>在所有服务器上持久存在的：（在响应远程过程调用 RPC 之前稳定存储的）</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>currentTerm</td>\n<td>服务器最后知道的任期号（从0开始递增）</td>\n</tr>\n<tr>\n<td>votedFor</td>\n<td>在当前任期内收到选票的候选人 id（如果没有就为 null）</td>\n</tr>\n<tr>\n<td>log[]</td>\n<td>日志条目；每个条目包含状态机的要执行命令和从领导人处收到时的任期号</td>\n</tr>\n</tbody>\n</table>\n<p>在所有服务器上不稳定存在的：</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>commitIndex</td>\n<td>已知的被提交的最大日志条目的索引值（从0开始递增）</td>\n</tr>\n<tr>\n<td>lastApplied</td>\n<td>被状态机执行的最大日志条目的索引值（从0开始递增）</td>\n</tr>\n</tbody>\n</table>\n<p>在领导人服务器上不稳定存在的：（在选举之后初始化的）</p>\n<table>\n<thead>\n<tr>\n<th>名称</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>nextIndex[]</td>\n<td>对于每一个服务器，记录需要发给它的下一个日志条目的索引（初始化为领导人上一条日志的索引值+1）</td>\n</tr>\n<tr>\n<td>matchIndex[]</td>\n<td>对于每一个服务器，记录已经复制到该服务器的日志的最高索引值（从0开始递增）</td>\n</tr>\n</tbody>\n</table>\n<p>表-2-i</p>\n<p><strong>附加日志远程过程调用 （AppendEntries RPC）</strong></p>\n<p>由领导人来调用复制日志（5.3节）；也会用作heartbeat</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>领导人的任期号</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>领导人的 id，为了其他服务器能重定向到客户端</td>\n</tr>\n<tr>\n<td>prevLogIndex</td>\n<td>最新日志之前的日志的索引值</td>\n</tr>\n<tr>\n<td>prevLogTerm</td>\n<td>最新日志之前的日志的领导人任期号</td>\n</tr>\n<tr>\n<td>entries[]</td>\n<td>将要存储的日志条目（表示 heartbeat 时为空，有时会为了效率发送超过一条）</td>\n</tr>\n<tr>\n<td>leaderCommit</td>\n<td>领导人提交的日志条目索引值</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>当前的任期号，用于领导人更新自己的任期号</td>\n</tr>\n<tr>\n<td>success</td>\n<td>如果其它服务器包含能够匹配上 prevLogIndex 和 prevLogTerm 的日志时为真</td>\n</tr>\n</tbody>\n</table>\n<p><strong>接受者需要实现：</strong></p>\n<ol>\n<li>如果 <code>term &lt; currentTerm</code>返回 false（5.1节）</li>\n<li>如果在<code>prevLogIndex</code>处的日志的任期号与<code>prevLogTerm</code>不匹配时，返回 false（5.3节）</li>\n<li>如果一条已经存在的日志与新的冲突（index 相同但是任期号 term 不同），则删除已经存在的日志和它之后所有的日志（5.3节）</li>\n<li>添加任何在已有的日志中不存在的条目</li>\n<li>如果<code>leaderCommit &gt; commitIndex</code>，将<code>commitIndex</code>设置为<code>leaderCommit</code>和最新日志条目索引号中较小的一个</li>\n</ol>\n<p>表-2-ii</p>\n<p><strong>投票请求 RPC（RequestVote RPC）</strong></p>\n<p>由候选人发起收集选票（5.2节）</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>候选人的任期号</td>\n</tr>\n<tr>\n<td>candidateId</td>\n<td>请求投票的候选人 id</td>\n</tr>\n<tr>\n<td>lastLogIndex</td>\n<td>候选人最新日志条目的索引值</td>\n</tr>\n<tr>\n<td>lastLogTerm</td>\n<td>候选人最新日志条目对应的任期号</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>目前的任期号，用于候选人更新自己</td>\n</tr>\n<tr>\n<td>voteGranted</td>\n<td>如果候选人收到选票为 true</td>\n</tr>\n</tbody>\n</table>\n<p><strong>接受者需要实现：</strong></p>\n<ol>\n<li>如果<code>term &lt; currentTerm</code>返回 false（5.1节）</li>\n<li>如果<code>votedFor</code>为空或者与<code>candidateId</code>相同，并且候选人的日志和自己的日志一样新，则给该候选人投票（5.2节 和 5.4节）</li>\n</ol>\n<p>表-2-iii</p>\n<p><strong>服务器需要遵守的规则：</strong></p>\n<p>所有服务器：</p>\n<ul>\n<li>如果<code>commitIndex &gt; lastApplied</code>，<code>lastApplied</code>自增，将<code>log[lastApplied]</code>应用到状态机（5.3节）</li>\n<li>如果 RPC 的请求或者响应中包含一个 term T 大于 <code>currentTerm</code>，则<code>currentTerm</code>赋值为 T，并切换状态为追随者（Follower）（5.1节）</li>\n</ul>\n<p>追随者（followers）: 5.2节</p>\n<ul>\n<li>响应来自候选人和领导人的 RPC</li>\n<li>如果在超过选取领导人时间之前没有收到来自当前领导人的<code>AppendEntries RPC</code>或者没有收到候选人的投票请求，则自己转换状态为候选人</li>\n</ul>\n<p>候选人：5.2节</p>\n<ul>\n<li>转变为选举人之后开始选举：<ul>\n<li><code>currentTerm</code>自增</li>\n<li>给自己投票</li>\n<li>重置选举计时器</li>\n<li>向其他服务器发送<code>RequestVote RPC</code></li>\n</ul>\n</li>\n<li>如果收到了来自大多数服务器的投票：成为领导人</li>\n<li>如果收到了来自新领导人的<code>AppendEntries RPC（heartbeat）</code>：转换状态为追随者</li>\n<li>如果选举超时：开始新一轮的选举</li>\n</ul>\n<p>领导人：</p>\n<ul>\n<li><p>一旦成为领导人：向其他所有服务器发送空的<code>AppendEntries RPC（heartbeat）</code>;在空闲时间重复发送以防止选举超时（5.2节）</p>\n</li>\n<li><p>如果收到来自客户端的请求：向本地日志增加条目，在该条目应用到状态机后响应客户端（5.3节）</p>\n</li>\n<li><p>对于一个追随者来说，如果上一次收到的日志索引大于将要收到的日志索引（nextIndex）：通过</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AppendEntries RPC</span><br></pre></td></tr></table></figure>\n<p>将 nextIndex 之后的所有日志条目发送出去</p>\n<ul>\n<li>如果发送成功：将该追随者的 <code>nextIndex</code>和<code>matchIndex</code>更新</li>\n<li>如果由于日志不一致导致<code>AppendEntries RPC</code>失败：<code>nextIndex</code>递减并且重新发送（5.3节）</li>\n</ul>\n</li>\n<li><p>如果存在一个满足<code>N &gt; commitIndex</code>和<code>matchIndex[i] &gt;= N</code>并且<code>log[N].term == currentTerm</code>的 N，则将<code>commitIndex</code>赋值为 N</p>\n</li>\n</ul>\n<p>表-2-iv</p>\n<p>表-2：Raft 一致性算法的总结（不包括成员变化 membership changes 和日志压缩 log compaction）</p>\n<table>\n<thead>\n<tr>\n<th>性质</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>选举安全原则（Election Safety）</td>\n<td>一个任期（term）内最多允许有一个领导人被选上（5.2节）</td>\n</tr>\n<tr>\n<td>领导人只增加原则（Leader Append-Only）</td>\n<td>领导人永远不会覆盖或者删除自己的日志，它只会增加条目</td>\n</tr>\n<tr>\n<td>日志匹配原则（Log Matching）</td>\n<td>如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同（5.3 节）</td>\n</tr>\n<tr>\n<td>领导人完全原则（Leader Completeness)</td>\n<td>如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中</td>\n</tr>\n<tr>\n<td>状态机安全原则（State Machine Safety）</td>\n<td>如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目（5.4.3节）</td>\n</tr>\n</tbody>\n</table>\n<p>表-3：Raft 算法保证这些特性任何时刻都成立</p>\n<p>Raft 通过首先选出一个领导人来实现一致性，然后给予领导人完全管理复制日志（replicated log）的责任。领导人接收来自客户端的日志条目，并把它们复制到其他的服务器上，领带人还要告诉服务器们什么时候将日志条目应用到它们的状态机是安全的。通过选出领导人能够简化复制日志的管理工作。例如，领导人能够决定将新的日志条目放到哪，而并不需要和其他的服务器商议，数据流被简化成从领导人流向其他服务器。如果领导人宕机或者和其他服务器失去连接，就可以选取下一个领导人。</p>\n<p>通过选出领导人，Raft 将一致性问题分解成为三个相对独立的子问题：</p>\n<ul>\n<li><strong>领导人选取（Leader election）：</strong> 在一个领导人宕机之后必须要选取一个新的领导人（5.2节）</li>\n<li><strong>日志复制（Log replication）：</strong> 领导人必须从客户端接收日志然后复制到集群中的其他服务器，并且强制要求其他服务器的日志保持和自己相同</li>\n<li><strong>安全性（Safety）：</strong> Raft 的关键的安全特性是 表-3 中提到的状态机安全原则（State Machine Safety）:如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。5.4节阐述了 Raft 是如何保证这条原则的，解决方案涉及到一个对于选举机制另外的限制，这一部分会在 5.2节 中说明。</li>\n</ul>\n<p>在说明了一致性算法之后，本章会讨论有关可用性（availability）的问题和系统中时序（timing）的问题。</p>\n<h2 id=\"5-1-Raft-基础\"><a href=\"#5-1-Raft-基础\" class=\"headerlink\" title=\"5.1 Raft 基础\"></a>5.1 Raft 基础</h2><p>一个 Raft 集群包括若干服务器；对于一个典型的 5 服务器集群，该集群能够容忍 2 台机器不能正常工作，而整个系统保持正常。在任意的时间，每一个服务器一定会处于以下三种状态中的一个：<em>领导人</em>、<em>候选人</em>、<em>追随者</em>。在正常情况下，只有一个服务器是领导人，剩下的服务器是追随者。追随者们是被动的：他们不会发送任何请求，只是响应来自领导人和候选人的请求。领导人来处理所有来自客户端的请求（如果一个客户端与追随者进行通信，追随者会将信息发送给领导人）。候选人是用来选取一个新的领导人的，这一部分会在 5.2节 进行阐释。图-4 阐述了这些状态，和它们之间的转换；它们的转换会在下边进行讨论。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fc9uv9fx6wj20hn07xt9z.jpg\" alt=\"img\"></a></p>\n<p>图-4：服务器的状态。追随者只响应其他服务器的请求。如果追随者没有收到任何消息，它会成为一个候选人并且开始一次选举。收到大多数服务器投票的候选人会成为新的领导人。领导人在它们宕机之前会一直保持领导人的状态。</p>\n<p><a href=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fc9vx0s6l6j20ef05odg9.jpg\" alt=\"img\"></a></p>\n<p>图-5：时间被分为一个个的任期（term），每一个任期的开始都是领导人选举。在成功选举之后，一个领导人会在任期内管理整个集群。如果选举失败，该任期就会因为没有领带人而结束。这个转变会在不同的时间的不同服务器上观察到。</p>\n<p>如 图-5 所示，Raft 算法将时间划分成为任意不同长度的任期（term）。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），就像 5.2节 所描述的那样，一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。Raft 算法保证在给定的一个任期最少要有一个领导人。</p>\n<p>不同的服务器可能会在任期内观察到多次不同的状态转换，在某些情况下，一台服务器可能看不到一次选举或者一个完整的任期。任期在 Raft 中充当逻辑时钟的角色，并且它们允许服务器检测过期的信息，比如过时的领导人。每一台服务器都存储着一个当前任期的数字，这个数字会单调的增加。当服务器之间进行通信时，会互相交换当前任期号；如果一台服务器的当前任期号比其它服务器的小，则更新为较大的任期号。如果一个候选人或者领导人意识到它的任期号过时了，它会立刻转换为追随者状态。如果一台服务器收到的请求的任期号是过时的，那么它会拒绝此次请求。</p>\n<p>Raft 中的服务器通过远程过程调用（RPC）来通信，基本的 Raft 一致性算法仅需要 2 种 RPC。RequestVote RPC 是候选人在选举过程中触发的（5.2节），AppendEntries RPC 是领导人触发的，为的是复制日志条目和提供一种心跳（heartbeat）机制（5.3节）。第7章加入了第三种 RPC 来在各个服务器之间传输快照（snapshot）。如果服务器没有及时收到 RPC 的响应，它们会重试，并且它们能够并行的发出 RPC 来获得最好的性能。</p>\n<h2 id=\"5-2-领导人选取\"><a href=\"#5-2-领导人选取\" class=\"headerlink\" title=\"5.2 领导人选取\"></a>5.2 领导人选取</h2><p>Raft 使用一种心跳机制（heartbeat）来触发领导人的选取。当服务器启动时，它们会初始化为追随者。一太服务器会一直保持追随者的状态只要它们能够收到来自领导人或者候选人的有效 RPC。领导人会向所有追随者周期性发送心跳（heartbeat，不带有任何日志条目的 AppendEntries RPC）来保证它们的领导人地位。如果一个追随者在一个周期内没有收到心跳信息，就叫做选举超时（election timeout）,然后它就会假定没有可用的领导人并且开始一次选举来选出一个新的领导人。</p>\n<p>为了开始选举，一个追随者会自增它的当前任期并且转换状态为候选人。然后，它会给自己投票并且给集群中的其他服务器发送 RequestVote RPC。一个候选人会一直处于该状态，直到下列三种情形之一发生：</p>\n<ul>\n<li>它赢得了选举；</li>\n<li>另一台服务器赢得了选举；</li>\n<li>一段时间后没有任何一台服务器赢得了选举</li>\n</ul>\n<p>这些情形会在下面的章节中分别讨论。</p>\n<p>一个候选人如果在一个任期内收到了来自集群中大多数服务器的投票就会赢得选举。在一个任期内，一台服务器最多能给一个候选人投票，按照先到先服务原则（first-come-first-served）（注意：在 5.4节 针对投票添加了一个额外的限制）。大多数原则使得在一个任期内最多有一个候选人能赢得选举（表-3 中提到的选举安全原则）。一旦有一个候选人赢得了选举，它就会成为领导人。然后它会像其他服务器发送心跳信息来建立自己的领导地位并且组织新的选举。</p>\n<p>当一个候选人等待别人的选票时，它有可能会收到来自其他服务器发来的声明其为领导人的 AppendEntries RPC。如果这个领导人的任期（包含在它的 RPC 中）比当前候选人的当前任期要大，则候选人认为该领导人合法，并且转换自己的状态为追随者。如果在这个 RPC 中的任期小于候选人的当前任期，则候选人会拒绝此次 RPC， 继续保持候选人状态。</p>\n<p>第三种情形是一个候选人既没有赢得选举也没有输掉选举：如果许多追随者在同一时刻都成为了候选人，选票会被分散，可能没有候选人能获得大多数的选票。当这种情形发生时，每一个候选人都会超时，并且通过自增任期号和发起另一轮 RequestVote RPC 来开始新的选举。然而，如果没有其它手段来分配选票的话，这种情形可能会无限的重复下去。</p>\n<p>Raft 使用随机的选举超时时间来确保第三种情形很少发生，并且能够快速解决。为了防止在一开始是选票就被瓜分，选举超时时间是在一个固定的间隔内随机选出来的（例如，150~300ms）。这种机制使得在大多数情况下只有一个服务器会率先超时，它会在其它服务器超时之前赢得选举并且向其它服务器发送心跳信息。同样的机制被用于选票一开始被瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，在超时进行下一次选举之前一直等待。这能够减小在新的选举中一开始选票就被瓜分的可能性。9.3节 展示了这种方法能够快速的选出一个领导人。</p>\n<p>选举是一个理解性引导我们设计替代算法的一个例子。最开始时，我们计划使用一种排名系统：给每一个候选人分配一个唯一的排名，用于在竞争的候选人之中选择领导人。如果一个候选人发现了另一个比它排名高的候选人，那么它会回到追随者的状态，这样排名高的候选人会很容易地赢得选举。但是我们发现这种方法在可用性方面有一点问题（一个低排名的服务器在高排名的服务器宕机后，需要等待超时才能再次成为候选人，但是如果它这么做的太快，它能重置选举领带人的过程）。我们对这个算法做了多次调整，但是每次调整后都会出现一些新的问题。最终我们认为随机重试的方法是更明确并且更易于理解的。</p>\n<h2 id=\"5-3-日志复制\"><a href=\"#5-3-日志复制\" class=\"headerlink\" title=\"5.3 日志复制\"></a>5.3 日志复制</h2><p>一旦选出了领导人，它就开始接收客户端的请求。每一个客户端请求都包含一条需要被复制状态机（replicated state machine）执行的命令。领导人把这条命令作为新的日志条目加入到它的日志中去，然后并行的向其他服务器发起 AppendEntries RPC ，要求其它服务器复制这个条目。当这个条目被安全的复制之后（下面的部分会详细阐述），领导人会将这个条目应用到它的状态机中并且会向客户端返回执行结果。如果追随者崩溃了或者运行缓慢或者是网络丢包了，领导人会无限的重试 AppendEntries RPC（甚至在它向客户端响应之后）知道所有的追随者最终存储了所有的日志条目。</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fc9zgzzchkj20hl0d7wg4.jpg\" alt=\"img\"></a></p>\n<p>图-6：日志由有序编号的日志条目组成。每个日志条目包含它被创建时的任期号（每个方块中的数字），并且包含用于状态机执行的命令。如果一个条目能够被状态机安全执行，就被认为可以提交了。</p>\n<p>日志就像 图-6 所示那样组织的。每个日志条目存储着一条被状态机执行的命令和当这条日志条目被领导人接收时的任期号。日志条目中的任期号用来检测在不同服务器上日志的不一致性，并且能确保 图-3 中的一些特性。每个日志条目也包含一个整数索引来表示它在日志中的位置。</p>\n<p>领导人决定什么时候将日志条目应用到状态机是安全的；这种条目被称为可被提交（commited）。Raft 保证可被提交（commited）的日志条目是持久化的并且最终会被所有可用的状态机执行。一旦被领导人创建的条目已经复制到了大多数的服务器上，这个条目就称为可被提交的（例如，图-6中的7号条目）。领导人日志中之前的条目都是可被提交的（commited），包括由之前的领导人创建的条目。5.4节将会讨论当领导人更替之后这条规则的应用问题的细节，并且也讨论了这种提交方式是安全的。领导人跟踪记录它所知道的被提交条目的最大索引值，并且这个索引值会包含在之后的 AppendEntries RPC 中（包括心跳 heartbeat 中），为的是让其他服务器都知道这条条目已经提交。一旦一个追随者知道了一个日志条目已经被提交，它会将该条目应用至本地的状态机（按照日志顺序）。</p>\n<p>我们设计了 Raft 日志机制来保证不同服务器上日志的一致性。这样做不仅简化了系统的行为使得它更可预测，并且也是保证安全性不可或缺的一部分。Raft 保证以下特性，并且也保证了 表-3 中的日志匹配原则（Log Matching Property）:</p>\n<ul>\n<li>如果在不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。</li>\n<li>如果在不同日志中的两个条目有着相同的索引和任期号，则它们之间的所有条目都是完全一样的。</li>\n</ul>\n<p>第一条特性源于领导人在一个任期里在给定的一个日志索引位置最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变。第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，领导人会把新日志条目紧接着之前的条目的索引位置和任期号都包含在里面。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fca1aw93cnj20gf0baabf.jpg\" alt=\"img\"></a></p>\n<p>图-7：当最上边的领导人掌权之后，追随者日志可能有以下情况（a~f）。一个格子表示一个日志条目；格子中的数字是它的任期。一个追随者可能会丢失一些条目（a, b）；可能多出来一些未提交的条目（c, d）；或者两种情况都有（e, f）。例如，场景 f 在如下情况下就会发生：如果一台服务器在任期2时是领导人并且往它的日志中添加了一些条目，然后在将它们提交之前就宕机了，之后它很快重启了，成为了任期3的领导人，又往它的日志中添加了一些条目，然后在任期2和任期3中的条目提交之前它又宕机了并且几个任期内都一直处于宕机状态。</p>\n<p>在一般情况下，领导人和追随者们的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，领导人的崩溃会导致日志不一致（旧的领导人可能没有完全复制完日志中的所有条目）。这些不一致会导致一系列领导人和追随者崩溃。图-7 阐述了一些追随者可能和新的领导人日志不同的情况。一个追随者可能会丢失掉领导人上的一些条目，也有可能包含一些领导人没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。</p>\n<p>在 Raft 算法中，领导人通过强制追随者们复制它的日志来处理日志的不一致。这就意味着，在追随者上的冲突日志会被领导者的日志覆盖。5.4节会说明当添加了一个额外的限制之后这是安全的。</p>\n<p>为了使得追随者的日志同自己的一致，领导人需要找到追随者同它的日志一致的地方，然后删除追随者在该位置之后的条目，然后将自己在该位置之后的条目发送给追随者。这些操作都在 AppendEntries RPC 进行一致性检查时完成。领导人给每一个追随者维护了一个<code>nextIndex</code>，它表示领导人将要发送给该追随者的下一条日志条目的索引。当一个领导人开始掌权时，它会将<code>nextIndex</code>初始化为它的最新的日志条目索引数+1（图-7 中的 11）。如果一个追随者的日志和领导者的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，领导人会将<code>nextIndex</code>递减然后重试 AppendEntries RPC。最终<code>nextIndex</code>会达到一个领导人和追随者日志一致的地方。这时，AppendEntries 会返回成功，追随者中冲突的日志条目都被移除了，并且添加所缺少的上了领导人的日志条目。一旦 AppendEntries 返回成功，追随者和领导人的日志就一致了，这样的状态会保持到该任期结束。</p>\n<p>如果需要的话，算法还可以进行优化来减少 AppendEntries RPC 失败的次数。例如，当拒绝了一个 AppendEntries 请求，追随者可以记录下冲突日志条目的任期号和自己存储那个任期的最早的索引。通过这些信息，领导人能够直接递减<code>nextIndex</code>跨过那个任期内所有的冲突条目；这样的话，一个冲突的任期需要一次 AppendEntries RPC，而不是每一个冲突条目需要一次 AppendEntries RPC。在实践中，我们怀疑这种优化是否是必要的，因为AppendEntries 一致性检查很少失败并且也不太可能出现大量的日志条目不一致的情况。</p>\n<p>通过这种机制，一个领导人在掌权时不需要采取另外特殊的方式来恢复日志的一致性。它只需要使用一些常规的操作，通过响应 AppendEntries 一致性检查的失败能使得日志自动的趋于一致。一个领导人从来不会覆盖或者删除自己的日志（表-3 中的领导人只增加原则）。</p>\n<p>这个日志复制机制展示了在第2章中阐述的所希望的一致性特性：Raft 能够接受，复制并且应用新的日志条目只要大部分的服务器是正常的。在通常情况下，一条新的日志条目可以在一轮 RPC 内完成在集群的大多数服务器上的复制；并且一个速度很慢的追随者并不会影响整体的性能。</p>\n<p>##5.4 安全性</p>\n<p>之前的章节中讨论了 Raft 算法是如何进行领导选取和复制日志的。然而，到目前为止这个机制还不能保证每一个状态机能按照相同的顺序执行同样的指令。例如，当领导人提交了若干日志条目的同时一个追随者可能宕机了，之后它又被选为了领导人然后用新的日志条目覆盖掉了旧的那些，最后，不同的状态机可能执行不同的命令序列。</p>\n<p>这一节通过在领带人选取部分加入了一个限制来完善了 Raft 算法。这个限制能够保证对于固定的任期，任何的领导人都拥有之前任期提交的全部日志条目（表-3 中的领导人完全原则）。有了这一限制，日志提交的规则就更清晰了。最后，我们提出了对于领导人完全原则的简单证明并且展示了它是如何修正复制状态机的行为的。</p>\n<p>###5.4.1 选举限制</p>\n<p>在所有的以领导人为基础的一致性算法中，领导人最终必须要存储全部已经提交的日志条目。在一些一致性算法中，例如：<a href=\"http://people.csail.mit.edu/cowling/vr/vr-revisited.pdf\" target=\"_blank\" rel=\"noopener\">Viewstamped Replication</a>，即使一开始没有包含全部已提交的条目也可以被选为领导人。这些算法都有一些另外的机制来保证找到丢失的条目并将它们传输给新的领导人，这个过程要么在选举过程中完成，要么在选举之后立即开始。不幸的是，这种方式大大增加了复杂性。Raft 使用了一种更简单的方式来保证在新的领导人开始选举的时候在之前任期的所有已提交的日志条目都会出现在上边，而不需要将这些条目传送给领导人。这就意味着日志条目只有一个流向：从领导人流向追随者。领导人永远不会覆盖已经存在的日志条目。</p>\n<p>Raft 使用投票的方式来阻止没有包含全部日志条目的服务器赢得选举。一个候选人为了赢得选举必须要和集群中的大多数进行通信，这就意味着每一条已经提交的日志条目最少在其中一台服务器上出现。如果候选人的日志至少和大多数服务器上的日志一样新（up-to-date，这个概念会在下边有详细介绍），那么它一定包含有全部的已经提交的日志条目。RequestVote RPC 实现了这个限制：这个 RPC（远程过程调用）包括候选人的日志信息，如果它自己的日志比候选人的日志要新，那么它会拒绝候选人的投票请求。</p>\n<p>Raft 通过比较日志中最后一个条目的索引和任期号来决定两个日志哪一个更新。如果两个日志的任期号不同，任期号大的更新；如果任期号相同，更长的日志更新。</p>\n<p>###5.4.2 提交之前任期的日志条目</p>\n<p><a href=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx4.sinaimg.cn/mw690/4858d6a8ly1fcc521xcvnj20ih08o75m.jpg\" alt=\"img\"></a></p>\n<p>图-8：如图的时间序列说明了为什么领导人不能通过之前任期的日志条目判断它的提交状态。（a）中的 S1 是领导人并且部分复制了索引2上的日志条目。（b）中 S1 崩溃了；S5 通过 S3，S4 和自己的选票赢得了选举，并且在索引2上接收了另一条日志条目。（c）中 S5 崩溃了，S1 重启了，通过 S2，S3 和自己的选票赢得了选举，并且继续索引2处的复制，这时任期2的日志条目已经在大部分服务器上完成了复制，但是还并没有提交。如果在（d）时刻 S1 崩溃了，S5 会通过 S2，S3，S4 的选票成为领导人，然后用它自己在任期3的日志条目覆盖掉其他服务器的日志条目。然而，如果在崩溃之前，S1 在它的当前任期在大多数服务器上复制了一条日志条目，就像在（e）中那样，那么这条条目就会被提交（S5就不会赢得选举）。在这时，之前的日志条目就会正常被提交。</p>\n<p>正如 5.3节 中描述的那样，只要一个日志条目被存在了在多数的服务器上，领导人就知道当前任期就可以提交该条目了。如果领导人在提交之前就崩溃了，之后的领导人会试着继续完成对日志的复制。然而，领导人并不能断定存储在大多数服务器上的日志条目一定在之前的任期中被提交了。图-8 说明了一种情况，一条存储在了大多数服务器上的日志条目仍然被新上任的领导人覆盖了。</p>\n<p>为了消除 图-8 中描述的问题，Raft 从来不会通过计算复制的数目来提交之前人气的日志条目。只有领导人当前任期的日志条目才能通过计算数目来进行提交。一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配原则（Log Matching Property），之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，通过观察该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。</p>\n<p>因为当领导人从之前任期复制日志条目时日志条目保留了它们最开始的任期号，所以这使得 Raft 在提交规则中增加了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要从之前的任期中复制日志条目，它必须要使用当前的新任期号。Raft 的方法使得判断日志更加容易，因为它们全程都保持着同样的任期号。另外，和其它的一致性算法相比，Raft 算法中的新领导人会发送更少的之前任期的日志条目（其他算法必须要发送冗余的日志条目并且在它们被提交之前来重新排序）。</p>\n<h3 id=\"5-4-3-安全性论证\"><a href=\"#5-4-3-安全性论证\" class=\"headerlink\" title=\"5.4.3 安全性论证\"></a>5.4.3 安全性论证</h3><p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fcc713vey3j20d3075js9.jpg\" alt=\"img\"></a></p>\n<p>图-9：如果 S1（任期 T 的领导人）在它的任期提交了一条日志条目，并且 S5 在之后的任期 U 成为了领导人，那么最少会有一台服务器（S3）接收了这条日志条目并且会给 S5 投票。</p>\n<p>给出了完整的 Raft 算法，现在我们能够更精确的论证领导人完全原则（Leader Completeness)（这基于 9.2节 提出的安全性证明）。我们假定领导人完全原则是不成立的，然后推导出矛盾。假定任期 T 的领导人 leaderT在它的任期提交了一个日志条目，但是这条日志条目并没有存储在之后的任期中的领导人上。我们设大于 T 的最小的任期 U 的领导人（leaderU） 没有存储这条日志条目。</p>\n<ol>\n<li>在 leaderU 选举时一定没有那条被提交的日志条目（领导人从来不会删除或者覆盖日志条目）。</li>\n<li>leaderT 复制了这个条目到集群的大多数的服务器上。因此，只是有一台服务器（投票者）即接收了来自 leaderT 的日志条目并且给 leaderU 投票，就像 图-9 中所示那样。这个投票者是产生矛盾的关键。</li>\n<li>投票者必须在给 leaderU 投票之前接收来自 leaderT 的日志条目；否则它会拒绝来自 leaderT 的 AppendEntries 请求（它的当前任期会比 T 要大）。</li>\n<li>投票者会在它给 leaderU 投票时存储那个条目，因为任何中间的领导人都保有该条目（基于假设），领导人从来不会移除这个条目，并且追随者也只会在和领导人冲突时才会移除日志条目。</li>\n<li>投票者给 leaderU 投票了，所以 leaderU 的日志必须和投票者的一样新。这就导致了一个矛盾。</li>\n<li>首先，如果投票者和 leaderU 最后一条日志条目的任期号相同，那么 leaderU 的日志一定和投票者的一样长，因此它的日志包含全部投票者的日志条目。这是矛盾的，因为在假设中投票者和 leaderU 包含的已提交条目是不同的。</li>\n<li>除此之外， leaderU 的最后一条日志的任期号一定比投票者的大。另外，它也比 T 要大，因为投票者的最后一条日志条目的任期号最小也要是 T（它包含了所有任期 T 提交的日志条目）。创建 leaderU 最后一条日志条目的上一任领导人必须包含已经提交的日志条目（基于假设）。那么，根据日志匹配原则（Log Matching），leaderU 也一定包含那条提交的日志条目，这也是矛盾的。</li>\n<li>这时就完成了矛盾推导。因此，所有比任期 T 大的领导人一定包含所有在任期 T 提交的日志条目。</li>\n<li>日志匹配原则（Log Matching）保证了未来的领导人也会包含被间接提交的日志条目，就像 图-8 中（d）时刻索引为2的条目。</li>\n</ol>\n<p>通过给出了 领导人完全原则（Leader Completeness)，我们能够证明 表-3 中的状态机安全原则（State Machine Safety），状态机安全原则（State Machine Safety）讲的是如果一台服务器将给定索引上的日志条目应用到了它自己的状态机上，其它服务器的同一索引位置不可能应用的是其它条目。在一个服务器应用一条日志条目到它自己的状态机中时，它的日志必须和领导人的日志在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性（Log Completeness Property）保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。</p>\n<p>最后，Raft 算法需要服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。</p>\n<p>##5.5 追随者和候选人崩溃</p>\n<p>截止到目前，我们只讨论了领导人崩溃的问题。追随者和候选人崩溃的问题解决起来要比领导人崩溃要简单得多，这两者崩溃的处理方式是一样的。如果一个追随者或者候选人崩溃了，那么之后的发送给它的 RequestVote RPC 和 AppendEntries RPC 会失败。Raft 通过无限的重试来处理这些失败；如果崩溃的服务器重启了，RPC 就会成功完成。如果一个服务器在收到了 RPC 之后但是在响应之前崩溃了，那么它会在重启之后再次收到同一个 RPC。因为 Raft 中的 RPC 都是幂等的，因此不会有什么问题。例如，如果一个追随者收到了一个已经包含在它的日志中的 AppendEntries 请求，它会忽视这个新的请求。</p>\n<p>##5.6 时序和可用性</p>\n<p>我们对于 Raft 的要求之一就是安全性不依赖于时序（timing）：系统不能仅仅因为一些事件发生的比预想的快一些或慢一些就产生错误。然而，可用性（系统可以及时响应客户端的特性）不可避免的要依赖时序。例如，如果消息交换在服务器崩溃时花费更多的时间，候选人不会等待太长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。</p>\n<p>领导人选取是 Raft 中对时序要求最关键的地方。Raft 会选出并且保持一个稳定的领导人只有系统满足下列时序要求（timing requirement）：</p>\n<p>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</p>\n<p>在这个不等式中，<code>broadcastTime</code>指的是一台服务器并行的向集群中的其他服务器发送 RPC 并且收到它们的响应的平均时间；<code>electionTimeout</code>指的就是在 5.2节 描述的选举超时时间；<code>MTBF</code>指的是单个服务器发生故障的间隔时间的平均数。<code>broadcastTime</code>应该比<code>electionTimeout</code>小一个数量级，为的是使领导人能够持续发送心跳信息（heartbeat）来阻止追随者们开始选举；根据已经给出的随机化选举超时时间方法，这个不等式也使得瓜分选票的情况变成不可能。<code>electionTimeout</code>也要比<code>MTBF</code>小几个数量级，为的是使得系统稳定运行。当领导人崩溃时，整个大约会在<code>electionTimeout</code>的时间内不可用；我们希望这种情况仅占全部时间的很小的一部分。</p>\n<p><code>broadcastTime</code>和<code>MTBF</code>是由系统决定的性质，但是<code>electionTimeout</code>是我们必须做出选择的。Raft 的 RPC 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，这取决于存储的技术。因此，<code>electionTimeout</code>一般在 10ms 到 500ms 之间。大多数的服务器的<code>MTBF</code>都在几个月甚至更长，很容易满足这个时序需求。</p>\n<p>#6 集群成员变化</p>\n<p>截止到目前，我们都假定集群的配置（加入到一致性算法的服务器集合）是固定的。在实际中，我们会经常更改配置，例如，替换掉那些崩溃的机器或者更改复制级别。虽然通过关闭整个集群，升级配置文件，然后重启整个集群也可以解决这个问题，但是这回导致在更改配置的过程中，整个集群不可用。另外，如果存在需要手工操作，那么就会有操作失误的风险。为了避免这些问题，我们决定采用自动改变配置并且把这部分加入到了 Raft 一致性算法中。</p>\n<p>为了让配置修改机制能够安全，那么在转换的过程中在任何时间点两个领导人不能再同一个任期被同时选为领导人。不幸的是，服务器集群从旧的配置直接升级到新的配置的任何方法都是不安全的，一次性自动的转换所有服务器是不可能的，所以集群可以在转换的过程中划分成两个单独的组（如 图-10 所示）。</p>\n<p><a href=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx3.sinaimg.cn/mw690/4858d6a8ly1fccbvshy16j20f00a374x.jpg\" alt=\"img\"></a></p>\n<p>图-10：从一个配置切换到另一个配置是不安全的因为不同的服务器会在不同的时间点进行切换。在这个例子中，集群数量从三台转换成五台。不幸的是，在一个时间点有两个服务器能被选举成为领导人，一个是在使用旧的配置的机器中（Cold）选出的领导人，另一个领导人是通过新的配置（Cnew）选出来的。</p>\n<p>为了保证安全性，集群配置的调整必须使用两阶段（two-phase）方法。有许多种实现两阶段方法的实现。例如，一些系统在第一个阶段先把旧的配置设为无效使得它无法处理客户端请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡配置，我们称其为共同一致（joint consensus）；一旦共同一致被提交了，然后系统再切换到新的配置。共同一致是旧的配置和新的配置的组合：</p>\n<ul>\n<li>日志条目被复制给集群中新、老配置的所有服务器。</li>\n<li>新、老配置的服务器都能成为领导人。</li>\n<li>需要分别在两种配置上获得大多数的支持才能达成一致（针对选举和提交）</li>\n</ul>\n<p>共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然能够响应服务器请求。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccchdn7t2j20gq088q3t.jpg\" alt=\"img\"></a></p>\n<p>图-11：集群配置变更的时间线。虚线表示的是已经被创建但是还没提交的配置条目，实线表示的是最新提交的配置条目。领导人首先在它的日志中创建 Cold,new配置条目并且将它提交到Cold,new（使用旧配置的大部分服务器和使用新配置的大部分服务器）。然后创建它创建Cnew配置条目并且将它提交到使用新配置的大部分机器上。这样就不存在Cold和Cnew能够分别同时做出决定的时刻。</p>\n<p>集群配置在复制日志中用特殊的日志条目来存储和通信；图-11 展示了配置变更的过程。当一个领导人接收到一个改变配置 Cold 为 Cnew 的请求，它会为了共同一致以前面描述的日志条目和副本的形式将配置存储起来（图中的 Cold,new）。一旦一个服务器将新的配置日志条目增加到它的日志中，它就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论它是否已经被提交）。这意味着领导人要使用 Cold,new 的规则来决定日志条目 Cold,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 Cold 配置也可能是 Cold,new 配置，这取决于赢得选举的候选人是否已经接收到了 Cold,new 配置。在任何情况下， Cnew 配置在这一时期都不会单方面的做出决定。</p>\n<p>一旦 Cold,new 被提交，那么无论是 Cold 还是 Cnew，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 Cnew 配置的日志条目并复制给集群就是安全的了。另外，每个服务器在收到新的配置的时候就会立即生效。当新的配置在 Cnew 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如 图-11，Cold 和 Cnew 没有任何机会同时做出单方面的决定；这就保证了安全性。</p>\n<p>针对重新配置提出了三个问题。第一个问题是一开始的时候新的服务器可能没有任何日志条目。如果它们在这个状态下加入到集群中，那么它们需要一段时间来更新追赶，在这个阶段它们还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权的身份加入到集群中来（领导人复制日志给他们，但是不把它们考虑到大多数中）。一旦新的服务器追赶上了集群中的其它机器，重新配置可以像上面描述的一样处理。</p>\n<p>第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 Cnew 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括自己；它复制日志但是不把它自己看作是大多数之一。当 Cnew 被提交时，会发生领导人过渡，因为这时是新的配置可以独立工作的最早的时间点（总是能够在 Cnew 配置下选出新的领导人）。在此之前，可能只能从 Cold 中选出领导人。</p>\n<p>第三个问题是，移除不在 Cnew 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳（heartbeat），所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新的任期号的 RequestVote RPC，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。</p>\n<p>为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略 RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个 RequestVote RPC，它不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么它就不会被更大的任期号废除。</p>\n<p>#7 日志压缩</p>\n<p>Raft 产生的日志在持续的正常操作中不断增长，但是在实际的系统中，它不会无限的增长下去。随着日志的不断增长，它会占据越来越多的空间并且花费更多的时间重置。如果没有一个机制使得它能够废弃在日志中不断累积的过时的信息就会引起可用性问题。</p>\n<p>快照（snapshot）是最简单的压缩方式。在快照中，全部的当前系统状态都被写入到快照中，存储到持久化的存储中，然后在那个时刻之前的全部日志都可以被丢弃。在 Chubby 和 ZooKeeper 中都使用了快照技术，这一章的剩下的部分会介绍 Raft 中使用的快照技术。</p>\n<p>增量压缩（incremental approaches）的方法，例如日志清理（log cleaning）或者日志结构合并树（log-structured merge trees），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以使用和快照相同的接口来实现 LSM tree ，但是日志清除方法就需要修改 Raft 了。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccdvbs2y7j20g70ae75i.jpg\" alt=\"img\"></a></p>\n<p>图-12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。</p>\n<p>图-12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也将一些少量的元数据包含到快照中：最后被包含的索引（last included index）指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期（last included term）指的是该条目的任期号。保留这些数据是为了支持快照前的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 章），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。</p>\n<p>尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 章）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给它们。</p>\n<p><strong>安装快照 RPC（InstallSnapshot RPC）</strong></p>\n<p>在领导人发送快照给跟随者时使用调用。领导人总是按顺序发送。</p>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>领导人的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>为了追随者能重定向到客户端</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照中包含的最后日志条目的索引值</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>快照中包含的最后日志条目的任期号</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>分块在快照中的偏移量</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>快照块的原始数据</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果是最后一块数据则为真</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>返回值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>term</td>\n<td>currentTerm，用于领导人更新自己</td>\n</tr>\n</tbody>\n</table>\n<p>接受者需要实现：</p>\n<ol>\n<li>如果<code>term &lt; currentTerm</code>立刻回复</li>\n<li>如果是第一个分块（offset 为 0）则创建新的快照</li>\n<li>在指定的偏移量写入数据</li>\n<li>如果 <code>done</code>为 false，则回复并继续等待之后的数据</li>\n<li>保存快照文件，丢弃所有存在的或者部分有着更小索引号的快照</li>\n<li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保留并且回复</li>\n<li>丢弃全部日志</li>\n<li>能够使用快照来恢复状态机（并且装载快照中的集群配置）</li>\n</ol>\n<p>表-13：InstallSnapshot RPC 的总结。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生存的信号，所以跟随者可以重置选举超时计时器。</p>\n<p>在这种情况下领导人使用一种叫做安装快照（InstallSnapshot）的新的 RPC 来发送快照给太落后的跟随者；见 表-13。当跟随者通过这种 RPC 接收到快照时，它必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃它所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须是正确的和并且被保留下来。</p>\n<p>这种快照的方式背离了 Raft 的强领导人原则（strong leader principle），因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织它们的数据了。</p>\n<p>我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。</p>\n<p>还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，它就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。</p>\n<p>第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制（copy-on-write）的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。</p>\n<p>#8 客户端交互</p>\n<p>这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端是如何发现领导人的和 Raft 是如何支持线性化语义（linearizable semantics）的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。</p>\n<p>Raft 中的客户端将所有请求发送给领导人。当客户端启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供它最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。</p>\n<p>我们 Raft 的目标是要实现线性化语义（linearizable semantics）（每一次操作立即执行，在它调用和收到回复之间只执行一次）。但是，如上述所说，Raft 是可以多次执行同一条命令的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。</p>\n<p>只读（read-only）的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回过期数据(stale data)的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是它还不知道。线性化的读操作必须不能返回过期数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全原则（Leader Completeness Property）保证了领导人一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来进行实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废除了（如果一个更新的领导人被选举出来，它自己的信息就已经过期了）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳（heartbeat）信息来处理这个问题。另外，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时序来保证安全性（它假设时间误差是有界的）。</p>\n<p>#9 实现和评价</p>\n<p>我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。</p>\n<p>这一章会从三个方面来评估 Raft 算法：可理解性、正确性和性能。</p>\n<p>##9.1 可理解性</p>\n<p>为了比较 Paxos 和 Raft 算法的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文除了日志压缩之外的所有内容；Paxos 课程包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者独立的区别从第一个算法处学来的经验。我们计算参加人员的每一个小测验的得分来看参与者是否对 Raft 的理解更好。</p>\n<table>\n<thead>\n<tr>\n<th>因素</th>\n<th>消除偏见的手段</th>\n<th>复习材料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>相同的讲课质量</td>\n<td>使用相同的讲师。Paxos 的讲义是基于之前在几所大学中使用的材料的并且做了改进。Paxos 的讲义要长 14%</td>\n<td>视频</td>\n</tr>\n<tr>\n<td>相同的测试难度</td>\n<td>用难度给问题分组，在测试中成对出现</td>\n<td>测验</td>\n</tr>\n<tr>\n<td>公平的打分</td>\n<td>使用红字标题。随机顺序打分，两个测验交替进行。</td>\n<td>红字标题</td>\n</tr>\n</tbody>\n</table>\n<p>表-1：考虑到的可能造成偏见的因素，以及解决方案和对应的复习材料</p>\n<p>我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表-1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccg1gulewj20dy0cjjsf.jpg\" alt=\"img\"></a></p>\n<p>图-14：表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩的散点图。在对角线之上的点表示在 Raft 获得了更高分数的学生。</p>\n<p>参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图-14 展示了每个参与者的得分。一对 t -测试表明，拥有 95% 的可信度，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。</p>\n<p>我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型显示，对小测验的选择会产生 12.5 分的差别在对 Raft 的好感度上。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft 的小测验得分会比 Paxos 低 6.3 分；我们不知道为什么，但这在统计学上是这样的。</p>\n<p><a href=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx2.sinaimg.cn/mw690/4858d6a8ly1fccg5giieaj20io082wfk.jpg\" alt=\"img\"></a></p>\n<p>图-15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。</p>\n<p>我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图-15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。</p>\n<p>关于 Raft 用户学习有一个更加详细的讨论，详见<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/ ̃ongaro/thesis.pdf</a></p>\n<p>##9.2 正确性</p>\n<p>在第5章，我们已经进行了一个<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">正式的说明</a>，和对一致性机制的安全性证明。这个正式说明通过 <a href=\"https://www.amazon.com/Specifying-Systems-Language-Hardware-Engineers/dp/032114306X\" target=\"_blank\" rel=\"noopener\">TLA+</a> 让 表-2 中的信息非常清晰。它大约有 400 行并且充当了证明的主题。同时对于任何想实现的人也是十分有用的。我们非常机械的通过 TLA 证明系统证明了日志完全特性（Log Completeness Property）。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明这个说明中的类型安全 type safety）。而且，我们已经写了一个<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">非正式的证明</a>关于状态机安全性质是完备的，并且是相当清晰的（大约 3500 个词）。</p>\n<p>##9.3 性能</p>\n<p>Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。</p>\n<p>我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答以下两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？</p>\n<p><a href=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://wx1.sinaimg.cn/mw690/4858d6a8ly1fccglcmgnjj20i20gc0vq.jpg\" alt=\"img\"></a></p>\n<p>图-16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。</p>\n<p>为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图-16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。</p>\n<p>图-16 上面的图表表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程由于太多的选票瓜分的情况往往都需要花费超过 10 秒钟。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。</p>\n<p>图-16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。</p>\n<p>#10 相关工作</p>\n<p>已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：</p>\n<ul>\n<li>Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰的论文。</li>\n<li>关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。</li>\n<li>实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 有着很大的差别。</li>\n<li>Paxos 可以应用的性能优化。</li>\n<li>Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。</li>\n</ul>\n<p>Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。</p>\n<p>像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。</p>\n<p>和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。</p>\n<p>Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。</p>\n<p>一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致（joint consensus）的方法因为它对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Raft 没有采用 Lamport 的基于 α 的方法是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较而言，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。和 VR、SMART 比较而言，Raft 的方法同时需要更少的额外机制来实现。</p>\n<p>#11 总结</p>\n<p>算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。</p>\n<p>在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。</p>\n<p>#12 鸣谢</p>\n<p>这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。</p>\n<p>#引用</p>\n<ol>\n<li>BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154.</li>\n<li>BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350.</li>\n<li>CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317.</li>\n<li>CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407.</li>\n<li>CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218.</li>\n<li>CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264.</li>\n<li>COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154.</li>\n<li>GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43.</li>\n<li>GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210.</li>\n<li>HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492.</li>\n<li>HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158.</li>\n<li>JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems &amp; Networks (2011), IEEE Computer Society, pp. 245–256.</li>\n<li>KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008.</li>\n<li>LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565.</li>\n<li>LAMPORT, L. The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169.</li>\n<li>LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25.</li>\n<li>LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002.</li>\n<li>LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005.</li>\n<li>LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103.</li>\n<li>LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17.</li>\n<li>LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13.</li>\n<li>LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.</li>\n<li>LogCabin source code. logcabin/logcabin.</li>\n<li>LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115.</li>\n<li>MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384.</li>\n<li>MAZIE` RES, D. Paxos made practical.<a href=\"http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf\" target=\"_blank\" rel=\"noopener\">http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf</a> , Jan. 2007.</li>\n<li>MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM.</li>\n<li>Raft user study. <a href=\"http://ramcloud.stanford.edu/~ongaro/userstudy/\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/~ongaro/userstudy/</a>.</li>\n<li>OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17.</li>\n<li>O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385.</li>\n<li>ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress).<a href=\"http://ramcloud.stanford.edu/~ongaro/thesis.pdf\" target=\"_blank\" rel=\"noopener\">http://ramcloud.stanford.edu/~ongaro/thesis.pdf</a></li>\n<li>ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX.</li>\n<li>OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130.</li>\n<li>Raft consensus algorithm website. <a href=\"http://raftconsensus.github.io/\" target=\"_blank\" rel=\"noopener\">http://raftconsensus.github.io</a>.</li>\n<li>REED, B. Personal communications, May 17, 2013.</li>\n<li>ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52.</li>\n<li>SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319.</li>\n<li>SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10.</li>\n<li>VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.<a href=\"https://github.com/iissnan/hexo-theme-next\" target=\"_blank\" rel=\"noopener\">https://github.com/iissnan/hexo-theme-next</a>)</li>\n</ol>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjw6dubl50000amumwyr0c0la","tag_id":"cjw6dublb0004amumi4807rbb","_id":"cjw6dublf0009amumqzcri9dm"},{"post_id":"cjw6dubla0002amumm7wnvpj4","tag_id":"cjw6dublf0008amumtpwu5nyd","_id":"cjw6dublk000eamumymm47yv8"},{"post_id":"cjw6dubld0005amumubm9j4o3","tag_id":"cjw6dubli000camumgzjvky2s","_id":"cjw6dublm000iamumfyzxzq1x"},{"post_id":"cjw6duble0006amumxtevoxh3","tag_id":"cjw6dubll000gamumlshxp9fv","_id":"cjw6dublo000mamumi282ypge"},{"post_id":"cjw6duble0007amumh6i3miaz","tag_id":"cjw6dubln000kamumhn1ikn7q","_id":"cjw6dublp000oamumrp6tg5bt"},{"post_id":"cjw6dublg000aamumwsy1z052","tag_id":"cjw6dubln000kamumhn1ikn7q","_id":"cjw6dublp000qamum4sgcsyaj"},{"post_id":"cjw6dublh000bamum9gph33mp","tag_id":"cjw6dublp000pamum8uvu60hk","_id":"cjw6dublq000samumkyhg7zf6"},{"post_id":"cjw6dubli000damumanv3gj9r","tag_id":"cjw6dublp000pamum8uvu60hk","_id":"cjw6dublq000uamumemqn2k12"},{"post_id":"cjw6dublk000famum7ma2hxog","tag_id":"cjw6dubln000kamumhn1ikn7q","_id":"cjw6dublq000wamumjevhqiry"},{"post_id":"cjw6dublm000jamum5yuk8ixk","tag_id":"cjw6dublq000vamum5shtlgkj","_id":"cjw6dublr000yamumfjzlys21"},{"post_id":"cjw6dubln000lamumuoymnvzo","tag_id":"cjw6dublr000xamum5w75s8x6","_id":"cjw6dublr000zamumbnfd1jsm"},{"post_id":"cjw6dubp90014amum7o72n2g8","tag_id":"cjw6dublq000vamum5shtlgkj","_id":"cjw6dubpb0015amumkvjylj4i"},{"post_id":"cjw6dubp50010amumw7uzhhfu","tag_id":"cjw6dubp90013amum85mbnv1y","_id":"cjw6dubpc0017amumxf5r2cis"},{"post_id":"cjw6dubp70012amuma89f9mh4","tag_id":"cjw6dubpb0016amumkjkxirwk","_id":"cjw6dubpc0018amumpe62skib"},{"post_id":"cjw6dubq90019amum4pazjjv9","tag_id":"cjw6dubli000camumgzjvky2s","_id":"cjw6dubqa001bamumm8jgh2um"},{"post_id":"cjw6dubq9001aamumia3zpx9n","tag_id":"cjw6dublp000pamum8uvu60hk","_id":"cjw6dubqb001damumnb194hdy"},{"post_id":"cjw6dubqa001camumlrbmu6k4","tag_id":"cjw6dubqb001eamumhf6pl5j0","_id":"cjw6dubqc001famumotwwlatc"},{"post_id":"cjw6dubr8001gamumv7qurlyd","tag_id":"cjw6dublq000vamum5shtlgkj","_id":"cjw6dubra001iamum5s1zy7xx"},{"post_id":"cjw6dubr9001hamum6glteraa","tag_id":"cjw6dubli000camumgzjvky2s","_id":"cjw6dubra001jamumi6puw2xf"},{"post_id":"cjw6dubsh001kamumd6n5f4lt","tag_id":"cjw6dubsi001lamuma75kd855","_id":"cjw6dubsi001mamum25ps8v84"},{"post_id":"cjw6dubsm001namumxh9ow6lr","tag_id":"cjw6dubsn001oamumjv1m01yx","_id":"cjw6dubsn001pamum8tzv8s6i"}],"Tag":[{"name":"大数据","_id":"cjw6dublb0004amumi4807rbb"},{"name":"Linux","_id":"cjw6dublf0008amumtpwu5nyd"},{"name":"kafka","_id":"cjw6dubli000camumgzjvky2s"},{"name":"lighttpd","_id":"cjw6dubll000gamumlshxp9fv"},{"name":"linux","_id":"cjw6dubln000kamumhn1ikn7q"},{"name":"nginx","_id":"cjw6dublp000pamum8uvu60hk"},{"name":"go","_id":"cjw6dublq000vamum5shtlgkj"},{"name":"Spark","_id":"cjw6dublr000xamum5w75s8x6"},{"name":"python","_id":"cjw6dubp90013amum85mbnv1y"},{"name":"zookeeper","_id":"cjw6dubpb0016amumkjkxirwk"},{"name":"分布式","_id":"cjw6dubqb001eamumhf6pl5j0"},{"name":"NoSQL","_id":"cjw6dubsi001lamuma75kd855"},{"name":"raft","_id":"cjw6dubsn001oamumjv1m01yx"}]}}